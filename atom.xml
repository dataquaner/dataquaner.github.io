<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DataQuaner</title>
  
  <subtitle>DataQuaner</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://dataquaner.github.io/"/>
  <updated>2020-06-21T12:27:36.753Z</updated>
  <id>https://dataquaner.github.io/</id>
  
  <author>
    <name>Leon</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark面试问题梳理：选择题</title>
    <link href="https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/"/>
    <id>https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/</id>
    <published>2020-06-21T06:35:00.000Z</published>
    <updated>2020-06-21T12:27:36.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Spark-的四大组件下面哪个不是-D"><a href="#1-Spark-的四大组件下面哪个不是-D" class="headerlink" title="1. Spark 的四大组件下面哪个不是 (D )"></a>1. <strong>Spark 的四大组件下面哪个不是 (D )</strong></h2><p>A.Spark Streaming    B. Mlib </p><p>C Graphx    D.Spark R</p><h2 id="2-下面哪个端口不是-spark-自带服务的端口-C"><a href="#2-下面哪个端口不是-spark-自带服务的端口-C" class="headerlink" title="2. 下面哪个端口不是 spark 自带服务的端口 (C )"></a>2. 下面哪个端口不是 spark 自带服务的端口 (C )</h2><p>A.8080 B.4040 C.8090 D.18080</p><p>备注：8080：spark集群web ui端口，4040：sparkjob监控端口，18080：jobhistory端口</p><h2 id="3-spark-1-4-版本的最大变化-B"><a href="#3-spark-1-4-版本的最大变化-B" class="headerlink" title="3. spark 1.4 版本的最大变化 (B )"></a><strong>3. spark 1.4 版本的最大变化 (B )</strong></h2><p>A spark sql Release 版本  B .引入 Spark R </p><p>C DataFrame D.支持动态资源分配</p><h2 id="4-Spark-Job-默认的调度模式-A"><a href="#4-Spark-Job-默认的调度模式-A" class="headerlink" title="4. Spark Job 默认的调度模式 (A )"></a>4. Spark Job 默认的调度模式 (A )</h2><p>A FIFO   B FAIR   </p><p>C 无   D 运行时指定</p><blockquote><p>备注：Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。</p></blockquote><h2 id="5-哪个不是本地模式运行的条件-D"><a href="#5-哪个不是本地模式运行的条件-D" class="headerlink" title="5.哪个不是本地模式运行的条件 ( D)"></a>5.哪个不是本地模式运行的条件 ( D)</h2><p>A spark.localExecution.enabled=true  </p><p>B 显式指定本地运行</p><p>C finalStage 无父 Stage</p><p>D partition默认值</p><blockquote><p>备注：【问题】Spark在windows能跑集群模式吗？</p><p>我认为是可以的，但是需要详细了解cmd命令行的写法。目前win下跑spark的单机模式是没有问题的。</p></blockquote><blockquote><p>【关键点】spark启动机制容易被windows的命令行cmd坑</p><p>　　1、带空格、奇怪字符的安装路径，cmd不能识别。最典型的坑就是安装在Program Files文件夹下的程序，因为Program和Files之间有个空格，所以cmd竟不能识别。之前就把JDK安装在了Program Files下面，然后启动spark的时候，总是提示我找不到JDK。我明明配置了环境变量了啊？这就是所谓了《已经配置环境变量，spark 仍然找不到Java》的错误问题。至于奇怪的字符，如感叹号!，我经常喜欢用来将重要的文件夹排在最前面，但cmd命令提示符不能识别。</p><p>　　2、是否需要配置hadoop的路径的问题——答案是需要用HDFS或者yarn就配，不需要用则不需配置。目前大多数的应用场景里面，Spark大规模集群基本安装在Linux服务器上，而自己用windows跑spark的情景，则大多基于学习或者实验性质，如果我们所要读取的数据文件从本地windows系统的硬盘读取（比如说d:\data\ml.txt），基本上不需要配置hadoop路径。我们都知道，在编spark程序的时候，可以指定spark的启动模式，而启动模式有这么三中（以python代码举例）：</p><p>　　　（2.1）本地情况，conf = SparkConf().setMaster(“local[*]”) ——&gt;也就是拿本机的spark来跑程序</p><p>　　　（2.2）远程情况，conf = SparkConf().setMaster(“spark://remotehost:7077”) ——&gt;远程spark主机</p><p>　　　（2.3）yarn情况，conf = SparkConf().setMaster(“yarn-client”) ——&gt;远程或本地 yarn集群代理spark</p><p>针对这3种情况，配置hadoop安装路径都有什么作用呢？（2.1）本地的情况，直接拿本机安装的spark来运行spark程序（比如d:\spark-1.6.2），则配不配制hadoop路径取决于是否需要使用hdfs。java程序的情况就更为简单，只需要导入相应的hadoop的jar包即可，是否配置hadoop路径并不重要。（2.2）的情况大体跟（2.1）的情况相同，虽然使用的远程spark，但如果使用本地数据，则运算的元数据也是从本地上传到远程spark集群的，无需配置hdfs。而（2.3）的情况就大不相同，经过我搜遍baidu、google、bing引擎，均没找到SparkConf直接配置远程yarn地址的方法，唯一的一个帖子介绍可以使用yarn://remote:8032的形式，则会报错“无法解析 地址”。查看Spark的官方说明，Spark其实是通过hadoop路径下的etc\hadoop文件夹中的配置文件来寻找yarn集群的。因此，需要使用yarn来运行spark的情况，在spark那配置好hadoop的目录就尤为重要。后期经过虚拟机的验证，表明，只要windows本地配置的host地址等信息与linux服务器端相同（注意应更改hadoop-2/etc/hadoop 下各种文件夹的配置路径，使其与windows本地一致），是可以直接在win下用yarn-client提交spark任务到远程集群的。</p><p>3、是否需要配置环境变量的问题，若初次配置，可以考虑在IDE里面配置，或者在程序本身用setProperty函数进行配置。因为配置windows下的hadoop、spark环境是个非常头疼的问题，有可能路径不对而导致无法找到相应要调用的程序。待实验多次成功率提高以后，再直接配置windows的全局环境变量不迟。</p><p>　　4、使用Netbeans这个IDE的时候，有遇到Netbeans不能清理构建的问题。原因，极有可能是导入了重复的库，spark里面含有hadoop包，记得检查冲突。同时，在清理构建之前，记得重新编译一遍程序，再进行清理并构建。</p><p>　　５、经常遇到WARN YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources资源不足无法运行的问题，添加conf.set(“spark.executor.memory”, “512m”);语句进行资源限制。先前在虚拟机跑spark，由于本身机子性能不高，给虚拟机设置的内存仅仅2G，导致hadoop和spark双开之后系统资源严重不足。因此可以缩小每个executor的运算规模。其他资源缺乏问题的解决方法参考<a href="http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html</a></p></blockquote><h2 id="6-下面哪个不是-RDD-的特点-C"><a href="#6-下面哪个不是-RDD-的特点-C" class="headerlink" title="6.下面哪个不是 RDD 的特点 (C )"></a>6.下面哪个不是 RDD 的特点 (C )</h2><p>A. 可分区   B 可序列化   C 可修改   D 可持久化</p><h2 id="7-关于广播变量，下面哪个是错误的-D"><a href="#7-关于广播变量，下面哪个是错误的-D" class="headerlink" title="7. 关于广播变量，下面哪个是错误的 (D )"></a>7. 关于广播变量，下面哪个是错误的 (D )</h2><p>A 任何函数调用    B 是只读的  </p><p>C 存储在各个节点    D 存储在磁盘或 HDFS</p><h2 id="8-关于累加器，下面哪个是错误的-D"><a href="#8-关于累加器，下面哪个是错误的-D" class="headerlink" title="8. 关于累加器，下面哪个是错误的 (D )"></a>8. 关于累加器，下面哪个是错误的 (D )</h2><p>A 支持加法 B 支持数值类型 </p><p>C 可并行 D 不支持自定义类型</p><h2 id="9-Spark-支持的分布式部署方式中哪个是错误的-D"><a href="#9-Spark-支持的分布式部署方式中哪个是错误的-D" class="headerlink" title="9.Spark 支持的分布式部署方式中哪个是错误的 (D )"></a>9.Spark 支持的分布式部署方式中哪个是错误的 (D )</h2><p>A standalone B spark on mesos  </p><p>C spark on YARN D Spark on local</p><h2 id="10-Stage-的-Task-的数量由什么决定-A"><a href="#10-Stage-的-Task-的数量由什么决定-A" class="headerlink" title="10.Stage 的 Task 的数量由什么决定 (A )"></a>10.Stage 的 Task 的数量由什么决定 (A )</h2><p>A Partition B Job C Stage D TaskScheduler</p><h2 id="11-下面哪个操作是窄依赖-B"><a href="#11-下面哪个操作是窄依赖-B" class="headerlink" title="11.下面哪个操作是窄依赖 (B )"></a>11.下面哪个操作是窄依赖 (B )</h2><p>A join B filter </p><p>C group D sort</p><h2 id="12-下面哪个操作肯定是宽依赖-C"><a href="#12-下面哪个操作肯定是宽依赖-C" class="headerlink" title="12.下面哪个操作肯定是宽依赖 (C )"></a>12.下面哪个操作肯定是宽依赖 (C )</h2><p>A map B flatMap </p><p>C reduceByKey D sample</p><h2 id="13-spark-的-master-和-worker-通过什么方式进行通信的？-D"><a href="#13-spark-的-master-和-worker-通过什么方式进行通信的？-D" class="headerlink" title="13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )"></a>13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )</h2><p>A http B nio C netty D Akka</p><blockquote><p>备注：从spark1.3.1之后，netty完全代替 了akka</p><p>一直以来，基于Akka实现的RPC通信框架是Spark引以为豪的主要特性，也是与Hadoop等分布式计算框架对比过程中一大亮点，但是时代和技术都在演化，从Spark1.3.1版本开始，为了解决大数据块（如shuffle）的传输问题，Spark引入了Netty通信框架，到了1.6.0版本，Netty居然完全取代了Akka，承担Spark内部所有的RPC通信以及数据流传输。</p><p>那么Akka又是什么东西？从Akka出现背景来说，它是基于Actor的RPC通信系统，它的核心概念也是Message，它是基于协程的，性能不容置疑；基于scala的偏函数，易用性也没有话说，但是它毕竟只是RPC通信，无法适用大的package/stream的数据传输，这也是Spark早期引入Netty的原因。</p><p>那么Netty为什么可以取代Akka？首先不容置疑的是Akka可以做到的，Netty也可以做到，但是Netty可以做到，Akka却无法做到，原因是啥？在软件栈中，Akka相比Netty要Higher一点，它专门针对RPC做了很多事情，而Netty相比更加基础一点，可以为不同的应用层通信协议（RPC，FTP，HTTP等）提供支持，在早期的Akka版本，底层的NIO通信就是用的Netty；其次一个优雅的工程师是不会允许一个系统中容纳两套通信框架，恶心！最后，虽然Netty没有Akka协程级的性能优势，但是Netty内部高效的Reactor线程模型，无锁化的串行设计，高效的序列化，零拷贝，内存池等特性也保证了Netty不会存在性能问题。</p><p>那么Spark是怎么用Netty来取代Akka呢？一句话，利用偏函数的特性，基于Netty“仿造”出一个简约版本的Actor模型！！</p></blockquote><h2 id="14-默认的存储级别-A"><a href="#14-默认的存储级别-A" class="headerlink" title="14. 默认的存储级别 (A )"></a>14. 默认的存储级别 (A )</h2><p>A MEMORY_ONLY B MEMORY_ONLY_SER</p><p>C MEMORY_AND_DISK D MEMORY_AND_DISK_SER</p><pre class="line-numbers language-scala"><code class="language-scala">备注：<span class="token comment" spellcheck="true">//不会保存任务数据 </span><span class="token keyword">val</span> NONE <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//直接将RDD的partition保存在该节点的Disk上 </span><span class="token keyword">val</span> DISK_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//直接将RDD的partition保存在该节点的Disk上,在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> DISK_ONLY_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition对应的原生的Java Object保存在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //那么这些partition将不会缓存,并且需要的时候被重新计算,默认缓存的级别 </span><span class="token keyword">val</span> MEMORY_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition对应的原生的Java Object保存在JVM中,在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> MEMORY_ONLY_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_ONLY_SER <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_ONLY_SER_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition反序列化后的对象存储在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //超出的partition将被保存在Disk上,并且在需要时读取 </span><span class="token keyword">val</span> MEMORY_AND_DISK <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> MEMORY_AND_DISK_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_AND_DISK_SER <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_AND_DISK_SER_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition序列化后存储在Tachyon中 </span><span class="token keyword">val</span> OFF_HEAP <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="15-spark-deploy-recoveryMode-不支持那种-D"><a href="#15-spark-deploy-recoveryMode-不支持那种-D" class="headerlink" title="15 spark.deploy.recoveryMode 不支持那种 (D )"></a>15 spark.deploy.recoveryMode 不支持那种 (D )</h2><p>A.ZooKeeper B. FileSystem </p><p>D NONE D Hadoop</p><h2 id="16-下列哪个不是-RDD-的缓存方法-C"><a href="#16-下列哪个不是-RDD-的缓存方法-C" class="headerlink" title="16.下列哪个不是 RDD 的缓存方法 (C )"></a>16.下列哪个不是 RDD 的缓存方法 (C )</h2><p>A persist() B Cache() </p><p>C Memory()</p><h2 id="17-Task-运行在下来哪里个选项中-Executor-上的工作单元-C"><a href="#17-Task-运行在下来哪里个选项中-Executor-上的工作单元-C" class="headerlink" title="17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )"></a>17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )</h2><p>A Driver program B. spark master </p><p>C.worker node D Cluster manager</p><h2 id="18-hive-的元数据存储在-derby-和-MySQL-中有什么区别-B"><a href="#18-hive-的元数据存储在-derby-和-MySQL-中有什么区别-B" class="headerlink" title="18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )"></a>18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )</h2><p>A.没区别 B.多会话</p><p>C.支持网络环境 D数据库的区别</p><pre class="line-numbers language-sql"><code class="language-sql">备注：  Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。内置的derby主要问题是并发性能很差，可以理解为单线程操作。Derby还有一个特性。更换目录执行操作，会找不到相关表等<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="19-DataFrame-和-RDD-最大的区别-B"><a href="#19-DataFrame-和-RDD-最大的区别-B" class="headerlink" title="19.DataFrame 和 RDD 最大的区别 (B )"></a>19.DataFrame 和 RDD 最大的区别 (B )</h2><p>A.科学统计支持 B.多了 schema </p><p>C.存储方式不一样 D.外部数据源支持</p><blockquote><p>备注：</p><p><strong>上图直观体现了RDD与DataFrame的区别</strong>：左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><p><strong>提升执行效率</strong>： RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。</p><p><strong>减少数据读取</strong>：分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。</p></blockquote><blockquote><p>上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。</p><p>  对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过（例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200）。</p><p>  此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。</p><p> 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><p>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。</p><p>对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。</p></blockquote><ul><li><p><strong>RDD和Dataset</strong></p><p>​    DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。</p><p>​    DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为Spark</p><p>SQl类型，然而RDD依赖于运行时反射机制。</p></li><li><p><strong>DataFrame和Dataset</strong></p><p>​    Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。因此具有如下三个特点：</p><p>​    DataSet可以在编译时检查类型</p><p>并且是面向对象的编程接口。</p></li></ul><h2 id="20-Master-的-ElectedLeader-事件后做了哪些操作-D"><a href="#20-Master-的-ElectedLeader-事件后做了哪些操作-D" class="headerlink" title="20.Master 的 ElectedLeader 事件后做了哪些操作 (D )"></a>20.Master 的 ElectedLeader 事件后做了哪些操作 (D )</h2><p>A. 通知 driver B.通知 worker </p><p>C.注册 application D.直接 ALIVE</p><h2 id="34-cache后面能不能接其他算子-它是不是action操作？"><a href="#34-cache后面能不能接其他算子-它是不是action操作？" class="headerlink" title="34.cache后面能不能接其他算子,它是不是action操作？"></a>34.cache后面能不能接其他算子,它是不是action操作？</h2><p>答：cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。</p><p>cache不是action操作</p><h2 id="35-reduceByKey是不是action？"><a href="#35-reduceByKey是不是action？" class="headerlink" title="35.reduceByKey是不是action？"></a>35.reduceByKey是不是action？</h2><p>答：不是，很多人都会以为是action，reduce rdd是action</p><h2 id="36-数据本地性是在哪个环节确定的？"><a href="#36-数据本地性是在哪个环节确定的？" class="headerlink" title="36.数据本地性是在哪个环节确定的？"></a>36.数据本地性是在哪个环节确定的？</h2><p>具体的task运行在那他机器上，dag划分stage的时候确定的</p><h2 id="37-RDD的弹性表现在哪几点？"><a href="#37-RDD的弹性表现在哪几点？" class="headerlink" title="37.RDD的弹性表现在哪几点？"></a>37.RDD的弹性表现在哪几点？</h2><p>1）自动的进行内存和磁盘的存储切换；</p><p>2）基于Lingage的高效容错；</p><p>3）task如果失败会自动进行特定次数的重试；</p><p>4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；</p><p>5）checkpoint和persist，数据计算之后持久化缓存</p><p>6）数据调度弹性，DAG TASK调度和资源无关</p><p>7）数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par</p><h2 id="38-常规的容错方式有哪几种类型？"><a href="#38-常规的容错方式有哪几种类型？" class="headerlink" title="38.常规的容错方式有哪几种类型？"></a>38.常规的容错方式有哪几种类型？</h2><p>1）.数据检查点,会发生拷贝，浪费资源</p><p>2）.记录数据的更新，每次更新都会记录下来，比较复杂且比较消耗性能</p><h2 id="39-RDD通过Linage（记录数据更新）的方式为何很高效？"><a href="#39-RDD通过Linage（记录数据更新）的方式为何很高效？" class="headerlink" title="39.RDD通过Linage（记录数据更新）的方式为何很高效？"></a>39.RDD通过Linage（记录数据更新）的方式为何很高效？</h2><p>1）lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且rDD</p><p>之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就</p><p>产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条</p><p>将复杂计算链条存储下来，计算的时候从后往前回溯</p><p>900步是上一个stage的结束，要么就checkpoint</p><p>2）记录原数据，是每次修改都记录，代价很大</p><p>如果修改一个集合，代价就很小，官方说rdd是</p><p>粗粒度的操作，是为了效率，为了简化，每次都是</p><p>操作数据集合，写或者修改操作，都是基于集合的</p><p>rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的</p><p>也可以是细粒度，读可以读其中的一条条的记录。</p><p>3）简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景</p><p>如网络爬虫，现实世界中，大多数写是粗粒度的场景</p><h2 id="40-RDD有哪些缺陷？"><a href="#40-RDD有哪些缺陷？" class="headerlink" title="40.RDD有哪些缺陷？"></a>40.RDD有哪些缺陷？</h2><p>1）不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的</p><p>所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是</p><p>说可以一条条的读</p><p>2）不支持增量迭代计算，Flink支持</p><h2 id="41-说一说Spark程序编写的一般步骤？"><a href="#41-说一说Spark程序编写的一般步骤？" class="headerlink" title="41.说一说Spark程序编写的一般步骤？"></a>41.说一说Spark程序编写的一般步骤？</h2><p>答：初始化，资源，数据源，并行化，rdd转化，action算子打印输出结果或者也可以存至相应的数据存储介质，具体的可看下图：</p><p>file:///E:/%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6/qq19B99AF2399E52F466CC3CF7E3B24ED5/069fa7b471f54e038440faf63233acce/640.webp</p><h2 id="42-Spark有哪两种算子？"><a href="#42-Spark有哪两种算子？" class="headerlink" title="42. Spark有哪两种算子？"></a>42. Spark有哪两种算子？</h2><p>答：Transformation（转化）算子和Action（执行）算子。</p><h2 id="43-Spark提交你的jar包时所用的命令是什么？"><a href="#43-Spark提交你的jar包时所用的命令是什么？" class="headerlink" title="43. Spark提交你的jar包时所用的命令是什么？"></a>43. Spark提交你的jar包时所用的命令是什么？</h2><p>答：spark-submit。</p><h2 id="44-Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？"><a href="#44-Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？" class="headerlink" title="44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？"></a>44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？</h2><p>答：在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h2 id="45-你所理解的Spark的shuffle过程？"><a href="#45-你所理解的Spark的shuffle过程？" class="headerlink" title="45. 你所理解的Spark的shuffle过程？"></a>45. 你所理解的Spark的shuffle过程？</h2><p>答：从下面三点去展开</p><p>1）shuffle过程的划分</p><p>2）shuffle的中间结果如何存储</p><p>3）shuffle的数据如何拉取过来</p><p>可以参考这篇博文：<a href="http://www.cnblogs.com/jxhd1/p/6528540.html" target="_blank" rel="noopener">http://www.cnblogs.com/jxhd1/p/6528540.html</a></p><blockquote><p><strong>Shuffle后续优化方向</strong>：通过上面的介绍，我们了解到，Shuffle过程的主要存储介质是磁盘，尽量的减少IO是Shuffle的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，Shuffle过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有“土豪”内存节点，减少Shuffle IO的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW计算引擎运行地更好。</p><p><strong>MapReduce Shuffle后续优化方向</strong>：压缩：对数据进行压缩，减少写读数据量；</p><p>减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；<br>内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；<br>网络框架：netty的性能据说要占优了；<br><strong>本节点上的数据不走网络框架</strong>：对于本节点上的Map输出，Reduce直接去读吧，不需要绕道网络框架。<br>Spark Shuffle后续优化方向：Spark作为MapReduce的进阶架构，对于Shuffle过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是Spark的Shuffle对于我们来说在一些方面还是需要优化的。</p><p>压缩：对数据进行压缩，减少写读数据量；<br><strong>内存化</strong>：Spark历史版本中是有这样设计的：Map写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。</p></blockquote><h2 id="46-你如何从Kafka中获取数据？"><a href="#46-你如何从Kafka中获取数据？" class="headerlink" title="46. 你如何从Kafka中获取数据？"></a>46. 你如何从Kafka中获取数据？</h2><p><strong>1) 基于Receiver的方式</strong></p><p>这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。</p><p><strong>2) 基于Direct的方式</strong></p><p>这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据</p><h2 id="47-对于Spark中的数据倾斜问题你有什么好的方案？"><a href="#47-对于Spark中的数据倾斜问题你有什么好的方案？" class="headerlink" title="47. 对于Spark中的数据倾斜问题你有什么好的方案？"></a>47. 对于Spark中的数据倾斜问题你有什么好的方案？</h2><p>1）前提是定位数据倾斜，是OOM了，还是任务执行缓慢，看日志，看WebUI</p><p>2)解决方法，有多个方面</p><p>· 避免不必要的shuffle，如使用广播小表的方式，将reduce-side-join提升为map-side-join</p><p>·分拆发生数据倾斜的记录，分成几个部分进行，然后合并join后的结果</p><p>·改变并行度，可能并行度太少了，导致个别task数据压力大</p><p>·两阶段聚合，先局部聚合，再全局聚合</p><p>·自定义paritioner，分散key的分布，使其更加均匀</p><p>详细解决方案参考博文《Spark数据倾斜优化方法》</p><h2 id="48-RDD创建有哪几种方式？"><a href="#48-RDD创建有哪几种方式？" class="headerlink" title="48.RDD创建有哪几种方式？"></a>48.RDD创建有哪几种方式？</h2><p>1).使用程序中的集合创建rdd</p><p>2).使用本地文件系统创建rdd</p><p>3).使用hdfs创建rdd，</p><p>4).基于数据库db创建rdd</p><p>5).基于Nosql创建rdd，如hbase</p><p>6).基于s3创建rdd，</p><p>7).基于数据流，如socket创建rdd</p><p>如果只回答了前面三种，是不够的，只能说明你的水平还是入门级的，实践过程中有很多种创建方式。</p><h2 id="49-Spark并行度怎么设置比较合适"><a href="#49-Spark并行度怎么设置比较合适" class="headerlink" title="49.Spark并行度怎么设置比较合适"></a>49.Spark并行度怎么设置比较合适</h2><p>答：spark并行度，每个core承载2<del>4个partition,如，32个core，那么64</del>128之间的并行度，也就是</p><p>设置64~128个partion，并行读和数据规模无关，只和内存使用量和cpu使用</p><p>时间有关</p><h2 id="50-Spark中数据的位置是被谁管理的？"><a href="#50-Spark中数据的位置是被谁管理的？" class="headerlink" title="50.Spark中数据的位置是被谁管理的？"></a>50.Spark中数据的位置是被谁管理的？</h2><p>答：每个数据分片都对应具体物理位置，数据的位置是被blockManager，无论</p><p>数据是在磁盘，内存还是tacyan，都是由blockManager管理</p><p>答：Spark中的数据本地性有三种：</p><p>a.PROCESS_LOCAL是指读取缓存在本地节点的数据</p><p>b.NODE_LOCAL是指读取本地节点硬盘数据</p><p>c.ANY是指读取非本地节点数据</p><p>通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。</p><h2 id="52-rdd有几种操作类型？"><a href="#52-rdd有几种操作类型？" class="headerlink" title="52.rdd有几种操作类型？"></a>52.rdd有几种操作类型？</h2><p>1）transformation，rdd由一种转为另一种rdd</p><p>2）action，</p><p>3）cronroller，crontroller是控制算子,cache,persist，对性能和效率的有很好的支持</p><p>三种类型，不要回答只有2中操作</p><h2 id="53-Spark如何处理不能被序列化的对象？"><a href="#53-Spark如何处理不能被序列化的对象？" class="headerlink" title="53.Spark如何处理不能被序列化的对象？"></a>53.Spark如何处理不能被序列化的对象？</h2><p>将不能序列化的内容封装成object</p><h2 id="54-collect功能是什么，其底层是怎么实现的？"><a href="#54-collect功能是什么，其底层是怎么实现的？" class="headerlink" title="54.collect功能是什么，其底层是怎么实现的？"></a>54.collect功能是什么，其底层是怎么实现的？</h2><p>答：driver通过collect把集群中各个节点的内容收集过来汇总成结果，collect返回结果是Array类型的，collect把各个节点上的数据抓过来，抓过来数据是Array型，collect对Array抓过来的结果进行合并，合并后Array中只有一个元素，是tuple类型（KV类型的）的。</p><h2 id="55-Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"><a href="#55-Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？" class="headerlink" title="55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"></a>55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？</h2><p>答：</p><p>1）因为输入数据有很多task，尤其是有很多小文件的时候，有多少个输入block就会有多少个task启动；</p><p>2）spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。</p><p>3）参数可以通过spark_home/conf/spark-default.conf配置文件设置:</p><p>spark.sql.shuffle.partitions 50 spark.default.parallelism 10</p><p>第一个是针对spark sql的task数量</p><p>第二个是非spark sql程序设置生效</p><h2 id="56-为什么Spark-Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生"><a href="#56-为什么Spark-Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生" class="headerlink" title="56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?"></a>56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?</h2><p>答：会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑</p><p>否则很容易出现长时间分配不到资源，job一直不能运行的情况。</p><h2 id="57-map与flatMap的区别"><a href="#57-map与flatMap的区别" class="headerlink" title="57.map与flatMap的区别"></a>57.map与flatMap的区别</h2><blockquote><p>map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象</p><p>flatMap：对RDD每个元素转换，然后再扁平化</p><p>将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组</p><p>对象，会抛弃值为null的值</p></blockquote><h2 id="58-列举你常用的action？"><a href="#58-列举你常用的action？" class="headerlink" title="58.列举你常用的action？"></a>58.列举你常用的action？</h2><p>collect，reduce,take,count,saveAsTextFile等</p><h2 id="59-Spark为什么要持久化，一般什么场景下要进行persist操作？"><a href="#59-Spark为什么要持久化，一般什么场景下要进行persist操作？" class="headerlink" title="59.Spark为什么要持久化，一般什么场景下要进行persist操作？"></a>59.Spark为什么要持久化，一般什么场景下要进行persist操作？</h2><blockquote><p>为什么要进行持久化？</p><p>spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤</p><p>只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。</p></blockquote><blockquote><p>以下场景会使用persist</p><p>1）某个步骤计算非常耗时，需要进行persist持久化</p><p>2）计算链条非常长，重新恢复要算很多步骤，很好使，persist</p><p>3）checkpoint所在的rdd要持久化persist，</p><p>lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前</p><p>要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。</p><p>4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</p><p>5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</p><p>60.为什么要进行序列化</p><p>序列化可以减少数据的体积，减少存储空间，高效存储和传输数据，不好的是使用的时候要反序列化，非常消耗CPU</p></blockquote><h2 id="61-介绍一下join操作优化经验？"><a href="#61-介绍一下join操作优化经验？" class="headerlink" title="61.介绍一下join操作优化经验？"></a>61.介绍一下join操作优化经验？</h2><blockquote><p>答：join其实常见的就分为两类： map-side join 和  reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。</p><p>备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。</p></blockquote><h2 id="62-介绍一下cogroup-rdd实现原理，你在什么场景下用过这个rdd？"><a href="#62-介绍一下cogroup-rdd实现原理，你在什么场景下用过这个rdd？" class="headerlink" title="62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？"></a>62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？</h2><blockquote><p>答：cogroup的函数实现:这个实现根据两个要进行合并的两个RDD操作,生成一个CoGroupedRDD的实例,这个RDD的返回结果是把相同的key中两个RDD分别进行合并操作,最后返回的RDD的value是一个Pair的实例,这个实例包含两个Iterable的值,第一个值表示的是RDD1中相同KEY的值,第二个值表示的是RDD2中相同key的值.由于做cogroup的操作,需要通过partitioner进行重新分区的操作,因此,执行这个流程时,需要执行一次shuffle的操作(如果要进行合并的两个RDD的都已经是shuffle后的rdd,同时他们对应的partitioner相同时,就不需要执行shuffle,)，</p><p>场景：表关联查询</p></blockquote><p>63下面这段代码输出结果是什么？</p><hr><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">def</span> joinRdd<span class="token punctuation">(</span>sc<span class="token operator">:</span>SparkContext<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token keyword">val</span> name<span class="token operator">=</span> Array<span class="token punctuation">(</span>Tuple2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">"spark"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">"tachyon"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token string">"hadoop"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">val</span> score<span class="token operator">=</span> Array<span class="token punctuation">(</span>Tuple2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">val</span> namerdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">val</span> scorerdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>score<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">val</span> result <span class="token operator">=</span> namerdd<span class="token punctuation">.</span>join<span class="token punctuation">(</span>scorerdd<span class="token punctuation">)</span><span class="token punctuation">;</span>result <span class="token punctuation">.</span>collect<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>答案<span class="token operator">:</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>Spark<span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">(</span>tachyon<span class="token punctuation">,</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">(</span>hadoop<span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Spark-的四大组件下面哪个不是-D&quot;&gt;&lt;a href=&quot;#1-Spark-的四大组件下面哪个不是-D&quot; class=&quot;headerlink&quot; title=&quot;1. Spark 的四大组件下面哪个不是 (D )&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;Spark 的四
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark面试问题梳理</title>
    <link href="https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/"/>
    <id>https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/</id>
    <published>2020-06-21T06:35:00.000Z</published>
    <updated>2020-06-21T12:25:46.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题一：Spark中的RDD是什么，有哪些特性？"><a href="#问题一：Spark中的RDD是什么，有哪些特性？" class="headerlink" title="问题一：Spark中的RDD是什么，有哪些特性？"></a><strong>问题一：Spark中的RDD是什么，有哪些特性？</strong></h2><h3 id="1-RDD是什么？"><a href="#1-RDD是什么？" class="headerlink" title="1.RDD是什么？"></a><strong>1.RDD是什么？</strong></h3><blockquote><p><strong>RDD</strong>（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合</p></blockquote><ul><li><strong>Dataset</strong>：就是一个集合，用于存放数据的</li><li><strong>Destributed</strong>：分布式，可以并行在集群计算</li><li><strong>Resilient</strong>：表示弹性的，弹性表示<ul><li>RDD中的数据可以存储在内存或者磁盘中；</li><li>RDD中的分区是可以改变的；</li></ul></li></ul><h3 id="2-五大特性："><a href="#2-五大特性：" class="headerlink" title="2. 五大特性："></a><strong>2. 五大特性：</strong></h3><ol><li><strong>A list of partitions</strong>：一个分区列表，RDD中的数据都存储在一个分区列表中</li><li><strong>A function for computing each split</strong>：作用在每一个分区中的函数</li><li><strong>A list of dependencies on other RDDs</strong>：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的</li><li><strong>Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)</strong>：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向</li><li>可选项，数据本地性，数据位置最优</li></ol><h2 id="问题二：-概述一下spark中的常用算子区别（map-mapPartitions，foreach，foreachPatition）"><a href="#问题二：-概述一下spark中的常用算子区别（map-mapPartitions，foreach，foreachPatition）" class="headerlink" title="问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）"></a>问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）</h2><p>常用算子：</p><ul><li><strong>map</strong>：用于遍历RDD，将函数应用于每一个元素，返回新的RDD（transformation算子）</li><li><strong>foreach</strong>：用于遍历RDD，将函数应用于每一个元素，无返回值（action算子）</li><li><strong>mapPatitions</strong>：用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子）</li><li><strong>foreachPatition</strong>：用于遍历操作RDD中的每一个分区，无返回值（action算子）</li></ul><p><strong>总结</strong>：一般使用<strong>mapPatitions</strong>和<strong>foreachPatition</strong>算子比map和foreach更加高效，推荐使用</p><h2 id="问题三：-谈谈spark中的宽窄依赖："><a href="#问题三：-谈谈spark中的宽窄依赖：" class="headerlink" title="问题三：.谈谈spark中的宽窄依赖："></a><strong>问题三：.谈谈spark中的宽窄依赖</strong>：</h2><p>答：RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题四：spark中如何划分stage："><a href="#问题四：spark中如何划分stage：" class="headerlink" title="问题四：spark中如何划分stage："></a>问题四：spark中如何划分stage：</h2><ul><li><strong>概念</strong>：</li></ul><blockquote><p>​       Spark任务会根据<strong>RDD</strong>之间的依赖关系，形成一个<strong>DAG有向无环图</strong>，<strong>DAG</strong>会提交给<strong>DAGScheduler</strong>，<strong>DAGScheduler</strong>会把<strong>DAG</strong>划分相互依赖的多个<strong>stage</strong>，划分依据就是<strong>宽窄依赖</strong>，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以<strong>taskSet</strong>的形式提交给<strong>TaskScheduler</strong>运行，stage是由一组并行的task组成。</p><p>​       Spark程序中可以因为不同的action触发众多的job，一个程序中可以有很多的job，每一个job是由一个或者多个stage构成的，后面的stage依赖于前面的stage，也就是说只有前面依赖的stage计算完毕后，后面的stage才会运行；</p><p>​        stage 的划分标准就是宽依赖：何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；</p><p>​       切割规则：从后往前，遇到宽依赖就切割stage；</p></blockquote><ul><li><strong>图解</strong>：</li></ul><p><img src="https://img-blog.csdn.net/20180909161915933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><ul><li><strong>计算格式</strong>：pipeline管道计算模式，piepeline只是一种计算思想，一种模式。</li></ul><blockquote><p>​        spark的pipeline管道计算模式相当于执行了一个高阶函数，也就是说来一条数据然后计算一条数据，会把所有的逻辑走完，然后落地，而MapReduce是1+1=2，2+1=3这样的计算模式，也就是计算完落地，然后再计算，然后再落地到磁盘或者内存，最后数据是落在计算节点上，按reduce的hash分区落地。管道计算模式完全基于内存计算，所以比MapReduce快的原因。</p><p>管道中的RDD何时落地：shuffle write的时候，对RDD进行持久化的时候。</p><p>​    stage的task的并行度是由stage的最后一个RDD的分区数来决定的，一般来说，一个partition对应一个task，但最后reduce的时候可以手动改变reduce的个数，也就是改变最后一个RDD的分区数，也就改变了并行度。例如：reduceByKey(<em>+</em>,3)</p></blockquote><ul><li><strong>优化</strong>：提高stage的并行度：reduceByKey(<em>+</em>,patition的个数) ，join(<em>+</em>,patition的个数)</li></ul><h2 id="问题五：DAGScheduler分析："><a href="#问题五：DAGScheduler分析：" class="headerlink" title="问题五：DAGScheduler分析："></a>问题五：DAGScheduler分析：</h2><p>答：</p><ul><li><p><strong>概述</strong>：<strong>DAGScheduler</strong>是一个面向stage 的调度器；</p></li><li><p><strong>主要入参</strong>：</p><ul><li>dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,resultHandler, localProperties.get)</li><li>rdd： final RDD；</li><li>cleanedFunc： 计算每个分区的函数；</li><li>resultHander： 结果侦听器；</li></ul></li><li><p><strong>主要功能</strong>：</p><ol><li><p>接受用户提交的job；</p></li><li><p>将job根据类型划分为不同的stage，记录那些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</p></li><li><p>决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给<strong>TaskScheduler</strong>；</p></li><li><p>重新提交<strong>shuffle</strong>输出丢失的stage给taskScheduler；</p></li></ol></li></ul><p>注：一个stage内部的错误不是由shuffle输出丢失造成的，DAGScheduler是不管的，由TaskScheduler负责尝试重新提交task执行。</p><h2 id="问题六：Job的生成："><a href="#问题六：Job的生成：" class="headerlink" title="问题六：Job的生成："></a><strong>问题六：Job的生成：</strong></h2><p>​        一旦driver程序中出现action，就会生成一个job，比如count等，向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。</p><p>​        每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是<strong>宽依赖</strong>，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p><h2 id="问题七：有向无环图："><a href="#问题七：有向无环图：" class="headerlink" title="问题七：有向无环图："></a><strong>问题七：有向无环图：</strong></h2><p>​    <strong>DAG</strong>，有向无环图，简单的来说，就是一个由顶点和有方向性的边构成的图中，从任意一个顶点出发，没有任意一条路径会将其带回到出发点的顶点位置，为每个spark job计算具有依赖关系的多个stage任务阶段，通常根据shuffle来划分stage，如reduceByKey,groupByKey等涉及到shuffle的transformation就会产生新的stage ，然后将每个stage划分为具体的一组任务，以TaskSets的形式提交给底层的任务调度模块来执行，其中不同stage之前的RDD为宽依赖关系，TaskScheduler任务调度模块负责具体启动任务，监控和汇报任务运行情况。</p><h2 id="问题八：RDD是什么以及它的分类："><a href="#问题八：RDD是什么以及它的分类：" class="headerlink" title="问题八：RDD是什么以及它的分类："></a><strong>问题八：RDD是什么以及它的分类：</strong></h2><p><img src="https://img-blog.csdn.net/20180909162040887?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题九：RDD的操作"><a href="#问题九：RDD的操作" class="headerlink" title="问题九：RDD的操作"></a><strong>问题九：RDD的操作</strong></h2><p><img src="https://img-blog.csdn.net/20180909162041368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/2018090916204272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162101528?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162110376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/2018090916212499?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162136156?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163215275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163232304?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163244483?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163257291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题十-RDD缓存："><a href="#问题十-RDD缓存：" class="headerlink" title="问题十: RDD缓存："></a><strong>问题十: RDD缓存</strong>：</h2><p>​        Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 <strong>transformation</strong>自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。</p><p>​         Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。</p><p>​          Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li><p>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</p></li><li><p>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p></li><li><p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p></li><li><p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p></li><li><p>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</p></li><li><p>在不会使用cached RDD的时候，及时使用unpersist方法来释放它。</p></li></ul><h2 id="问题十一：RDD共享变量："><a href="#问题十一：RDD共享变量：" class="headerlink" title="问题十一：RDD共享变量："></a>问题十一：RDD共享变量：</h2><p>​       在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。</p><p>(1). <strong>广播变量（Broadcast Variables）</strong></p><p>– 广播变量缓存到各个节点的内存中，而不是每个 Task</p><p>– 广播变量被创建后，能在集群中运行的任何函数调用</p><p>– 广播变量是只读的，不能在被广播后修改</p><p>– 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本</p><p>val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量<br>(2). <strong>累加器</strong></p><p>​    累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值</p><h2 id="问题十二：spark-submit的时候如何引入外部jar包："><a href="#问题十二：spark-submit的时候如何引入外部jar包：" class="headerlink" title="问题十二：spark-submit的时候如何引入外部jar包："></a>问题十二：spark-submit的时候如何引入外部jar包：</h2><p>在通过spark-submit提交任务时，可以通过添加配置参数来指定 </p><p>–driver-class-path 外部jar包<br>–jars 外部jar包</p><h2 id="问题十三：spark如何防止内存溢出："><a href="#问题十三：spark如何防止内存溢出：" class="headerlink" title="问题十三：spark如何防止内存溢出："></a>问题十三：spark如何防止内存溢出：</h2><ul><li><p>driver端的内存溢出 </p><ul><li><strong>可以增大driver的内存参数</strong>：<code>spark.driver.memory (default 1g)</code><br>这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。</li><li><strong>map过程产生大量对象导致内存溢出</strong><br>这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。<br>面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区， 不会有shuffle的过程。</li></ul></li><li><p>数据不平衡导致内存溢出 </p><pre><code> 数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。</code></pre></li><li><p>shuffle后内存溢出 </p><pre><code>  shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。</code></pre></li><li><p>standalone模式下资源分配不均匀导致内存溢出</p><pre><code>  在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</code></pre><p>​      这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()<br>rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。</p></li></ul><h2 id="问题十四：spark中cache和persist的区别："><a href="#问题十四：spark中cache和persist的区别：" class="headerlink" title="问题十四：spark中cache和persist的区别："></a>问题十四：spark中cache和persist的区别：</h2><p><strong>cache</strong>：缓存数据，默认是缓存在内存中，其本质还是调用persist<br><strong>persist</strong>: 缓存数据，有丰富的数据缓存策略。数据可以保存在内存也可以保存在磁盘中，使用的时候指定对应的缓存级别就可以了。</p><h2 id="问题十五：spark中的数据倾斜的现象，原因，后果："><a href="#问题十五：spark中的数据倾斜的现象，原因，后果：" class="headerlink" title="问题十五：spark中的数据倾斜的现象，原因，后果："></a>问题十五：spark中的数据倾斜的现象，原因，后果：</h2><p><strong>(1)、数据倾斜的现象</strong><br>        多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。<br><strong>(2)、数据倾斜的原因</strong> </p><ul><li><p>数据问题<br>1、key本身分布不均衡（包括大量的key为空）<br>2、key的设置不合理</p></li><li><p>spark使用问题<br>1、shuffle时的并发度不够<br>2、计算方式有误</p></li></ul><p><strong>(3)、数据倾斜的后果</strong> </p><ol><li><p>spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）</p></li><li><p>过多的数据在同一个task中运行，将会把executor撑爆。</p></li></ol><h2 id="问题十六：spark数据倾斜的处理："><a href="#问题十六：spark数据倾斜的处理：" class="headerlink" title="问题十六：spark数据倾斜的处理："></a>问题十六：spark数据倾斜的处理：</h2><p>发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首<strong>先要检查数据本身</strong>，是否存在异常数据。</p><ol><li><p><strong>数据问题造成的数据倾斜</strong><br><strong>找出异常的key</strong><br>如果任务长时间卡在最后最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个。<br>比如:</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>“key”<span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span><span class="token operator">+</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><p>   如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况: </p><p>1、null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。<br>2、无效数据，大量重复的测试数据或是对结果影响不大的有效数据。<br>3、有效数据，业务导致的正常数据分布。</p><p><strong>解决办法</strong> </p><p>第1，2种情况，直接对数据进行过滤即可（因为该数据对当前业务不会产生影响）。<br>第3种情况则需要进行一些特殊操作，常见的有以下几种做法<br>(1) <strong>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作</strong>。<br>(2) <strong>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</strong><br>(3) 使用reduceByKey 代替 groupByKey(reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义.)<br>(4) 使用<strong>map join</strong>。</p><p><strong>案例</strong> </p><p>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:<br>(1) 将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>(2) 对数据进行 reduceByKey(func)<br>(3) 将 key + 随机值 转成 key<br>(4) 再对数据进行 reduceByKey(func)</p><p><strong>案例操作流程分析</strong>： </p><p>假设说有倾斜的Key，我们给所有的Key加上一个随机数，然后进行reduceByKey操作；此时同一个Key会有不同的随机数前缀，在进行reduceByKey操作的时候原来的一个非常大的倾斜的Key就分而治之变成若干个更小的Key，不过此时结果和原来不一样，怎么破？进行map操作，目的是把随机数前缀去掉，然后再次进行reduceByKey操作。（当然，如果你很无聊，可以再次做随机数前缀），这样我们就可以把原本倾斜的Key通过分而治之方案分散开来，最后又进行了全局聚合<br>注意1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。<br>注意2: 单独处理异常数据时，可以配合使用Map Join解决。</p><p>2.<strong>spark使用不当造成的数据倾斜</strong></p><ul><li><p><strong>提高shuffle并行度</strong><br>dataFrame和sparkSql可以设置spark.sql.shuffle.partitions参数控制shuffle的并发度，默认为200。<br>rdd操作可以设置spark.default.parallelism控制并发度，默认参数由不同的Cluster Manager控制。</p><ul><li><strong>局限性:</strong> 只是让每个task执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些key的大      小非常大，即使一个task单独执行它，也会受到数据倾斜的困扰。</li></ul></li><li><p><strong>使用map join 代替reduce join</strong></p><pre><code>  在小表不是特别大(取决于你的executor大小)的情况下使用，可以使程序避免shuffle的过程，自然也就没有数据倾斜的困扰了.（详细见http://blog.csdn.net/lsshlsw/article/details/50834858、http://blog.csdn.net/lsshlsw/article/details/48694893）</code></pre><ul><li><p><strong>局限性</strong>: 因为是先将小数据发送到每个executor上，所以数据量不能太大。</p><p>​    </p><h2 id="问题十七：spark中map-side-join关联优化："><a href="#问题十七：spark中map-side-join关联优化：" class="headerlink" title="问题十七：spark中map-side-join关联优化："></a>问题十七：spark中map-side-join关联优化：</h2></li></ul></li></ul><p>​      将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。</p><p>如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。</p><p>何时使用：<strong>在海量数据中匹配少量特定数据</strong></p><p>原理：reduce-side-join 的缺陷在于会将key相同的数据发送到同一个partition中进行运算，大数据集的传输需要长时间的IO，同时任务并发度收到限制，还可能造成数据倾斜。</p><p>reduce-side-join 运行图如下</p><p><img src="https://img-blog.csdn.net/20180909162441742?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>map-side-join 运行图如下：</p><p><img src="https://img-blog.csdn.net/20180909162409221?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>将少量的数据转化为Map进行广播，广播会将此 Map 发送到每个节点中，如果不进行广播，每个task执行时都会去获取该Map数据，造成了性能浪费。对大数据进行遍历，使用mapPartition而不是map，因为mapPartition是在每个partition中进行操作，因此可以减少遍历时新建broadCastMap.value对象的空间消耗，同时匹配不到的数据也不会返回。</p><h2 id="问题十八：kafka整合sparkStreaming问题："><a href="#问题十八：kafka整合sparkStreaming问题：" class="headerlink" title="问题十八：kafka整合sparkStreaming问题："></a>问题十八：kafka整合sparkStreaming问题：</h2><p><strong>(1)、如何实现sparkStreaming读取kafka中的数据</strong><br>可以这样说：在kafka0.10版本之前有二种方式与sparkStreaming整合，一种是基于<strong>receiver</strong>，一种是<strong>direct</strong>,然后分别阐述这2种方式分别是什么 </p><ul><li><p><strong>receiver</strong>：是采用了kafka高级api,利用receiver接收器来接受kafka topic中的数据，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据，kafka中topic的偏移量是保存在zk中的。<br>基本使用：还有几个需要注意的点： </p><p>​       在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度.<br>​         对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。<br>在默认配置下，这种方式可能会因为底层的失败而丢失数据. 因为receiver一直在接收数据,在其已经通知zookeeper数据接收完成但是还没有处理的时候,executor突然挂掉(或是driver挂掉通知executor关闭),缓存在其中的数据就会丢失. 如果希望做到高可靠, 让数据零丢失,如果我们启用了Write Ahead Logs(spark.streaming.receiver.writeAheadLog.enable=true）该机制会同步地将接收到的Kafka数据写入分布式文件系统(比如HDFS)上的预写日志中. 所以, 即使底层节点出现了失败, 也可以使用预写日志中的数据进行恢复. 复制到文件系统如HDFS，那么storage level需要设置成 StorageLevel.MEMORY_AND_DISK_SER，也就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER)</p></li></ul><ul><li><strong>direct</strong>: 在spark1.3之后，引入了Direct方式。不同于Receiver的方式，Direct方式没有receiver这一层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，之后根据设定的maxRatePerPartition来处理每个batch。（设置spark.streaming.kafka.maxRatePerPartition=10000。限制每秒钟从topic的每个partition最多消费的消息条数）</li></ul><p><strong>(2) 对比这2中方式的优缺点：</strong></p><ul><li><p>采用receiver方式：这种方式可以保证数据不丢失，但是无法保证数据只被处理一次，WAL实现的是At-least-once语义（至少被处理一次），如果在写入到外部存储的数据还没有将offset更新到zookeeper就挂掉,这些数据将会被反复消费. 同时,降低了程序的吞吐量。</p></li><li><p>采用direct方式:    相比Receiver模式而言能够确保机制更加健壮. 区别于使用Receiver来被动接收数据, Direct模式会周期性地主动查询Kafka, 来获得每个topic+partition的最新的offset, 从而定义每个batch的offset的范围. 当处理数据的job启动时, 就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 </p><pre><code>**优点**： **1、简化并行读取** 如果要读取多个partition, 不需要创建多个输入DStream然后对它们进行union操作. Spark会创建跟Kafka partition一样多的RDD partition, 并且会并行从Kafka中读取数据. 所以在Kafka partition和RDD partition之间, 有一个一对一的映射关系.**2、高性能** 如果要保证零数据丢失, 在基于receiver的方式中, 需要开启WAL机制. 这种方式其实效率低下, 因为数据实际上被复制了两份, Kafka自己本身就有高可靠的机制, 会对数据复制一份, 而这里又会复制一份到WAL中. 而基于direct的方式, 不依赖Receiver, 不需要开启WAL机制, 只要Kafka中作了数据的复制, 那么就可以通过Kafka的副本进行恢复.**3、一次且仅一次的事务机制** 基于receiver的方式, 是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的. 这是消费Kafka数据的传统方式. 这种方式配合着WAL机制可以保证数据零丢失的高可靠性, 但是却无法保证数据被处理一次且仅一次, 可能会处理两次. 因为Spark和ZooKeeper之间可能是不同步的. 基于direct的方式, 使用kafka的简单api, Spark Streaming自己就负责追踪消费的offset, 并保存在checkpoint中. Spark自己一定是同步的, 因此可以保证数据是消费一次且仅消费一次。不过需要自己完成将offset写入zk的过程,在官方文档中都有相应介绍. </code></pre><pre class="line-numbers language-scala"><code class="language-scala"><span class="token operator">-</span><span class="token operator">*</span>简单代码实例： messages<span class="token punctuation">.</span>foreachRDD<span class="token punctuation">(</span>rdd<span class="token keyword">=></span><span class="token punctuation">{</span> <span class="token keyword">val</span> message <span class="token operator">=</span> rdd<span class="token punctuation">.</span>map<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//对数据进行一些操作 </span>message<span class="token punctuation">.</span>map<span class="token punctuation">(</span>method<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//更新zk上的offset (自己实现) </span>updateZKOffsets<span class="token punctuation">(</span>rdd<span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>sparkStreaming程序自己消费完成后，自己主动去更新zk上面的偏移量。也可以将zk中的偏移量保存在mysql或者redis数据库中，下次重启的时候，直接读取mysql或者redis中的偏移量，获取到上次消费的偏移量，接着读取数据。</p></li></ul><h2 id="问题十九：利用scala语言进行排序"><a href="#问题十九：利用scala语言进行排序" class="headerlink" title="问题十九：利用scala语言进行排序"></a>问题十九：利用scala语言进行排序</h2><p><strong>1.冒泡</strong>：</p><p><img src="https://img-blog.csdn.net/20180909162545316?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><strong>2.快读排序</strong>：</p><p><img src="https://img-blog.csdn.net/20180909162606931?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162624563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题二十：spark-master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？"><a href="#问题二十：spark-master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？" class="headerlink" title="问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？"></a>问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？</h2><p>​        Spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括worker,master,application,executors.standby节点要从zk中获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的，另外，master切换需要注意两点：</p><p>​    1. 在master切换的过程中，所有的已经在运行的程序皆正常运行，因为spark application在运行前就已经通过cluster manager获得了计算资源，所以在运行时job本身的调度和处理master是没有任何关系的；</p><p>​    2. 在master的切换过程中唯一的影响是不能提交新的job，一方面不能提交新的应用程序给集群，因为只有Active master才能接受新的程序的提交请求，另外一方面，已经运行的程序也不能action操作触发新的job提交请求。</p><p><strong>问题二十一：spark master HA主从切换过程不会影响集群已有的作业运行，为什么？</strong></p><p>答：因为程序在运行之前，已经向集群申请过资源，这些资源已经提交给driver了，也就是说已经分配好资源了，这是粗粒度分配，一次性分配好资源后不需要再关心资源分配，在运行时让driver和executor自动交互，弊端是如果资源分配太多，任务运行完不会很快释放，造成资源浪费，这里不适用细粒度分配的原因是因为任务提交太慢。</p><h2 id="问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？"><a href="#问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？" class="headerlink" title="问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？"></a>问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？</h2><p><strong>1.粗粒度</strong>：启动时就分配好资源，程序启动，后续具体使用就使用分配好的资源，不需要再分配资源。好处：作业特别多时，资源复用率较高，使用粗粒度。缺点：容易资源浪费，如果一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度。如果有999个资源闲置在那里，会造成资源大量浪费。</p><p><strong>2.细粒度</strong>：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。</p><h2 id="问题二十三：driver的功能是什么："><a href="#问题二十三：driver的功能是什么：" class="headerlink" title="问题二十三：driver的功能是什么："></a><strong>问题二十三：driver的功能是什么</strong>：</h2><p>1.一个spark作业运行时包括一个driver进程，也就是作业的主进程，具有main函数，并且有sparkContext的实例，是程序的入口；</p><p><strong>2.功能</strong>：负责向集群申请资源，向master注册信息，负责了作业的调度，负责了作业的解析，生成stage并调度task到executor上，包括DAGScheduler，TaskScheduler。</p><h2 id="问题二十四：spark的有几种部署模式，每种模式特点？"><a href="#问题二十四：spark的有几种部署模式，每种模式特点？" class="headerlink" title="问题二十四：spark的有几种部署模式，每种模式特点？"></a>问题二十四：spark的有几种部署模式，每种模式特点？</h2><p><strong>1）本地模式</strong></p><p>Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类</p><p>·  local：只启动一个executor</p><p>·  local[k]:启动k个executor</p><p>·  local：启动跟cpu数目相同的 executor</p><p><strong>2)standalone模式</strong></p><p>分布式部署集群， 自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础，</p><p><strong>3)Spark on yarn模式</strong></p><p>分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端</p><p><strong>4）Spark On Mesos模式。</strong>官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序：</p><p><strong>1)   粗粒度模式</strong>（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p><p><strong>2)   细粒度模式（Fine-grained Mode）</strong>：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。</p><h2 id="问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a><strong>问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</strong></h2><p><strong>1）Spark core</strong>：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。</p><p><strong>2）SparkStreaming</strong>是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。</p><p><strong>3）Spark sql</strong>：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析</p><p><strong>4）BlinkDB</strong> ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。</p><p><strong>5）MLBase</strong>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib，MLI、ML Optimizer和MLRuntime。</p><p><strong>6）GraphX</strong>是Spark中用于图和图并行计算</p><h2 id="问题二十六：spark中worker-的主要工作是什么？"><a href="#问题二十六：spark中worker-的主要工作是什么？" class="headerlink" title="问题二十六：spark中worker 的主要工作是什么？"></a><strong>问题二十六：spark中worker 的主要工作是什么？</strong></h2><p><strong>主要功能</strong>：管理当前节点内存，CPU的使用情况，接受master发送过来的资源指令，通过executorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务，需要注意的是：</p><p><strong>1.worker</strong>会不会汇报当前信息给master？worker心跳给master主要只有workid，不会以心跳的方式发送资源信息给master，这样master就知道worker是否存活，只有故障的时候才会发送资源信息；</p><p><strong>2.worker</strong>不会运行代码，具体运行的是executor，可以运行具体application斜的业务逻辑代码，操作代码的节点，不会去运行代码。</p><h2 id="问题二十七：简单说一下hadoop和spark的shuffle相同和差异？"><a href="#问题二十七：简单说一下hadoop和spark的shuffle相同和差异？" class="headerlink" title="问题二十七：简单说一下hadoop和spark的shuffle相同和差异？"></a><strong>问题二十七：简单说一下hadoop和spark的shuffle相同和差异？</strong></h2><p><strong>1）从 high-level 的角度来看，两者并没有大的差别</strong>。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</p><p><strong>2）从 low-level 的角度来看，两者差别不小。</strong> Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。</p><p><strong>3）从实现角度来看，两者也有不少差别。</strong> Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。</p><p>如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？ </p><p>Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</p><h2 id="问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别"><a href="#问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别" class="headerlink" title="问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别"></a>问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别</h2><p><strong>两者都是用mr模型来进行并行计算:</strong></p><p>1) hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。 </p><p>2) spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 </p><p>3) hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 </p><p>spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。</p><h2 id="问题二十九：RDD机制？"><a href="#问题二十九：RDD机制？" class="headerlink" title="问题二十九：RDD机制？"></a>问题二十九：RDD机制？</h2><p>rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 </p><p>所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。 </p><p>rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。</p><h2 id="问题三十：spark有哪些组件？"><a href="#问题三十：spark有哪些组件？" class="headerlink" title="问题三十：spark有哪些组件？"></a><strong>问题三十：spark有哪些组件？</strong></h2><p>答：主要有如下组件：</p><p>1）master：管理集群和节点，不参与计算。 </p><p>2）worker：计算节点，进程本身不参与计算，和master汇报。 </p><p>3）Driver：运行程序的main方法，创建spark context对象。 </p><p>4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 </p><p>5）client：用户提交程序的入口。</p><h2 id="问题三十一：spark工作机制？"><a href="#问题三十一：spark工作机制？" class="headerlink" title="问题三十一：spark工作机制？"></a><strong>问题三十一：spark工作机制？</strong></h2><p>答：用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 </p><p>执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。 task scheduler会将stage划分为task set分发到各个节点的executor中执行。</p><h2 id="问题三十二：spark的优化怎么做？"><a href="#问题三十二：spark的优化怎么做？" class="headerlink" title="问题三十二：spark的优化怎么做？"></a><strong>问题三十二：spark的优化怎么做？</strong></h2><p>答： spark调优比较复杂，但是大体可以分为三个方面来进行，</p><p><strong>1）平台层面的调优：</strong>防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet，</p><p><strong>2）应用程序层面的调优</strong>：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等，</p><p><strong>3）JVM层面的调优</strong>：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等</p><p>​       序列化在分布式系统中扮演着重要的角色，优化Spark程序时，首当其冲的就是对序列化方式的优化。Spark为使用者提供两种序列化方式：</p><ul><li>Java serialization: 默认的序列化方式。</li></ul><ul><li>Kryo serialization: 相较于 Java serialization 的方式，速度更快，空间占用更小，但并不支持所有的序列化格式，同时使用的时候需要注册class。spark-sql中默认使用的是kyro的序列化方式。<br>可以在spark-default.conf设置全局参数，也可以代码中初始化时对SparkConf设置 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) ，该参数会同时作用于机器之间数据的shuffle操作以及序列化rdd到磁盘，内存。<br>Spark不将Kyro设置成默认的序列化方式是因为它需要对类进行注册，官方强烈建议在一些网络数据传输很大的应用中使用kyro序列化。</li></ul><p>如果你要序列化的对象比较大，可以增加参数spark.kryoserializer.buffer所设置的值。</p><p>如果你没有注册需要序列化的class，Kyro依然可以照常工作，但会存储每个对象的全类名(full class name)，这样的使用方式往往比默认的 Java serialization 还要浪费更多的空间。</p><p>可以设置 spark.kryo.registrationRequired 参数为 true，使用kyro时如果在应用中有类没有进行注册则会报错：</p><p>如上这个错误需要添加</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题一：Spark中的RDD是什么，有哪些特性？&quot;&gt;&lt;a href=&quot;#问题一：Spark中的RDD是什么，有哪些特性？&quot; class=&quot;headerlink&quot; title=&quot;问题一：Spark中的RDD是什么，有哪些特性？&quot;&gt;&lt;/a&gt;&lt;strong&gt;问题一：Sp
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>HiveSQL优化 hive参数版总结</title>
    <link href="https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/"/>
    <id>https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/</id>
    <published>2020-06-11T06:50:00.000Z</published>
    <updated>2020-06-11T10:22:08.086Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。</strong></p></blockquote><h2 id="1-Hive优化目标"><a href="#1-Hive优化目标" class="headerlink" title="1. Hive优化目标"></a><strong>1. Hive优化目标</strong></h2><ul><li><p>在有限的资源下，执行效率更高</p></li><li><p>常见问题</p><ul><li><p>数据倾斜</p></li><li><p>map数设置</p></li><li><p>reduce数设置</p></li><li><p>其他</p></li></ul></li></ul><h2 id="2-Hive执行优化"><a href="#2-Hive执行优化" class="headerlink" title="2. Hive执行优化"></a><strong>2. Hive执行优化</strong></h2><ul><li><p><code>HQL --&gt; Job --&gt; Map/Reduce</code></p></li><li><p>执行计划<code>explain [extended] hql</code></p><p>样例</p><p><code>select col,count(1) from test2 group by col;</code></p><p><code>explain select col,count(1) from test2 group by col;</code></p></li></ul><h2 id="3-Hive表优化"><a href="#3-Hive表优化" class="headerlink" title="3. Hive表优化"></a><strong>3. Hive表优化</strong></h2><ul><li><p>分区</p><p><code>set hive.exec.dynamic.partition=true;</code></p><p><code>set hive.exec.dynamic.partition.mode=nonstrict;</code></p><p>​    静态分区</p><p>​    动态分区</p></li><li><p>分桶</p><p>  <code>set hive.enforce.bucketing=true;</code></p><p>  <code>set hive.enforce.sorting=true;</code></p></li><li><p>数据</p><p>相同数据尽量聚集在一起</p></li></ul><h2 id="4-Hive-Job优化"><a href="#4-Hive-Job优化" class="headerlink" title="4. Hive Job优化"></a><strong>4. Hive Job优化</strong></h2><h3 id="并行化执行"><a href="#并行化执行" class="headerlink" title="并行化执行"></a>并行化执行</h3><p>每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间</p><ul><li><code>set hive.exec.parallel= true;</code></li><li><code>set hive.exec.parallel.thread.numbe=8;</code></li></ul><h3 id="本地化执行"><a href="#本地化执行" class="headerlink" title="本地化执行"></a>本地化执行</h3><ul><li>​    job的输入数据大小必须小于参数:<code>hive.exec.mode.local.auto.inputbytes.max(默认128MB)</code></li><li>​    job的map数必须小于参数:<code>hive.exec.mode.local.auto.tasks.max(默认4)</code></li><li>​    job的reduce数必须为0或者1</li><li>​    <code>set hive.exec.mode.local.auto=true;</code></li></ul><p>​       当一个job满足如上条件才能真正使用本地模式:</p><h3 id="job合并输入小文件"><a href="#job合并输入小文件" class="headerlink" title="job合并输入小文件"></a>job合并输入小文件</h3><ul><li><code>set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</code></li><li>合并文件数由mapred.max.split.size限制的大小决定</li></ul><h3 id="job合并输出小文件"><a href="#job合并输出小文件" class="headerlink" title="job合并输出小文件"></a>job合并输出小文件</h3><ul><li><p><code>set hive.merge.smallfiles.avgsize=256000000;</code>当输出文件平均小于该值，启动新job合并文件</p></li><li><p><code>set hive.merge.size.per.task=64000000;</code>合并之后的文件大小</p></li></ul><h3 id="JVM重利用"><a href="#JVM重利用" class="headerlink" title="JVM重利用"></a>JVM重利用</h3><ul><li><p><code>set mapred.job.reuse.jvm.num.tasks=20;</code></p><p>JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。</p></li></ul><h3 id="压缩数据"><a href="#压缩数据" class="headerlink" title="压缩数据"></a>压缩数据</h3><p><code>set hive.exec.compress.output=true;</code></p><p><code>set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec;</code></p><p><code>set mapred.output.compression.type=BLOCK;</code></p><p><code>set hive.exec.compress.intermediate=true;</code></p><p><code>set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</code></p><p><code>set hive.intermediate.compression.type=BLOCK;</code></p><p>中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式</p><p>hive查询最终的输出也可以压缩</p><h2 id="5-Hive-Map优化"><a href="#5-Hive-Map优化" class="headerlink" title="5. Hive Map优化"></a><strong>5. Hive Map优化</strong></h2><p><code>set mapred.map.tasks =10;</code> 无效</p><p> <strong>(1) 默认map个数</strong></p><p><code>default_num = total_size / block_size;</code></p><p>如果想增加map个数，则设置<code>mapred.map.tasks</code>为一个较大的值</p><p>如果想减小map个数，则设置<code>mapred.min.split.size</code>为一个较大的值</p><p>情况1：输入文件size巨大，但不是小文件</p><p>情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于<code>blockSize</code>。这种情况通过增大<code>mapred.min.split.size</code>不可行，需要使用<code>combineFileInputFormat</code>将多个input path合并成一个<code>InputSplit</code>送给mapper处理，从而减少<code>mapper</code>的数量。</p><h2 id="6-Hive-Shuffle优化"><a href="#6-Hive-Shuffle优化" class="headerlink" title="6. Hive Shuffle优化"></a><strong>6. Hive Shuffle优化</strong></h2><ul><li>Map端</li></ul><p>​    <code>io.sort.mb</code></p><p>​    <code>io.sort.spill.percent</code></p><p>​    <code>min.num.spill.for.combine</code></p><p>​    <code>io.sort.factor</code></p><p>​    <code>io.sort.record.percent</code></p><ul><li><p>Reduce端</p><p><code>mapred.reduce.parallel.copies</code></p><p><code>mapred.reduce.copy.backoff</code></p><p><code>io.sort.factor</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p></li></ul><h2 id="7-Hive-Reduce优化"><a href="#7-Hive-Reduce优化" class="headerlink" title="7. Hive Reduce优化"></a><strong>7. Hive Reduce优化</strong></h2><ul><li><p>需要reduce操作的查询</p><p>group by,</p><p>join,</p><p>distribute by,</p><p>cluster by…</p><p>order by 比较特殊,只需要一个reduce</p></li><li><p>sum,count,distinct…</p></li><li><p>聚合函数</p><p>高级查询</p></li><li><p>推测执行</p><p>mapred.reduce.tasks.speculative.execution</p><p>hive.mapred.reduce.tasks.speculative.execution</p></li><li><p>Reduce优化</p><p>numRTasks = min[maxReducers,input.size/perReducer]</p><p>maxReducers=hive.exec.reducers.max</p><p>perReducer = hive.exec.reducers.bytes.per.reducer</p><p>hive.exec.reducers.max 默认 ：999</p><p>hive.exec.reducers.bytes.per.reducer 默认:1G</p></li><li><p>set mapred.reduce.tasks=10;直接设置</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。&lt;/strong&gt;&lt;/p&gt;
&lt;/blo
      
    
    </summary>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>6.Hadoop面试系列之UDF</title>
    <link href="https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/"/>
    <id>https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/</id>
    <published>2020-06-10T14:57:19.767Z</published>
    <updated>2020-06-10T14:57:19.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-开发步骤"><a href="#1-开发步骤" class="headerlink" title="1. 开发步骤"></a>1. <strong>开发步骤</strong></h2><p>​       UDF简称自定义函数，它是Hive函数库的扩展，自定义函数UDF在MapReduce执行阶段发挥作用。开发步骤如下：</p><blockquote><p>1）  给hive.ql.exec.UDF包开发一个自定义函数类，从UDF继承。自定义函数类实现evaluate方法。</p><p>2）  在FunctionRegistry类中注册开发的自定义函数类。</p><p>3）  打包发布至Hive客户端。</p></blockquote><h3 id="1-1-开发工具"><a href="#1-1-开发工具" class="headerlink" title="1.1 开发工具"></a><strong>1.1 开发工具</strong></h3><p>​      Eclipse是一款开源的、基于Java的可扩展开发平台。Hadoop开发人员可通过在Eclipse上面开发UDF。</p><h3 id="1-2-UDF函数案例"><a href="#1-2-UDF函数案例" class="headerlink" title="1.2 UDF函数案例"></a>1.2 UDF函数案例</h3><h4 id="1）开发UDF函数类"><a href="#1）开发UDF函数类" class="headerlink" title="1）开发UDF函数类"></a>1）开发UDF函数类</h4><p>文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHelloWorld.java</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">.</span>UDF<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">UDFHelloWorld</span> <span class="token keyword">extends</span> <span class="token class-name">UDF</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> String <span class="token function">evaluate</span><span class="token punctuation">(</span>String str<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>str <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> null<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> <span class="token string">"HelloWorld "</span> <span class="token operator">+</span> str<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        helloUDF uf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">helloUDF</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//Text t = new Text("gfsg");</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>uf<span class="token punctuation">.</span><span class="token function">evaluate</span><span class="token punctuation">(</span><span class="token string">"nihao"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2）UDF类注册，注册方法"><a href="#2）UDF类注册，注册方法" class="headerlink" title="2）UDF类注册，注册方法"></a>2）UDF类注册，注册方法</h4><p>文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>UDFHelloWorld<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*** FunctionRegistry.*/</span><span class="token keyword">public</span> <span class="token keyword">final</span> <span class="token keyword">class</span> <span class="token class-name">FunctionRegistry</span> <span class="token punctuation">{</span>    <span class="token keyword">static</span> <span class="token punctuation">{</span>        <span class="token function">registerGenericUDF</span><span class="token punctuation">(</span><span class="token string">"concat"</span><span class="token punctuation">,</span> GenericUDFConcat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"substr"</span><span class="token punctuation">,</span> UDFSubstr<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"substring"</span><span class="token punctuation">,</span> UDFSubstr<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"space"</span><span class="token punctuation">,</span> UDFSpace<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"repeat"</span><span class="token punctuation">,</span> UDFRepeat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"ascii"</span><span class="token punctuation">,</span> UDFAscii<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"lpad"</span><span class="token punctuation">,</span> UDFLpad<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"rpad"</span><span class="token punctuation">,</span> UDFRpad<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"Hello"</span><span class="token punctuation">,</span> UDFHelloWorld<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerGenericUDF</span><span class="token punctuation">(</span><span class="token string">"size"</span><span class="token punctuation">,</span> GenericUDFSize<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3）Jar包发布路径"><a href="#3）Jar包发布路径" class="headerlink" title="3）Jar包发布路径"></a>3）Jar包发布路径</h4><p>发布路径：/opt/boh/hive/lib/hive-exec-0.12.0-cdh5.0.0.jar</p><p>上传至hadoop集群执行脚本的hive客户端。</p><h3 id="1-3Hive-UDF函数"><a href="#1-3Hive-UDF函数" class="headerlink" title="1.3Hive UDF函数"></a>1.3Hive UDF函数</h3><h4 id="1-3-1UDF函数列表"><a href="#1-3-1UDF函数列表" class="headerlink" title="1.3.1UDF函数列表"></a>1.3.1UDF函数列表</h4><ul><li>函数清单及其功能</li></ul><p><code>TO_DATE(string date,'format')</code></p><ul><li>格式化所需要的日期</li></ul><p><code>ADD_MONTHS(Timestamp date,int n)</code></p><ul><li>增加月数</li></ul><p><code>date_tostring(Timestamp date,'format')</code></p><ul><li>转换Date类型为指定格式字符串</li></ul><p><code>MONTHS_BETWEEN（Timestamp date1，Timestamp date2）</code></p><ul><li>返回两个日期之间的月数</li></ul><p><code>f_age(string identityId)</code></p><ul><li>验证身份证合法性并返回性别年龄</li></ul><p><code>f_checkidcard(string identityId)</code></p><ul><li>验证身份证合法性</li></ul><h4 id="1-3-2-UDF函数说明"><a href="#1-3-2-UDF函数说明" class="headerlink" title="1.3.2 UDF函数说明"></a>1.3.2 UDF函数说明</h4><ul><li>TO_DATE函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">Select</span>  to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">09</span> <span class="token number">11</span>:<span class="token number">11</span>:<span class="token number">11</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>ADD_MONTHS函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> add_months<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">10</span><span class="token operator">-</span><span class="token number">09</span> <span class="token number">11</span>:<span class="token number">11</span>:<span class="token number">11</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>date_tostring函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> date_tostring<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'YYYY-MM-DD'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">09</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>MONTHS_BETWEEN函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> MONTHS_BETWEEN<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>to_date<span class="token punctuation">(</span><span class="token string">'20140706111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span>返回结果：<span class="token number">2.096774193548387</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>f_age函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> f_age<span class="token punctuation">(</span><span class="token string">'511024198710148199'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">127</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>f_checkidcard函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> f_checkidcard<span class="token punctuation">(</span><span class="token string">'511024198710148199'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-开发步骤&quot;&gt;&lt;a href=&quot;#1-开发步骤&quot; class=&quot;headerlink&quot; title=&quot;1. 开发步骤&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;开发步骤&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;​       UDF简称自定义函数，它是Hive函数库的扩展，自定义
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hive开窗函数梳理</title>
    <link href="https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/"/>
    <id>https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/</id>
    <published>2020-06-10T06:35:00.000Z</published>
    <updated>2020-06-10T11:15:10.879Z</updated>
    
    <content type="html"><![CDATA[<p>本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。</p><p>假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。</p><p><img src="https://pic3.zhimg.com/80/v2-008682fd90478af4fb84e88bccd480ee_1440w.jpg" alt="img"></p><p><strong>1.SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">/*前三次贷款的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token number">3</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token keyword">CURRENT</span> <span class="token keyword">ROW</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv1<span class="token punctuation">,</span><span class="token comment" spellcheck="true">/*历史所有贷款 累加到下一次贷款 的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token keyword">UNBOUNDED</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token number">1</span> <span class="token keyword">FOLLOWING</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv2<span class="token keyword">FROM</span> loan <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。</p><p>窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用：</p><ul><li>n PRECEDING : 前n行    n preceding</li><li>n FOLLOWING：后n行</li><li>CURRENT ROW ： 当前行</li></ul><p>如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为:</p><p>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.</p><p>窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。</p><p><strong>2.NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#将数据按name切分成10区，并返回属于第几个分区</span>NTILE<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>ROW_NUMBER<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f3</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>DENSE_RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f4</span><span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中第一个函数<a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">NTILE(10)</a>是将数据按name切分成10区，并返回属于第几个分区。</p><blockquote><p>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p><p>   然后可以根据桶号，选取前或后 n分之几的数据。</p></blockquote><p>后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。</p><ul><li>ROW_NUMBER()返回的是一列连续的序号。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-0f2c6da71227f7840aea5257acf8d88b_1440w.png" alt="img"></p><ul><li>RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}.</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f0bd3985ddacd2f347151dacc24cce_1440w.png" alt="img"></p><ul><li>DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-ac60db4d8a9c658ddf17fb43f09f616d_1440w.png" alt="img"></p><p><strong>3.LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取上一笔贷款的日期,缺失默认填NULL</span>LAG<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> last_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取下一笔贷款的日期,缺失指定填'1970-1-1'</span>LEAD<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'1970-1-1'</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> next_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取最早一笔贷款的日期</span>FIRST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> first_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取新一笔贷款的日期</span>LAST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> latest_dt<span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/pelifymeng2/article/details/70313943" target="_blank" rel="noopener">LAG(n)</a>将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录</p><p><strong>4.GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdateGROUPING SETS<span class="token punctuation">(</span>substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> orderdate<span class="token punctuation">)</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。</p><p>比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5.</p><p>如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null.</p><pre class="line-numbers language-text"><code class="language-text">SELECT A,B,CFROM loan#分别按照月份和日进行分区GROUP BY substring(orderdate,1,7),orderdatewith CUBEORDER BY GROUPING__ID; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。</p><pre class="line-numbers language-text"><code class="language-text">SELECT A,B,CFROM loan#分别按照月份和日进行分区GROUP BY substring(orderdate,1,7),orderdatewith ROLLUPORDER BY GROUPING__ID; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。&lt;/p&gt;
&lt;p&gt;假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-008
      
    
    </summary>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
      <category term="开窗函数" scheme="https://dataquaner.github.io/tags/%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop核心知识之MapReduce原理</title>
    <link href="https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/"/>
    <id>https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/</id>
    <published>2020-06-08T13:14:00.000Z</published>
    <updated>2020-06-08T14:31:06.843Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>MapReduce是一个基于集群的计算<strong>平台</strong>，是一个简化分布式编程的计算<strong>框架</strong>，是一个将分布式计算抽象为Map和Reduce两个阶段的编程<strong>模型</strong>。<em>（这句话记住了是可以用来装逼的）</em></p></blockquote><h2 id="1-MapReduce工作流程"><a href="#1-MapReduce工作流程" class="headerlink" title="1.MapReduce工作流程"></a>1.MapReduce工作流程</h2><p>0)   用户提交任务 （含数据）</p><p>1)    集群首先对输入数据源进行<strong>切片</strong></p><p>2)    master 调度 worker 执行 map 任务</p><p>3)    worker 读取输入源片段</p><p>4)    worker 执行 map 任务，将任务输出保存在本地<br>5)    master 调度 worker 执行 reduce 任务，reduce worker 读取 map 任务的输出文件</p><p>6） 执行 reduce 任务，将任务输出保存到 HDFS</p><p><img src="https://img-blog.csdnimg.cn/20181215100217729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIzMTM3Mw==,size_16,color_FFFFFF,t_70" alt="MapReduce工作流程"></p><p>由上至下依次执行</p><table><thead><tr><th>过程</th><th>过程描述</th></tr></thead><tbody><tr><td></td><td>用户提交任务job 给集群</td></tr><tr><td>切片</td><td>集群查找源数据 对源数据做基本处理</td></tr><tr><td>分词(每行执行一次map函数)</td><td>集群(yarn的appliction)分配map任务节点worker</td></tr><tr><td>映射</td><td>其中间数据(存在本地)</td></tr><tr><td>分区(partition)</td><td>中间数据</td></tr><tr><td>排序 (或二次排序)</td><td>中间数据</td></tr><tr><td>聚合(combine有无key聚合后key无序)</td><td>中间数据  分组(group) 发生在排序后混洗前(个人理解)</td></tr><tr><td>混洗(shuffle后key有序)</td><td>混洗横跨mapper和reducer，其发生在mapper的输出和reducer的输入阶段</td></tr><tr><td>规约(reduce)</td><td>集群(yarn的appliction)分配reduce任务节点worker</td></tr></tbody></table><p><img src="https://upload-images.jianshu.io/upload_images/12652505-f3e65e7fc499b579.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1172/format/webp" alt="MapReduce工作原理"></p><p>下面针对具体过程详细介绍：</p><h2 id="2-切片split"><a href="#2-切片split" class="headerlink" title="2.切片split"></a>2.切片split</h2><p>​       HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</p><p><strong>Map任务的数量</strong>：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。<strong>大多数情况下，理想的分片大小是一个HDFS块</strong></p><p><strong>Reduce任务的数量：</strong> <strong>最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</strong> 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</p><h2 id="3-Map-阶段"><a href="#3-Map-阶段" class="headerlink" title="3.Map()阶段"></a>3.Map()阶段</h2><blockquote><ol><li><p>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数</p></li><li><p>重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出</p></li><li><p>对输出的key、value进行分区</p></li><li><p>对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中</p></li></ol></blockquote><h2 id="4-Reduce阶段"><a href="#4-Reduce阶段" class="headerlink" title="4. Reduce阶段"></a>4. Reduce阶段</h2><blockquote><p>多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上</p></blockquote><blockquote><p>对多个map的输出进行合并、排序。</p></blockquote><blockquote><p>重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出</p></blockquote><blockquote><p>把reduce的输出保存到文件中</p></blockquote><p>特别说明：</p><blockquote><p>切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理</p><p>分词，映射，分区，排序，聚合 都属map阶段</p><p>混洗  横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段</p><p>规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义</p><p>分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容</p></blockquote><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>执行步骤：</p><p>1）map任务处理——&gt;切片</p><ul><li>读取输入文件内容，解析成key、value对，输入文件的每一行，就是一个key、value对，对应调用一次map函数。</li><li>写自己的逻辑，对输入的key、value（k1,v1）处理，转换成新的key、value(k2,v2)输出。</li></ul><p>2）reduce任务处理——&gt;计算</p><ul><li>在reduce之前，有一个shuffle的过程对多个map任务的输出进行合并、排序、分组等操作。</li><li>写reduce函数自己的逻辑，对输入的key、value（k2,{v2,…}）处理，转换成新的key、value(k3,v3)输出。</li><li>把reduce的输出保存到文件中。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;MapReduce是一个基于集群的计算&lt;strong&gt;平台&lt;/strong&gt;，是一个简化分布式编程的计算&lt;strong&gt;框架&lt;/strong&gt;，是一个将分布式计算抽象为Map和Reduce两个阶段的编程&lt;strong&gt;模型&lt;/strong&gt;。&lt;em
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="https://dataquaner.github.io/tags/MapReduce/"/>
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>hadoop shell命令</title>
    <link href="https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/"/>
    <id>https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/</id>
    <published>2020-06-08T10:40:00.000Z</published>
    <updated>2020-06-08T11:13:27.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><p>FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 </p><p>所有的的FS shell命令使用URI路径作为参数。URI格式是<code>scheme://authority/path</code>。</p><p>对HDFS文件系统，scheme是hdfs，</p><p>对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。</p><p>一个HDFS文件或目录比如<code>/parent/child</code>可以表示成<code>hdfs://namenode:namenodeport/parent/child</code>，或者更简单的<code>/parent/child</code>（假设你配置文件中的默认值是<code>namenode:namenodeport</code>）。</p><p>大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。</p><h2 id="1-hadoop-fs-命令列表"><a href="#1-hadoop-fs-命令列表" class="headerlink" title="1. hadoop fs 命令列表"></a>1. hadoop fs 命令列表</h2><p><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#FS+Shell" target="_blank" rel="noopener">FS Shell</a></p><ul><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#cat" target="_blank" rel="noopener">cat</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chgrp" target="_blank" rel="noopener">chgrp</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chmod" target="_blank" rel="noopener">chmod</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chown" target="_blank" rel="noopener">chown</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#copyFromLocal" target="_blank" rel="noopener">copyFromLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#copyToLocal" target="_blank" rel="noopener">copyToLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#cp" target="_blank" rel="noopener">cp</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#du" target="_blank" rel="noopener">du</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#dus" target="_blank" rel="noopener">dus</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#expunge" target="_blank" rel="noopener">expunge</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#get" target="_blank" rel="noopener">get</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#getmerge" target="_blank" rel="noopener">getmerge</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#ls" target="_blank" rel="noopener">ls</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#lsr" target="_blank" rel="noopener">lsr</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#mkdir" target="_blank" rel="noopener">mkdir</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#movefromLocal" target="_blank" rel="noopener">movefromLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#mv" target="_blank" rel="noopener">mv</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#put" target="_blank" rel="noopener">put</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#rm" target="_blank" rel="noopener">rm</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#rmr" target="_blank" rel="noopener">rmr</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#setrep" target="_blank" rel="noopener">setrep</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#stat" target="_blank" rel="noopener">stat</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#tail" target="_blank" rel="noopener">tail</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test" target="_blank" rel="noopener">test</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#text" target="_blank" rel="noopener">text</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#touchz" target="_blank" rel="noopener">touchz</a></li></ul><p>特别说明： </p><blockquote><ul><li><code>hadoop fs</code>：通用的文件系统命令，针对任何系统，比如本地文件、HDFS文件、HFTP文件、S3文件系统等。</li><li><code>hadoop dfs</code>：特定针对HDFS的文件系统的相关操作，但是已经不推荐使用。</li><li><code>hdfs dfs</code>：与hadoop dfs类似，同样是针对HDFS文件系统的操作，替代hadoop dfs。</li></ul></blockquote><h2 id="2-各命令使用说明"><a href="#2-各命令使用说明" class="headerlink" title="2. 各命令使用说明"></a>2. 各命令使用说明</h2><ul><li><p><code>cat</code><br>使用方法：<code>hadoop fs -cat URI [URI …]</code></p><p>​    将路径指定文件的内容输出到<code>stdout</code>。</p><p>示例：    </p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2hadoop fs -cat file:///file3 /user/hadoop/file4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>返回值：</p><pre><code>  成功返回0，失败返回-1。</code></pre></li><li><p><code>chgrp</code><br>使用方法：<code>hadoop fs -chgrp [-R] GROUP URI [URI …]</code> </p><p>​        Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. </p><p>​        改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>chmod</code><br>使用方法：<code>hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …]</code></p><p>​      改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>chown</code><br>使用方法：<code>hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</code></p><p>​       改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>copyFromLocal</code><br>使用方法：<code>hadoop fs -copyFromLocal URI</code></p><p>​       除了限定源路径是一个本地文件外，和<code>put</code>命令相似。</p></li><li><p><code>copyToLocal</code><br>使用方法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI</code></p><p>​       除了限定目标路径是一个本地文件外，和get命令类似。</p></li><li><p><code>cp</code><br>使用方法：<code>hadoop fs -cp URI [URI …]</code></p><p>​     将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​     成功返回0，失败返回-1。</p><ul><li><p><code>du</code><br>使用方法：<code>hadoop fs -du URI [URI …]</code></p><p>​      显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>示例：    </p><pre class="line-numbers language-SJELL"><code class="language-SJELL">hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回值：</p><pre><code> 成功返回0，失败返回-1。</code></pre></li><li><p><code>dus</code><br>使用方法：<code>hadoop fs -dus</code></p><p>​    显示文件的大小。</p></li><li><p><code>expunge</code><br>使用方法：<code>hadoop fs -expunge</code></p></li><li><p>清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。</p></li><li><p><code>get</code><br>使用方法：<code>hadoop fs -get [-ignorecrc] [-crc]</code></p></li><li><p>​      复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。</p></li><li><p>示例：</p><pre class="line-numbers language-SHELL"><code class="language-SHELL">hadoop fs -get /user/hadoop/file localfilehadoop fs -get hdfs://host:port/user/hadoop/file localfile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​     成功返回0，失败返回-1。</p><ul><li><p>getmerge<br>使用方法：<code>hadoop fs -getmerge [addnl]</code></p></li><li><p>接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。</p></li><li><p>ls<br>使用方法：<code>hadoop fs -ls</code></p><p>​      如果是文件，则按照如下格式返回文件信息：<br>​          文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>​      如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>​         目录名</p><p>​         修改日期 修改时间 权限 用户ID 组ID<br>示例：       </p><pre class="line-numbers language-SHELL"><code class="language-SHELL">hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回值：</p><pre><code>    成功返回0，失败返回-1。</code></pre></li><li><p>mkdir<br>使用方法：hadoop fs -mkdir</p><p>​    接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。</p><p>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​    成功返回0，失败返回-1。</p><ul><li><p><code>movefromLocal</code><br>使用方法：<code>dfs -moveFromLocal</code></p><p> 输出一个”not implemented“信息。</p></li><li><p>mv<br>使用方法：<code>hadoop fs -mv URI [URI …]</code></p><p>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​    成功返回0，失败返回-1。</p><ul><li><p><code>put</code><br>使用方法：hadoop fs -put …</p><p>​      从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -put localfile /user/hadoop/hadoopfilehadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdirhadoop fs -put localfile hdfs://host:port/hadoop/hadoopfilehadoop fs -put - hdfs://host:port/hadoop/hadoopfile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>从标准输入中读取输入。<br>返回值：</p><p>​    成功返回0，失败返回-1。</p></li><li><p><code>rm</code><br>使用方法：<code>hadoop fs -rm URI [URI …]</code></p><p>​    删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  成功返回0，失败返回-1。</p><ul><li><p><code>rmr</code><br>使用方法：<code>hadoop fs -rmr URI [URI …]</code></p></li><li><p>delete的递归版本。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -rmr /user/hadoop/dirhadoop fs -rmr hdfs://host:port/user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  成功返回0，失败返回-1。</p><ul><li><p>setrep<br>使用方法：<code>hadoop fs -setrep [-R]</code></p><p>   改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。</p></li><li><p>示例：</p><p><code>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</code><br>返回值：</p><p>   成功返回0，失败返回-1。</p></li><li><p>stat<br>使用方法：hadoop fs -stat URI [URI …]</p><p>   返回指定路径的统计信息。</p></li><li><p>示例：</p></li><li><p><code>hadoop fs -stat path</code><br>返回值：<br>  成功返回0，失败返回-1。</p></li><li><p>tail<br>使用方法：<code>hadoop fs -tail [-f] URI</code></p><p>  将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。</p></li><li><p>示例：</p><p><code>hadoop fs -tail pathname</code><br>返回值：<br>   成功返回0，失败返回-1。</p></li><li><p>test<br>使用方法：hadoop fs -test -[ezd] URI</p><p>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。</p></li><li><p>示例：</p><p><code>hadoop fs -test -e filename</code><br>text<br>使用方法：hadoop fs -text</p><p>   将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</p></li><li><p>touchz<br>使用方法：<code>hadoop fs -touchz URI [URI …]</code></p><p>   创建一个0字节的空文件。</p></li><li><p>示例：</p><p><code>hadoop -touchz pathname</code><br>返回值：<br>成功返回0，失败返回-1。</p></li></ul><p>更多详细信息访问：<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;p&gt;FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 &lt;/p&gt;
&lt;p&gt;所有的的FS
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/tags/Hadoop/"/>
    
      <category term="Shell" scheme="https://dataquaner.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>【Hive日常问题】导入数据成功，查询显示NULL</title>
    <link href="https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/"/>
    <id>https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/</id>
    <published>2020-05-06T13:40:00.000Z</published>
    <updated>2020-05-06T11:17:26.029Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><strong>问题描述</strong></h2><p>hive导入数据成功，但是查询结果为NULL：</p><pre class="line-numbers language-powershell"><code class="language-powershell">load <span class="token keyword">data</span> local inpath <span class="token string">'/user/hive/student.txt'</span> into table hive_test<span class="token punctuation">.</span>students<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-powershell"><code class="language-powershell">Loading <span class="token keyword">data</span> to table hive_test<span class="token punctuation">.</span>studentsOK<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> hive_test<span class="token punctuation">.</span>students<span class="token punctuation">;</span>OK<span class="token boolean">NULL</span>    <span class="token boolean">NULL</span><span class="token boolean">NULL</span>    <span class="token boolean">NULL</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格：</p><p>OK<br>NULL    NULL<br>NULL    NULL<br>查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格：</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> students<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> students<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span> <span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">' '</span> <span class="token keyword">LINES</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">'\n'</span> STORED <span class="token keyword">AS</span> TEXTFILE<span class="token punctuation">;</span>OK<span class="token number">1</span>    sun<span class="token number">2</span>    lin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;hive导入数据成功，但是查询结果为NULL：&lt;/p&gt;
&lt;pre class=&quot;line-
      
    
    </summary>
    
    
      <category term="Data Question" scheme="https://dataquaner.github.io/categories/Data-Question/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Python高级特性之切片</title>
    <link href="https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/"/>
    <id>https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/</id>
    <published>2020-04-25T14:14:00.000Z</published>
    <updated>2020-04-25T10:10:47.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Python可切片对象的索引方式"><a href="#1-Python可切片对象的索引方式" class="headerlink" title="1. Python可切片对象的索引方式"></a>1. Python可切片对象的索引方式</h2><p>​       包括：正索引和负索引两部分，如下图所示，以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例：</p><p><img src="https:////upload-images.jianshu.io/upload_images/14029140-3da45bbfe1029df4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/464/format/webp" alt="img"></p><h2 id="2-Python切片操作的一般方式"><a href="#2-Python切片操作的一般方式" class="headerlink" title="2. Python切片操作的一般方式"></a>2. Python切片操作的一般方式</h2><p>​       一个完整的切片表达式包含两个“:”，用于分隔三个参数(start_index、end_index、step)。</p><ul><li>当只有一个“:”时，默认第三个参数step=1；</li><li>当一个“:”也没有时，start_index=end_index，表示切取start_index指定的那个元素。</li></ul><p>​       切片操作基本表达式：object[start_index:end_index:step]</p><ul><li>step：正负数均可，其绝对值大小决定了切取数据时的‘‘步长”，而正负号决定了“切取方向”，正表示“从左往右”取值，负表示“从右往左”取值。当step省略时，默认为1，即从左往右以步长1取值。“切取方向非常重要！”“切取方向非常重要！”“切取方向非常重要！”，重要的事情说三遍！</li><li>start_index：表示起始索引（包含该索引对应值）；该参数省略时，表示从对象“端点”开始取值，至于是从“起点”还是从“终点”开始，则由step参数的正负决定，step为正从“起点”开始，为负从“终点”开始。</li><li>end_index：表示终止索引（不包含该索引对应值）；该参数省略时，表示一直取到数据“端点”，至于是到“起点”还是到“终点”，同样由step参数的正负决定，step为正时直到“终点”，为负时直到“起点”。</li></ul><h2 id="3-Python切片操作详细例子"><a href="#3-Python切片操作详细例子" class="headerlink" title="3. Python切片操作详细例子"></a>3. Python切片操作详细例子</h2><p>​      以下示例均以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="1-切取单个元素"><a href="#1-切取单个元素" class="headerlink" title="1. 切取单个元素"></a>1. 切取单个元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">0</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">6</span>当索引只有一个数时，表示切取某一个元素。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-切取完整对象"><a href="#2-切取完整对象" class="headerlink" title="2. 切取完整对象"></a>2. 切取完整对象</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#从左往右</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#从左往右</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#从右往左</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-start-index和end-index全为正（-）索引的情况"><a href="#3-start-index和end-index全为正（-）索引的情况" class="headerlink" title="3. start_index和end_index全为正（+）索引的情况"></a>3. start_index和end_index全为正（+）索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，start_index<span class="token operator">=</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token number">6</span>同样表示从左往右取值。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>输出为空列表，说明没取到数据。step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，决定了从右往左取值，而start_index<span class="token operator">=</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token number">6</span>决定了从左往右取值，两者矛盾，所以为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>同样输出为空列表。step<span class="token operator">=</span><span class="token number">1</span>，决定了从左往右取值，而start_index<span class="token operator">=</span><span class="token number">6</span>到end_index<span class="token operator">=</span><span class="token number">2</span>决定了从右往左取值，两者矛盾，所以为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值<span class="token number">0</span>开始一直取到end_index<span class="token operator">=</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值<span class="token number">9</span>开始一直取到end_index<span class="token operator">=</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从start_index<span class="token operator">=</span><span class="token number">6</span>开始，一直取到“终点”值<span class="token number">9</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从start_index<span class="token operator">=</span><span class="token number">6</span>开始，一直取到“起点”<span class="token number">0</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-start-index和end-index全为负（-）索引的情况"><a href="#4-start-index和end-index全为负（-）索引的情况" class="headerlink" title="4. start_index和end_index全为负（-）索引的情况"></a>4. start_index和end_index全为负（-）索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，而start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>决定了从右往左取值，两者矛盾，所以为空。索引<span class="token operator">-</span><span class="token number">1</span>在<span class="token operator">-</span><span class="token number">6</span>的右边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>同样是从右往左取值。索引<span class="token operator">-</span><span class="token number">1</span>在<span class="token number">6</span>的右边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，而start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>同样是从左往右取值。索引<span class="token operator">-</span><span class="token number">6</span>在<span class="token operator">-</span><span class="token number">1</span>的左边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从“起点”开始一直取到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从“终点”开始一直取到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>开始，一直取到“终点”。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>开始，一直取到“起点”。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="5-start-index和end-index正（-）负（-）混合索引的情况"><a href="#5-start-index和end-index正（-）负（-）混合索引的情况" class="headerlink" title="5. start_index和end_index正（+）负（-）混合索引的情况"></a>5. start_index和end_index正（+）负（-）混合索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>的左边，因此从左往右取值，而step<span class="token operator">=</span><span class="token number">1</span>同样决定了从左往右取值，因此结果正确<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>的左边，因此从左往右取值，但step<span class="token operator">=</span><span class="token operator">-</span>则决定了从右往左取值，两者矛盾，因此为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token number">6</span>的右边，因此从右往左取值，但step<span class="token operator">=</span><span class="token number">1</span>则决定了从左往右取值，两者矛盾，因此为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token number">6</span>的右边，因此从右往左取值，而step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>同样决定了从右往左取值，因此结果正确。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="6-多层切片操作"><a href="#6-多层切片操作" class="headerlink" title="6. 多层切片操作"></a>6. 多层切片操作</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>相当于：a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>理论上可无限次多层切片操作，只要上一次返回的是非空可切片对象即可。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="7-切片操作的三个参数可以用表达式"><a href="#7-切片操作的三个参数可以用表达式" class="headerlink" title="7. 切片操作的三个参数可以用表达式"></a>7. 切片操作的三个参数可以用表达式</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>即：a<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="8-其他对象的切片操作"><a href="#8-其他对象的切片操作" class="headerlink" title="8. 其他对象的切片操作"></a>8. 其他对象的切片操作</h3><p>​       前面的切片操作以list对象为例进行说明，但实际上可进行切片操作的数据类型还有很多，包括元组、字符串等等。</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>元组的切片操作<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token string">'ABCDEFG'</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token string">'ACEG'</span>字符串的切片操作<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token function">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token function">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">87</span><span class="token number">90</span><span class="token number">93</span><span class="token number">96</span><span class="token number">99</span>就是利用<span class="token function">range</span><span class="token punctuation">(</span><span class="token punctuation">)</span>函数生成<span class="token number">1</span><span class="token operator">-</span><span class="token number">99</span>的整数，然后从start_index<span class="token operator">=</span><span class="token number">2</span>（即<span class="token number">3</span>）开始以step<span class="token operator">=</span><span class="token number">3</span>取值，直到终点，再在新序列中取最后五个数。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-常用切片操作"><a href="#4-常用切片操作" class="headerlink" title="4. 常用切片操作"></a>4. 常用切片操作</h2><h3 id="1-取偶数位置"><a href="#1-取偶数位置" class="headerlink" title="1.取偶数位置"></a>1.取偶数位置</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-取奇数位置"><a href="#2-取奇数位置" class="headerlink" title="2.取奇数位置"></a>2.取奇数位置</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-拷贝整个对象"><a href="#3-拷贝整个对象" class="headerlink" title="3.拷贝整个对象"></a>3.拷贝整个对象</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#41946376</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#41921864</span>或<span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">.</span><span class="token function">copy</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#39783752</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#39759176</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>需要注意的是：<strong>[:]和.copy()都属于“浅拷贝”，只拷贝最外层元素，内层嵌套元素则通过引用方式共享，而非独立分配内存</strong>，如果需要彻底拷贝则需采用“深拷贝”方式，如下例所示：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'a={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">9</span> <span class="token comment" spellcheck="true">#修改b的最外层元素，将1变成9</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'D'</span> <span class="token comment" spellcheck="true">#修改b的内嵌层元素</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'a={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'b={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'id(a)={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'id(b)={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#原始a</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#b修改内部元素A为D后，a中的A也变成了D，说明共享内部嵌套元素，但外部元素1没变。</span>b<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#修改后的b</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">38669128</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">38669192</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-修改单个元素"><a href="#4-修改单个元素" class="headerlink" title="4.修改单个元素"></a>4.修改单个元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="5-在某个位置插入元素"><a href="#5-在某个位置插入元素" class="headerlink" title="5.在某个位置插入元素"></a>5.在某个位置插入元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">,</span><span class="token string">'C'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-替换一部分元素"><a href="#6-替换一部分元素" class="headerlink" title="6.替换一部分元素"></a>6.替换一部分元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul><li>start_index、end_index、step三者可同为正、同为负，或正负混合。但必须遵循一个原则，即：当start_index表示的实际位置在end_index的左边时，从左往右取值，此时step必须是正数（同样表示从左往右）；当start_index表示的实际位置在end_index的右边时，表示从右往左取值，此时step必须是负数（同样表示从右往左），即两者的取值顺序必须相同。</li><li>当start_index或end_index省略时，取值的起始索引和终止索引由step的正负来决定，这种情况不会有取值方向矛盾（即不会返回空列表[]），但正和负取到的结果顺序是相反的，因为一个向左一个向右。</li><li>step的正负是必须要考虑的，尤其是当step省略时。比如a[-1:]，很容易就误认为是从“终点”开始一直取到“起点”，即a[-1:]= [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]，但实际上a[-1:]=[9]（注意不是9），原因在于step省略时step=1表示从左往右取值，而起始索引start_index=-1本身就是对象的最右边元素了，再往右已经没数据了，因此结果只含有9一个元素。</li><li>需要注意：“取单个元素（不带“:”）”时，返回的是对象的某个元素，其类型由元素本身的类型决定，而与母对象无关，如上面的a[0]=0、a[-4]=6，元素0和6都是“数值型”，而母对象a却是“list”型；“取连续切片（带“:”）”时，返回结果的类型与母对象相同，哪怕切取的连续切片只包含一个元素，如上面的a[-1:]=[9]，返回的是一个只包含元素“9”的list，而非数值型“9”。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Python可切片对象的索引方式&quot;&gt;&lt;a href=&quot;#1-Python可切片对象的索引方式&quot; class=&quot;headerlink&quot; title=&quot;1. Python可切片对象的索引方式&quot;&gt;&lt;/a&gt;1. Python可切片对象的索引方式&lt;/h2&gt;&lt;p&gt;​   
      
    
    </summary>
    
    
      <category term="Python" scheme="https://dataquaner.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://dataquaner.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode数组系列之#88：合并两个有序数组</title>
    <link href="https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/"/>
    <id>https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/</id>
    <published>2020-04-25T08:16:16.000Z</published>
    <updated>2020-04-25T11:02:08.816Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目：合并两个有序数组"><a href="#题目：合并两个有序数组" class="headerlink" title="题目：合并两个有序数组"></a>题目：合并两个有序数组</h3><h3 id="难度：Easy"><a href="#难度：Easy" class="headerlink" title="难度：Easy"></a>难度：Easy</h3><h3 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h3><blockquote><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>​        给你两个有序整数数组 nums1 和 nums2，请你将 nums2 合并到 nums1 中，使 nums1 成为一个有序数组。</p><h4 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h4><ul><li>初始化 nums1 和 nums2 的元素数量分别为 m 和 n 。</li><li>你可以假设 nums1 有足够的空间（空间大小大于或等于 m + n）来保存 nums2 中的元素。</li></ul><h4 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h4><p>​    输入:<br>​        nums1 = [1,2,3,0,0,0], m = 3<br>​        nums2 = [2,5,6],           n = 3</p><p>​     输出: </p><p>​        [1,2,2,3,5,6]</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/merge-sorted-array" target="_blank" rel="noopener">https://leetcode-cn.com/problems/merge-sorted-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="方法一：合并后排序"><a href="#方法一：合并后排序" class="headerlink" title="方法一：合并后排序"></a>方法一：合并后排序</h4><h5 id="直觉"><a href="#直觉" class="headerlink" title="直觉"></a>直觉</h5><p>​      最朴素的解法就是将两个数组合并之后再排序。</p><p>​      该算法只需要一行(Java是2行)，时间复杂度较差，为O((n + m)log(n + m))。这是由于这种方法没有利用两个数组本身已经有序这一点。</p><h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><h6 id="Python版"><a href="#Python版" class="headerlink" title="Python版"></a>Python版</h6><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> m<span class="token punctuation">:</span> int<span class="token punctuation">,</span> nums2<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Do not return anything, modify nums1 in-place instead.        """</span>         nums1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> sorted<span class="token punctuation">(</span>nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span> <span class="token operator">+</span> nums2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="Java版"><a href="#Java版" class="headerlink" title="Java版"></a>Java版</h5><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums1<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums2<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>nums2<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>    Arrays<span class="token punctuation">.</span><span class="token function">sort</span><span class="token punctuation">(</span>nums1<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="Scala版"><a href="#Scala版" class="headerlink" title="Scala版"></a>Scala版</h6><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">object</span> Solution <span class="token punctuation">{</span>    <span class="token keyword">def</span> merge<span class="token punctuation">(</span>nums1<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> m<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">,</span> nums2<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h5><ul><li>时间复杂度 : <em>O</em>((<em>n</em>+<em>m</em>)log(<em>n</em>+<em>m</em>))</li><li>空间复杂度 : O(1)</li></ul><h4 id="方法二：双指针-从前往后"><a href="#方法二：双指针-从前往后" class="headerlink" title="方法二：双指针 / 从前往后"></a>方法二：双指针 / 从前往后</h4><h5 id="直觉-1"><a href="#直觉-1" class="headerlink" title="直觉"></a>直觉</h5><p>​       一般而言，对于有序数组可以通过双指针法达到O(n+m)的时间复杂度。</p><p>最直接的算法实现是将指针p1 置为 nums1的开头， p2为 nums2的开头，在每一步将最小值放入输出数组中。</p><p>​      由于 nums1 是用于输出的数组，需要将nums1中的前m个元素放在其他地方，也就需要O(m) 的空间复杂度。</p><p><img src="https://pic.leetcode-cn.com/992f95361c37ad06deadb6f14a9970d0184fd47330365400dd1d6f7be239e0ff-image.png" alt="image.png"></p><h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><h5 id="Python版-1"><a href="#Python版-1" class="headerlink" title="Python版"></a>Python版</h5><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> nums2<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums1: List[int]        :type m: int        :type nums2: List[int]        :type n: int        :rtype: void Do not return anything, modify nums1 in-place instead.        """</span>        <span class="token comment" spellcheck="true"># Make a copy of nums1.</span>        nums1_copy <span class="token operator">=</span> nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span>         nums1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Two get pointers for nums1_copy and nums2.</span>        p1 <span class="token operator">=</span> <span class="token number">0</span>         p2 <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment" spellcheck="true"># Compare elements from nums1_copy and nums2</span>        <span class="token comment" spellcheck="true"># and add the smallest one into nums1.</span>        <span class="token keyword">while</span> p1 <span class="token operator">&lt;</span> m <span class="token operator">and</span> p2 <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>             <span class="token keyword">if</span> nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">:</span>                 nums1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">]</span><span class="token punctuation">)</span>                p1 <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">)</span>                p2 <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># if there are still elements to add</span>        <span class="token keyword">if</span> p1 <span class="token operator">&lt;</span> m<span class="token punctuation">:</span>             nums1<span class="token punctuation">[</span>p1 <span class="token operator">+</span> p2<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> p2 <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>            nums1<span class="token punctuation">[</span>p1 <span class="token operator">+</span> p2<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a><strong>复杂度分析</strong></h5><ul><li>时间复杂度 : O(n + m)</li><li>空间复杂度 : O(m)</li></ul><h4 id="方法三-双指针-从后往前"><a href="#方法三-双指针-从后往前" class="headerlink" title="方法三 : 双指针 / 从后往前"></a>方法三 : 双指针 / 从后往前</h4><h5 id="直觉-2"><a href="#直觉-2" class="headerlink" title="直觉"></a>直觉</h5><p>​       方法二已经取得了最优的时间复杂度O(n + m)，但需要使用额外空间。这是由于在从头改变nums1的值时，需要把nums1中的元素存放在其他位置。</p><p>​       如果我们从结尾开始改写 nums1 的值又会如何呢？这里没有信息，因此不需要额外空间。</p><p>这里的指针 p 用于追踪添加元素的位置。</p><p><img src="https://pic.leetcode-cn.com/57c1daae7dab21c175f0a3acc18e4535aecde350c5100832bd2fdb0e4279180e-image.png" alt="img"></p><p><img src="https://pic.leetcode-cn.com/bac9fc86e104b5fa65f144e0604e0f4ffe4585efac12c1942b618be1c70363ca-image.png" alt="img"></p><h5 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h5><p>Python版</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> nums2<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums1: List[int]        :type m: int        :type nums2: List[int]        :type n: int        :rtype: void Do not return anything, modify nums1 in-place instead.        """</span>        <span class="token comment" spellcheck="true"># two get pointers for nums1 and nums2</span>        p1 <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token number">1</span>        p2 <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># set pointer for nums1</span>        p <span class="token operator">=</span> m <span class="token operator">+</span> n <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># while there are still elements to compare</span>        <span class="token keyword">while</span> p1 <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">and</span> p2 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span>                p2 <span class="token operator">-=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span>  nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span>                p1 <span class="token operator">-=</span> <span class="token number">1</span>            p <span class="token operator">-=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># add missing elements from nums2</span>        nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span><span class="token punctuation">:</span>p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Java版</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums1<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums2<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// two get pointers for nums1 and nums2</span>    <span class="token keyword">int</span> p1 <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> p2 <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// set pointer for nums1</span>    <span class="token keyword">int</span> p <span class="token operator">=</span> m <span class="token operator">+</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// while there are still elements to compare</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>p1 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>p2 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">// compare two elements from nums1 and nums2 </span>      <span class="token comment" spellcheck="true">// and add the largest one in nums1 </span>      nums1<span class="token punctuation">[</span>p<span class="token operator">--</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">?</span> nums2<span class="token punctuation">[</span>p2<span class="token operator">--</span><span class="token punctuation">]</span> <span class="token operator">:</span> nums1<span class="token punctuation">[</span>p1<span class="token operator">--</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// add missing elements from nums2</span>    System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>nums2<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h5><ul><li>时间复杂度 : O(n + m)</li><li>空间复杂度 : O(1)</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;题目：合并两个有序数组&quot;&gt;&lt;a href=&quot;#题目：合并两个有序数组&quot; class=&quot;headerlink&quot; title=&quot;题目：合并两个有序数组&quot;&gt;&lt;/a&gt;题目：合并两个有序数组&lt;/h3&gt;&lt;h3 id=&quot;难度：Easy&quot;&gt;&lt;a href=&quot;#难度：Easy&quot; c
      
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://dataquaner.github.io/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://dataquaner.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>数据倾斜问题总结</title>
    <link href="https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/"/>
    <id>https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/</id>
    <published>2020-04-22T14:14:00.000Z</published>
    <updated>2020-04-22T14:35:55.073Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-什么是数据倾斜"><a href="#0-什么是数据倾斜" class="headerlink" title="0. 什么是数据倾斜"></a>0. 什么是数据倾斜</h2><blockquote><p>​        对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。</p><p>来源百度百科</p></blockquote><p>​        对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><h2 id="1-数据倾斜的现象"><a href="#1-数据倾斜的现象" class="headerlink" title="1. 数据倾斜的现象"></a>1. 数据倾斜的现象</h2><p>​       多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。</p><h2 id="2-数据倾斜的影响"><a href="#2-数据倾斜的影响" class="headerlink" title="2. 数据倾斜的影响"></a>2. 数据倾斜的影响</h2><p>1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：<strong>Out Of Memory</strong>。</p><p>2）<strong>运行速度慢</strong> ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。</p><p><img src="https://pic1.zhimg.com/80/v2-b26e15f4b1c3ce2f78fba64397b6fd60_1440w.jpg" alt="img"></p><p><strong><em>一个经验结论是：一般情况下，OOM的原因都是数据倾斜\</em></strong></p><h2 id="3-如何定位数据倾斜"><a href="#3-如何定位数据倾斜" class="headerlink" title="3. 如何定位数据倾斜"></a>3. 如何定位数据倾斜</h2><p>​         数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p><p><strong>原因</strong>： 查看任务-》查看Stage-》查看代码</p><p>​        某个task执行特别慢的情况</p><p>​        某个task莫名其妙内存溢出的情况</p><p>​        查看导致数据倾斜的key的数据分布情况</p><p><img src="https://pic1.zhimg.com/80/v2-b1b26a9b5e6a1d68d9aea4d1f2bc551c_1440w.jpg" alt="img"></p><p>也可从以下几种情况考虑：</p><p>1、是不是有OOM情况出现，一般是少数内存溢出的问题</p><p>2、是不是应用运行时间差异很大，总体时间很长</p><p>3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题</p><p>4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断</p><p>5、看看代码里面是否有一些导致Shuffle的算子出现</p><h2 id="4-数据倾斜的几种典型情况（重点）"><a href="#4-数据倾斜的几种典型情况（重点）" class="headerlink" title="4. 数据倾斜的几种典型情况（重点）"></a><strong>4. 数据倾斜的几种典型情况（重点）</strong></h2><ul><li>数据源中的数据分布不均匀，Spark需要频繁交互</li><li>数据集中的不同Key由于分区方式，导致数据倾斜</li><li>JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</li><li>聚合操作中，数据集中的数据分布不均匀（主要）</li><li>JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</li><li>JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</li><li>数据集中少数几个key数据量很大，不重要，其他数据均匀</li></ul><p>注意：</p><ul><li><p>需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题</p></li><li><p>只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以</p></li></ul><h2 id="5-数据倾斜的处理方法"><a href="#5-数据倾斜的处理方法" class="headerlink" title="5. 数据倾斜的处理方法"></a>5. 数据倾斜的处理方法</h2><p>​         发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。</p><h3 id="5-1-检查数据，找出异常的key"><a href="#5-1-检查数据，找出异常的key" class="headerlink" title="5.1 检查数据，找出异常的key"></a>5.1 检查数据，找出异常的key</h3><p>​          如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。</p><p>选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"key"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​        如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况:</p><ul><li><p>null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。</p></li><li><p>无效数据，大量重复的测试数据或是对结果影响不大的有效数据。</p></li><li><p>有效数据，业务导致的正常数据分布。</p></li></ul><p><strong>解决办法</strong><br>  第1，2种情况，直接对数据进行过滤即可。</p><p>  第3种情况则需要进行一些特殊操作，常见的有以下几种做法。</p><ul><li><p>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。</p></li><li><p>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</p></li><li><p>使用reduceByKey 代替 groupByKey</p></li><li><p>使用map join。</p><p><strong>举例</strong>：<br>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下：</p><p>将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>对数据进行 reduceByKey(func)<br>将 key + 随机值 转成 key<br>再对数据进行 reduceByKey(func)<br>tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。</p><p>tips2: 单独处理异常数据时，可以配合使用Map Join解决</p></li></ul><h4 id="5-1-1-数据源中的数据分布不均匀，Spark需要频繁交互"><a href="#5-1-1-数据源中的数据分布不均匀，Spark需要频繁交互" class="headerlink" title="5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互"></a><strong>5.1.1</strong> 数据源中的数据分布不均匀，Spark需要频繁交互</h4><p><strong>解决方案</strong>1：避免数据源的数据倾斜</p><p><strong>实现原理</strong>：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive或者Kafka中还是会发生数据倾斜。</p><p><strong>适用情况</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>总结</strong>：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题</p><h4 id="5-1-2-数据集中的不同Key由于分区方式，导致数据倾斜"><a href="#5-1-2-数据集中的不同Key由于分区方式，导致数据倾斜" class="headerlink" title="5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜"></a><strong>5.1.2</strong> 数据集中的不同Key由于分区方式，导致数据倾斜</h4><p><strong>解决方案1</strong>：调整并行度</p><p><strong>实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。</p><p><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。</p><p><img src="https://pic4.zhimg.com/80/v2-9a1722a9ceb6fe125f7b36715f6dcfff_1440w.jpg" alt="img"></p><p><strong>总结</strong>：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效</p><p><strong>解决方案2</strong>：</p><p><strong>缓解数据倾斜**</strong>（自定义Partitioner）**</p><p><strong>适用场景</strong>：大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p><p><strong>解决方案</strong>： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p><p><strong>优势</strong>： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong>： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><h3 id="5-2-检查Spark运行过程相关操作"><a href="#5-2-检查Spark运行过程相关操作" class="headerlink" title="5.2 检查Spark运行过程相关操作"></a>5.2 检查Spark运行过程相关操作</h3><h4 id="5-2-1-JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）"><a href="#5-2-1-JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）" class="headerlink" title="5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）"></a>5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</h4><p><strong>解决方案</strong>：Reduce side Join转变为Map side Join</p><p><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。</p><p><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。</p><h4 id="5-2-2-聚合操作中，数据集中的数据分布不均匀（主要）"><a href="#5-2-2-聚合操作中，数据集中的数据分布不均匀（主要）" class="headerlink" title="5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）"></a>5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）</h4><p><strong>解决方案</strong>：两阶段聚合（局部聚合+全局聚合）</p><p><strong>适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案</p><p><strong>实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案</p><p>将相同key的数据分拆处理</p><p><img src="https://pic3.zhimg.com/80/v2-495a5fed7eb38db37d2f0bd13c45a30e_1440w.jpg" alt="img"></p><h4 id="5-2-3-JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀"><a href="#5-2-3-JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀" class="headerlink" title="5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀"></a><strong>5.2.3</strong> JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</h4><p><strong>解决方案</strong>：为倾斜key增加随机前/后缀</p><p><strong>适用场景</strong>：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong>：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong>：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong>：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>注意</strong>：具有倾斜Key的RDD数据集中，key的数量比较少</p><p><img src="https://pic4.zhimg.com/80/v2-248b0cead5e9fb8a7b1cec840dd61b2f_1440w.jpg" alt="img"></p><h4 id="5-2-4-JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀"><a href="#5-2-4-JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀" class="headerlink" title="5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀"></a><strong>5.2.4</strong> JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</h4><p><strong>解决方案</strong>：随机前缀和扩容RDD进行join</p><p><strong>适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。</p><p><strong>实现思路</strong>：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><p>注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀）</p><p><img src="https://pic4.zhimg.com/80/v2-fa2211e3a343d7b68e83bfe83d67f0cb_1440w.jpg" alt="img"></p><h4 id="5-2-5-数据集中少数几个key数据量很大，不重要，其他数据均匀"><a href="#5-2-5-数据集中少数几个key数据量很大，不重要，其他数据均匀" class="headerlink" title="5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀"></a><strong>5.2.5</strong> 数据集中少数几个key数据量很大，不重要，其他数据均匀</h4><p><strong>解决方案</strong>：过滤少数倾斜Key</p><p><strong>适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-什么是数据倾斜&quot;&gt;&lt;a href=&quot;#0-什么是数据倾斜&quot; class=&quot;headerlink&quot; title=&quot;0. 什么是数据倾斜&quot;&gt;&lt;/a&gt;0. 什么是数据倾斜&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;​        对于集群系统，一般缓存是分布式的，即
      
    
    </summary>
    
    
      <category term="Data Question" scheme="https://dataquaner.github.io/categories/Data-Question/"/>
    
    
      <category term="数据倾斜" scheme="https://dataquaner.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
    
  </entry>
  
  <entry>
    <title>1.数据开发工程师面试题目必知必会</title>
    <link href="https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/"/>
    <id>https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/</id>
    <published>2020-04-21T10:59:38.680Z</published>
    <updated>2020-04-21T10:59:38.680Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面试题目梳理"><a href="#面试题目梳理" class="headerlink" title="面试题目梳理"></a>面试题目梳理</h2><h3 id="一-数据结构和算法-LeetCode"><a href="#一-数据结构和算法-LeetCode" class="headerlink" title="一. 数据结构和算法 LeetCode"></a>一. 数据结构和算法 <a href="https://leetcode-cn.com/problemset/all/" target="_blank" rel="noopener">LeetCode</a></h3><ol><li>合并数组</li><li>二元查找树转双向链表</li><li>二叉树层次遍历</li><li>堆 最小堆</li><li>排序算法</li><li>动态规划</li><li>青蛙跳台阶</li><li>贪心算法</li><li>字符串转换成整数</li><li>链表中倒数第K个结点</li><li><ol start="11"><li>二维数组中的查找</li></ol></li><li><ol start="12"><li>替换空格</li></ol></li><li><ol start="13"><li>从尾到头打印链表</li></ol></li><li><ol start="14"><li>重建二叉树</li></ol></li><li>用两个栈实现队列</li><li>斐波那契数列及变形题</li><li>二进制中1的个数</li><li>在O(1)时间删除链表结点</li><li>调整数组顺序使奇数位于偶数前面</li><li>反转链表</li><li>合并两个排序的链表</li><li>树的子结构</li><li>二叉树的镜像</li><li>顺时针打印矩阵</li><li>栈的压入、弹出序列</li><li>二叉搜索树的后序遍历序列</li><li>二叉树中和为某一值的路径</li><li>数组中出现次数超过一半的数字</li><li>最小的k个数</li><li>连续子数组的最大和</li><li>第一个只出现一次的字符</li><li>两个链表的第一个公共结点</li><li>链表中环的入口结点</li><li>二叉树的镜像</li><li>跳台阶</li><li>变态跳台阶</li><li>矩形覆盖</li><li>从上往下打印二叉树</li><li>二叉搜索树的第K个结点</li></ol><h3 id="二-计算平台"><a href="#二-计算平台" class="headerlink" title="二. 计算平台"></a>二. 计算平台</h3><h4 id="1-Hadoop"><a href="#1-Hadoop" class="headerlink" title="1. Hadoop"></a>1. Hadoop</h4><ol><li><p>数据倾斜问题</p></li><li><p>hive开窗函数</p></li><li><p>hive UDF UDAF<br>Mapreduce原理</p></li><li><p>MR的Shuffle过程</p></li><li><p>Yarn的工作机制，以及MR Job提交运行过程</p></li><li><p>MapReduce1的工作机制和过程</p></li><li><p>HDFS写入过程</p></li><li><p>Fsimage 与 EditLog定义及合并过程</p></li><li><p>HDFS读过程</p></li><li><p>HDFS简介</p></li><li><p>在向HDFS中写数据的时候，当写某一副本时出错怎么处理？</p></li><li><p>namenode的HA实现</p></li><li><p>简述联邦HDFS</p></li><li><p>HDFS源码解读–create()</p></li><li><p>NameNode高可用中editlog同步的过程</p></li><li><p>HDFS写入过程客户端奔溃怎么处理？（租约恢复）</p></li></ol><h4 id="2-Hive"><a href="#2-Hive" class="headerlink" title="2. Hive"></a>2. Hive</h4><ol><li>Hive内部表与外部表的区别</li><li>Hive与传统数据库的区别</li><li>Hiverc文件</li><li>Hive分区</li><li>Hive分区过多有何坏处以及分区时的注意事项</li><li>Hive中复杂数据类型的使用好处与坏处</li><li>hive分桶？</li><li>Hive元数据库是用来做什么的，存储哪些信息？</li><li>为何不使用Derby作为元数据库？</li><li>Hive什么情况下可以避免进行mapreduce？</li><li>Hive连接？</li><li>Hive MapJoin?</li><li>Hive的sort by, order by, distribute by, cluster by区别？</li><li>Hadoop计算框架特性</li><li>Hive优化常用手段</li><li>数据倾斜整理(转)</li><li>使用Hive如何进行抽样查询？</li></ol><h4 id="3-Spark"><a href="#3-Spark" class="headerlink" title="3. Spark"></a>3. Spark</h4><ol><li>Spark的运行模式</li><li>RDD是如何容错的？</li><li>Spark和MapReduce的区别</li><li>说一下Spark的RDD</li><li>自己实现一个RDD，需要实现哪些函数或者部分？</li><li>MapReduce和Spark的区别</li><li>Spark的Stage是怎么划分的？如何优化？</li><li>宽依赖与窄依赖区别</li><li>Spark性能调优</li><li>Flink、Storm与Spark Stream的区别（未）</li><li>说下spark中的transform和action</li><li>RDD、DataFrame和DataSet的区别</li><li>Spark执行任务流程（standalone、yarn）</li><li>Spark的数据容错机制</li><li>Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</li><li>Spark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？以及要注意的地方</li><li>driver的功能是什么？</li><li>spark端口</li><li>RDD有哪几种创建方式</li><li>map和flatmap的区别</li><li>Spark的基本工作流程</li></ol><h4 id="4-Flink"><a href="#4-Flink" class="headerlink" title="4. Flink"></a>4. Flink</h4><h5 id="4-1-Flink核心概念和基础"><a href="#4-1-Flink核心概念和基础" class="headerlink" title="4.1 Flink核心概念和基础"></a>4.1 Flink核心概念和基础</h5><blockquote><p>第一部分：Flink 中的核心概念和基础篇，包含了 Flink 的整体介绍、核心概念、算子等考察点。</p></blockquote><p>第二部分：Flink 进阶篇，包含了 Flink 中的数据传输、容错机制、序列化、数据热点、反压等实际生产环境中遇到的问题等考察点。</p><p>第三部分：Flink 源码篇，包含了 Flink 的核心代码实现、Job 提交流程、数据交换、分布式快照机制、Flink SQL 的原理等考察点。</p><h4 id="5-Storm："><a href="#5-Storm：" class="headerlink" title="5. Storm："></a><strong>5. Storm：</strong></h4><p>Storm的可靠性如何实现？包括spout和bolt两部分</p><p>怎么提高Storm的并发度？</p><p>Storm如何处理反压机制？</p><p>Storm中的Stream grouping有哪几种方式？</p><p>Storm的组件介绍</p><p>Storm怎么完成对单词的计数？</p><p>简述Strom的计算结构</p><h4 id="6-kafka："><a href="#6-kafka：" class="headerlink" title="6. kafka："></a><strong>6. kafka：</strong></h4><p>kafka介绍</p><p>Kafka与传统消息队列的区别？</p><p>kafka的零拷贝</p><p>kafka消息持久化和顺序读写？</p><h4 id="7-Kylin"><a href="#7-Kylin" class="headerlink" title="7. Kylin"></a>7. Kylin</h4><p>简介Kylin</p><p>Kylin的工作原理</p><p>Kylin的技术框架</p><p>Cube、Cuboid 和 Cube Segment</p><p>Kylin 对维度表的的要求</p><p>Cube的构建过程</p><p>全量构建和增量构建的区别</p><p>流式构建原理</p><h3 id="三-数据库"><a href="#三-数据库" class="headerlink" title="三. 数据库"></a>三. 数据库</h3><pre><code>     1）两大引擎Innodb引擎和MyIASM引擎，      2）mysql索引原理和底层实现BTREE、B+ TREE</code></pre><h3 id="四-数据仓库"><a href="#四-数据仓库" class="headerlink" title="四. 数据仓库"></a>四. 数据仓库</h3><p>​    1）拉链表<br>​    2）星型模型和雪花模型<br>​    3）维度建模过程</p><h3 id="五-操作系统"><a href="#五-操作系统" class="headerlink" title="五. 操作系统"></a>五. 操作系统</h3><p>   1）线程和进程，进程间的通信方式<br>   2）死锁<br>   3）内存分页<br>   4）同步异步阻塞</p><h3 id="六-计算机网络"><a href="#六-计算机网络" class="headerlink" title="六. 计算机网络"></a>六. 计算机网络</h3><ol><li>简述TCP和UDP的区别</li><li>七层协议每一层的任务及作用</li><li>简述http状态码</li><li>简述http协议与https协议</li><li>简述SSL协议</li><li>解析DNS过程</li><li>三次握手，四次挥手的过程？？为什么三握？</li></ol><h3 id="七-Linux"><a href="#七-Linux" class="headerlink" title="七. Linux"></a>七. Linux</h3><h4 id="1-比较常用Linux指令"><a href="#1-比较常用Linux指令" class="headerlink" title="1. 比较常用Linux指令"></a>1. 比较常用Linux指令</h4><p>　　1.1、ls/ll、cd、mkdir、rm-rf、cp、mv、ps -ef | grep xxx、kill、free-m、tar -xvf file.tar、（说那么十几二十来个估计差不多了）</p><h4 id="2-进程相关"><a href="#2-进程相关" class="headerlink" title="2. 进程相关"></a>2. 进程相关</h4><h5 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h5><p>　　2.1、ps -ef | grep xxx</p><p>　　2.2、ps -aux | grep xxx（-aux显示所有状态）</p><h5 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h5><p>　　3.1、kill -9[PID]  —(PID用查看进程的方式查找)</p><p>4、启动/停止服务</p><p>　　4.1、cd到bin目录cd/</p><p>　　4.2、./startup.sh  –打开（先确保有足够的权限）</p><p>　　4.3、./shutdown.sh —关闭</p><p>5、查看日志</p><p>　　5.1、cd到服务器的logs目录（里面有xx.out文件）</p><p>　　5.2、tail -f xx.out –此时屏幕上实时更新日志。ctr+c停止</p><p>　　5.3、查看最后100行日志 tail -100 xx.out </p><p>　　5.4、查看关键字附件的日志。如：cat filename | grep -C 5 ‘关键字’（关键字前后五行。B表示前，A表示后，C表示前后） —-使用不多**<br>**</p><p>　　5.5、还有vi查询啥的。用的也不多。</p><p>6、查看端口：（如查看某个端口是否被占用）</p><p>　　6.1、netstat -anp | grep 端口号（状态为LISTEN表示被占用）</p><p>7、查找文件</p><p>　　7.1、查找大小超过xx的文件： find . -type f -size +xxk —–(find . -type f -mtime -1 -size +100k -size-400k)–查区间大小的文件</p><p>　　7.2、通过文件名：find / -name xxxx  —整个硬盘查找</p><p>　　其余的基本上不常用</p><p>8、vim（vi）编辑器　　</p><p>　　有命令模式、输入模式、末行模式三种模式。<br>　　命令模式：查找内容(/abc、跳转到指定行(20gg)、跳转到尾行(G)、跳转到首行(gg)、删除行(dd)、插入行(o)、复制粘贴(yy,p)<br>　　输入模式：编辑文件内容<br>　　末行模式：保存退出(wq)、强制退出(q!)、显示文件行号(set number)<br>　　在命令模式下，输入a或i即可切换到输入模式，输入冒号(:)即可切换到末行模式；在输入模式和末行模式下，按esc键切换到命令模式</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;面试题目梳理&quot;&gt;&lt;a href=&quot;#面试题目梳理&quot; class=&quot;headerlink&quot; title=&quot;面试题目梳理&quot;&gt;&lt;/a&gt;面试题目梳理&lt;/h2&gt;&lt;h3 id=&quot;一-数据结构和算法-LeetCode&quot;&gt;&lt;a href=&quot;#一-数据结构和算法-LeetCode&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>IDEA环境下Git使用总结</title>
    <link href="https://dataquaner.github.io/2020/04/20/idea-xia-shi-yong-git-cao-zuo/"/>
    <id>https://dataquaner.github.io/2020/04/20/idea-xia-shi-yong-git-cao-zuo/</id>
    <published>2020-04-20T13:40:00.000Z</published>
    <updated>2020-04-20T04:13:12.538Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>工作中多人使用版本控制软件协作开发，常见的应用场景归纳如下：</p><p>假设小组中有两个人，组长小张，组员小袁</p><p>[TOC]</p><h2 id="场景一：小张创建项目并提交到远程Git仓库"><a href="#场景一：小张创建项目并提交到远程Git仓库" class="headerlink" title="场景一：小张创建项目并提交到远程Git仓库"></a>场景一：小张创建项目并提交到远程Git仓库</h2><p>创建好项目，选择VCS - &gt; Import into Version Control -&gt; Create Git Repository</p><p><img src="https://img-blog.csdn.net/20160912161234797" alt="img"></p><p>接下来指定本地仓库的位置，按个人习惯指定即可，例如这里选择了项目源代码同目录</p><p><img src="https://img-blog.csdn.net/20160912161334752" alt="img"></p><p>点击OK后创建完成本地仓库，注意，这里仅仅是本地的。下面把项目源码添加到本地仓库。</p><p>下图是Git与提交有关的三个命令对应的操作，Add命令是把文件从IDE的工作目录添加到本地仓库的stage区，Commit命令把stage区的暂存文件提交到当前分支的仓库，并清空stage区。Push命令把本地仓库的提交同步到远程仓库。</p><p><img src="https://img-blog.csdn.net/20160912164147415" alt="img"></p><p>IDEA中对操作做了一定的简化，Commit和Push可以在一步中完成。</p><p>具体操作，在项目上点击右键，选择Git菜单</p><p><img src="https://img-blog.csdn.net/20160912165901032" alt="img"></p><p><img src="https://img-blog.csdn.net/20160912165911954" alt="img"></p><p><img src="https://img-blog.csdn.net/20160912165921938" alt="img"></p><p>因为是第一次提交，Push前需要指定远程仓库的地址。如下图，点击Define remote后，在弹出的窗口中输入远程仓库地址。</p><p><img src="https://img-blog.csdn.net/20160912165942829" alt="img"></p><h2 id="场景二：小袁从远程Git仓库上获取项目源码"><a href="#场景二：小袁从远程Git仓库上获取项目源码" class="headerlink" title="场景二：小袁从远程Git仓库上获取项目源码"></a>场景二：小袁从远程Git仓库上获取项目源码</h2><p>即克隆项目，操作如下：</p><p><img src="https://img-blog.csdn.net/20160912170148207" alt="img"></p><p>输入小张Push时填写的远程仓库地址</p><p><img src="https://img-blog.csdn.net/20160912170214880" alt="img"></p><p>接下来按向导操作，即可把项目从远程仓库克隆到本地仓库和IDE工作区。</p><h2 id="场景三：小袁修改了部分源码，提交到远程仓库"><a href="#场景三：小袁修改了部分源码，提交到远程仓库" class="headerlink" title="场景三：小袁修改了部分源码，提交到远程仓库"></a>场景三：小袁修改了部分源码，提交到远程仓库</h2><p>这个操作和首次提交的流程基本一致，分别是 Add -&gt; Commit -&gt; Push。请参考场景一</p><h2 id="场景四：小张从远程仓库获取小袁的提交"><a href="#场景四：小张从远程仓库获取小袁的提交" class="headerlink" title="场景四：小张从远程仓库获取小袁的提交"></a>场景四：小张从远程仓库获取小袁的提交</h2><p>获取更新有两个命令：Fetch和Pull，Fetch是从远程仓库下载文件到本地的origin/master，然后可以手动对比修改决定是否合并到本地的master库。Push则是直接下载并合并。如果各成员在工作中都执行修改前先更新的规范，则可以直接使用Pull方式以简化操作。</p><p><img src="https://img-blog.csdn.net/20160912170628933" alt="img"></p><h2 id="场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发"><a href="#场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发" class="headerlink" title="场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发"></a>场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发</h2><p>建分支也是一个常用的操作，例如临时修改bug、开发不确定是否加入的功能等，都可以创建一个分支，再等待合适的时机合并到主干。</p><p>创建流程如下：</p><p><img src="https://img-blog.csdn.net/20160912171844429" alt="img"></p><p>选择New Branch并输入一个分支的名称</p><p><img src="https://img-blog.csdn.net/20160912171858663" alt="img"></p><p>创建完成后注意IDEA的右下角，如下图，Git: wangpangzi_branch表示已经自动切换到wangpangzi_branch分支，当前工作在这个分支上。</p><p>点击后弹出一个小窗口，在Local Branches中有其他可用的本地分支选项，点击后选择Checkout即可切换当前工作的分支。</p><p><img src="https://img-blog.csdn.net/20160912173123122" alt="img"></p><p>如下图，点击Checkout</p><p><img src="https://img-blog.csdn.net/20160912173307202" alt="img"></p><p>注意，这里创建的分支仅仅在本地仓库，如果想让组长小张获取到这个分支，还需要提交到远程仓库。</p><h2 id="场景六：小袁把分支提交到远程Git仓库"><a href="#场景六：小袁把分支提交到远程Git仓库" class="headerlink" title="场景六：小袁把分支提交到远程Git仓库"></a>场景六：小袁把分支提交到远程Git仓库</h2><p>切换到新建的分支，使用Push功能</p><p><img src="https://img-blog.csdn.net/20160912173718844" alt="img"></p><p><img src="https://img-blog.csdn.net/20160912174243815" alt="img"></p><h2 id="场景七：小张获取小袁提交的分支"><a href="#场景七：小张获取小袁提交的分支" class="headerlink" title="场景七：小张获取小袁提交的分支"></a>场景七：小张获取小袁提交的分支</h2><p>使用Pull功能打开更新窗口，点击Remote栏后面的刷新按钮，会在Branches to merge栏中刷新出新的分支。这里并不想做合并，所以不要选中任何分支，直接点击Pull按钮完成操作。</p><p><img src="https://img-blog.csdn.net/20160912174329143" alt="img"></p><p>更新后，再点击右下角，可以看到在Remote Branches区已经有了新的分支，点击后在弹出的子菜单中选择Checkout as new local branch，在本地仓库中创建该分支。完成后在Local Branches区也会出现该分支的选项，可以按上面的方法，点击后选择Checkout切换。</p><p><img src="https://img-blog.csdn.net/20160912174729488" alt="img"></p><h2 id="场景八：小张把分支合并到主干"><a href="#场景八：小张把分支合并到主干" class="headerlink" title="场景八：小张把分支合并到主干"></a>场景八：小张把分支合并到主干</h2><p>新功能开发完成，体验很好，项目组决定把该功能合并到主干上。</p><p>切换到master分支，选择Merge Changes</p><p><img src="https://img-blog.csdn.net/20160912175201306" alt="img"></p><p>选择要合并的分支，点击Merge完成</p><p><img src="https://img-blog.csdn.net/20160912175359903" alt="img"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;工作中多人使用版本控制软件协作开发，常见的应用场景归纳如下：&lt;/p&gt;
&lt;p&gt;假设小组中有两个人，组长小张，组员小袁&lt;/p&gt;
&lt;p&gt;[TOC]
      
    
    </summary>
    
    
      <category term="Git" scheme="https://dataquaner.github.io/categories/Git/"/>
    
    
      <category term="IDEA" scheme="https://dataquaner.github.io/tags/IDEA/"/>
    
      <category term="Git" scheme="https://dataquaner.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Flink面试问题梳理(基础+进阶+源码)</title>
    <link href="https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/"/>
    <id>https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/</id>
    <published>2020-04-18T13:40:00.000Z</published>
    <updated>2020-04-18T14:39:18.053Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一部分：Flink-面试基础篇"><a href="#第一部分：Flink-面试基础篇" class="headerlink" title="第一部分：Flink 面试基础篇"></a><strong>第一部分：Flink 面试基础篇</strong></h2><h3 id="1-简单介绍一下-Flink"><a href="#1-简单介绍一下-Flink" class="headerlink" title="1. 简单介绍一下 Flink"></a><strong>1. 简单介绍一下 Flink</strong></h3><p>​        Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。</p><p>​        Flink提供了诸多高抽象层的API以便用户编写分布式任务：</p><ul><li><p>DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。</p></li><li><p>DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。</p></li><li><p>Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。</p></li></ul><p>​      此外，Flink 还针对特定的应用领域提供了领域库，例如：Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。</p><p>根据官网的介绍，Flink 的特性包含：</p><blockquote><p>支持高吞吐、低延迟、高性能的流处理<br>支持带有事件时间的窗口 （Window） 操作<br>支持有状态计算的 Exactly-once 语义<br>支持高度灵活的窗口 （Window） 操作，支持基于 time、count、session 以及 data-driven 的窗口操作<br>支持具有 Backpressure 功能的持续流模型<br>支持基于轻量级分布式快照（Snapshot）实现的容错<br>一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理<br>Flink 在 JVM 内部实现了自己的内存管理<br>支持迭代计算<br>支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存</p></blockquote><h3 id="2-Flink-相比传统的-Spark-Streaming-有什么区别"><a href="#2-Flink-相比传统的-Spark-Streaming-有什么区别" class="headerlink" title="2. Flink 相比传统的 Spark Streaming 有什么区别?"></a><strong>2. Flink 相比传统的 Spark Streaming 有什么区别?</strong></h3><p>​       这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来：</p><blockquote><p><strong>Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。</strong></p></blockquote><p>下面我们就分几个方面介绍两个框架的主要区别：</p><h4 id="1-架构模型"><a href="#1-架构模型" class="headerlink" title="[1] 架构模型"></a><strong>[1] 架构模型</strong></h4><p>​      Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。</p><h4 id="2-任务调度"><a href="#2-任务调度" class="headerlink" title="[2] 任务调度"></a><strong>[2] 任务调度</strong></h4><p>​       Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。</p><p>​       Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。</p><h4 id="3-时间机制"><a href="#3-时间机制" class="headerlink" title="[3] 时间机制"></a><strong>[3] 时间机制</strong></h4><p>​        Spark Streaming 支持的时间机制有限，只支持<strong>处理时间</strong>。Flink 支持了流处理程序在时间上的三个定义：<strong>处理时间、事件时间、注入时间</strong>。同时也支持 <strong>watermark</strong> 机制来处理滞后数据。</p><h4 id="4-容错机制"><a href="#4-容错机制" class="headerlink" title="[4] 容错机制"></a><strong>[4] 容错机制</strong></h4><p>​        对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。</p><p>​         Flink 则使用两阶段提交协议来解决这个问题。</p><h3 id="3-Flink-的组件栈有哪些？"><a href="#3-Flink-的组件栈有哪些？" class="headerlink" title="3. Flink 的组件栈有哪些？"></a><strong>3. Flink 的组件栈有哪些？</strong></h3><p>​        根据 Flink 官网描述，Flink 是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。</p><p><img src="https://pic1.zhimg.com/80/v2-3d7d9b6e80a843212a4d168b500af1d8_1440w.jpg" alt="img"></p><p>图片来源于：<a href="https://link.zhihu.com/?target=https%3A//flink.apache.org">https://flink.apache.org</a></p><p>​         自下而上，每一层分别代表：</p><blockquote><ul><li>Deploy 层：该层主要涉及了Flink的部署模式，在上图中我们可以看出，Flink 支持包括local、Standalone、Cluster、Cloud等多种部署模式。</li><li>Runtime 层：Runtime层提供了支持 Flink 计算的核心实现，比如：支持分布式 Stream 处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。</li><li>API层：API 层主要实现了面向流（Stream）处理和批（Batch）处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API，后续版本，Flink有计划将DataStream和DataSet API进行统一。</li><li>Libraries层：该层称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。</li></ul></blockquote><h3 id="4-Flink-的运行必须依赖-Hadoop组件吗？"><a href="#4-Flink-的运行必须依赖-Hadoop组件吗？" class="headerlink" title="4. Flink 的运行必须依赖 Hadoop组件吗？"></a><strong>4. Flink 的运行必须依赖 Hadoop组件吗？</strong></h3><p>​        Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadoop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点。</p><h3 id="5-你们的Flink集群规模多大？"><a href="#5-你们的Flink集群规模多大？" class="headerlink" title="5. 你们的Flink集群规模多大？"></a><strong>5. 你们的Flink集群规模多大？</strong></h3><p>​      大家注意，这个问题看起来是问你实际应用中的Flink集群规模，其实还隐藏着另一个问题：Flink可以支持多少节点的集群规模？</p><p>​      在回答这个问题时候，可以将自己生产环节中的集群规模、节点、内存情况说明，同时说明部署模式（一般是Flink on Yarn），除此之外，用户也可以同时在小集群（少于5个节点）和拥有 TB 级别状态的上千个节点上运行 Flink 任务。</p><h3 id="6-Flink的基础编程模型了解吗？"><a href="#6-Flink的基础编程模型了解吗？" class="headerlink" title="6. Flink的基础编程模型了解吗？"></a><strong>6. Flink的基础编程模型了解吗？</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-2e5a594aaa7dd3efdcbb2c3f9e5a8fa9_1440w.jpg" alt="img"></p><p>​        上图是来自Flink官网的运行流程图。通过上图我们可以得知，Flink 程序的基本构建是数据输入来自一个 Source，Source 代表数据的输入端，经过 Transformation 进行转换，然后在一个或者多个Sink接收器中结束。数据流（stream）就是一组永远不会停止的数据记录流，而转换（transformation）是将一个或多个流作为输入，并生成一个或多个输出流的操作。执行时，Flink程序映射到 streaming dataflows，由流（streams）和转换操作（transformation operators）组成。</p><h3 id="7-Flink集群有哪些角色？各自有什么作用？"><a href="#7-Flink集群有哪些角色？各自有什么作用？" class="headerlink" title="7. Flink集群有哪些角色？各自有什么作用？"></a><strong>7. Flink集群有哪些角色？各自有什么作用？</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-a0c80153cec85fc9cd165d862b4d489e_1440w.jpg" alt="img"></p><p>​       Flink 程序在运行时主要有 TaskManager，JobManager，Client三种角色。其中JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。</p><p>​      TaskManager是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。</p><p>​      Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。</p><h3 id="8-说说-Flink-资源管理中-Task-Slot-的概念"><a href="#8-说说-Flink-资源管理中-Task-Slot-的概念" class="headerlink" title="8. 说说 Flink 资源管理中 Task Slot 的概念"></a><strong>8. 说说 Flink 资源管理中 Task Slot 的概念</strong></h3><p><img src="https://pic4.zhimg.com/80/v2-c692077d1cd718ad5671fa7d6128db93_1440w.jpg" alt="img"></p><p>​        在Flink架构角色中我们提到，TaskManager是实际负责执行计算的Worker，TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 Task Slot 的概念。</p><p>​       简单的说，TaskManager会将自己节点上管理的资源分为不同的Slot：固定大小的资源子集。这样就避免了不同Job的Task互相竞争内存资源，但是需要主要的是，Slot只会做内存的隔离。没有做CPU的隔离。</p><h3 id="9-说说-Flink-的常用算子？"><a href="#9-说说-Flink-的常用算子？" class="headerlink" title="9. 说说 Flink 的常用算子？"></a><strong>9. 说说 Flink 的常用算子？</strong></h3><p>​        Flink 最常用的常用算子包括：Map：DataStream → DataStream，输入一个参数产生一个参数，map的功能是对输入的参数进行转换操作。Filter：过滤掉指定条件的数据。KeyBy：按照指定的key进行分组。Reduce：用来进行结果汇总合并。Window：窗口函数，根据某些特性将每个key的数据进行分组（例如：在5s内到达的数据）</p><h3 id="10-说说你知道的Flink分区策略？"><a href="#10-说说你知道的Flink分区策略？" class="headerlink" title="10. 说说你知道的Flink分区策略？"></a><strong>10. 说说你知道的Flink分区策略？</strong></h3><p>​       什么要搞懂什么是分区策略。分区策略是用来决定数据如何发送至下游。目前 Flink 支持了8中分区策略的实现。</p><p><img src="https://pic2.zhimg.com/80/v2-135f2ee43f63fd4275f5e437fb21dda9_1440w.jpg" alt="img"></p><p>上图是整个Flink实现的分区策略继承图：</p><blockquote><ul><li><strong>GlobalPartitioner</strong>数据会被分发到下游算子的第一个实例中进行处理。</li><li><strong>ShufflePartitioner</strong>数据会被随机分发到下游算子的每一个实例中进行处理。</li><li><strong>RebalancePartitioner</strong>数据会被循环发送到下游的每一个实例中进行处理。</li><li><strong>RescalePartitioner</strong>这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为2，编号为A和B。下游并行度为4，编号为1，2，3，4。那么A则把数据循环发送给1和2，B则把数据循环发送给3和4。假设上游并行度为4，编号为A，B，C，D。下游并行度为2，编号为1，2。那么A和B则把数据发送给1，C和D则把数据发送给2。</li><li><strong>BroadcastPartitioner</strong>广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做join的场景。</li><li><strong>ForwardPartitioner</strong> 用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。</li><li><strong>KeyGroupStreamPartitioner</strong> Hash分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。</li><li><strong>CustomPartitionerWrapper</strong>用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑。</li></ul></blockquote><h3 id="11-Flink的并行度了解吗？Flink的并行度设置是怎样的？"><a href="#11-Flink的并行度了解吗？Flink的并行度设置是怎样的？" class="headerlink" title="11. Flink的并行度了解吗？Flink的并行度设置是怎样的？"></a><strong>11. Flink的并行度了解吗？Flink的并行度设置是怎样的？</strong></h3><p>​        Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。这些并行实例的数量被称为并行度。</p><p>​      我们在实际生产环境中可以从四个不同层面设置并行度：</p><ul><li><p>操作算子层面(Operator Level)</p></li><li><p>执行环境层面(Execution Environment Level)</p></li><li><p>客户端层面(Client Level)</p></li><li><p>系统层面(System Level)</p></li></ul><p>需要注意的优先级：算子层面&gt;环境层面&gt;客户端层面&gt;系统层面。</p><h3 id="12-Flink的Slot和parallelism有什么区别？"><a href="#12-Flink的Slot和parallelism有什么区别？" class="headerlink" title="12. Flink的Slot和parallelism有什么区别？"></a><strong>12. Flink的Slot和parallelism有什么区别？</strong></h3><p>官网上十分经典的图：</p><p><img src="https://pic2.zhimg.com/80/v2-d4ee22523e404ce88195bf5c4696f361_1440w.jpg" alt="img"></p><p>slot是指taskmanager的并发执行能力，假设我们将 taskmanager.numberOfTaskSlots 配置为3那么每一个 taskmanager 中分配3个 TaskSlot, 3个 taskmanager 一共有9个TaskSlot。</p><p><img src="https://pic4.zhimg.com/80/v2-855ff152aad46e75981864e13df3740b_1440w.jpg" alt="img"></p><p>parallelism是指taskmanager实际使用的并发能力。假设我们把 parallelism.default 设置为1，那么9个 TaskSlot 只能用1个，有8个空闲。</p><h3 id="13-Flink有没有重启策略？说说有哪几种？"><a href="#13-Flink有没有重启策略？说说有哪几种？" class="headerlink" title="13. Flink有没有重启策略？说说有哪几种？"></a><strong>13. Flink有没有重启策略？说说有哪几种？</strong></h3><p>Flink 实现了多种重启策略。</p><ul><li><p>固定延迟重启策略（Fixed Delay Restart Strategy）</p></li><li><p>故障率重启策略（Failure Rate Restart Strategy）</p></li><li><p>没有重启策略（No Restart Strategy）</p></li><li><p>Fallback重启策略（Fallback Restart Strategy）</p></li></ul><h3 id="14-用过Flink中的分布式缓存吗？如何使用？"><a href="#14-用过Flink中的分布式缓存吗？如何使用？" class="headerlink" title="14. 用过Flink中的分布式缓存吗？如何使用？"></a><strong>14. 用过Flink中的分布式缓存吗？如何使用？</strong></h3><p>​        Flink实现的分布式缓存和Hadoop有异曲同工之妙。目的是在本地读取文件，并把他放在 taskmanager 节点中，防止task重复拉取。</p><pre class="line-numbers language-text"><code class="language-text">val env = ExecutionEnvironment.getExecutionEnvironment// register a file from HDFSenv.registerCachedFile("hdfs:///path/to/your/file", "hdfsFile")// register a local executable file (script, executable, ...)env.registerCachedFile("file:///path/to/exec/file", "localExecFile", true)// define your program and execute...val input: DataSet[String] = ...val result: DataSet[Integer] = input.map(new MyMapper())...env.execute()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="15-说说Flink中的广播变量，使用时需要注意什么？"><a href="#15-说说Flink中的广播变量，使用时需要注意什么？" class="headerlink" title="15. 说说Flink中的广播变量，使用时需要注意什么？"></a><strong>15. 说说Flink中的广播变量，使用时需要注意什么？</strong></h3><p>​        我们知道Flink是并行的，计算过程可能不在一个 Slot 中进行，那么有一种情况即：当我们需要访问同一份数据。那么Flink中的广播变量就是为了解决这种情况。</p><p>​        我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。</p><h3 id="16-说说Flink中的窗口？"><a href="#16-说说Flink中的窗口？" class="headerlink" title="16. 说说Flink中的窗口？"></a><strong>16. 说说Flink中的窗口？</strong></h3><p>​      来一张官网经典的图：</p><p><img src="https://pic3.zhimg.com/80/v2-c4faa8da0a50b8dbf8f07ed978890106_1440w.jpg" alt="img"></p><p>​        Flink 支持两种划分窗口的方式，按照time和count。如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window。</p><p>​        flink支持窗口的两个重要属性（size和interval）</p><p>​        如果size=interval,那么就会形成tumbling-window(无重叠数据)如果size&gt;interval,那么就会形成sliding-window(有重叠数据)如果size&lt; interval, 那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。</p><p>​        通过组合可以得出四种基本窗口：</p><ul><li><p>time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5))</p></li><li><p>time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3))</p></li><li><p>count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5)</p></li><li><p>count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3)</p></li></ul><h3 id="17-说说Flink中的状态存储？"><a href="#17-说说Flink中的状态存储？" class="headerlink" title="17. 说说Flink中的状态存储？"></a><strong>17. 说说Flink中的状态存储？</strong></h3><p>​        Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和 checkpoint 交互。</p><p>​        Flink提供了三种状态存储方式：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。</p><h3 id="18-Flink-中的时间有哪几类"><a href="#18-Flink-中的时间有哪几类" class="headerlink" title="18. Flink 中的时间有哪几类"></a><strong>18. Flink 中的时间有哪几类</strong></h3><p>​        Flink 中的时间和其他流式计算系统的时间一样分为三类：事件时间，摄入时间，处理时间三种。</p><ul><li>如果以 EventTime 为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime。</li><li>如果以 IngesingtTime 为基准来定义时间窗口将形成 IngestingTimeWindow,以 source 的systemTime为准。</li><li>如果以 ProcessingTime 基准来定义时间窗口将形成 ProcessingTimeWindow，以 operator 的systemTime 为准。</li></ul><h3 id="19-Flink-中水印是什么概念，起到什么作用？"><a href="#19-Flink-中水印是什么概念，起到什么作用？" class="headerlink" title="19. Flink 中水印是什么概念，起到什么作用？"></a><strong>19. Flink 中水印是什么概念，起到什么作用？</strong></h3><p>​       Watermark 是 Apache Flink 为了处理 EventTime 窗口计算提出的一种机制, 本质上是一种时间戳。一般来讲Watermark经常和Window一起被用来处理乱序事件。</p><h3 id="20-Flink-Table-amp-SQL-熟悉吗？TableEnvironment这个类有什么作用"><a href="#20-Flink-Table-amp-SQL-熟悉吗？TableEnvironment这个类有什么作用" class="headerlink" title="20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用"></a><strong>20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用</strong></h3><p>​       TableEnvironment是Table API和SQL集成的核心概念。</p><p>这个类主要用来：</p><ul><li><p>在内部catalog中注册表</p></li><li><p>注册外部catalog</p></li><li><p>执行SQL查询</p></li><li><p>注册用户定义（标量，表或聚合）函数</p></li><li><p>将DataStream或DataSet转换为表</p></li><li><p>持有对ExecutionEnvironment或StreamExecutionEnvironment的引用</p></li></ul><h3 id="21-Flink-SQL的实现原理是什么？-是如何实现-SQL-解析的呢？"><a href="#21-Flink-SQL的实现原理是什么？-是如何实现-SQL-解析的呢？" class="headerlink" title="21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？"></a><strong>21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？</strong></h3><p>​      首先大家要知道 Flink 的SQL解析是基于Apache Calcite这个开源框架。</p><p><img src="https://pic2.zhimg.com/80/v2-c3877f035a976a213eab756465c2ebfd_1440w.jpg" alt="img"></p><p>基于此，一次完整的SQL解析过程如下：</p><blockquote><ul><li><p>用户使用对外提供Stream SQL的语法开发业务应用</p></li><li><p>用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划</p></li><li><p>采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划</p></li><li><p>对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行</p></li></ul></blockquote><h2 id="第二部分：Flink-面试进阶篇"><a href="#第二部分：Flink-面试进阶篇" class="headerlink" title="第二部分：Flink 面试进阶篇"></a><strong>第二部分：Flink 面试进阶篇</strong></h2><h3 id="1-Flink是如何支持批流一体的？"><a href="#1-Flink是如何支持批流一体的？" class="headerlink" title="1. Flink是如何支持批流一体的？"></a><strong>1. Flink是如何支持批流一体的？</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-36a937f492576709a89e8f25c1bed4a9_1440w.jpg" alt="img"></p><p>​       本道面试题考察的其实就是一句话：Flink的开发者认为批处理是流处理的一种特殊情况。批处理是有限的流处理。Flink 使用一个引擎支持了DataSet API 和 DataStream API。</p><h3 id="2-Flink是如何做到高效的数据交换的？"><a href="#2-Flink是如何做到高效的数据交换的？" class="headerlink" title="2. Flink是如何做到高效的数据交换的？"></a><strong>2. Flink是如何做到高效的数据交换的？</strong></h3><p>​         在一个Flink Job中，数据需要在不同的task中进行交换，整个数据交换是有 TaskManager 负责的，TaskManager 的网络组件首先从缓冲buffer中收集records，然后再发送。Records 并不是一个一个被发送的，二是积累一个批次再发送，batch 技术可以更加高效的利用网络资源。</p><h3 id="3-Flink是如何做容错的？"><a href="#3-Flink是如何做容错的？" class="headerlink" title="3. Flink是如何做容错的？"></a><strong>3. Flink是如何做容错的？</strong></h3><p>​        Flink 实现容错主要靠强大的CheckPoint机制和State机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态。</p><h3 id="4-Flink-分布式快照的原理是什么？"><a href="#4-Flink-分布式快照的原理是什么？" class="headerlink" title="4. Flink 分布式快照的原理是什么？"></a><strong>4. Flink 分布式快照的原理是什么？</strong></h3><p>​        Flink的分布式快照是根据Chandy-Lamport算法量身定做的。简单来说就是持续创建分布式数据流及其状态的一致快照。</p><p><img src="https://pic2.zhimg.com/80/v2-4207ea26cfbf1c4d6528137a50b7add9_1440w.jpg" alt="img"></p><p>​       核心思想是在 input source 端插入 barrier，控制 barrier 的同步来实现 snapshot 的备份和 exactly-once 语义。</p><h3 id="5-Flink-是如何保证Exactly-once语义的？"><a href="#5-Flink-是如何保证Exactly-once语义的？" class="headerlink" title="5. Flink 是如何保证Exactly-once语义的？"></a><strong>5. Flink 是如何保证Exactly-once语义的？</strong></h3><p>​      Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。分为以下几个步骤：</p><ul><li><p>开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面</p></li><li><p>预提交（preCommit）将内存中缓存的数据写入文件并关闭</p></li><li><p>正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟</p></li><li><p>丢弃（abort）丢弃临时文件</p></li></ul><p>​    若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。</p><h3 id="6-Flink-的-kafka-连接器有什么特别的地方？"><a href="#6-Flink-的-kafka-连接器有什么特别的地方？" class="headerlink" title="6. Flink 的 kafka 连接器有什么特别的地方？"></a><strong>6. Flink 的 kafka 连接器有什么特别的地方？</strong></h3><p>​     Flink源码中有一个独立的connector模块，所有的其他connector都依赖于此模块，Flink 在1.9版本发布的全新kafka连接器，摒弃了之前连接不同版本的kafka集群需要依赖不同版本的connector这种做法，只需要依赖一个connector即可。</p><h3 id="7-说说-Flink的内存管理是如何做的"><a href="#7-说说-Flink的内存管理是如何做的" class="headerlink" title="7. 说说 Flink的内存管理是如何做的?"></a><strong>7. 说说 Flink的内存管理是如何做的?</strong></h3><p>​       Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。</p><p>理论上Flink的内存管理分为三部分：</p><ul><li><p>Network Buffers：这个是在TaskManager启动的时候分配的，这是一组用于缓存网络数据的内存，每个块是32K，默认分配2048个，可以通过“taskmanager.network.numberOfBuffers”修改</p></li><li><p>Memory Manage pool：大量的Memory Segment块，用于运行时的算法（Sort/Join/Shuffle等），这部分启动的时候就会分配。下面这段代码，根据配置文件中的各种参数来计算内存的分配方法。（heap or off-heap，这个放到下节谈），内存的分配支持预分配和lazy load，默认懒加载的方式。</p></li><li><p>User Code，这部分是除了Memory Manager之外的内存用于User code和TaskManager本身的数据结构。</p></li></ul><h3 id="8-说说-Flink的序列化如何做的"><a href="#8-说说-Flink的序列化如何做的" class="headerlink" title="8. 说说 Flink的序列化如何做的?"></a><strong>8. 说说 Flink的序列化如何做的?</strong></h3><p>​       Java本身自带的序列化和反序列化的功能，但是辅助信息占用空间比较大，在序列化对象时记录了过多的类信息。</p><p>​      Apache Flink摒弃了Java原生的序列化方法，以独特的方式处理数据类型和序列化，包含自己的类型描述符，泛型类型提取和类型序列化框架。</p><p>​     TypeInformation 是所有类型描述符的基类。它揭示了该类型的一些基本属性，并且可以生成序列化器。TypeInformation 支持以下几种类型：</p><ul><li><p>BasicTypeInfo: 任意Java 基本类型或 String 类型</p></li><li><p>BasicArrayTypeInfo: 任意Java基本类型数组或 String 数组</p></li><li><p>WritableTypeInfo: 任意 Hadoop Writable 接口的实现类</p></li><li><p>TupleTypeInfo: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现</p></li><li><p>CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples)</p></li><li><p>PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法</p></li><li><p>GenericTypeInfo: 任意无法匹配之前几种类型的类</p></li></ul><p>​       针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。</p><h3 id="9-Flink中的Window出现了数据倾斜，你有什么解决办法？"><a href="#9-Flink中的Window出现了数据倾斜，你有什么解决办法？" class="headerlink" title="9.  Flink中的Window出现了数据倾斜，你有什么解决办法？"></a><strong>9.  Flink中的Window出现了数据倾斜，你有什么解决办法？</strong></h3><p>​       window产生数据倾斜指的是数据在不同的窗口内堆积的数据量相差过多。本质上产生这种情况的原因是数据源头发送的数据量速度不同导致的。出现这种情况一般通过两种方式来解决：</p><ul><li><p>在数据进入窗口前做预聚合</p></li><li><p>重新设计窗口聚合的key</p></li></ul><h3 id="10-Flink中在使用聚合函数-GroupBy、Distinct、KeyBy-等函数时出现数据热点该如何解决？"><a href="#10-Flink中在使用聚合函数-GroupBy、Distinct、KeyBy-等函数时出现数据热点该如何解决？" class="headerlink" title="10.  Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？"></a><strong>10.  Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？</strong></h3><p>​      数据倾斜和数据热点是所有大数据框架绕不过去的问题。处理这类问题主要从3个方面入手：</p><ul><li>在业务上规避这类问题</li></ul><p>​      例如一个假设订单场景，北京和上海两个城市订单量增长几十倍，其余城市的数据量不变。这时候我们在进行聚合的时候，北京和上海就会出现数据堆积，我们可以单独数据北京和上海的数据。</p><ul><li>Key的设计上</li></ul><p>​     把热key进行拆分，比如上个例子中的北京和上海，可以把北京和上海按照地区进行拆分聚合。</p><ul><li>参数设置</li></ul><p>​     Flink 1.9.0 SQL(Blink Planner) 性能优化中一项重要的改进就是升级了微批模型，即 MiniBatch。原理是缓存一定的数据后再触发处理，以减少对State的访问，从而提升吞吐和减少数据的输出量。</p><h3 id="11-Flink任务延迟高，想解决这个问题，你会如何入手？"><a href="#11-Flink任务延迟高，想解决这个问题，你会如何入手？" class="headerlink" title="11. Flink任务延迟高，想解决这个问题，你会如何入手？"></a><strong>11. Flink任务延迟高，想解决这个问题，你会如何入手？</strong></h3><p>​     在Flink的后台任务管理中，我们可以看到Flink的哪个算子和task出现了反压。最主要的手段是资源调优和算子调优。资源调优即是对作业中的Operator的并发数（parallelism）、CPU（core）、堆内存（heap_memory）等参数进行调优。作业参数调优包括：并行度的设置，State的设置，checkpoint的设置。</p><h3 id="12-Flink是如何处理反压的？"><a href="#12-Flink是如何处理反压的？" class="headerlink" title="12. Flink是如何处理反压的？"></a><strong>12. Flink是如何处理反压的？</strong></h3><p>​       Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。下游消费者消费变慢，上游就会受到阻塞。</p><h3 id="13-Flink的反压和Storm有哪些不同？"><a href="#13-Flink的反压和Storm有哪些不同？" class="headerlink" title="13. Flink的反压和Storm有哪些不同？"></a><strong>13. Flink的反压和Storm有哪些不同？</strong></h3><p>​      Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。</p><p>​      Flink中的反压使用了高效有界的分布式阻塞队列，下游消费变慢会导致发送端阻塞。</p><p>​      二者最大的区别是Flink是逐级反压，而Storm是直接从源头降速。</p><h3 id="14-Operator-Chains（算子链）这个概念你了解吗？"><a href="#14-Operator-Chains（算子链）这个概念你了解吗？" class="headerlink" title="14. Operator Chains（算子链）这个概念你了解吗？"></a><strong>14. Operator Chains（算子链）这个概念你了解吗？</strong></h3><p>​      为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。这就是我们所说的算子链。</p><h3 id="15-Flink什么情况下才会把Operator-chain在一起形成算子链？"><a href="#15-Flink什么情况下才会把Operator-chain在一起形成算子链？" class="headerlink" title="15. Flink什么情况下才会把Operator chain在一起形成算子链？"></a><strong>15. Flink什么情况下才会把Operator chain在一起形成算子链？</strong></h3><p>​    两个operator chain在一起的的条件：</p><ul><li><p>上下游的并行度一致</p></li><li><p>下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</p></li><li><p>上下游节点都在同一个 slot group 中（下面会解释 slot group）</p></li><li><p>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</p></li><li><p>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</p></li><li><p>两个节点间数据分区方式是 forward（参考理解数据流的分区）</p></li><li><p>用户没有禁用 chain</p></li></ul><h3 id="16-说说Flink1-9的新特性？"><a href="#16-说说Flink1-9的新特性？" class="headerlink" title="16. 说说Flink1.9的新特性？"></a><strong>16. 说说Flink1.9的新特性？</strong></h3><ul><li><p>支持hive读写，支持UDF</p></li><li><p>Flink SQL TopN和GroupBy等优化</p></li><li><p>Checkpoint跟savepoint针对实际业务场景做了优化</p></li><li><p>Flink state查询</p></li></ul><h3 id="17-消费kafka数据的时候，如何处理脏数据？"><a href="#17-消费kafka数据的时候，如何处理脏数据？" class="headerlink" title="17. 消费kafka数据的时候，如何处理脏数据？"></a><strong>17. 消费kafka数据的时候，如何处理脏数据？</strong></h3><p>   可以在处理前加一个fliter算子，将不符合规则的数据过滤出去。</p><h2 id="第三部分：Flink-面试源码篇"><a href="#第三部分：Flink-面试源码篇" class="headerlink" title="第三部分：Flink 面试源码篇"></a><strong>第三部分：Flink 面试源码篇</strong></h2><h3 id="1-Flink-Job的提交流程"><a href="#1-Flink-Job的提交流程" class="headerlink" title="1. Flink Job的提交流程"></a><strong>1. Flink Job的提交流程</strong></h3><p>​       用户提交的Flink Job会被转化成一个DAG任务运行，分别是：StreamGraph、JobGraph、ExecutionGraph，Flink中JobManager与TaskManager，JobManager与Client的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。</p><h3 id="2-Flink所谓”三层图”结构是哪几个”图”？"><a href="#2-Flink所谓”三层图”结构是哪几个”图”？" class="headerlink" title="2. Flink所谓”三层图”结构是哪几个”图”？"></a><strong>2. Flink所谓”三层图”结构是哪几个”图”？</strong></h3><blockquote><p>一个Flink任务的DAG生成计算图大致经历以下三个过程：</p><ul><li><p>StreamGraph<br>最接近代码所表达的逻辑层面的计算拓扑结构，按照用户代码的执行顺序向StreamExecutionEnvironment添加StreamTransformation构成流式图。</p></li><li><p>JobGraph<br>从StreamGraph生成，将可以串联合并的节点进行合并，设置节点之间的边，安排资源共享slot槽位和放置相关联的节点，上传任务所需的文件，设置检查点配置等。相当于经过部分初始化和优化处理的任务图。</p></li><li><p>ExecutionGraph<br>由JobGraph转换而来，包含了任务具体执行所需的内容，是最贴近底层实现的执行图。</p></li></ul></blockquote><h3 id="3-JobManger在集群中扮演了什么角色？"><a href="#3-JobManger在集群中扮演了什么角色？" class="headerlink" title="3. JobManger在集群中扮演了什么角色？"></a><strong>3. JobManger在集群中扮演了什么角色？</strong></h3><p>​      JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。</p><p>​     JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务管理和资源管理。</p><p>​     JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。</p><p>​    同时在任务执行的过程中，Flink JobManager 会触发 Checkpoint 操作，每个 TaskManager 节点 收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Fink JobManager 中完成。</p><p>​     当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。</p><h3 id="4-JobManger在集群启动过程中起到什么作用？"><a href="#4-JobManger在集群启动过程中起到什么作用？" class="headerlink" title="4. JobManger在集群启动过程中起到什么作用？"></a><strong>4. JobManger在集群启动过程中起到什么作用？</strong></h3><p>​     JobManager的职责主要是接收Flink作业，调度Task，收集作业状态和管理TaskManager。它包含一个Actor，并且做如下操作：</p><ul><li><p>RegisterTaskManager: 它由想要注册到JobManager的TaskManager发送。注册成功会通过AcknowledgeRegistration消息进行Ack。</p></li><li><p>SubmitJob: 由提交作业到系统的Client发送。提交的信息是JobGraph形式的作业描述信息。</p></li><li><p>CancelJob: 请求取消指定id的作业。成功会返回CancellationSuccess，否则返回CancellationFailure。</p></li><li><p>UpdateTaskExecutionState: 由TaskManager发送，用来更新执行节点(ExecutionVertex)的状态。成功则返回true，否则返回false。</p></li><li><p>RequestNextInputSplit: TaskManager上的Task请求下一个输入split，成功则返回NextInputSplit，否则返回null。</p></li><li><p>JobStatusChanged： 它意味着作业的状态(RUNNING, CANCELING, FINISHED,等)发生变化。这个消息由ExecutionGraph发送。</p></li></ul><h3 id="5-TaskManager在集群中扮演了什么角色？"><a href="#5-TaskManager在集群中扮演了什么角色？" class="headerlink" title="5. TaskManager在集群中扮演了什么角色？"></a><strong>5. TaskManager在集群中扮演了什么角色？</strong></h3><p>​     TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。</p><p>​    客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。</p><p>​     TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。</p><p>​      可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进行的方式有很大的区别，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。</p><h3 id="6-TaskManager在集群启动过程中起到什么作用？"><a href="#6-TaskManager在集群启动过程中起到什么作用？" class="headerlink" title="6. TaskManager在集群启动过程中起到什么作用？"></a><strong>6. TaskManager在集群启动过程中起到什么作用？</strong></h3><p>​     TaskManager的启动流程较为简单：启动类：org.apache.flink.runtime.taskmanager.TaskManager核心启动方法 ： selectNetworkInterfaceAndRunTaskManager启动后直接向JobManager注册自己，注册完成后，进行部分模块的初始化。</p><h3 id="7-Flink-计算资源的调度是如何实现的？"><a href="#7-Flink-计算资源的调度是如何实现的？" class="headerlink" title="7. Flink 计算资源的调度是如何实现的？"></a><strong>7. Flink 计算资源的调度是如何实现的？</strong></h3><p>​     TaskManager中最细粒度的资源是Task slot，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。</p><p>​     通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。</p><p>​      而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot：</p><p><img src="https://pic2.zhimg.com/80/v2-00e4b6d488ca209d7b02580ece13a905_1440w.jpg" alt="img"></p><h3 id="8-简述Flink的数据抽象及数据交换过程？"><a href="#8-简述Flink的数据抽象及数据交换过程？" class="headerlink" title="8. 简述Flink的数据抽象及数据交换过程？"></a><strong>8. 简述Flink的数据抽象及数据交换过程？</strong></h3><p>​        Flink 为了避免JVM的固有缺陷例如java对象存储密度低，FGC影响吞吐和响应等，实现了自主管理内存。MemorySegment就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。</p><p>​        在MemorySegment这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是Buffer。</p><p>​        对接从Java对象转为Buffer的中间对象是另一个抽象StreamRecord。</p><h3 id="9-Flink-中的分布式快照机制是如何实现的？"><a href="#9-Flink-中的分布式快照机制是如何实现的？" class="headerlink" title="9. Flink 中的分布式快照机制是如何实现的？"></a><strong>9. Flink 中的分布式快照机制是如何实现的？</strong></h3><p>​        Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。</p><p><img src="https://pic3.zhimg.com/80/v2-897c8116fc9c17b50068810949311d52_1440w.jpg" alt="img"></p><p>​       barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。</p><p>​       然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。 一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。在所有sink确认快照后，意味快照着已完成。</p><p>​        一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。</p><h3 id="10-简单说说FlinkSQL的是如何实现的？"><a href="#10-简单说说FlinkSQL的是如何实现的？" class="headerlink" title="10. 简单说说FlinkSQL的是如何实现的？"></a><strong>10. 简单说说FlinkSQL的是如何实现的？</strong></h3><p>​        Flink 将 SQL 校验、SQL 解析以及 SQL 优化交给了Apache Calcite。Calcite 在其他很多开源项目里也都应用到了，譬如 Apache Hive, Apache Drill, Apache Kylin, Cascading。Calcite 在新的架构中处于核心的地位，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-6f52103fd8b0bbd77e3c35f9cdbf4df9_1440w.jpg" alt="img"></p><p>构建抽象语法树的事情交给了 Calcite 去做。SQL query 会经过 Calcite 解析器转变成 SQL 节点树，通过验证后构建成 Calcite 的抽象语法树（也就是图中的 Logical Plan）。另一边，Table API 上的调用会构建成 Table API 的抽象语法树，并通过 Calcite 提供的 RelBuilder 转变成 Calcite 的抽象语法树。然后依次被转换成逻辑执行计划和物理执行计划。</p><p>在提交任务后会分发到各个 TaskManager 中运行，在运行时会使用 Janino 编译器编译代码后运行。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第一部分：Flink-面试基础篇&quot;&gt;&lt;a href=&quot;#第一部分：Flink-面试基础篇&quot; class=&quot;headerlink&quot; title=&quot;第一部分：Flink 面试基础篇&quot;&gt;&lt;/a&gt;&lt;strong&gt;第一部分：Flink 面试基础篇&lt;/strong&gt;&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="Flink" scheme="https://dataquaner.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://dataquaner.github.io/tags/Flink/"/>
    
      <category term="面试必备" scheme="https://dataquaner.github.io/tags/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系列之决策树算法（03）：决策树的剪枝</title>
    <link href="https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/"/>
    <id>https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/</id>
    <published>2020-04-11T11:32:09.079Z</published>
    <updated>2020-04-11T11:32:09.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现<strong>过拟合现象</strong>。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为<strong>剪枝(pruning)。</strong></p><p>具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。</p><blockquote><p>决策树的剪枝往往通过<strong>极小化</strong>决策树整体的<strong>损失函数(loss funtion)</strong>或<strong>代价函数(cost funtion)</strong>来实现。</p></blockquote><h1 id="2-剪枝算法"><a href="#2-剪枝算法" class="headerlink" title="2.剪枝算法"></a>2.剪枝算法</h1><h2 id="2-1-为什么要剪枝"><a href="#2-1-为什么要剪枝" class="headerlink" title="2.1 为什么要剪枝"></a>2.1 为什么要剪枝</h2><p><strong>现象</strong></p><p>接上一次讲的生成决策树，下面给出一张图。</p><img src="https://pic2.zhimg.com/80/v2-d6588457cc144c1bad2f87ec77081af1_hd.jpg" alt="决策树学习中的过渡拟合" style="zoom:50%;"><ul><li>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。</li><li>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</li></ul><p><strong>可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</strong></p><p><strong>原因</strong></p><p><img src="https://pic2.zhimg.com/80/v2-677a5e08d5b55b3b4cb14f7ad6f8eb31_hd.jpg" alt="原因"></p><ul><li>原因1：噪声、样本冲突，即错误的样本数据。</li><li>原因2：特征即属性不能完全作为分类标准。</li><li>原因3：巧合的规律性，数据量不够大。</li></ul><p>这个时候，就需要对生成树进行修剪，也就是<strong>剪枝</strong>。</p><h2 id="2-2-如何进行剪枝"><a href="#2-2-如何进行剪枝" class="headerlink" title="2.2 如何进行剪枝"></a>2.2 如何进行剪枝</h2><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><strong>预剪枝</strong></h3><p>预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法:<br>        (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。<br>        (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。<br>        (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。</p><p>(4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。</p><p><strong>优点&amp;缺点</strong></p><ul><li><p>由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【<strong>怎样精确地估计何时停止树的增长是相当困难的</strong>】。</p></li><li><p>预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。</p></li></ul><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a><strong>后剪枝</strong></h3><p>后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。</p><p>这里主要介绍四种：</p><ul><li>REP-错误率降低剪枝</li><li>PEP-悲观剪枝</li><li>CCP-代价复杂度剪枝</li><li>MEP-最小错误剪枝</li></ul><h4 id="REP-Reduced-Error-Pruning-方法"><a href="#REP-Reduced-Error-Pruning-方法" class="headerlink" title="REP(Reduced Error Pruning)方法"></a><strong>REP(Reduced Error Pruning)方法</strong></h4><blockquote><p>对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S</p></blockquote><p><img src="https://pic2.zhimg.com/80/v2-ff11945f2e5a8319d82ab53c363ef441_hd.jpg" alt="img"></p><p><strong>优点：</strong></p><ul><li>REP 是当前最简单的事后剪枝方法之一。</li><li>它的计算复杂性是线性的。</li><li>和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。</li></ul><p><strong>缺点：</strong></p><ul><li>但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意.</li></ul><h4 id="PEP-Pessimistic-Error-Pruning-方法"><a href="#PEP-Pessimistic-Error-Pruning-方法" class="headerlink" title="PEP(Pessimistic Error Pruning)方法"></a><strong>PEP(Pessimistic Error Pruning)方法</strong></h4><blockquote><p>为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, <strong>PEP 方法对误差估计增加了连续性校正(continuity correction)</strong>。关于PEP方法的数据解释待后续开专题梳理。</p></blockquote><p><strong>优点：</strong></p><ul><li>PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一</li><li>PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利</li><li>它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .</li></ul><p><strong>缺点：</strong></p><p>PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。</p><p><strong>TIPS：</strong></p><p>个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。</p><h4 id="MEP-Minimum-Error-Pruning-方法"><a href="#MEP-Minimum-Error-Pruning-方法" class="headerlink" title="MEP(Minimum Error Pruning)方法"></a><strong>MEP(Minimum Error Pruning)方法</strong></h4><blockquote><p>MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。</p></blockquote><p><strong>优点：</strong></p><ul><li>MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集.</li><li>它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .</li></ul><p><strong>缺点：</strong></p><p>类别平均分配的前提假设现实几率不大&amp;对K太过敏感</p><p><img src="https://pic3.zhimg.com/80/v2-5e7deee0ee978be2eec60328192affc6_hd.jpg" alt="img"></p><p>对此，也有改进算法，我没有深入研究。</p><p><img src="https://pic1.zhimg.com/80/v2-3ffc529242dbb52dcb4946e37fed92f0_hd.jpg" alt="img"></p><h4 id="CCP-Cost-Complexity-Pruning-方法"><a href="#CCP-Cost-Complexity-Pruning-方法" class="headerlink" title="CCP(Cost-Complexity Pruning)方法"></a><strong>CCP(Cost-Complexity Pruning)方法</strong></h4><blockquote><p>CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤:<br>                (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。</p><p>​        (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。</p></blockquote><p><strong>缺点：</strong></p><p>生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的.</p><p><strong>对比四种方法</strong></p><table><thead><tr><th><strong>剪枝名称</strong></th><th><strong>剪枝方式</strong></th><th><strong>计算复杂度</strong></th><th><strong>误差估计</strong></th></tr></thead><tbody><tr><td>REP</td><td>自底向上</td><td>0(n)</td><td>剪枝集上误差估计</td></tr><tr><td>PEP</td><td>自顶向下</td><td>o(n)</td><td>使用连续纠正</td></tr><tr><td>CCP</td><td>自底向上</td><td>o(n2)</td><td>标准误差</td></tr><tr><td>MEP</td><td>自底向上</td><td>o(n)</td><td>使用连续纠正</td></tr></tbody></table><p>① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。</p><p>② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。</p><p>③ 如果数据集多（REP&amp;CCP←复杂但树小）</p><p>④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定）</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h1><p>决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。</p><p>剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。</p><p><strong>预剪枝：</strong></p><p>预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。</p><p>简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。</p><p>这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。</p><p><strong>后剪枝：</strong></p><p>后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。</p><p>同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。</p><p>后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。</p><p><strong>剪枝方法的选择</strong></p><p>如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。</p><p>后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。</p><p>有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。</p><p>经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。</p><p>无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。</p><p>函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点</p><p>函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。</p><p>函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值</p><p>上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      决策树
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Decision Tree" scheme="https://dataquaner.github.io/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系列之决策树算法（02）：决策树的生成</title>
    <link href="https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/"/>
    <id>https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/</id>
    <published>2020-04-11T11:31:53.052Z</published>
    <updated>2020-04-11T11:31:53.052Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。</p><h2 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h2><p> <strong>示例数据表格</strong></p><p>    文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中</p><p><img src="https://pic3.zhimg.com/80/v2-393024075528471f43f52891d29320be_hd.jpg" alt="img"></p><h3 id="2-1-相关概念阐述"><a href="#2-1-相关概念阐述" class="headerlink" title="2.1 相关概念阐述"></a><strong>2.1 相关概念阐述</strong></h3><h4 id="2-1-1-决策树"><a href="#2-1-1-决策树" class="headerlink" title="2.1.1 决策树"></a><strong>2.1.1 决策树</strong></h4><p> 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。</p><h4 id="2-1-2-ID3、C4-5、CART"><a href="#2-1-2-ID3、C4-5、CART" class="headerlink" title="2.1.2 ID3、C4.5、CART"></a><strong>2.1.2 ID3、C4.5、CART</strong></h4><p>在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【<a href="https://dataquaner.github.io/2019/12/17/机器学习系列之决策树算法（01）：决策树特征选择/">机器学习系列之决策树算法（01）：决策树特征选择</a>】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。</p><h5 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a><strong>信息熵</strong></h5><p> 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。</p><p>​                                                            Entopy(t)=-Σp(i|t)log2p(i|t)</p><h5 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a><strong>信息增益</strong></h5><p>信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为：</p><p>​                                        Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di)</p><h5 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a><strong>信息增益率</strong></h5><p> 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7：</p><p>​                H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7))</p><h5 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a><strong>基尼系数</strong></h5><p> 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0">基尼系数</a></p><p> 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低.</p><p> CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分.</p><p> 基尼系数的计算公式如下:</p><p>​                            Gini = 1 – Σ (Pi)2 for i=1 to number of classes</p><h3 id="2-2-完整生成过程"><a href="#2-2-完整生成过程" class="headerlink" title="2.2 完整生成过程"></a><strong>2.2 完整生成过程</strong></h3><p> 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例</p><h4 id="2-2-1-根节点的选择"><a href="#2-2-1-根节点的选择" class="headerlink" title="2.2.1 根节点的选择"></a><strong>2.2.1 根节点的选择</strong></h4><p> 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数</p><p> 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为：</p><p>​                        Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985</p><h5 id="天气"><a href="#天气" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-4977ea2a875ea67cb75588eb04b6aee5_hd.jpg" alt="img"></p><h6 id="信息增益计算"><a href="#信息增益计算" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下：</p><p>D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981</p><p>D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为：</p><p>3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965</p><p>其信息增益为：</p><p>Gain(天气)=0.985 - 0.965 = 0.020</p><h6 id="信息增益率计算"><a href="#信息增益率计算" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p> 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为：</p><p>H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556</p><p> 则其信息增益率为：</p><p>Gain_ratio(天气)=0.020/1.556=0.012</p><h6 id="基尼系数计算"><a href="#基尼系数计算" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9</li><li>Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21</li></ul><h5 id="温度"><a href="#温度" class="headerlink" title="温度"></a><strong>温度</strong></h5><p>  其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-74ad946b56a27f3bc480ba07f31552de_hd.jpg" alt="img"></p><h6 id="信息增益计算-1"><a href="#信息增益计算-1" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857</p><p>    其信息增益为：</p><p>Gain(温度)=0.985 - 0.857 = 0.128</p><h6 id="信息增益率计算-1"><a href="#信息增益率计算-1" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378</p><p>    则其信息增益率为：</p><p>Gain_ratio(温度)=0.128/1.378=0.0928</p><h6 id="基尼系数计算-1"><a href="#基尼系数计算-1" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0</li><li>Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7</li></ul><h5 id="湿度"><a href="#湿度" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic1.zhimg.com/80/v2-423f80f054a8d86eed652afff6a6c914_hd.jpg" alt="img"></p><h6 id="信息增益计算-2"><a href="#信息增益计算-2" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>    作为子节点的归一化信息熵为：</p><p>4/7 * 1.0 + 3/7 * 0.918 = 0.964</p><p>    其信息增益为：</p><p>Gain(湿度)=0.985 - 0.964 = 0.021</p><h6 id="信息增益率计算-2"><a href="#信息增益率计算-2" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=0.021/0.985=0.021</p><h6 id="基尼系数计算-2"><a href="#基尼系数计算-2" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</li><li>Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><h6 id="刮风"><a href="#刮风" class="headerlink" title="刮风"></a><strong>刮风</strong></h6><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-edb75f5790519fcee2dd6317f4d5557d_hd.jpg" alt="img"></p><h6 id="信息增益计算-3"><a href="#信息增益计算-3" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>    作为子节点的归一化信息熵为：</p><p>3/7 * 1.0 + 4/7 * 0.918 = 0.964</p><p>    其信息增益为：</p><p>Gain(刮风)=0.985 - 0.964 = 0.021</p><h6 id="信息增益率计算-3"><a href="#信息增益率计算-3" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0.021/0.985=0.021</p><h6 id="基尼系数计算-3"><a href="#基尼系数计算-3" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</li><li>Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><h5 id="根节点的选择"><a href="#根节点的选择" class="headerlink" title="根节点的选择"></a><strong>根节点的选择</strong></h5><p>  如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度</p><p><strong>信息增益</strong></p><ul><li><p>Gain(天气)=0.985 - 0.965 = 0.020</p></li><li><p>Gain(温度)=0.985 - 0.857 = 0.128</p></li><li><p>Gain(湿度)=0.985 - 0.964 = 0.021</p></li><li><p>Gain(刮风)=0.985 - 0.964 = 0.021</p></li></ul><p><strong>信息增益率</strong></p><ul><li>Gain_ratio(天气)=0.020/1.556=0.012</li><li>Gain_ratio(温度)=0.128/1.378=0.0928</li><li>Gain_ratio(湿度)=0.021/0.985=0.021</li><li>Gain_ratio(刮风)=0.021/0.985=0.021</li></ul><p><strong>基尼系数</strong></p><ul><li>Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619</li><li>Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857</li><li>Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li><li>Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><p>确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择:</p><ul><li>根节点<ul><li>子节点温度高:(待进一步进行选择)</li><li>子节点温度中:(待进一步进行选择)</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul><h4 id="2-2-2-子节点温度高的选择"><a href="#2-2-2-子节点温度高的选择" class="headerlink" title="2.2.2 子节点温度高的选择"></a><strong>2.2.2 子节点温度高的选择</strong></h4><p>    其剩下的数据集如下,温度不再进行下面的节点选择参与:</p><p><img src="https://pic1.zhimg.com/80/v2-86c2e574fc2309e9dff98e8205c0dff4_hd.jpg" alt="img"></p><p>    根据信息熵的计算公式可以得到子节点温度高的信息熵为：</p><p>​                                                Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><h5 id="天气-1"><a href="#天气-1" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-72771013ffea869ed0ce76c7f8998f79_hd.jpg" alt="img"></p><h6 id="信息增益计算-4"><a href="#信息增益计算-4" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    相应的信息熵如下：</p><p>D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>    归一化信息熵为：</p><p>2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5</p><p>    其信息增益为：</p><p>Gain(天气)=1.0 - 0.5 = 0.5</p><h6 id="信息增益率计算-4"><a href="#信息增益率计算-4" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    对应的属性熵为：</p><p>H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5</p><p>    则其信息增益率为：</p><p>Gain_ratio(天气)=0.5/1.5=0.33333</p><h6 id="基尼系数计算-4"><a href="#基尼系数计算-4" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25</li></ul><h5 id="湿度-1"><a href="#湿度-1" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-ea98c22052cf1246618f266c52be998e_hd.jpg" alt="img"></p><h6 id="信息增益计算-5"><a href="#信息增益计算-5" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0</p><p>D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>2/4 * 0.0 + 2/4 * 0.0 = 0.0</p><p>    其信息增益为：</p><p>Gain(湿度)=1.0 - 0.0 = 1.0</p><h6 id="信息增益率计算-5"><a href="#信息增益率计算-5" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=1.0/1.0=1.0</p><h6 id="基尼系数计算-5"><a href="#基尼系数计算-5" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0</li><li>Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0</li><li>Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0</li></ul><h5 id="刮风-1"><a href="#刮风-1" class="headerlink" title="刮风"></a><strong>刮风</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-ba94b90f801904bae450796e8a5db0be_hd.jpg" alt="img"></p><h6 id="信息增益计算-6"><a href="#信息增益计算-6" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0</p><p>D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>    作为子节点的归一化信息熵为：</p><p>1/4 * 0.0 + 3/4 * 0.918 = 0.688</p><p>    其信息增益为：</p><p>Gain(刮风)=1.0 - 0.688 = 0.312</p><h6 id="信息增益率计算-6"><a href="#信息增益率计算-6" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0.312/0.918=0.349</p><h6 id="基尼系数计算-6"><a href="#基尼系数计算-6" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li><p>Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0</p></li><li><p>Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</p></li><li><p>Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333</p></li></ul><p><strong>子节点温度高的选择</strong></p><p>    如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度</p><ul><li>Gain(天气)=1.0 - 0.5 = 0.5</li><li>Gain(湿度)=1.0 - 0.0 = 1.0</li><li>Gain(刮风)=1.0 - 0.688 = 0.312</li><li>Gain_ratio(天气)=0.5/1.5=0.33333</li><li>Gain_ratio(湿度)=1.0/1.0=1.0</li><li>Gain_ratio(刮风)=0.312/0.918=0.349</li><li>Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25</li><li>Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0</li><li>Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333</li></ul><p>    确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果:</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中:(待进一步进行选择)<ul><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h4 id="2-2-3-子节点温度中的选择"><a href="#2-2-3-子节点温度中的选择" class="headerlink" title="2.2.3 子节点温度中的选择"></a><strong>2.2.3 子节点温度中的选择</strong></h4><p>    其剩下的数据集如下,温度不再进行下面的节点选择参与:</p><p><img src="https://pic1.zhimg.com/80/v2-3f1b0c5e060288599f90c428441aaabc_hd.jpg" alt="img"></p><p>    根据信息熵的计算公式可以得到子节点温度高的信息熵为：</p><p>Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><h5 id="天气-2"><a href="#天气-2" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic4.zhimg.com/80/v2-7ac9cd99fc8388c6c3f34fdb2e132b57_hd.jpg" alt="img"></p><h6 id="信息增益计算-7"><a href="#信息增益计算-7" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    相应的信息熵如下：</p><p>D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D</p><p>(阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>    归一化信息熵为：</p><p>1/2 * 0.0 + 1/2 * 0.0 = 0</p><p>    其信息增益为：</p><p>Gain(天气)=1.0 - 0 = 1.0</p><h6 id="信息增益率计算-7"><a href="#信息增益率计算-7" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    对应的属性熵为：</p><p>H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(天气)=1.0/1.0=1.0</p><h6 id="基尼系数计算-7"><a href="#基尼系数计算-7" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0</li><li>Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0</li></ul><h5 id="湿度-2"><a href="#湿度-2" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-a27cfcdbbde0c07ab39cbdac34b8e6da_hd.jpg" alt="img"></p><h6 id="信息增益计算-8"><a href="#信息增益计算-8" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>1/2 * 0.0 + 1/2 * 0.0 = 0</p><p>    其信息增益为：</p><p>Gain(湿度)=1.0 - 0.0 = 1.0</p><h6 id="信息增益率计算-8"><a href="#信息增益率计算-8" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=1.0/1.0=1.0</p><h6 id="基尼系数计算-8"><a href="#基尼系数计算-8" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0</li><li>Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0</li></ul><h5 id="刮风-2"><a href="#刮风-2" class="headerlink" title="刮风"></a><strong>刮风</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-887ebcf7eecd07ee13ca02d024a3b321_hd.jpg" alt="img"></p><h6 id="信息增益计算-9"><a href="#信息增益计算-9" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    作为子节点的归一化信息熵为：</p><p>1/1 * 1.0 = 1.0</p><p>    其信息增益为：</p><p>Gain(刮风)=1.0 - 1.0 = 0</p><h6 id="信息增益率计算-9"><a href="#信息增益率计算-9" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(2/2 * log2(2/2) = 0.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0/0 = 0</p><h6 id="基尼系数计算-9"><a href="#基尼系数计算-9" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5</li><li>Gini(刮风)=2/2 * 0.5 = 0.5</li></ul><p><strong>子节点温度中的选择</strong></p><p>如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧</p><ul><li>Gain(天气)=1.0 - 0 = 1.0</li><li>Gain(湿度)=1.0 - 0.0 = 1.0</li><li>Gain(刮风)=1.0 - 1.0 = 0</li><li>Gain_ratio(天气)=1.0/1.0=1.0</li><li>Gain_ratio(湿度)=1.0/1.0=1.0</li><li>Gain_ratio(刮风)=0/0 = 0</li><li>Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0</li><li>Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0</li><li>Gini(刮风)=2/2 * 0.5 = 0.5</li></ul><p>    确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果:</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中<ul><li>叶节点天气晴：打篮球</li><li>叶节点天气阴：不打篮球</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h4 id="2-2-4-最终的决策树"><a href="#2-2-4-最终的决策树" class="headerlink" title="2.2.4 最终的决策树"></a><strong>2.2.4 最终的决策树</strong></h4><p>    在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树：</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中<ul><li>叶节点天气晴：打篮球</li><li>叶节点天气阴：不打篮球</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a><strong>3. 思考</strong></h2><p> 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？</p><p> 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      决策树
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Decision Tree" scheme="https://dataquaner.github.io/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>数据存储之MySQL系列（01）：MySQL体系结构</title>
    <link href="https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/"/>
    <id>https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/</id>
    <published>2020-04-11T11:31:10.255Z</published>
    <updated>2020-04-11T11:31:10.255Z</updated>
    
    <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      MySQL
    
    </summary>
    
    
      <category term="DataBase" scheme="https://dataquaner.github.io/categories/DataBase/"/>
    
    
      <category term="MySQL" scheme="https://dataquaner.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>xgboost算法模型输出的解释</title>
    <link href="https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/"/>
    <id>https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/</id>
    <published>2020-04-11T11:30:52.923Z</published>
    <updated>2020-04-11T11:30:52.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p> 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。</p><h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2. 数据集"></a>2. 数据集</h2><p> 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasetsiris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>data <span class="token operator">=</span> iris<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span> data<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true">#(100L, 4L)</span><span class="token comment" spellcheck="true">#一共有100个样本数据, 维度为4维</span>label <span class="token operator">=</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span> label<span class="token comment" spellcheck="true">#正好选取label为0和1的数据</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-训练集与测试集"><a href="#3-训练集与测试集" class="headerlink" title="3. 训练集与测试集"></a>3. 训练集与测试集</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cross_validation <span class="token keyword">import</span> train_test_splittrain_x<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="4-Xgboost建模"><a href="#4-Xgboost建模" class="headerlink" title="4. Xgboost建模"></a>4. Xgboost建模</h2><h3 id="4-1-模型初始化设置"><a href="#4-1-模型初始化设置" class="headerlink" title="4.1 模型初始化设置"></a>4.1 模型初始化设置</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgbdtrain<span class="token operator">=</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>train_x<span class="token punctuation">,</span>label<span class="token operator">=</span>train_y<span class="token punctuation">)</span>dtest<span class="token operator">=</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>test_x<span class="token punctuation">)</span>params<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'booster'</span><span class="token punctuation">:</span><span class="token string">'gbtree'</span><span class="token punctuation">,</span>    <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'binary:logistic'</span><span class="token punctuation">,</span>    <span class="token string">'eval_metric'</span><span class="token punctuation">:</span> <span class="token string">'auc'</span><span class="token punctuation">,</span>    <span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token string">'lambda'</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">,</span>    <span class="token string">'subsample'</span><span class="token punctuation">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>    <span class="token string">'colsample_bytree'</span><span class="token punctuation">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>    <span class="token string">'min_child_weight'</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span>    <span class="token string">'eta'</span><span class="token punctuation">:</span> <span class="token number">0.025</span><span class="token punctuation">,</span>    <span class="token string">'seed'</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token string">'nthread'</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">,</span>     <span class="token string">'silent'</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">}</span>watchlist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>dtrain<span class="token punctuation">,</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-建模与预测"><a href="#4-2-建模与预测" class="headerlink" title="4.2 建模与预测"></a>4.2 建模与预测</h3><pre class="line-numbers language-python"><code class="language-python">bst<span class="token operator">=</span>xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>params<span class="token punctuation">,</span>dtrain<span class="token punctuation">,</span>num_boost_round<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>evals<span class="token operator">=</span>watchlist<span class="token punctuation">)</span>ypred<span class="token operator">=</span>bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 设置阈值, 输出一些评价指标</span>y_pred <span class="token operator">=</span> <span class="token punctuation">(</span>ypred <span class="token operator">>=</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1</span><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token keyword">print</span> <span class="token string">'AUC: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>roc_auc_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>ypred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'ACC: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'Recall: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>recall_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'F1-score: %.4f'</span> <span class="token operator">%</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'Precesion: %.4f'</span> <span class="token operator">%</span>metrics<span class="token punctuation">.</span>precision_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Out[23]:</p><pre class="line-numbers language-python"><code class="language-python">AUC<span class="token punctuation">:</span> <span class="token number">1.0000</span>ACC<span class="token punctuation">:</span> <span class="token number">1.0000</span>Recall<span class="token punctuation">:</span> <span class="token number">1.0000</span>F1<span class="token operator">-</span>score<span class="token punctuation">:</span> <span class="token number">1.0000</span>Precesion<span class="token punctuation">:</span> <span class="token number">1.0000</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>int64<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Yeah, 完美的模型, 完美的预测!</p><h3 id="4-3-可视化输出"><a href="#4-3-可视化输出" class="headerlink" title="4.3 可视化输出"></a>4.3 可视化输出</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#对于预测的输出有三种方式</span>?bst<span class="token punctuation">.</span>predictSignature<span class="token punctuation">:</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>data<span class="token punctuation">,</span> output_margin<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> ntree_limit<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> pred_leaf<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> pred_contribs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> approx_contribs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>pred_leaf <span class="token punctuation">:</span> bool    When this option <span class="token keyword">is</span> on<span class="token punctuation">,</span> the output will be a matrix of <span class="token punctuation">(</span>nsample<span class="token punctuation">,</span> ntrees<span class="token punctuation">)</span>    <span class="token keyword">with</span> each record indicating the predicted leaf index of each sample <span class="token keyword">in</span> each tree<span class="token punctuation">.</span>    Note that the leaf index of a tree <span class="token keyword">is</span> unique per tree<span class="token punctuation">,</span> so you may find leaf <span class="token number">1</span>    <span class="token keyword">in</span> both tree <span class="token number">1</span> <span class="token operator">and</span> tree <span class="token number">0</span><span class="token punctuation">.</span>pred_contribs <span class="token punctuation">:</span> bool    When this option <span class="token keyword">is</span> on<span class="token punctuation">,</span> the output will be a matrix of <span class="token punctuation">(</span>nsample<span class="token punctuation">,</span> nfeats<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> each record indicating the feature contributions <span class="token punctuation">(</span>SHAP values<span class="token punctuation">)</span> <span class="token keyword">for</span> that    prediction<span class="token punctuation">.</span> The sum of all feature contributions <span class="token keyword">is</span> equal to the prediction<span class="token punctuation">.</span>    Note that the bias <span class="token keyword">is</span> added <span class="token keyword">as</span> the final column<span class="token punctuation">,</span> on top of the regular features<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-3-1-得分"><a href="#4-3-1-得分" class="headerlink" title="4.3.1 得分"></a>4.3.1 得分</h4><p>默认的输出就是得分, 这没什么好说的, 直接上code.</p><pre class="line-numbers language-python"><code class="language-python">ypred <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span>ypred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[32]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>        <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>        <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>        <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>        <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出.</p><h4 id="4-3-2-所属的叶子节点"><a href="#4-3-2-所属的叶子节点" class="headerlink" title="4.3.2 所属的叶子节点"></a>4.3.2 所属的叶子节点</h4><p>当设置<code>pred_leaf=True</code>的时候, 这时就会输出每个样本在所有树中的叶子节点</p><pre class="line-numbers language-python"><code class="language-python">ypred_leaf <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">,</span> pred_leaf<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>ypred_leaf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[33]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以<code>ypred_leaf</code>的维度为<code>[100*100]</code>.</p><p>对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树).</p><p>那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型.</p><pre class="line-numbers language-python"><code class="language-python">xgb<span class="token punctuation">.</span>to_graphviz<span class="token punctuation">(</span>bst<span class="token punctuation">,</span> num_trees<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可视化第一棵树的生成情况</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://images2017.cnblogs.com/blog/957413/201710/957413-20171017204407818-1932629185.png" alt="img"></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#直接输出模型的迭代工程</span>bst<span class="token punctuation">.</span>dump_model<span class="token punctuation">(</span><span class="token string">"model.txt"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">booster<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f3<span class="token operator">&lt;</span><span class="token number">0.75</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.019697</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0214286</span>booster<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f2<span class="token operator">&lt;</span><span class="token number">2.35</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.0212184</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0212</span>booster<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f2<span class="token operator">&lt;</span><span class="token number">2.35</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.0197404</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0197235</span>booster<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">:</span> ……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值.</p><p>在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值.</p><p>同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值.</p><pre class="line-numbers language-python"><code class="language-python">leaf1   <span class="token operator">-</span><span class="token number">1.381214</span>leaf2    <span class="token number">1.410950</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了<code>'objective': 'binary:logistic'</code>, 因此使用函数将累加的值转换为实际的打分:</p><p>f(x)=1/(1+exp(−x))</p><pre class="line-numbers language-python"><code class="language-python"><span class="token number">1</span><span class="token operator">/</span>float<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1.38121416</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">0.20081407112186503</span><span class="token number">1</span><span class="token operator">/</span>float<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.410950</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">0.8039157403338895</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这就与<code>ypred = bst.predict(dtest)</code> 的分值相对应上了.</p><h4 id="4-3-2-特征重要性"><a href="#4-3-2-特征重要性" class="headerlink" title="4.3.2 特征重要性"></a>4.3.2 特征重要性</h4><p>接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性.</p><pre class="line-numbers language-python"><code class="language-python">ypred_contribs <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">,</span> pred_contribs<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>ypred_contribs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[37]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的<code>ypred_contribs</code>的维度为<code>[100,5]</code>, 通过阅读前面的文档注释就可以知道, 最后一列是<code>bias</code>, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的.</p><p>通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例.</p><pre class="line-numbers language-python"><code class="language-python">score_a <span class="token operator">=</span> sum<span class="token punctuation">(</span>ypred_contribs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> score_a<span class="token comment" spellcheck="true"># -1.38121373579</span>score_b <span class="token operator">=</span> sum<span class="token punctuation">(</span>ypred_contribs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> score_b<span class="token comment" spellcheck="true"># 1.41094945744</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相同的分值, 相同的处理情况.</p><p>到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      xgboost算法模型输出的解释
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="XGBoost" scheme="https://dataquaner.github.io/tags/XGBoost/"/>
    
  </entry>
  
  <entry>
    <title>LightGBM算法基础系列之基础理论篇（1）</title>
    <link href="https://dataquaner.github.io/2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/"/>
    <id>https://dataquaner.github.io/2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/</id>
    <published>2020-04-11T11:30:37.965Z</published>
    <updated>2020-04-11T11:30:37.965Z</updated>
    
    <content type="html"><![CDATA[<p>这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      LightGBM分类算法
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="LightGBM" scheme="https://dataquaner.github.io/tags/LightGBM/"/>
    
  </entry>
  
  <entry>
    <title>零基础自学人工智能路径规划，附资源，亲身经验</title>
    <link href="https://dataquaner.github.io/2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/"/>
    <id>https://dataquaner.github.io/2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/</id>
    <published>2020-04-11T01:25:00.000Z</published>
    <updated>2020-04-11T12:08:44.555Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>下面的每个资源都是我亲身学过的，且是网上公开公认最优质的资源。</p><p>下面的每个学习步骤也是我一步步走过来的。希望大家以我为参考，少走弯路。</p><p>请大家不要浪费时间找非常多的资料，只看最精华的！</p><p>综述，机器学习的自学简单来说分为三个步骤</p><p><strong>前期</strong>：知识储备包括数学知识，机器学习经典算法知识，编程技术（python）的掌握</p><p><strong>中期</strong>：算法的代码实现</p><p><strong>后期</strong>：实战水平提升</p><p><strong>机器学习路径规划图</strong></p><p><img src="https://pic2.zhimg.com/80/v2-513e28c795a43b69a0ae88d49d722205_hd.jpg" alt="img"></p><h1 id="1-数学基础"><a href="#1-数学基础" class="headerlink" title="1. 数学基础"></a><strong>1. 数学基础</strong></h1><p>很多人看到数学知识的时候就望而却步，数学是需要的，但是作为入门水平，对数学的要求没有那么的高。<strong><em>假设你上过大学的数学课（忘了也没事），需要的数学知识啃一啃还是基本能理解下来的。</em></strong></p><h2 id="1-1-数学内容"><a href="#1-1-数学内容" class="headerlink" title="1.1 数学内容"></a>1.1 数学内容</h2><p><strong>线性代数</strong>：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等</p><p><strong>统计与概率</strong>：概率分布，独立性与贝叶斯，最大似然(MLE)和最大后验估计(MAP)等</p><p><strong>优化</strong>：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的如梯度下降、牛顿法等</p><p><strong>微积分</strong>：偏微分，链式法则，矩阵求导等</p><p><strong>信息论</strong>、<strong>数值理论</strong>等</p><p>上面的看不太懂没事，不是特别难，学习一下就能理解了。</p><h2 id="1-2-数学资源"><a href="#1-2-数学资源" class="headerlink" title="1.2 数学资源"></a>1.2 数学资源</h2><p>网上有很多人会列举大量大量的课程资源，这是非常不负责任的事，学完那些我头发都得白了。<strong><em>实际上，我们只需要学习其中的一部分就够了。</em></strong></p><h3 id="1-2-1-吴恩达的斯坦福大学机器学习王牌课程CS229"><a href="#1-2-1-吴恩达的斯坦福大学机器学习王牌课程CS229" class="headerlink" title="1.2.1 吴恩达的斯坦福大学机器学习王牌课程CS229"></a>1.2.1 吴恩达的斯坦福大学机器学习王牌课程CS229</h3><p>课后就有对学生数学知识的要求和补充，这些数学知识是完全符合机器学习要求的，不多也不少。墙裂推荐要看，不过只有英文版的。</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1NrCAW38C9lXFqPwqTlrVRA">https://pan.baidu.com/s/1NrCAW38C9lXFqPwqTlrVRA</a><br>密码：3k3m</p><h3 id="1-2-2-深度学习的三大开山鼻祖之一Yoshua-Bengio写的深度学习（包含了机器学习）领域的教科书"><a href="#1-2-2-深度学习的三大开山鼻祖之一Yoshua-Bengio写的深度学习（包含了机器学习）领域的教科书" class="headerlink" title="1.2.2 深度学习的三大开山鼻祖之一Yoshua Bengio写的深度学习（包含了机器学习）领域的教科书"></a>1.2.2 深度学习的三大开山鼻祖之一Yoshua Bengio写的深度学习（包含了机器学习）领域的教科书</h3><p>现在以开源的形式在网上公开。这部书被誉为深度学习的圣经。在这里我们只看这本书的第一部分，也就是数学基础。囊括了机器学习所需的所有必备数学基础，而且是从最基础的说起，也不多，必读的。</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1GmmbqFewyCuEA7blXNC-7g">https://pan.baidu.com/s/1GmmbqFewyCuEA7blXNC-7g</a><br>密码：6qqm</p><h3 id="1-2-3-跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。"><a href="#1-2-3-跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。" class="headerlink" title="1.2.3 跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。"></a>1.2.3 跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。</h3><p>链接：<a href="https://zhuanlan.zhihu.com/p/25197792，知乎专栏上的一篇好文章，囊括了所有的应用知识点。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25197792，知乎专栏上的一篇好文章，囊括了所有的应用知识点。</a></p><p>好了，数学方面我只推荐上面三个资源，三个都是必看的。里面很多可能你现在看不太懂，没关系。先大概过一遍，知道自己的数学水平在哪。在看到算法知识的时候，不懂的再回来补就好。后期需要更多的数学资料我会再更新的。</p><h1 id="2-编程技术"><a href="#2-编程技术" class="headerlink" title="2 编程技术"></a><strong>2 编程技术</strong></h1><p>编程语言：python3.5及以上，python易学，这个这期先不细讲。</p><h1 id="3-经典算法知识"><a href="#3-经典算法知识" class="headerlink" title="3 经典算法知识"></a><strong>3 经典算法知识</strong></h1><p>算法包括机器学习和深度学习，机器学习是深度学习的基础。所以务必先学机器学习的经典算法，再学深度学习的算法。</p><h2 id="3-1-机器学习"><a href="#3-1-机器学习" class="headerlink" title="3.1 机器学习"></a>3.1 机器学习</h2><h3 id="3-1-1-课程资料"><a href="#3-1-1-课程资料" class="headerlink" title="3.1.1 课程资料"></a>3.1.1 课程资料</h3><p>首推吴恩达的CS229，经典中的经典，在网易公开课里有视频，翻译，课程讲义，笔记是非常非常完备的。墙裂推荐。<strong><em>这个课程对数学有一定的要求</em></strong>，但我觉得只要你上过大学的数学，然后补一下上面的数学，完全可以直接来看这个CS229。</p><p>假设你的数学真的很差的话，怎么办？吴恩达在coursera上也开了一门跟CS229完全匹配的课程，coursera机器学习课。这门课是CS229的翻版，唯一不同的是它对数学基本是没有要求了，如果你对数学真的不懂的话，那就先看这个的教程吧。它跟CS229的关系就是同样的广度，但是深度浅很多，不过你学完coursera还是要回过头来看CS229的。这个也是免费的。</p><p>CS229课程视频：<a href="https://link.zhihu.com/?target=http%3A//open.163.com/special/opencourse/machinelearning.html">http://open.163.com/special/opencourse/machinelearning.html</a></p><p>课程讲义和中文笔记：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1MC_yWjcz_m5YoZFNBcsRSQ">https://pan.baidu.com/s/1MC_yWjcz_m5YoZFNBcsRSQ</a><br>密码：6rw6</p><h3 id="3-1-2-配套书籍："><a href="#3-1-2-配套书籍：" class="headerlink" title="3.1.2 配套书籍："></a>3.1.2 配套书籍：</h3><p>机器学习实战，必看。用代码实现了一遍各大经典机器学习算法，必须看，对你理解算法有很大帮助，同时里面也有应用。<strong><em>如果大家看上面纯理论的部分太枯燥了，就可以来看看这本书来知道在现实中机器学习算法是怎么应用的，会很大程度提升你的学习兴趣</em></strong>，当初我可是看了好几遍。</p><p>书籍及课后代码：链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/15XtFOH18si316076GLKYfg">https://pan.baidu.com/s/15XtFOH18si316076GLKYfg</a><br>密码：sawb</p><p>李航《统计学习方法》，配合着看</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1Mk_O71k-H8GHeaivWbzM-Q">https://pan.baidu.com/s/1Mk_O71k-H8GHeaivWbzM-Q</a><br>密码：adep，配合着看</p><p>周志华《机器学习》，机器学习的百科全书，配合着看。</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1lJoQnWToonvBU6cYwjrRKg">https://pan.baidu.com/s/1lJoQnWToonvBU6cYwjrRKg</a><br>密码：7rzl</p><h2 id="3-2-深度学习"><a href="#3-2-深度学习" class="headerlink" title="3.2 深度学习"></a>3.2 深度学习</h2><p>说到深度学习，我们不得不提斯坦福的另一门王牌课程CS231，李飞飞教授的。这门课的课程，课后习题，堪称完美。必须必须看。整个系列下来，特别是课后的习题要做，做完之后你会发现，哇哦！它的课后习题就是写代码来实现算法的。这个在网易云课堂上有。</p><p>视频地址：<a href="https://link.zhihu.com/?target=http%3A//study.163.com/course/introduction.htm%3FcourseId%3D1004697005">http://study.163.com/course/introduction.htm?courseId=1004697005</a></p><p>课程笔记翻译，知乎专栏：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a></p><p>墙裂建议要阅读这个知乎专栏，关于怎么学这门课，这个专栏写的很清楚。</p><p>课后作业配套答案：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/bigdatadigest/article/category/7425092">https://blog.csdn.net/bigdatadigest/article/category/7425092</a></p><h2 id="3-3-学习时间"><a href="#3-3-学习时间" class="headerlink" title="3.3 学习时间"></a>3.3 学习时间</h2><p>到这里了，你的机器学习和深度学习算是入门了。学完上面这些，按一天6小时，一周六天的话，起码也得三个月吧。上面是基本功一定要认真学。但是，还找不了工作。因为你还没把这些知识应用到实际当中。</p><h2 id="3-4-实战部分"><a href="#3-4-实战部分" class="headerlink" title="3.4 实战部分"></a>3.4 实战部分</h2><h3 id="3-4-1-实战基础"><a href="#3-4-1-实战基础" class="headerlink" title="3.4.1 实战基础"></a>3.4.1 实战基础</h3><p>这一个阶段，你要开始用tensorflow（谷歌的深度学习框架）、scikit-learn（python的机器学习框架），这两个框架极大程度地集成了各大算法。其实上面在学习cs231n的时候你就会用到一部分。</p><p>scikit-learn的学习：<a href="https://link.zhihu.com/?target=http%3A//sklearn.apachecn.org/cn/0.19.0/">http://sklearn.apachecn.org/cn/0.19.0/</a></p><p>这是scikit-learn的官方文档中文版翻译，有理论有实战，最好的库学习资源，没有之一。认真看，传统的机器学习就是用这个库来实现的。</p><p>Tensorflow的学习：<a href="https://link.zhihu.com/?target=https%3A//tensorflow.google.cn/api_docs/python/%3Fhl%3Dzh-cn">https://tensorflow.google.cn/api_docs/python/?hl=zh-cn</a></p><p>官方文档很详尽，还有实战例子，学习tensorflow的不二之选</p><h3 id="3-4-2-实战进阶"><a href="#3-4-2-实战进阶" class="headerlink" title="3.4.2 实战进阶"></a>3.4.2 实战进阶</h3><p>仅仅看这两个教程是不够的，你需要更多地去应用这两个库。</p><p>接下来推荐一部神书，机器学习和深度学习的实战教学，非常非常的棒，网上有很多号称实战的书或者例子，我看了基本就是照搬官网的，只有这一本书，是完全按照工业界的流程解决方案进行实战，你不仅能学习到库的应用，还能深入了解工业界的流程解决方案，最好的实战教学书，没有之一。书名是hands-on-ml-with-sklearn-and-tf</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1x318qTHGt9oZKQwHkoUvKA">https://pan.baidu.com/s/1x318qTHGt9oZKQwHkoUvKA</a><br>密码：xssj</p><h3 id="3-4-3-实战最终阶段"><a href="#3-4-3-实战最终阶段" class="headerlink" title="3.4.3 实战最终阶段"></a>3.4.3 实战最终阶段</h3><p>kaggle数据竞赛，如果你已经学到了这一步，恭喜你离梦想越来越近了：对于我们初学者来说，没有机会接触到机器学习真正的应用项目，所以一些比赛平台是我们不错的选择。参加kaggle竞赛可以给你的简历增分不少，里面有入门级别到专家级别的实战案例，满足你的各方面需求。如果你能学到这一步了，我相信也不需要再看这个了。</p><p><strong><em>上述所有资料的合集：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1tPqsSmSMZa6qLyD0ng87IQ">https://pan.baidu.com/s/1tPqsSmSMZa6qLyD0ng87IQ</a></em></strong><br><strong><em>密码：ve75</em></strong></p><p>补充：</p><p>学到这个水平，应该是能够实习的水平了，还有很多后面再说吧。比如深度学习和机器学习的就业方向，深度学习得看论文，找工作还得对你得编程基础进行加强，具体就是数据结构与算法，我当年在这个上面可是吃了很大的亏。</p><p>这里面关于深度学习和机器学习的就业其实是两个方向，上面的其实也没有说全。一般来说，你得选择一个方向专攻。我建议的是，自学的最好在后期侧重机器学习方向，而不是深度学习。深度学习的岗位实在是太少，要求太高。机器学习还算稍微好点。</p><p><strong>重点：上面的学习路径是主要框架，但是不意味着仅仅学习这些就够了。根据每个人基础的不同，你有可能需要另外的学习资料补充。但是，我希望大家可以按照上面的主框架走，先按上面我推荐的资源学，有需要的再去看别的（我之后也会推荐），上面的我能列出来的都是最经典的，最有效率，而且我亲身学过的。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;下面的每个资源都是我亲身学过的，且是网上公开公认最优质的资源。&lt;/p&gt;
&lt;p&gt;下面的每个学习步骤也是我一步步走过来的。希
      
    
    </summary>
    
    
      <category term="LearnPath" scheme="https://dataquaner.github.io/categories/LearnPath/"/>
    
    
      <category term="LearnPath" scheme="https://dataquaner.github.io/tags/LearnPath/"/>
    
      <category term="AI" scheme="https://dataquaner.github.io/tags/AI/"/>
    
  </entry>
  
</feed>
