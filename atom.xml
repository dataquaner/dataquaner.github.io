<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DataQuaner</title>
  
  <subtitle>DataQuaner</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://dataquaner.github.io/"/>
  <updated>2020-09-13T05:19:56.268Z</updated>
  <id>https://dataquaner.github.io/</id>
  
  <author>
    <name>Leon</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【hive日常使用问题记录】Hive建表导致的ORC序列化错误</title>
    <link href="https://dataquaner.github.io/2020/09/13/hive-ri-chang-shi-yong-wen-ti-ji-lu-hive-jian-biao-dao-zhi-de-orc-xu-lie-hua-cuo-wu/"/>
    <id>https://dataquaner.github.io/2020/09/13/hive-ri-chang-shi-yong-wen-ti-ji-lu-hive-jian-biao-dao-zhi-de-orc-xu-lie-hua-cuo-wu/</id>
    <published>2020-09-13T08:00:00.000Z</published>
    <updated>2020-09-13T05:19:56.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p><strong>hive表在创建时候指定存储格式</strong></p><pre class="line-numbers language-sql"><code class="language-sql">STORED <span class="token keyword">AS</span> ORC tblproperties <span class="token punctuation">(</span><span class="token string">'orc.compress'</span><span class="token operator">=</span><span class="token string">'SNAPPY'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> <strong>当insert数据到表时抛出异常</strong></p><pre><code>Caused by: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.hive.ql.io.orc.OrcSerde$OrcSerdeRow    at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:98)    at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:743)    at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)    at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:97)    at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)    at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:115)    at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:169)    at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:561)</code></pre><p> <strong>此时查看表结构</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">desc</span> formatted persons_orc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​    <img src="https://img2018.cnblogs.com/blog/932932/201812/932932-20181218160535441-989917770.png" alt="img"></p><p><strong>可以看到SerDe Library 的格式是LazySimpleSerDe，序列化格式不是orc的,所以抛出异常</strong></p><h2 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h2><p><strong>这里将表的序列化方式修改为orc即可</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> persons_orc <span class="token keyword">SET</span> FILEFORMAT ORC<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>再看序列化格式已经是orc，使用insert(insert overwrite table persons_orc select * from persons;)插入数据可以ok</strong></p><p><img src="https://img2018.cnblogs.com/blog/932932/201812/932932-20181218160848723-111121235.png" alt="img"></p><p> 可以参考详细解释:<a href="http://www.imooc.com/article/252830" target="_blank" rel="noopener">http://www.imooc.com/article/252830</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述：&quot;&gt;&lt;a href=&quot;#问题描述：&quot; class=&quot;headerlink&quot; title=&quot;问题描述：&quot;&gt;&lt;/a&gt;问题描述：&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;hive表在创建时候指定存储格式&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;line-num
      
    
    </summary>
    
    
      <category term="-- Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="-- Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师面试资料汇总</title>
    <link href="https://dataquaner.github.io/2020/07/06/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20200705/"/>
    <id>https://dataquaner.github.io/2020/07/06/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20200705/</id>
    <published>2020-07-05T17:25:00.000Z</published>
    <updated>2020-07-05T16:21:35.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>[TOC]</p><h1 id="一-Hadoop篇"><a href="#一-Hadoop篇" class="headerlink" title="一. Hadoop篇"></a>一. Hadoop篇</h1><h2 id="1-并行计算模型MapReduce"><a href="#1-并行计算模型MapReduce" class="headerlink" title="1. 并行计算模型MapReduce"></a>1. 并行计算模型MapReduce</h2><h3 id="1-1-MapReduce-的工作原理"><a href="#1-1-MapReduce-的工作原理" class="headerlink" title="1.1 MapReduce 的工作原理"></a>1.1 MapReduce 的工作原理</h3><blockquote><p>​       MapReduce是一个基于集群的计算<strong>平台</strong>，是一个简化分布式编程的计算<strong>框架</strong>，是一个将分布式计算抽象为<strong>Map</strong>和<strong>Reduce</strong>两个阶段的编程<strong>模型</strong>。<em>（这句话记住了是可以用来装逼的）</em></p></blockquote><p><img src="http://dl.iteye.com/upload/attachment/0066/0130/e1090dee-ee98-30d1-ad55-2f88f774fa73.jpg" alt="img"></p><p><strong>执行步骤：</strong>切片&gt;分词&gt;映射&gt;分区&gt;排序&gt;聚合&gt;shuffle&gt;reduce</p><p>1）<strong>Map()阶段</strong></p><ul><li><p>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数</p></li><li><p>重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出</p></li><li><p>对输出的key、value进行分区</p></li><li><p>对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中</p></li></ul><p>2）<strong>Reduce阶段</strong></p><ul><li>多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上</li><li>对多个map的输出进行合并、排序。</li><li>重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出</li><li>把reduce的输出保存到文件中</li></ul><p><strong>特别说明：</strong></p><p><strong>切片</strong> 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理</p><p><strong>分词</strong>，<strong>映射</strong>，<strong>分区</strong>，<strong>排序</strong>，<strong>聚合</strong> 都属map阶段</p><p><strong>混洗</strong>  横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段</p><p><strong>规约</strong> 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义</p><p>分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容</p><p><strong>补充</strong></p><p><strong>切片：</strong>HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</p><p><strong>Map任务的数量</strong>：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。<strong>大多数情况下，理想的分片大小是一个HDFS块</strong></p><p><strong>Reduce任务的数量：</strong> <strong>最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</strong> 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</p><h3 id="1-2-MapReduce中shuffle工作流程及优化"><a href="#1-2-MapReduce中shuffle工作流程及优化" class="headerlink" title="1.2 MapReduce中shuffle工作流程及优化"></a>1.2 MapReduce中shuffle工作流程及优化</h3><blockquote><p>shuffle主要功能是把map task的输出结果有效地传送到reduce端。</p><p>简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p><p>前奏：</p><p><strong>1.</strong> 在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。</p><p><strong>2.</strong> 在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。前面我们知道这个job有3个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。</p></blockquote><p>主要工作流程map端分区，排序，溢写，拷贝，reduce端合并</p><h4 id="1）Map端shuffle"><a href="#1）Map端shuffle" class="headerlink" title="1）Map端shuffle"></a>1）Map端shuffle</h4><p><img src="https://img-blog.csdnimg.cn/20190705232803270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190705232832585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><ul><li><p><strong>分区Partition</strong></p><p>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。 </p></li><li><p><strong>写入内存缓冲区：</strong> 在我们的例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入<strong>内存缓冲区</strong>中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。 这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为<strong>Spill</strong>，中文可译为<strong>溢写</strong>，字面意思很直观。</p></li><li><p><strong>溢写Spill：</strong> 这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</p></li><li><p><strong>排序Sort：</strong> 当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。 </p></li><li><p><strong>合并Map端</strong>：在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节在于，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p></li><li><p><strong>CombineReduce端</strong>：在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。 </p></li></ul><h4 id="2）Reduce端shuffle"><a href="#2）Reduce端shuffle" class="headerlink" title="2）Reduce端shuffle"></a>2）Reduce端shuffle</h4><p><strong>1.</strong> <strong>Copy过程</strong>，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 </p><p><strong>2.</strong> <strong>Merge阶段</strong>。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。</p><p>增加combiner，压缩溢写的文件。</p><p><strong>3.</strong> <strong>Reducer的输入文件</strong>。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。至于怎样才能让这个文件出现在内存中，之后的<a href="http://langyu.iteye.com/blog/1341267" target="_blank" rel="noopener">性能优化篇</a>我再说。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。 </p><p><img src="https://img-blog.csdnimg.cn/20190705233112741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190705233139761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="3）shuffle优化"><a href="#3）shuffle优化" class="headerlink" title="3）shuffle优化"></a>3）shuffle优化</h4><ul><li><p><strong>压缩</strong>：对数据进行压缩，减少写读数据量；</p></li><li><p><strong>减少不必要的排序</strong>：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；</p></li><li><p><strong>内存化</strong>：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；</p><p>补充</p><blockquote><p><strong>1. Map端</strong></p><p><strong>1) io.sort.mb</strong></p><p>用于map输出排序的内存缓冲区大小</p><p>类型：Int</p><p>默认：100mb</p><p>备注：如果能估算map输出大小，就可以合理设置该值来尽可能<strong>减少溢出写的次数</strong>，这对调优很有帮助。</p><p><strong>2) io.sort.spill.percent</strong></p><p>map输出排序时的spill阀值（即使用比例达到该值时，将缓冲区中的内容spill 到磁盘）</p><p>类型：float</p><p>默认：0.80</p><p><strong>3) io.sort.factor</strong></p><p>归并因子（归并时的最多合并的流数），map、reduce阶段都要用到</p><p>类型：Int</p><p>默认：10</p><p>备注：将此值增加到100是很常见的。</p><p><strong>4) min.num.spills.for.combine</strong></p><p>运行combiner所需的最少溢出写文件数（如果已指定combiner）</p><p>类型：Int</p><p>默认：3</p><p><strong>5) mapred.compress.map.output</strong></p><p>map输出是否压缩</p><p>类型：Boolean</p><p>默认：false</p><p>备注：如果map输出的数据量非常大，那么在写入磁盘时压缩数据往往是个很好的主意，因为这样会让写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量。</p><p><strong>6) mapred.map.output.compression.codec</strong></p><p>用于map输出的压缩编解码器</p><p>类型：Classname</p><p>默认：org.apache.hadoop.io.compress.DefaultCodec</p><p>备注：推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的 TeraSort作业，运行时间减少60%，且明显快于Zlib压缩。</p><p><strong>7) tasktracker.http.threads</strong></p><p>每个tasktracker的工作线程数，用于将map输出到reducer。</p><p>（注：这是集群范围的设置，不能由单个作业设置）</p><p>类型：Int</p><p>默认：40</p><p>备注：tasktracker开http服务的线程数。用于reduce拉取map输出数据，大集群可以将其设为40~50。</p><p><strong>2. reduce端</strong></p><p><strong>1) mapred.reduce.slowstart.completed.maps</strong></p><p>调用reduce之前，map必须完成的最少比例</p><p>类型：float</p><p>默认：0.05</p><p><strong>2) mapred.reduce.parallel.copies</strong></p><p>reducer在copy阶段同时从mapper上拉取的文件数</p><p>类型：int</p><p>默认：5</p><p><strong>3) mapred.job.shuffle.input.buffer.percent</strong></p><p>在shuffle的复制阶段，分配给map输出的缓冲区占堆空间的百分比</p><p>类型：float</p><p>默认：0.70</p><p><strong>4) mapred.job.shuffle.merge.percent</strong></p><p>map输出缓冲区（由mapred.job.shuffle.input.buffer.percent定义）使用比例阀值，当达到此阀值，缓冲区中的数据将会被归并然后spill 到磁盘。</p><p>类型：float</p><p>默认：0.66</p><p><strong>5) mapred.inmem.merge.threshold</strong></p><p>map输出缓冲区中文件数</p><p>类型：int</p><p>默认：1000</p><p>备注：0或小于0的数意味着没有阀值限制，溢出写将有mapred.job.shuffle.merge.percent单独控制。</p><p><strong>6) mapred.job.reduce.input.buffer.percent</strong></p><p>在reduce过程中，在内存中保存map输出的空间占整个堆空间的比例。</p><p>类型：float</p><p>默认：0.0</p><p>备注：reduce阶段开始时，内存中的map输出大小不能大于该值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存。然而，如果reducer需要的内存较少，则可以增加此值来最小化访问磁盘的次数，以提高reduce性能。</p></blockquote></li></ul><h2 id="2-分布式文件系统HDFS"><a href="#2-分布式文件系统HDFS" class="headerlink" title="2. 分布式文件系统HDFS"></a>2. 分布式文件系统HDFS</h2><h3 id="2-1-HDFS-的体系架构和读写流程"><a href="#2-1-HDFS-的体系架构和读写流程" class="headerlink" title="2.1 HDFS 的体系架构和读写流程"></a>2.1 HDFS 的体系架构和读写流程</h3><h4 id="1）体系架构"><a href="#1）体系架构" class="headerlink" title="1）体系架构"></a>1）体系架构</h4><p><img src="https://img-blog.csdn.net/20161114151112205" alt="img"></p><p><strong>采用Master-Slaver模式：</strong></p><ul><li><p>NameNode中心服务器（Master）:维护文件系统树、以及整棵树内的文件目录、负责整个数据集群的管理。</p></li><li><p>DataNode分布在不同的机架上（Slaver）：在客户端或者NameNode的调度下，存储并检索数据块，并且定期向NameNode发送所存储的数据块的列表。</p></li><li><p>客户端与NameNode获取元数据；与DataNode交互获取数据。</p></li><li><p>默认情况下，每个DataNode都保存了3个副本，其中两个保存在同一个机架的两个不同的节点上。另一个副本放在不同机架上的节点上。</p></li></ul><blockquote><p>补充</p><p>基本概念</p><p>机架：HDFS集群，由分布在多个机架上的大量DataNode组成，不同机架之间节点通过交换机通信，HDFS通过机架感知策略，使NameNode能够确定每个DataNode所属的机架ID，使用副本存放策略，来改进数据的可靠性、可用性和网络带宽的利用率。</p><p>数据块(block)：HDFS最基本的存储单元，默认为64M，用户可以自行设置大小。</p><p>元数据：指HDFS文件系统中，文件和目录的属性信息。HDFS实现时，采用了 镜像文件（Fsimage） + 日志文件（EditLog）的备份机制。文件的镜像文件中内容包括：修改时间、访问时间、数据块大小、组成文件的数据块的存储位置信息。目录的镜像文件内容包括：修改时间、访问控制权限等信息。日志文件记录的是：HDFS的更新操作。</p><p>NameNode启动的时候，会将镜像文件和日志文件的内容在内存中合并。把内存中的元数据更新到最新状态。</p><p>用户数据：HDFS存储的大部分都是用户数据，以数据块的形式存放在DataNode上。</p><p>在HDFS中，NameNode 和 DataNode之间使用TCP协议进行通信。DataNode每3s向NameNode发送一个心跳。每10次心跳后，向NameNode发送一个数据块报告自己的信息，通过这些信息，NameNode能够重建元数据，并确保每个数据块有足够的副本。</p></blockquote><h4 id="2）HDFS文件读流程"><a href="#2）HDFS文件读流程" class="headerlink" title="2）HDFS文件读流程"></a>2）HDFS文件读流程</h4><p><img src="https://img-blog.csdn.net/20161110132352177?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p>（1）客户端通过调用<strong>FileSystem</strong>的<strong>open</strong>方法获取需要读取的数据文件，对<strong>HDFS</strong>来说该<strong>FileSystem</strong>就是<strong>DistributeFileSystem</strong></p><p>（2）<strong>DistributeFileSystem</strong>通过<strong>RPC</strong>来调用<strong>NameNode</strong>，获取到要读的数据文件对应的<strong>bock</strong>存储在哪些<strong>NataNode</strong>之上</p><p>（3）客户端先到最佳位置（距离最近）的<strong>DataNode</strong>上调用<strong>FSDataInputStream</strong>的<strong>read</strong>方法，通过反复调用<strong>read</strong>方法，可以将数据从<strong>DataNode</strong>传递到客户端</p><p>（4）当读取完所有的数据之后，<strong>FSDataInputStream</strong>会关闭与<strong>DataNode</strong>的连接，然后寻找下一块的最佳位置，客户端只需要读取连续的流。</p><p>（5）一旦客户端完成读取操作之后，就对<strong>FSDataInputStream</strong>调用<strong>close</strong>方法来完成资源的关闭操作</p><h4 id="3）HDFS文件写操作"><a href="#3）HDFS文件写操作" class="headerlink" title="3）HDFS文件写操作"></a>3）HDFS文件写操作</h4><p><img src="https://img-blog.csdn.net/20161110132117505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p>（1）客户端通过调用<strong>DistributeFileSystem</strong>的<strong>create</strong>方法来创建一个文件</p><p>（2）<strong>DistributeFileSystem</strong>会对<strong>NameNode</strong>发起<strong>RPC</strong>请求，在文件系统的名称空间中创建一个新的文件，此时会进行各种检查，比如：检查要创建的文件是否已经存在，如果该文件不存在，<strong>NameNode</strong>就会为该文件创建一条元数据记录</p><p>（3）客户端调用<strong>FSDataOututStream</strong>的<strong>write</strong>方法将数据写到一个内部队列中。假设副本数为3，那么将队列中的数据写到3个副本对应的存储的DataNode上。</p><p>（4）<strong>FSDataOututStream</strong>内部维护着一个确认队列，当接收到所有<strong>DataNode</strong>确认写完的消息后，数据才会从确认队列中删除</p><p>（5）当客户端完成数据的写入后，会对数据流调用close方法来关闭相关资源</p><p>补充</p><blockquote><p>写入过程客户端奔溃怎么处理？（租约恢复）</p></blockquote><h3 id="2-2-HDFS-常用操作命令"><a href="#2-2-HDFS-常用操作命令" class="headerlink" title="2.2 HDFS 常用操作命令"></a>2.2 HDFS 常用操作命令</h3><h4 id="1）查看文件常用命令"><a href="#1）查看文件常用命令" class="headerlink" title="1）查看文件常用命令"></a>1）查看文件常用命令</h4><ul><li>命令格式<br>1.hdfs dfs -ls path 查看文件列表<br>2.hdfs dfs -lsr path 递归查看文件列表<br>3.hdfs dfs -du path 查看path下的磁盘情况，单位字节</li><li>使用示例<br>1.hdfs dfs -ls / 查看当前目录<br>2.hdfs dfs - lsr / 递归查看当前目录</li></ul><h4 id="2）创建文件夹"><a href="#2）创建文件夹" class="headerlink" title="2）创建文件夹"></a>2）创建文件夹</h4><ul><li>命令格式<br>hdfs dfs -mkdir path</li><li>使用用例<br>hdfs dfs -mkdir /user/iron<br>注：该命令可递归创建文件夹，不可重复创建，在Linux文件系统中不可见</li></ul><h4 id="3）创建文件"><a href="#3）创建文件" class="headerlink" title="3）创建文件"></a>3）创建文件</h4><ul><li>命令格式<br>hdfs dfs -touchz path</li><li>使用用例<br>hdfs dfs -touchz /user/iron/iron.txt<br>注：该命令不可递归创建文件即当该文件的上级目录不存在时无法创建该文件，可重复创建但会覆盖原有的内容</li></ul><h4 id="4）复制文件和目录"><a href="#4）复制文件和目录" class="headerlink" title="4）复制文件和目录"></a>4）复制文件和目录</h4><ul><li>命令格式<br>hdfs dfs -cp 源目录 目标目录</li><li>使用用例<br>hdfs dfs -cp /user/iron /user/iron01<br>注：该命令会将源目录的整个目录结构都复制到目标目录中<br>hdfs dfs -cp /user/iron/* /user/iron01<br>注：该命令只会将源目录中的文件及其文件夹都复制到目标目录中</li></ul><h4 id="5）移动文件和目录"><a href="#5）移动文件和目录" class="headerlink" title="5）移动文件和目录"></a>5）移动文件和目录</h4><ul><li>命令格式<br>hdfs dfs -mv 源目录 目标目录</li><li>使用用例<br>hdfs dfs -mv /user/iron /user/iron01</li></ul><h4 id="6）赋予权限"><a href="#6）赋予权限" class="headerlink" title="6）赋予权限"></a>6）赋予权限</h4><ul><li>命令格式<br>hdfs dfs -chmod [权限参数][拥有者][:[组]] path</li><li>使用用例<br>hdfs dfs -chmod 777 /user/*<br>注：该命令是将user目录下的所用文件及其文件夹（不包含子文件夹中的文件）赋予最高权限：读，写，执行<br>777表示该用户，该用户的同组用户，其他用户都具有最高权限</li></ul><h4 id="7）上传文件"><a href="#7）上传文件" class="headerlink" title="7）上传文件"></a>7）上传文件</h4><ul><li>命令格式<br>hdfs dfs -put 源文件夹 目标文件夹</li><li>使用用例<br>hdfs dfs -put /home/hadoop01/iron /user/iron01<br>注：该命令上传Linux文件系统中iron整个文件夹<br>hdfs dfs -put /home/hadoop01/iron/* /user/iron01<br>注：该命令上传Linux文件系统中iron文件夹中的所有文件（不包括文件夹）<br>类似命令：<br>hdfs dfs -copyFromLocal 源文件夹 目标文件夹 作用同put<br>hdfs dfs -moveFromLocal 源文件夹 目标文件夹 上传后删除本地</li></ul><h4 id="8）下载文件"><a href="#8）下载文件" class="headerlink" title="8）下载文件"></a>8）下载文件</h4><ul><li>命令格式<br>hdfs dfs -get源文件夹 目标文件夹</li><li>使用用例<br>hdfs dfs -get /user/iron01 /home/hadoop01/iron<br>注：该命令下载hdfs文件系统中的iron01整个文件夹到Linux文件系统中<br>hdfs dfs -get /user/iron01/* /home/hadoop01/iron<br>注：该命令下载hdfs文件系统中的iron01整个文件夹到Linux文件系统中（不包含文件夹）<br>类似命令<br>hdfs dfs -copyToLocal 源文件夹 目标文件夹 作用同get<br>hdfs dfs -moveToLocal 源文件夹 目标文件夹 get后删除源文件</li></ul><h4 id="9）查看文件内容"><a href="#9）查看文件内容" class="headerlink" title="9）查看文件内容"></a>9）查看文件内容</h4><ul><li>命令格式<br>hadoop fs -cat path 从头查看这个文件<br>hadoop fs -tail path 从尾部查看最后1K</li><li>使用用例<br>hadoop fs -cat /userjzl/home/book/1.txt<br>hadoop fs -tail /userjzl/home/book/1.txt</li></ul><h4 id="10）删除文件"><a href="#10）删除文件" class="headerlink" title="10）删除文件"></a>10）删除文件</h4><ul><li>命令格式<br>hdfs dfs -rm 目标文件<br>hdfs dfs -rmr 目标文件 递归删除（慎用）</li><li>使用用例<br>hdfs dfs -rm /user/test.txt 删除test.txt文件<br>hdfs dfs -rmr /user/testdir 递归删除testdir文件夹<br>注：rm不可以删除文件夹</li></ul><h2 id="3-通用资源管理器Yarn"><a href="#3-通用资源管理器Yarn" class="headerlink" title="3. 通用资源管理器Yarn"></a>3. 通用资源管理器Yarn</h2><h3 id="3-1-Yarn-的产生背景和架构"><a href="#3-1-Yarn-的产生背景和架构" class="headerlink" title="3.1 Yarn 的产生背景和架构"></a>3.1 Yarn 的产生背景和架构</h3><h3 id="3-2-Yarn-中的角色划分和各自的作用"><a href="#3-2-Yarn-中的角色划分和各自的作用" class="headerlink" title="3.2 Yarn 中的角色划分和各自的作用"></a>3.2 Yarn 中的角色划分和各自的作用</h3><h3 id="3-3-Yarn-的配置和常用的资源调度策略"><a href="#3-3-Yarn-的配置和常用的资源调度策略" class="headerlink" title="3.3 Yarn 的配置和常用的资源调度策略"></a>3.3 Yarn 的配置和常用的资源调度策略</h3><h3 id="3-4-Yarn-进行一次任务资源调度的过程"><a href="#3-4-Yarn-进行一次任务资源调度的过程" class="headerlink" title="3.4 Yarn 进行一次任务资源调度的过程"></a>3.4 Yarn 进行一次任务资源调度的过程</h3><h2 id="4-数据仓库工具Hive"><a href="#4-数据仓库工具Hive" class="headerlink" title="4. 数据仓库工具Hive"></a>4. 数据仓库工具Hive</h2><h4 id="4-1-Hive-和普通关系型数据库的区别"><a href="#4-1-Hive-和普通关系型数据库的区别" class="headerlink" title="4.1 Hive 和普通关系型数据库的区别"></a>4.1 Hive 和普通关系型数据库的区别</h4><ul><li>Hive和关系型数据库存储文件的系统不同, Hive使用的是HDFS(Hadoop的分布式文件系统),关系型数据则是服务器本地的文件系统。</li><li>Hive使用的计算模型是MapReduce,而关系型数据库则是自己设计的计算模型.</li><li>关系型数据库都是为实时查询业务设计的,而Hive则是为海量数据做挖掘而设计的,实时性差;实时性的区别导致Hive的应用场景和关系型数据库有很大区别。</li><li>Hive很容易扩展自己的存储能力和计算能力,这几是继承Hadoop的,而关系型数据库在这方面要比Hive差很多。</li></ul><h4 id="4-2-Hive内部表和外部表的区别"><a href="#4-2-Hive内部表和外部表的区别" class="headerlink" title="4.2 Hive内部表和外部表的区别"></a>4.2 Hive内部表和外部表的区别</h4><ul><li><strong>创建表时</strong>：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径， 不对数据的位置做任何改变。</li><li><strong>删除表时</strong>：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</li></ul><h4 id="4-3-Hive分区表和分桶表的区别"><a href="#4-3-Hive分区表和分桶表的区别" class="headerlink" title="4.3 Hive分区表和分桶表的区别"></a>4.3 Hive分区表和分桶表的区别</h4><p><strong>分区</strong>在HDFS上的表现形式是一个<strong>目录</strong>， <strong>分桶</strong>是一个单独的<strong>文件</strong></p><p><strong>分区</strong>: 细化数据管理，直接读对应目录，缩小mapreduce程序要扫描的数据量</p><p><strong>分桶</strong>： 1、提高join查询的效率（用分桶字段做连接字段）2、提高采样的效率</p><h4 id="4-4-Hive-支持哪些数据格式"><a href="#4-4-Hive-支持哪些数据格式" class="headerlink" title="4.4 Hive 支持哪些数据格式"></a>4.4 Hive 支持哪些数据格式</h4><p>可支持Text，SequenceFile，ParquetFile，ORC，RCFILE等</p><p><strong>补充</strong></p><blockquote><ul><li><p><strong>TextFile：</strong> TextFile文件不支持块压缩，默认格式，数据不做压缩，磁盘开销大，数据解析开销大。这边不做深入介绍。 </p></li><li><p><strong>RCFile：</strong> Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能，但是不支持模式演进。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</p></li><li><p><strong>ORCFile</strong>： 存储方式：数据按行分块 每块按照列存储 ，压缩快 快速列存取，效率比rcfile高,是rcfile的改良版本，相比RC能够更好的压缩，能够更快的查询，但还是不支持模式演进。</p></li><li><p><strong>Parquet：</strong> Parquet能够很好的压缩，有很好的查询性能，支持有限的模式演进。但是写速度通常比较慢。这中文件格式主要是用在Cloudera Impala上面的。</p></li></ul></blockquote><p><strong>性能对比</strong></p><ul><li><strong>读操作</strong></li></ul><img src="https://img-blog.csdn.net/20180206102745607?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl6enh5Y2o=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:50%;"><ul><li><p><strong>存储效率</strong></p><img src="https://img-blog.csdn.net/20180206102837722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl6enh5Y2o=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:50%;"></li></ul><h4 id="4-5-Hive元数据库作用及存储内容"><a href="#4-5-Hive元数据库作用及存储内容" class="headerlink" title="4.5 Hive元数据库作用及存储内容"></a>4.5 Hive元数据库作用及存储内容</h4><p>​     本质上只是用来存储hive中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。为数据库创建的目录一般在hive数据仓库目录下</p><h4 id="4-6-HiveSQL-支持的几种排序区别"><a href="#4-6-HiveSQL-支持的几种排序区别" class="headerlink" title="4.6 HiveSQL 支持的几种排序区别"></a>4.6 HiveSQL 支持的几种排序区别</h4><p><strong>1）Order By：全局排序，只有一个Reducer</strong></p><ul><li><p>使用 ORDER BY 子句排序</p><p>ASC（ascend）: 升序（默认）</p><p>DESC（descend）: 降序</p></li><li><p>ORDER BY 子句在SELECT语句的结尾</p></li><li><p>案例实操 </p><ul><li><p>查询员工信息按工资升序排列</p><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp <span class="token keyword">order</span> <span class="token keyword">by</span> sal<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>查询员工信息按工资降序排列</p><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp <span class="token keyword">order</span> <span class="token keyword">by</span> sal <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><p><strong>2）Sort By：每个MapReduce内部排序</strong><br>      Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ul><li>设置reduce个数</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>查看设置reduce个数</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>根据部门编号降序查看员工信息</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp sort <span class="token keyword">by</span> deptno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>将查询结果导入到文件中（按照部门编号降序排序）</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/opt/module/datas/sortby-result'</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp sort <span class="token keyword">by</span> deptno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>3）Distribute By：分区排序</strong><br>      Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>案例实操：</p><ul><li>先按照部门编号分区，再按照员工编号降序排序。</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/opt/module/datas/distribute-result'</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> deptno sort <span class="token keyword">by</span> empno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：</p><p>​    distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。<br>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br><strong>4）Cluster By</strong><br>​      当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><ul><li>以下两种写法等价</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp cluster <span class="token keyword">by</span> deptno<span class="token punctuation">;</span>hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> deptno sort <span class="token keyword">by</span> deptno<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</span><span class="token comment" spellcheck="true">--cluster by  ：sort by 和 distribute by的组合</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-7-Hive-的动态分区"><a href="#4-7-Hive-的动态分区" class="headerlink" title="4.7 Hive 的动态分区"></a>4.7 Hive 的动态分区</h4><blockquote><p>​        往hive分区表中插入数据时，如果需要创建的分区很多，比如以表中某个字段进行分区存储，则需要复制粘贴修改很多sql去执行，效率低。因为hive是批处理系统，<strong>所以hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。</strong></p></blockquote><ul><li><p><strong>使用动态分区表必须配置的参数</strong></p><ul><li><code>set hive.exec.dynamic.partition =true</code>（默认false）,表示开启动态分区功能；</li><li><code>set hive.exec.dynamic.partition.mode = nonstrict</code>(默认strict),表示允许所有分区都是动态的，否则必须有静态分区字段；</li></ul></li><li><p><strong>动态分区相关调优参数</strong></p><ul><li><code>set  hive.exec.max.dynamic.partitions.pernode=100</code> （默认100，一般可以设置大一点，比如1000）； 表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。</li><li><code>set hive.exec.max.dynamic.partitions =1000</code>(默认值) ；  表示一个动态分区语句可以创建的最大动态分区个数，超出报错；</li><li><code>set hive.exec.max.created.files =10000</code>(默认) 全局可以创建的最大文件个数，超出报错。</li></ul></li></ul><h4 id="4-8-Hive-MapJoin"><a href="#4-8-Hive-MapJoin" class="headerlink" title="4.8 Hive MapJoin"></a>4.8 Hive MapJoin</h4><blockquote><p>​     MapJoin是Hive的一种优化操作，其适用于小表JOIN大表的场景，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率</p></blockquote><p><strong>使用</strong></p><p><strong>方法一：</strong></p><p>在Hive0.11前，必须使用MAPJOIN来标记显示地启动该优化操作，由于其需要将小表加载进内存所以要注意小表的大小</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token comment" spellcheck="true">/*+ MAPJOIN(smalltable)*/</span>  <span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">,</span><span class="token keyword">value</span><span class="token keyword">FROM</span> smalltable <span class="token keyword">JOIN</span> bigtable <span class="token keyword">ON</span> smalltable<span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> bigtable<span class="token punctuation">.</span><span class="token keyword">key</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>方法二</strong>：</p><p>在Hive0.11后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机</p><pre class="line-numbers language-sql"><code class="language-sql">hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>默认值为true，自动开启MAPJOIN优化</p><pre class="line-numbers language-sql"><code class="language-sql">hive<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>smalltable<span class="token punctuation">.</span>filesize<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中 </p><p>注意：使用默认启动该优化的方式如果出现默名奇妙的BUG(比如MAPJOIN并不起作用),就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化</p><pre class="line-numbers language-shell"><code class="language-shell">hive.auto.convert.join=false(关闭自动MAPJOIN转换操作)hive.ignore.mapjoin.hint=false(不忽略MAPJOIN标记)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> 对于以下查询是不支持使用方法二(MAPJOIN标记)来启动该优化的</p><pre class="line-numbers language-shell"><code class="language-shell">select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                    ) firstjoin                                                              JOIN                                                                   smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是，如果使用的是方法一即没有MAPJOIN标记则以上查询语句将会被作为两个MJ执行，进一步的，如果预先知道表大小是能够被加载进内存的，则可以通过以下属性来将两个MJ合并成一个MJ</p><pre class="line-numbers language-shell"><code class="language-shell">hive.auto.convert.join.noconditionaltask：Hive在基于输入文件大小的前提下将普通JOIN转换成MapJoin，并是否将多个MJ合并成一个hive.auto.convert.join.noconditionaltask.size：多个MJ合并成一个MJ时，其表的总的大小须小于该值，同时hive.auto.convert.join.noconditionaltask必须为true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="4-9-HQL-和-SQL-有哪些常见的区别"><a href="#4-9-HQL-和-SQL-有哪些常见的区别" class="headerlink" title="4.9 HQL 和 SQL 有哪些常见的区别"></a>4.9 HQL 和 SQL 有哪些常见的区别</h4><ul><li><p><strong>总体一致</strong>：Hive-sql与SQL基本上一样，因为当初的设计目的，就是让会SQL不会编程MapReduce的也能使用Hadoop进行处理数据。</p></li><li><p><strong>区别：</strong>Hive没有delete和update。</p><ul><li><p><strong>Hive不支持等值连接</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--SQL中对两表内联可以写成：</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dual <span class="token number">a</span><span class="token punctuation">,</span>dual <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">--Hive中应为</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dual <span class="token number">a</span> <span class="token keyword">join</span> dual <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--而不是传统的格式：</span><span class="token keyword">SELECT</span> t1<span class="token number">.a1</span> <span class="token keyword">as</span> <span class="token number">c1</span><span class="token punctuation">,</span> t2<span class="token number">.b1</span> <span class="token keyword">as</span> c2FROM t1<span class="token punctuation">,</span> t2<span class="token keyword">WHERE</span> t1<span class="token number">.a2</span> <span class="token operator">=</span> t2<span class="token number">.b2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>分号字符</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如：</span><span class="token keyword">select</span> concat<span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span>concat<span class="token punctuation">(</span><span class="token string">';'</span><span class="token punctuation">,</span><span class="token keyword">key</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> dual<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--但HiveQL在解析语句时提示：</span>FAILED: Parse Error: line <span class="token number">0</span>:<span class="token operator">-</span><span class="token number">1</span> mismatched input <span class="token string">'&lt;EOF>'</span> expecting <span class="token punctuation">)</span> <span class="token operator">in</span> <span class="token keyword">function</span> specification<span class="token comment" spellcheck="true">--解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成：</span><span class="token keyword">select</span> concat<span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span>concat<span class="token punctuation">(</span><span class="token string">'\073'</span><span class="token punctuation">,</span><span class="token keyword">key</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> dual<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>IS [NOT] NULL</strong></p><pre class="line-numbers language-sql"><code class="language-sql"> <span class="token comment" spellcheck="true">--SQL中null代表空值, 值得警惕的是, </span> <span class="token comment" spellcheck="true">--在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>Hive不支持将数据插入现有的表或分区中</strong>        </p></li><li><p><strong>hive不支持INSERT INTO 表 Values（）, UPDATE, DELETE操作</strong></p></li><li><p><strong>hive支持嵌入mapreduce程序，来处理复杂的逻辑</strong><br>如：</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">FROM</span> <span class="token punctuation">(</span> <span class="token number">1</span><span class="token punctuation">.</span> MAP doctext <span class="token keyword">USING</span> <span class="token string">'python wc_mapper.py'</span> <span class="token keyword">AS</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> cnt<span class="token punctuation">)</span> <span class="token number">2</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> docs <span class="token number">3</span><span class="token punctuation">.</span> CLUSTER <span class="token keyword">BY</span> word <span class="token number">4</span><span class="token punctuation">.</span> <span class="token punctuation">)</span> <span class="token number">a</span> <span class="token number">5</span><span class="token punctuation">.</span> REDUCE word<span class="token punctuation">,</span> cnt <span class="token keyword">USING</span> <span class="token string">'python wc_reduce.py'</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--doctext: 是输入</span><span class="token comment" spellcheck="true">--word, cnt: 是map程序的输出</span><span class="token comment" spellcheck="true">--CLUSTER BY: 将wordhash后，又作为reduce程序的输入并且map程序、reduce程序可以单独使用，如：</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> <span class="token punctuation">(</span> <span class="token number">2</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> session_table <span class="token number">3</span><span class="token punctuation">.</span> <span class="token keyword">SELECT</span> sessionid<span class="token punctuation">,</span> tstamp<span class="token punctuation">,</span> <span class="token keyword">data</span> <span class="token number">4</span><span class="token punctuation">.</span> DISTRIBUTE <span class="token keyword">BY</span> sessionid SORT <span class="token keyword">BY</span> tstamp <span class="token number">5</span><span class="token punctuation">.</span> <span class="token punctuation">)</span> <span class="token number">a</span> <span class="token number">6</span><span class="token punctuation">.</span> REDUCE sessionid<span class="token punctuation">,</span> tstamp<span class="token punctuation">,</span> <span class="token keyword">data</span> <span class="token keyword">USING</span> <span class="token string">'session_reducer.sh'</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--DISTRIBUTE BY: 用于给reduce程序分配行数据</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h4 id="4-10-Hive开窗函数"><a href="#4-10-Hive开窗函数" class="headerlink" title="4.10 Hive开窗函数"></a>4.10 Hive开窗函数</h4><p>假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。</p><p><img src="https://pic3.zhimg.com/80/v2-008682fd90478af4fb84e88bccd480ee_1440w.jpg" alt="img"></p><p><strong>1. SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">/*前三次贷款的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token number">3</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token keyword">CURRENT</span> <span class="token keyword">ROW</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv1<span class="token punctuation">,</span><span class="token comment" spellcheck="true">/*历史所有贷款 累加到下一次贷款 的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token keyword">UNBOUNDED</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token number">1</span> <span class="token keyword">FOLLOWING</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv2<span class="token keyword">FROM</span> loan <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​      其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。</p><p>窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用：</p><ul><li>n PRECEDING : 前n行    n preceding</li><li>n FOLLOWING：后n行</li><li>CURRENT ROW ： 当前行</li></ul><p>如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为:</p><p>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.</p><p>窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。</p><p><strong>2. NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#将数据按name切分成10区，并返回属于第几个分区</span>NTILE<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>ROW_NUMBER<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f3</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>DENSE_RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f4</span><span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中第一个函数<a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">NTILE(10)</a>是将数据按name切分成10区，并返回属于第几个分区。</p><blockquote><p>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p><p>然后可以根据桶号，选取前或后 n分之几的数据。</p></blockquote><p>后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。</p><ul><li>ROW_NUMBER()返回的是一列连续的序号。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-0f2c6da71227f7840aea5257acf8d88b_1440w.png" alt="img"></p><ul><li>RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}.</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f0bd3985ddacd2f347151dacc24cce_1440w.png" alt="img"></p><ul><li>DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-ac60db4d8a9c658ddf17fb43f09f616d_1440w.png" alt="img"></p><p><strong>3. LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取上一笔贷款的日期,缺失默认填NULL</span>LAG<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> last_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取下一笔贷款的日期,缺失指定填'1970-1-1'</span>LEAD<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'1970-1-1'</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> next_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取最早一笔贷款的日期</span>FIRST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> first_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取新一笔贷款的日期</span>LAST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> latest_dt<span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/pelifymeng2/article/details/70313943" target="_blank" rel="noopener">LAG(n)</a>将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录</p><p><strong>4. GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdateGROUPING SETS<span class="token punctuation">(</span>substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> orderdate<span class="token punctuation">)</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。</p><p>比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5.</p><p>如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null.</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdate<span class="token keyword">with</span> CUBE<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdate<span class="token keyword">with ROLLUP</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。</p><h4 id="4-11-HiveUDF-UDAF-UDTF函数"><a href="#4-11-HiveUDF-UDAF-UDTF函数" class="headerlink" title="4.11 HiveUDF UDAF UDTF函数"></a>4.11 HiveUDF UDAF UDTF函数</h4><p><strong>UDF：一进一出</strong></p><p><strong>实现方法</strong>：</p><ol><li><p>继承UDF类</p></li><li><p>重写evaluate方法</p></li><li><p>将该java文件编译成jar</p></li><li><p>在终端输入如下命令：</p><pre class="line-numbers language-powershell"><code class="language-powershell">hive> add jar test<span class="token punctuation">.</span>jar<span class="token punctuation">;</span>hive> create temporary <span class="token keyword">function</span> function_name as <span class="token string">'com.hrj.hive.udf.UDFClass'</span><span class="token punctuation">;</span>hive> <span class="token function">select</span> function_name<span class="token punctuation">(</span>t<span class="token punctuation">.</span>col1<span class="token punctuation">)</span> <span class="token keyword">from</span> table t<span class="token punctuation">;</span>hive> drop temporary <span class="token keyword">function</span> function_name<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p><strong>UDAF：多进一出</strong></p><p><strong>实现方法</strong>: </p><ol><li><p>用户的UDAF必须继承了<code>org.apache.hadoop.hive.ql.exec.UDAF；</code></p></li><li><p>用户的UDAF必须包含至少一个实现了<code>org.apache.hadoop.hive.ql.exec</code>的静态类，诸如实现了 UDAFEvaluator</p></li><li><p>一个计算函数必须实现的5个方法的具体含义如下：</p><ul><li><strong>init()：</strong>主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。</li><li><strong>iterate()：</strong>每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。</li><li><strong>terminatePartial()：</strong>Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。</li><li><strong>merge()：</strong>Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。</li><li><strong>terminate()：</strong>Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。</li></ul></li><li><p>部分聚集结果的数据类型和最终结果的数据类型可以不同。</p></li></ol><p><strong>UDTF：一进多出</strong></p><p><strong>实现方法</strong>：</p><ol><li><p>继承<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</code></p></li><li><p><strong>initialize()</strong>：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）</p></li><li><p><strong>process：</strong>初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列      可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数</p></li><li><p>最后close()方法调用，对需要清理的方法进行清理</p></li></ol><h4 id="4-12-Hive数据倾斜问题"><a href="#4-12-Hive数据倾斜问题" class="headerlink" title="4.12 Hive数据倾斜问题"></a>4.12 Hive数据倾斜问题</h4><p><strong>0. 什么是数据倾斜</strong></p><blockquote><p>​        对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。</p><p>来源百度百科</p></blockquote><p>​        对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><p><strong>1. 数据倾斜的现象</strong></p><p>​       多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。</p><p><strong>2. 数据倾斜的影响</strong></p><p>1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：<strong>Out Of Memory</strong>。</p><p>2）<strong>运行速度慢</strong> ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。</p><p><img src="https://pic1.zhimg.com/80/v2-b26e15f4b1c3ce2f78fba64397b6fd60_1440w.jpg" alt="img"></p><p><strong><em>一个经验结论是：一般情况下，OOM的原因都是数据倾斜\</em></strong></p><p><strong>3. 如何定位数据倾斜</strong></p><p>​         数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p><p><strong>原因</strong>： 查看任务-》查看Stage-》查看代码</p><p>​        某个task执行特别慢的情况</p><p>​        某个task莫名其妙内存溢出的情况</p><p>​        查看导致数据倾斜的key的数据分布情况</p><p><img src="https://pic1.zhimg.com/80/v2-b1b26a9b5e6a1d68d9aea4d1f2bc551c_1440w.jpg" alt="img"></p><p>也可从以下几种情况考虑：</p><p>1、是不是有OOM情况出现，一般是少数内存溢出的问题</p><p>2、是不是应用运行时间差异很大，总体时间很长</p><p>3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题</p><p>4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断</p><p>5、看看代码里面是否有一些导致Shuffle的算子出现</p><p><strong>4. 数据倾斜的几种典型情况（重点）</strong></p><ul><li>数据源中的数据分布不均匀，Spark需要频繁交互</li><li>数据集中的不同Key由于分区方式，导致数据倾斜</li><li>JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</li><li>聚合操作中，数据集中的数据分布不均匀（主要）</li><li>JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</li><li>JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</li><li>数据集中少数几个key数据量很大，不重要，其他数据均匀</li></ul><p>注意：</p><ul><li><p>需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题</p></li><li><p>只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以</p></li></ul><p><strong>5. 数据倾斜的处理方法</strong></p><p>​         发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。</p><p><strong>5.1 检查数据，找出异常的key</strong></p><p>​          如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。</p><p>选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"key"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​        如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况:</p><ul><li><p>null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。</p></li><li><p>无效数据，大量重复的测试数据或是对结果影响不大的有效数据。</p></li><li><p>有效数据，业务导致的正常数据分布。</p></li></ul><p><strong>解决办法</strong><br>  第1，2种情况，直接对数据进行过滤即可。</p><p>  第3种情况则需要进行一些特殊操作，常见的有以下几种做法。</p><ul><li><p>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。</p></li><li><p>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</p></li><li><p>使用reduceByKey 代替 groupByKey</p></li><li><p>使用map join。</p><p><strong>举例</strong>：<br>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下：</p><p>将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>对数据进行 reduceByKey(func)<br>将 key + 随机值 转成 key<br>再对数据进行 reduceByKey(func)<br>tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。</p><p>tips2: 单独处理异常数据时，可以配合使用Map Join解决</p></li></ul><p><strong>5.1.1</strong> 数据源中的数据分布不均匀，Spark需要频繁交互</p><p><strong>解决方案</strong>1：避免数据源的数据倾斜</p><p><strong>实现原理</strong>：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive或者Kafka中还是会发生数据倾斜。</p><p><strong>适用情况</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>总结</strong>：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题</p><p><strong>5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜</strong></p><p><strong>解决方案1</strong>：调整并行度</p><p><strong>实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。</p><p><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。</p><p><img src="https://pic4.zhimg.com/80/v2-9a1722a9ceb6fe125f7b36715f6dcfff_1440w.jpg" alt="img"></p><p><strong>总结</strong>：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效</p><p><strong>解决方案2</strong>：</p><p><strong>缓解数据倾斜**</strong>（自定义Partitioner）**</p><p><strong>适用场景</strong>：大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p><p><strong>解决方案</strong>： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p><p><strong>优势</strong>： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong>： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><p><strong>5.2 检查Spark运行过程相关操作</strong></p><p><strong>5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</strong></p><p><strong>解决方案</strong>：Reduce side Join转变为Map side Join</p><p><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。</p><p><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。</p><p><strong>5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）</strong></p><p><strong>解决方案</strong>：两阶段聚合（局部聚合+全局聚合）</p><p><strong>适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案</p><p><strong>实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案</p><p>将相同key的数据分拆处理</p><p><img src="https://pic3.zhimg.com/80/v2-495a5fed7eb38db37d2f0bd13c45a30e_1440w.jpg" alt="img"></p><p><strong>5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</strong></p><p><strong>解决方案</strong>：为倾斜key增加随机前/后缀</p><p><strong>适用场景</strong>：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong>：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong>：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong>：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>注意</strong>：具有倾斜Key的RDD数据集中，key的数量比较少</p><p><img src="https://pic4.zhimg.com/80/v2-248b0cead5e9fb8a7b1cec840dd61b2f_1440w.jpg" alt="img"></p><p><strong>5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</strong></p><p><strong>解决方案</strong>：随机前缀和扩容RDD进行join</p><p><strong>适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。</p><p><strong>实现思路</strong>：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><p>注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀）</p><p><img src="https://pic4.zhimg.com/80/v2-fa2211e3a343d7b68e83bfe83d67f0cb_1440w.jpg" alt="img"></p><p><strong>5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀</strong></p><p><strong>解决方案</strong>：过滤少数倾斜Key</p><p><strong>适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h4 id="4-13-HiveSQL-的优化（系统参数调整、SQL-语句优化）"><a href="#4-13-HiveSQL-的优化（系统参数调整、SQL-语句优化）" class="headerlink" title="4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化）"></a>4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化）</h4><ul><li><p><strong>Hive优化目标</strong></p></li><li><ul><li>在有限的资源下，执行效率更高</li></ul></li><li><p>常见问题</p></li><li><ul><li>数据倾斜</li><li>map数设置</li><li>reduce数设置</li><li>其他</li></ul></li><li><p><strong>Hive执行</strong></p></li><li><ul><li><p>HQL –&gt; Job –&gt; Map/Reduce</p></li><li><p>执行计划</p></li><li><ul><li>explain [extended] hql</li><li>样例</li><li>select col,count(1) from test2 group by col;</li><li>explain select col,count(1) from test2 group by col;</li></ul></li></ul></li><li><p><strong>Hive表优化</strong></p></li><li><ul><li><p>分区</p></li><li><ul><li>set hive.exec.dynamic.partition=true;</li><li>set hive.exec.dynamic.partition.mode=nonstrict;</li><li>静态分区</li><li>动态分区</li></ul></li><li><p>分桶</p></li><li><ul><li>set hive.enforce.bucketing=true;</li><li>set hive.enforce.sorting=true;</li></ul></li><li><p>数据</p></li><li><ul><li>相同数据尽量聚集在一起</li></ul></li></ul></li><li><p><strong>Hive Job优化</strong></p></li><li><ul><li><p><strong>并行化执行</strong></p></li><li><ul><li>每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间</li><li>set hive.exec.parallel= true;</li><li>set hive.exec.parallel.thread.numbe=8;</li></ul></li><li><p><strong>本地化执行</strong></p></li><li><ul><li>job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB)</li><li>job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4)</li><li>job的reduce数必须为0或者1</li><li>set hive.exec.mode.local.auto=true;</li><li>当一个job满足如下条件才能真正使用本地模式:</li></ul></li><li><p><strong>job合并输入小文件</strong></p></li><li><ul><li>set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</li><li>合并文件数由mapred.max.split.size限制的大小决定</li></ul></li><li><p><strong>job合并输出小文件**</strong></p></li><li><ul><li>set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件</li><li>set hive.merge.size.per.task=64000000;合并之后的文件大小</li></ul></li><li><p><strong>JVM重利用</strong></p></li><li><ul><li>set mapred.job.reuse.jvm.num.tasks=20;</li><li>JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。</li></ul></li><li><p>压缩数据</p></li><li><ul><li>set hive.exec.compress.output=true;</li><li>set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec;</li><li>set mapred.output.compression.type=BLOCK;</li><li>set hive.exec.compress.intermediate=true;</li><li>set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</li><li>set hive.intermediate.compression.type=BLOCK;</li><li>中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式</li><li>hive查询最终的输出也可以压缩</li></ul></li></ul></li><li><p><strong>Hive Map优化</strong></p></li><li><ul><li><p>set mapred.map.tasks =10; 无效</p></li><li><p>(1)默认map个数</p></li><li><ul><li>default_num=total_size/block_size;</li></ul></li><li><p>(2)期望大小</p></li><li><ul><li>goal_num=mapred.map.tasks;</li></ul></li><li><p>(3)设置处理的文件大小</p></li><li><ul><li>split_size=max(mapred.min.split.size,block_size);</li><li>split_num=total_size/split_size;</li></ul></li><li><p>(4)计算的map个数</p></li><li><ul><li>compute_map_num=min(split_num,max(default_num,goal_num))</li></ul></li><li><p>经过以上的分析，在设置map个数的时候，可以简答的总结为以下几点：</p></li><li><ul><li>增大mapred.min.split.size的值</li><li>如果想增加map个数，则设置mapred.map.tasks为一个较大的值</li><li>如果想减小map个数，则设置mapred.min.split.size为一个较大的值</li><li>情况1：输入文件size巨大，但不是小文件</li><li>情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。</li></ul></li><li><p>map端聚合</p></li><li><ul><li>set hive.map.aggr=true;</li></ul></li><li><p>推测执行</p></li><li><ul><li>mapred.map.tasks.apeculative.execution</li></ul></li></ul></li><li><p><strong>Hive Shuffle优化</strong></p></li><li><ul><li><p><strong>Map端</strong></p></li><li><ul><li>io.sort.mb</li><li>io.sort.spill.percent</li><li>min.num.spill.for.combine</li><li>io.sort.factor</li><li>io.sort.record.percent</li></ul></li><li><p><strong>Reduce端</strong></p></li><li><ul><li>mapred.reduce.parallel.copies</li><li>mapred.reduce.copy.backoff</li><li>io.sort.factor</li><li>mapred.job.shuffle.input.buffer.percent</li><li>mapred.job.shuffle.input.buffer.percent</li><li>mapred.job.shuffle.input.buffer.percent</li></ul></li></ul></li><li><p><strong>Hive Reduce优化</strong></p></li><li><ul><li><p><strong>需要reduce操作的查询</strong></p></li><li><ul><li>group by,join,distribute by,cluster by…</li><li>order by比较特殊,只需要一个reduce</li><li>sum,count,distinct…</li><li>聚合函数</li><li>高级查询</li></ul></li><li><p><strong>推测执行</strong></p></li><li><ul><li>mapred.reduce.tasks.speculative.execution</li><li>hive.mapred.reduce.tasks.speculative.execution</li></ul></li><li><p><strong>Reduce优化</strong></p></li><li><ul><li>numRTasks = min[maxReducers,input.size/perReducer]</li><li>maxReducers=hive.exec.reducers.max</li><li>perReducer = hive.exec.reducers.bytes.per.reducer</li><li>hive.exec.reducers.max 默认 ：999</li><li>hive.exec.reducers.bytes.per.reducer 默认:1G</li><li>set mapred.reduce.tasks=10;直接设置</li><li>计算公式</li></ul></li></ul></li><li><p><strong>Hive查询操作优化</strong></p></li><li><p><strong>join优化</strong></p></li><li><ul><li>关联操作中有一张表非常小</li><li>不等值的链接操作</li><li>set hive.auto.current.join=true;</li><li>hive.mapjoin.smalltable.filesize默认值是25mb</li><li>select <code>/*+mapjoin(A)*/</code> f.a,f.b from A t join B f on (f.a=t.a)</li><li>hive.optimize.skewjoin=true;如果是Join过程出现倾斜，应该设置为true</li><li>set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化</li><li>mapjoin</li><li>简单总结下,mapjoin的使用场景:</li></ul></li><li><p><strong>Bucket join</strong></p></li><li><ul><li>两个表以相同方式划分桶</li><li>两个表的桶个数是倍数关系</li><li>crete table order(cid int,price float) clustered by(cid) into 32 buckets;</li><li>crete table customer(id int,first string) clustered by(id) into 32 buckets;</li><li>select price from order t join customer s on t.cid=s.id</li></ul></li><li><p><strong>join 优化前</strong></p></li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> m<span class="token punctuation">.</span>cid<span class="token punctuation">,</span>u<span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token keyword">order</span> m <span class="token keyword">join</span> customer u <span class="token keyword">on</span> m<span class="token punctuation">.</span>cid<span class="token operator">=</span>u<span class="token punctuation">.</span>id <span class="token keyword">where</span> m<span class="token punctuation">.</span>dt<span class="token operator">=</span><span class="token string">'2013-12-12'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><strong>join优化后</strong></li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> m<span class="token punctuation">.</span>cid<span class="token punctuation">,</span>u<span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> cid <span class="token keyword">from</span> <span class="token keyword">order</span> <span class="token keyword">where</span> dt<span class="token operator">=</span><span class="token string">'2013-12-12'</span><span class="token punctuation">)</span>m <span class="token keyword">join</span> customer u <span class="token keyword">on</span> m<span class="token punctuation">.</span>cid<span class="token operator">=</span>u<span class="token punctuation">.</span>id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p><strong>group by 优化</strong></p><p><code>hive.groupby.skewindata=true;</code>如果是group by 过程出现倾斜 应该设置为true</p><p><code>set hive.groupby.mapaggr.checkinterval=100000;-</code>-这个是group的键对应的记录条数超过这个值则会进行优化</p></li></ul><ul><li><p><strong>count distinct 优化</strong></p><ul><li><p>优化前</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> id<span class="token punctuation">)</span> <span class="token keyword">from</span> tablename<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>优化后</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token keyword">distinct</span> id <span class="token keyword">from</span> tablename<span class="token punctuation">)</span> tmp<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> tablename <span class="token keyword">group</span> <span class="token keyword">by</span> id<span class="token punctuation">)</span> tmp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>优化前</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> <span class="token number">c</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> <span class="token number">d</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>优化后</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">c</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">d</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">0</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">c</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">0</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token number">d</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">d</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span> test<span class="token punctuation">)</span>tmp1 <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><p>#二. Spark篇</p><h2 id="1-SparkCore"><a href="#1-SparkCore" class="headerlink" title="1. SparkCore"></a>1. SparkCore</h2><h3 id="1-1-Spark工作原理"><a href="#1-1-Spark工作原理" class="headerlink" title="1.1 Spark工作原理"></a>1.1 Spark工作原理</h3><h4 id="1-Spark是什么"><a href="#1-Spark是什么" class="headerlink" title="1. Spark是什么"></a>1. Spark是什么</h4><p>Spark是一种通用分布式并行计算框架。和Mapreduce最大不同就是spark是基于内存的迭代式计算。</p><blockquote><p>Spark的Job处理的中间输出结果可以保存在内存中，从而不再需要读写HDFS，除此之外，一个MapReduce 在计算过程中只有map 和reduce 两个阶段，处理之后就结束了，而在Spark的计算模型中，可以分为n阶段，因为它内存迭代式的，我们在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。 </p><p>因此<strong>Spark能更好地适用于数据挖掘与<a href="http://lib.csdn.net/base/machinelearning" target="_blank" rel="noopener">机器学习</a>等需要迭代的MapReduce的<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="noopener">算法</a></strong>。其<strong>不仅实现了MapReduce的算子map 函数和reduce函数及计算模型</strong>，还提供更为丰富的<strong>算子</strong>，如filter、join、groupByKey等。是一个用来实现快速而同用的集群计算的平台。 </p></blockquote><h4 id="2-Spark工作原理框图"><a href="#2-Spark工作原理框图" class="headerlink" title="2. Spark工作原理框图"></a>2. Spark工作原理框图</h4><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cimage-20200627101139297.png" alt="image-20200627101139297"></p><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cimage-20200627101150890.png" alt="image-20200627101150890"></p><h5 id="第一层级"><a href="#第一层级" class="headerlink" title="第一层级"></a>第一层级</h5><p>工作流程</p><p>a. 构建Spark Application的运行环境（启动SparkContext）</p><p>b. SparkContext在初始化过程中分别创建DAGScheduler作业调度和TaskScheduler任务调度两级调度模块</p><p>c. SparkContext向资源管理器（可以是Standalone、Mesos、Yarn）申请运行Executor资源；</p><p>d. 由资源管理器分配资源并启动StandaloneExecutorBackend，executor，之后向SparkContext申请Task；</p><p>e. DAGScheduler将job 划分为多个stage,并将Stage提交给TaskScheduler;</p><p>g. Task在Executor上运行，运行完毕释放所有资源。</p><h5 id="第二层级"><a href="#第二层级" class="headerlink" title="第二层级"></a>第二层级</h5><p><strong>DAGScheduler作业调度生成过程</strong></p><p><strong>DAGScheduler</strong>是一个面向stage 的作业调度器。</p><blockquote><p>作业调度模块是<strong>基于任务阶段</strong>的高层调度模块，它为每个Spark作业计算具有依赖关系的多个调度阶段（通常根据<strong>shuffle</strong>来划分），然后为每个阶段构建出一组具体的任务（通常会考虑数据的本地性等），然后以TaskSets（任务组）的形式提交给任务调度模块来具体执行。</p></blockquote><p><strong>主要三大功能</strong></p><ol><li><p><strong>接受用户提交的job</strong>。将job根据类型划分为不同的stage，记录哪些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</p></li><li><p><strong>决定每个task的最佳位置</strong>，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给<strong>TaskScheduler</strong>；</p></li><li><p><strong>重新提交shuffle输出丢失的stage给taskScheduler；</strong></p></li></ol><p><strong>DAG如何将Job划分为多个stage</strong></p><p><img src="https://img-blog.csdn.net/20180909161915933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="图解"></p><p><strong>划分依据：**</strong>宽窄依赖**。何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p><p><strong>切割规则：</strong>从后往前，遇到宽依赖就切割stage；</p><blockquote><p> Spark任务会根据<strong>RDD</strong>之间的依赖关系，形成一个<strong>DAG有向无环图</strong>，<strong>DAG</strong>会提交给<strong>DAGScheduler</strong>，<strong>DAGScheduler</strong>会把<strong>DAG</strong>划分相互依赖的多个<strong>stage</strong>，划分依据就是<strong>宽窄依赖</strong>，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以<strong>taskSet</strong>的形式提交给<strong>TaskScheduler</strong>运行，stage是由一组并行的task组成。  </p><p> <strong>一旦driver程序中出现action，就会生成一个job，比如count等</strong>，</p><p> ​     向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。</p><p> ​        每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是<strong>宽依赖</strong>，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p></blockquote><h5 id="第三层级"><a href="#第三层级" class="headerlink" title="第三层级"></a>第三层级</h5><p><strong>谈谈spark中的宽窄依赖</strong></p><p>RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="1-2-Spark的shuffle原理和过程"><a href="#1-2-Spark的shuffle原理和过程" class="headerlink" title="1.2 Spark的shuffle原理和过程"></a>1.2 Spark的shuffle原理和过程</h3><h5 id="Shuffle过程框图"><a href="#Shuffle过程框图" class="headerlink" title="Shuffle过程框图"></a>Shuffle过程框图</h5><p><img src="https://yqfile.alicdn.com/278d5baeff935b9f4339f94fc85ccab67241927b.png" alt="img"></p><p>主要逻辑如下：</p><blockquote><p>1）首先每一个MapTask会根据ReduceTask的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</p><p>2）其次MapTask产生的结果会根据设置的partition算法填充到每个bucket中。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中。</p><p>当ReduceTask启动时，它会根据自己task的id和所依赖的Mapper的id从远端或本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</p><p>这里的bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等。</p></blockquote><p>Spark shuffle可以分为两部分：Shuffle Write 和 Shuffle Fetch</p><h5 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h5><blockquote><p>由于不要求数据有序，shuffle write 的任务很简单：<strong>将数据 partition 好，并持久化</strong>。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</p></blockquote><p>shuffle write 的任务很简单，那么实现也很简单：将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。图示如下：</p><p><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/shuffle-write-no-consolidation.png" alt="shuffle write"></p><p>上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数），缓冲区被称为 bucket，其大小为<code>spark.shuffle.file.buffer.kb</code> ，默认是 32KB（Spark 1.1 版本以前是 100KB）。</p><blockquote><p>其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。</p></blockquote><p>ShuffleMapTask 的执行过程很简单：先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由<code>partitioner.partition(record.getKey()))</code>决定。每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 <strong>FileSegment</strong>。之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。</p><p>这样的实现很简单，但有几个问题：</p><ol><li><strong>产生的 FileSegment 过多。</strong>每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。</li><li><strong>缓冲区占用内存空间大。</strong>每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M <em>R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores</em> R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了<code>cores * R * 32 KB</code>。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。</li></ol><p>目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图：</p><p><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/shuffle-write-consolidation.png" alt="shuffle-write-consolidation"></p><p>可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。先执行完的 ShuffleMapTask 形成 ShuffleBlocki，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlocki’，每个 ShuffleBlock 被称为 <strong>FileSegment</strong>。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过<code>spark.shuffle.consolidateFiles=true</code>来开启。</p><h5 id="Shuffle-Fetch"><a href="#Shuffle-Fetch" class="headerlink" title="Shuffle Fetch"></a>Shuffle Fetch</h5><p>先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey：<br><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/reduceByKeyStage.png" alt="reduceByKey"></p><p>很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了：</p><ul><li>在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？</li><li>边 fetch 边处理还是一次性 fetch 完再处理？</li><li>fetch 来的数据存放到哪里？</li><li>怎么获得要 fetch 的数据的存放位置？</li></ul><blockquote><ul><li><p><strong>在什么时候 fetch？</strong>当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。理论上讲，一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 <code>spark.reducer.maxMbInFlight</code>，这里用 <strong>softBuffer</strong> 表示，默认大小为 48MB。一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。</p></li><li><p><strong>边 fetch 边处理还是一次性 fetch 完再处理？</strong>边 fetch 边处理。本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。<strong>那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？</strong>答案是使用可以 aggregate 的数据结构，比如 HashMap。每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 \ record，直接将其放进 HashMap 里面。如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 <code>func(hashMap.get(Key), Value)</code>，比如上面 WordCount 例子中的 func 就是 <code>hashMap.get(Key) ＋ Value</code>，并将 func 的结果重新 put(key) 到 HashMap 中去。这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。</p><pre class="line-numbers language-java"><code class="language-java">  <span class="token comment" spellcheck="true">// MapReduce</span>  <span class="token function">reduce</span><span class="token punctuation">(</span>K key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>V<span class="token operator">></span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>       result <span class="token operator">=</span> <span class="token function">process</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> values<span class="token punctuation">)</span>      <span class="token keyword">return</span> result      <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">// Spark</span>  <span class="token function">reduce</span><span class="token punctuation">(</span>K key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>V<span class="token operator">></span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>      result <span class="token operator">=</span> null       <span class="token keyword">for</span> <span class="token punctuation">(</span>V value <span class="token operator">:</span> values<span class="token punctuation">)</span>           result  <span class="token operator">=</span> <span class="token function">func</span><span class="token punctuation">(</span>result<span class="token punctuation">,</span> value<span class="token punctuation">)</span>      <span class="token keyword">return</span> result  <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。因此一些算法比如求平均数，在 process 里面很好实现，直接<code>sum(values)/values.length</code>，而在 Spark 中 func 可以实现<code>sum(values)</code>，但不好实现<code>/values.length</code>。更多的 func 将会在下面的章节细致分析。</p></li><li><p><strong>fetch 来的数据存放到哪里？</strong>刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。如果<code>spark.shuffle.spill = false</code>就只用内存。内存使用的是<code>AppendOnlyMap</code> ，类似 Java 的<code>HashMap</code>，内存＋磁盘使用的是<code>ExternalAppendOnlyMap</code>，如果内存空间不足时，<code>ExternalAppendOnlyMap</code>可以将 \ records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。<strong>使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？</strong>在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。</p></li><li><p><strong>怎么获得要 fetch 的数据的存放位置？</strong>在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 <code>MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)</code>，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。</p></li></ul></blockquote><h3 id="1-3-Spark的Stage划分及优化"><a href="#1-3-Spark的Stage划分及优化" class="headerlink" title="1.3 Spark的Stage划分及优化"></a>1.3 Spark的Stage划分及优化</h3><blockquote><p>窄依赖指父RDD的每一个分区最多被一个子RDD的分区所用，表现为</p><p> 一个父RDD的分区对应于一个子RDD的分区<br> 两个父RDD的分区对应于一个子RDD 的分区。<br> 宽依赖指子RDD的每个分区都要依赖于父RDD的所有分区，这是shuffle类操作</p></blockquote><p><strong>Stage:</strong></p><blockquote><p>一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。Stage的划分，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。</p><p>比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；</p><p>如果是rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage.</p><p>会根据RDD之间的依赖关系将DAG图划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中</p></blockquote><p><strong>Stage划分思路</strong></p><p>因此spark划分stage的整体思路是：<strong>从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。</strong></p><p>在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！</p><p>而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。</p><p><strong>总结</strong></p><p>map,filter为窄依赖，<br> groupbykey为宽依赖<br> 遇到一个宽依赖就分一个stage</p><h3 id="1-4-Spark和MapReduce的区别"><a href="#1-4-Spark和MapReduce的区别" class="headerlink" title="1.4 Spark和MapReduce的区别"></a>1.4 Spark和MapReduce的区别</h3><p><strong>整体对比概念</strong></p><p>Spark Shuffle 与MapReduce Shuffle的设计思想相同，但是实现细节优化方式不同。</p><blockquote><p><strong>1. 从逻辑角度来讲</strong>，Shuffle 过程就是一个 GroupByKey 的过程，两者没有本质区别。<br>只是 MapReduce 为了方便 GroupBy 存在于不同 partition 中的 key/value records，就<u>提前对 key 进行排序</u>。Spark 认为很多应用不需要对 key 排序，就默认没有在 GroupBy 的过程中对 key 排序。</p><p><strong>2. 从数据流角度讲，两者有差别。</strong><br>  MapReduce 只能从一个 Map Stage shuffle 数据，Spark 可以从多个 Map Stages shuffle 数据</p><p><strong>3 .Shuffle write/read 实现上有一些区别。</strong><br>   以前对 shuffle write/read 的分类是 <strong>sort-based</strong> 和 <strong>hash-based</strong>。MapReduce 可以说是 <strong>sort-based</strong>，shuffle write 和 shuffle read 过程都是基于key sorting 的 (buffering records + in-memory sort + on-disk external sorting)。早期的 Spark 是 hash-based，shuffle write 和 shuffle read 都使用 HashMap-like 的数据结构进行 aggregate (without key sorting)。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based (only sort partition id, without key sorting)，shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经“你中有我，我中有你”，界限已经不那么清晰。</p><p><strong>4. 从数据 fetch 与数据计算的重叠粒度来讲，两者有细微区别。</strong><br>   MapReduce 是<strong>粗粒度</strong>，reducer fetch 到的 records 先被放到 <code>shuffle buffer</code> 中休息，当 shuffle buffer 快满时，才对它们进行 combine()。而 Spark 是<strong>细粒度</strong>，可以即时将 fetch 到的 record 与 HashMap 中相同 key 的 record 进行 aggregate。</p></blockquote><blockquote><p><img src="https://pic4.zhimg.com/b5a8d3294a7c99f065896fee00f910e4_r.jpg" alt="img"></p><p>解说：<br>1、MapReduce在Map阶段完成之后数据会被写入到内存中的一个环形缓冲区（后续的分区/分组/排序在这里完成）；Spark的Map阶段完成之后直接输出到磁盘。<br>2、受第一步的影响，MapReduce输出的数据是有序的（针对单个Map数据来说）；Spark的数据是无序的（可以使用RDD算子达到排序的效果）。<br>3、MapReduce缓冲区的数据处理完之后会spill到磁盘形成一个文件，文件数量达到阈值之后将会进行merge操作，将多个小文件合并为一个大文件；Spark没有merge过程，一个Map中如果有对应多个Reduce的数据，则直接写多个磁盘文件。<br>4、MapReduce全部通过网络来获得数据；对于本地数据Spark可以直接读取</p></blockquote><h3 id="1-5-宽依赖与窄依赖区别"><a href="#1-5-宽依赖与窄依赖区别" class="headerlink" title="1.5 宽依赖与窄依赖区别"></a>1.5 宽依赖与窄依赖区别</h3><p>RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="1-6-Spark-RDD-原理"><a href="#1-6-Spark-RDD-原理" class="headerlink" title="1.6 Spark RDD 原理"></a>1.6 Spark RDD 原理</h3><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1. RDD是什么"></a>1. RDD是什么</h4><blockquote><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合</p><ul><li><strong>Dataset</strong>：就是一个集合，用于存放数据的</li><li><strong>Destributed</strong>：分布式，可以并行在集群计算</li><li><strong>Resilient</strong>：表示弹性的，弹性表示<ul><li>RDD中的数据可以存储在内存或者磁盘中；</li><li>RDD中的分区是可以改变的；</li></ul></li></ul></blockquote><ol><li><strong>A list of partitions</strong>：一个分区列表，RDD中的数据都存储在一个分区列表中</li><li><strong>A function for computing each split</strong>：作用在每一个分区中的函数</li><li><strong>A list of dependencies on other RDDs</strong>：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的</li><li><strong>Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)</strong>：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向</li><li>可选项，数据本地性，数据位置最优</li></ol><h4 id="2-RDD操作"><a href="#2-RDD操作" class="headerlink" title="2. RDD操作"></a>2. RDD操作</h4><p>​        RDD创建后就可以在RDD上进行数据处理。RDD支持两种操作：<strong>转换</strong>（transformation），即从现有的数据集创建一个新的数据集；<strong>动作</strong>（action），即在数据集上进行计算后，返回一个值给Driver程序。</p><ul><li><strong>转换</strong>（transformation）</li></ul><blockquote><p>​        RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。</p><p>​        RDD中所有的Transformation都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的Action时，这些Transformation才会真正运行。</p></blockquote><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#### map(func)**</span>返回一个新的分布式数据集，该数据集由每一个输入元素经过func函数转换后组成<span class="token comment" spellcheck="true">#### **fitler(func)**</span>返回一个新的数据集，该数据集由经过func函数计算后返回值为true的输入元素组成<span class="token comment" spellcheck="true">#### **flatMap(func)**</span>类似于map，但是每一个输入元素可以被映射为<span class="token number">0</span>或多个输出元素（因此func返回一个序列，而不是单一元素）<span class="token comment" spellcheck="true">#### **mapPartitions(func)**</span>类似于map，但独立地在RDD上每一个分片上运行，因此在类型为T的RDD上运行时，func函数类型必须是Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token operator">=</span><span class="token operator">></span>Iterator<span class="token punctuation">[</span>U<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#### **mapPartitionsWithSplit(func)**</span>类似于mapPartitons，但func带有一个整数参数表示分片的索引值。因此在类型为T的RDD上运行时，func函数类型必须是<span class="token punctuation">(</span>Int<span class="token punctuation">,</span>Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">></span>Iterator<span class="token punctuation">[</span>U<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#### **sample(withReplacement,fraction,seed)**</span>根据fraction指定的比例对数据进行采样，可以选择是否用随机数进行替换，seed用于随机数生成器种子<span class="token comment" spellcheck="true">#### **union(otherDataSet)**</span>返回一个新数据集，新数据集是由原数据集和参数数据集联合而成<span class="token comment" spellcheck="true">#### **distinct([numTasks])**</span>返回一个包含原数据集中所有不重复元素的新数据集<span class="token comment" spellcheck="true">#### **groupByKey([numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">)</span>对的数据集。注意默认情况下，只有<span class="token number">8</span>个并行任务来操作，但是可以传入一个可选的numTasks参数来改变它<span class="token comment" spellcheck="true">#### **reduceByKey(func,[numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。与groupByKey类似，reduceByKey任务的个数是可以通过第二个可选参数来设置的<span class="token comment" spellcheck="true">#### **sortByKey([[ascending],numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对数据集。升序或降序由ascending布尔参数决定<span class="token comment" spellcheck="true">#### **join(otherDataset0,[numTasks])**</span>在类型为<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>和<span class="token punctuation">(</span>K<span class="token punctuation">,</span>W<span class="token punctuation">)</span>数据集上调用，返回一个相同的key对应的所有元素在一起的<span class="token punctuation">(</span>K<span class="token punctuation">,</span><span class="token punctuation">(</span>V<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">)</span>数据集<span class="token comment" spellcheck="true">#### **cogroup(otherDataset,[numTasks])**</span>在类型为<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>和<span class="token punctuation">(</span>K<span class="token punctuation">,</span>W<span class="token punctuation">)</span>数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>W<span class="token punctuation">]</span><span class="token punctuation">)</span>元祖的数据集。这个操作也可以称为groupwith<span class="token comment" spellcheck="true">#### **cartesain(ohterDataset)**</span>笛卡尔积，在类型为T和U类型的数据集上调用，返回一个<span class="token punctuation">(</span>T<span class="token punctuation">,</span>U<span class="token punctuation">)</span>对数据集<span class="token punctuation">(</span>两两的元素对<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>动作</strong>（action）</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#### **reduce(func)**</span>通过函数func<span class="token punctuation">(</span>接收两个参数，返回一个参数<span class="token punctuation">)</span>聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的并行运行<span class="token comment" spellcheck="true">#### **collect()**</span>在驱动程序中，以数组形式返回数据集中的所有元素。通常在使用filter或者其他操作返回一个足够小的数据子集后再使用会比较有用<span class="token comment" spellcheck="true">#### **count()**</span>返回数据集元素个数<span class="token comment" spellcheck="true">#### **first()**</span>返回数据集第一个元素<span class="token punctuation">(</span>类似于take<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#### **take(n)**</span>返回一个由数据集前n个元素组成的数组注意 这个操作目前并非并行执行，而是由驱动程序计算所有的元素<span class="token comment" spellcheck="true">#### **takeSample(withReplacement,num,seed)**</span>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否由随机数替换不足的部分，seed用户指定随机数生成器种子<span class="token comment" spellcheck="true">#### **saveAsTextFile(path)**</span>将数据集的元素以textfile的形式保存到本地文件系统—HDFS或者任何其他Hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行<span class="token comment" spellcheck="true">#### **saveAsSequenceFile(path)**</span>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是本地系统、HDFS或者任何其他的Hadoop支持的文件系统。这个只限于由key<span class="token operator">-</span>value对组成，并实现了Hadoop的Writable接口，或者可以隐式的转换为Writable的RDD<span class="token punctuation">(</span>Spark包括了基本类型转换，例如Int、Double、String等<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#### **countByKey()**</span>对<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>类型的RDD有效，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Int<span class="token punctuation">)</span>对的map，表示每一个key对应的元素个数<span class="token comment" spellcheck="true">#### **foreach(func)**</span>在数据集的每一个元素上，运行函数func进行更新。通常用于边缘效果，例如更新一个叠加器，或者和外部存储系统进行交互，如HBase<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-RDD共享变量"><a href="#3-RDD共享变量" class="headerlink" title="3. RDD共享变量"></a>3. RDD共享变量</h4><p>在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：<strong>广播变量和累加器</strong>。</p><p>(1). <strong>广播变量（Broadcast Variables）</strong></p><p>– 广播变量缓存到各个节点的内存中，而不是每个 Task</p><p>– 广播变量被创建后，能在集群中运行的任何函数调用</p><p>– 广播变量是只读的，不能在被广播后修改</p><p>– 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本</p><p>val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量<br>(2). <strong>累加器</strong></p><p>​    累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值。</p><h4 id="4-RDD缓存"><a href="#4-RDD缓存" class="headerlink" title="4. RDD缓存"></a>4. RDD缓存</h4><p>Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 <strong>transformation</strong>自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。</p><p>​         Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。</p><p>​          Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li><p>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</p></li><li><p>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p></li><li><p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p></li><li><p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p></li><li><p>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</p></li><li><p>在不会使用cached RDD的时候，及时使用unpersist方法来释放它。</p></li></ul><h3 id="1-7-RDD有哪几种创建方式"><a href="#1-7-RDD有哪几种创建方式" class="headerlink" title="1.7 RDD有哪几种创建方式"></a>1.7 RDD有哪几种创建方式</h3><p>1) 使用程序中的集合创建rdd<br>2) 使用本地文件系统创建rdd<br>3) 使用hdfs创建rdd，<br>4) 基于数据库db创建rdd<br>5) 基于Nosql创建rdd，如hbase<br>6) 基于s3创建rdd，<br>7) 基于数据流，如socket创建rdd</p><h3 id="1-8-Spark的RDD-DataFrame和DataSet的区别"><a href="#1-8-Spark的RDD-DataFrame和DataSet的区别" class="headerlink" title="1.8 Spark的RDD DataFrame和DataSet的区别"></a>1.8 Spark的RDD DataFrame和DataSet的区别</h3><p><strong>RDD的优点：</strong></p><ol><li>相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</li><li>面向对象编程，直接存储的java对象，类型转化也安全</li></ol><p><strong>RDD的缺点：</strong></p><ol><li>由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦</li><li>默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁</li></ol><p><strong>DataFrame的优点：</strong></p><ol><li><p>结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表</p></li><li><p>有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。</p></li><li><p>hive兼容，支持hql、udf等</p></li></ol><p><strong>DataFrame的缺点：</strong></p><ol><li>编译时不能类型转化安全检查，运行时才能确定是否有问题</li><li>对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象</li></ol><p><strong>DateSet的优点：</strong></p><ol><li><p>DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据</p></li><li><p>和RDD一样，支持自定义对象存储</p></li><li><p>和DataFrame一样，支持结构化数据的sql查询</p></li><li><p>采用堆外内存存储，gc友好</p></li><li><p>类型转化安全，代码友好</p><p>如此回答有3个坑（容易引起面试官追问）：</p><p>1）Spark shuffle 与 MapReduce shuffle（或者Spark 与 MR 的区别）</p><p>2）Spark内存模型 </p><p>3）对gc（垃圾回收）的了解</p></li></ol><h3 id="1-10-Spark-的通信机制"><a href="#1-10-Spark-的通信机制" class="headerlink" title="1.10 Spark 的通信机制"></a>1.10 Spark 的通信机制</h3><h4 id="分布式的通信方式"><a href="#分布式的通信方式" class="headerlink" title="分布式的通信方式"></a>分布式的通信方式</h4><ul><li>RPC</li><li>RMI</li><li>JMS</li><li>EJB</li><li>Web Serivice</li></ul><h4 id="通信框架Akka"><a href="#通信框架Akka" class="headerlink" title="通信框架Akka"></a>通信框架Akka</h4><p>​       Hadoop MR中的计算框架，jobTracker和TaskTracker间是由于通过<strong>heartbeat</strong>的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色的高效的<strong>Akka</strong>和<strong>netty</strong>通信系统</p><h3 id="1-11-Spark的数据容错机制"><a href="#1-11-Spark的数据容错机制" class="headerlink" title="1.11 Spark的数据容错机制"></a>1.11 Spark的数据容错机制</h3><p>一般而言，对于分布式系统，数据集的容错性通常有两种方式：</p><p>1） <strong>数据检查点（在Spark中对应Checkpoint机制）</strong>。</p><p>2） <strong>记录数据的更新（在Spark中对应Lineage血统机制</strong>）。</p><p>对于大数据分析而言，数据检查点操作成本较高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低，同时会消耗大量存储资源。</p><p>Spark选择记录更新的方式。但更新粒度过细时，记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列记录下来，以便恢复丢失的分区。</p><h4 id="Lineage（血统）机制"><a href="#Lineage（血统）机制" class="headerlink" title="Lineage（血统）机制"></a>Lineage（血统）机制</h4><p>​      每个RDD除了包含分区信息外，还包含它从父辈RDD变换过来的步骤，以及如何重建某一块数据的信息，因此RDD的这种容错机制又称“血统”（Lineage）容错。Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做以便恢复数据。</p><p>​       相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新计算和恢复丢失的数据分区。但这种数据模型粒度较粗，因此限制了Spark的应用场景。所以可以说Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能方面的提升。</p><p>​      RDD在Lineage容错方面采用如下两种依赖来保证容错方面的性能：</p><p><strong>窄依赖（Narrow Dependeny）</strong>：窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。其中，1个父RDD分区对应1个子RDD分区，可以分为如下两种情况：</p><p> 子RDD分区与父RDD分区一一对应（如map、filter等算子）。一个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。</p><p><strong>宽依赖（Wide Dependency，源码中称为Shuffle Dependency）：</strong></p><p>宽依赖是指一个父RDD分区对应多个子RDD分区，可以分为如下两种情况：</p><p>一个父RDD对应所有子RDD分区（未经协同划分的Join）。</p><p>一个父RDD对应多个RDD分区（非全部分区）（如groupByKey）。</p><p>窄依赖与宽依赖关系如图3-10所示。</p><p>从图3-10可以看出对依赖类型的划分：根据父RDD分区是对应一个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，对于需要重新计算的子分区而言，只需要父分区的一部分数据，因此其余数据的重算就导致了冗余计算。</p><p><img src="https://yqfile.alicdn.com/f319c81b2aaeb9f7b29dfeff3ef1cd19ec64ca9b.png" alt="f319c81b2aaeb9f7b29dfeff3ef1cd19ec64ca9b"></p><p>图3-10　两种依赖关系</p><p>对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，在通过重新计算恢复数据的情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。</p><p>窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点（前面调度机制中已讲过）。</p><p>依赖关系在lineage容错中的应用总结如下：</p><p>1）窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成，并且父RDD的计算结果进行hash并传到对应节点上之后，才能计算子RDD。</p><p>2）数据丢失时，对于窄依赖，只需要重新计算丢失的那一块数据来恢复；对于宽依赖，则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖时，需要在适当的时机设置数据检查点（checkpoint机制在下节讲述）。可见Spark在容错性方面要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。</p><p>在Spark容错机制中，如果一个节点宕机了，而且运算属于窄依赖，则只要重算丢失的父RDD分区即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。更深入地来说：在窄依赖关系中，当子RDD的分区丢失，重算其父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，因此不存在冗余计算。而在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区使用，其中有一部分数据对应的是其他不需要重新计算的子RDD分区中的数据，因此在宽依赖关系下，这样计算就会产生冗余开销，这也是宽依赖开销更大的原因。为了减少这种冗余开销，通常在Lineage血统链比较长，并且含有宽依赖关系的容错中使用Checkpoint机制设置检查点。</p><h4 id="Checkpoint（检查点）机制"><a href="#Checkpoint（检查点）机制" class="headerlink" title="Checkpoint（检查点）机制"></a>Checkpoint（检查点）机制</h4><p>通过上述分析可以看出Checkpoint的本质是将RDD写入Disk来作为检查点。这种做法是为了通过lineage血统做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p><h3 id="1-13-Spark性能调优"><a href="#1-13-Spark性能调优" class="headerlink" title="1.13 Spark性能调优"></a>1.13 Spark性能调优</h3><h4 id="1-常用参数说明"><a href="#1-常用参数说明" class="headerlink" title="1) 常用参数说明"></a>1) 常用参数说明</h4><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可</span><span class="token comment" spellcheck="true">--executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时处理大批量数据时容易内存不足，再多申请一点，如6G</span><span class="token comment" spellcheck="true">--num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等</span><span class="token comment" spellcheck="true">--executor-cores 2  : 每个executor内的核数，即每个executor中的任务task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce任务的并行度是executor数目*executor中的任务数</span>yarn集群中一般有资源申请上限，如，executor<span class="token operator">-</span>memory<span class="token operator">*</span>num<span class="token operator">-</span>executors <span class="token operator">&lt;</span> 400G 等，所以调试参数时要注意这一点—<span class="token operator">-</span>spark<span class="token punctuation">.</span><span class="token keyword">default</span><span class="token punctuation">.</span>parallelism <span class="token number">200</span> ： Spark作业的默认为<span class="token number">500</span><span class="token operator">~</span><span class="token number">1000</span>个比较合适<span class="token punctuation">,</span>如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num<span class="token operator">-</span>executors <span class="token operator">*</span> executor<span class="token operator">-</span>cores的<span class="token number">2</span><span class="token operator">~</span><span class="token number">3</span>倍比较合适。<span class="token comment" spellcheck="true">-- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor内存中能占的最大比例。默认值是0.6</span>—<span class="token operator">-</span>spark<span class="token punctuation">.</span>shuffle<span class="token punctuation">.</span>memoryFraction <span class="token number">0.2</span> ： 设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是<span class="token number">0.2</span>，如果shuffle聚合时使用的内存超出了这个<span class="token number">20</span><span class="token operator">%</span>的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能—<span class="token operator">-</span>spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor<span class="token operator">-</span>memory，所以会为executor额外预留一部分内存，spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead即代表这部分内存<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-Spark常用编程建议"><a href="#2-Spark常用编程建议" class="headerlink" title="2) Spark常用编程建议"></a>2) Spark常用编程建议</h4><ol><li><p>避免创建重复的RDD，尽量复用同一份数据。</p></li><li><p>尽量避免使用shuffle类算子，因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子</p></li><li><p>用aggregateByKey和reduceByKey替代groupByKey,因为前两个是预聚合操作，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。</p></li><li><p>repartition适用于RDD[V], partitionBy适用于RDD[K, V]</p></li><li><p>mapPartitions操作替代普通map，foreachPartitions替代foreach</p></li><li><p>filter操作之后进行coalesce操作，可以减少RDD的partition数量</p></li><li><p>如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER</p></li><li><p>尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作</p></li><li><p>多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow</p></li><li><p>spark中的Group/join/XXXByKey等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数</p></li><li><p>尽量保证每轮Stage里每个task处理的数据量&gt;128M</p></li><li><p>如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲</p></li><li><p>2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD)</p></li><li><p>若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores=1， executor-memory=m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores=size/m(向上取整)</p></li><li><p>如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以hash join的方式实现，具体原理参考下一节的shuffle过程</p></li><li><p>如果需要在repartition重分区之后还要进行排序，可直接使用repartitionAndSortWithinPartitions，比分解操作效率高，因为它可以一边shuffle一边排序</p></li></ol><h4 id="3-shuffle性能优化"><a href="#3-shuffle性能优化" class="headerlink" title="3) shuffle性能优化"></a>3) shuffle性能优化</h4><p><strong>3.1 什么是shuffle操作</strong></p><p>spark中的shuffle操作功能：将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join操作，类似洗牌的操作。这些分布在各个存储节点上的数据重新打乱然后汇聚到不同节点的过程就是shuffle过程。</p><p><strong>3.2 哪些操作中包含shuffle操作</strong></p><p>RDD的特性是不可变的带分区的记录集合，Spark提供了Transformation和Action两种操作RDD的方式。Transformation是生成新的RDD，包括map, flatMap, filter, union, sample, join, groupByKey, cogroup, ReduceByKey, cros, sortByKey, mapValues等；Action只是返回一个结果，包括collect，reduce，count，save，lookupKey等</p><p>Spark所有的算子操作中是否使用shuffle过程要看计算后对应多少分区：</p><ul><li>若一个操作执行过程中，结果RDD的每个分区只依赖上一个RDD的同一个分区，即属于窄依赖，如map、filter、union等操作，这种情况是不需要进行shuffle的，同时还可以按照pipeline的方式，把一个分区上的多个操作放在同一个Task中进行</li><li>若结果RDD的每个分区需要依赖上一个RDD的全部分区，即属于宽依赖，如repartition相关操作（repartition，coalesce）、*ByKey操作（groupByKey，ReduceByKey，combineByKey、aggregateByKey等）、join相关操作（cogroup，join）、distinct操作，这种依赖是需要进行shuffle操作的</li></ul><p><strong>3.3 shuffle操作过程</strong></p><p>shuffle过程分为shuffle write和shuffle read两部分</p><ul><li>shuffle write： 分区数由上一阶段的RDD分区数控制，shuffle write过程主要是将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上（当前stage结束之后，每个task处理的数据按key进行分类，数据先写入内存缓冲区，缓冲区满，溢写spill到磁盘文件，最终相同key被写入同一个磁盘文件）创建的磁盘文件数量=当前stage中task数量*下一个stage的task数量</li><li>shuffle read：从上游stage的所有task节点上拉取属于自己的磁盘文件，每个read task会有自己的buffer缓冲，每次只能拉取与buffer缓冲相同大小的数据，然后聚合，聚合完一批后拉取下一批，边拉取边聚合。分区数由Spark提供的一些参数控制，如果这个参数值设置的很小，同时shuffle read的数据量很大，会导致一个task需要处理的数据非常大，容易发生JVM crash，从而导致shuffle数据失败，同时executor也丢失了，就会看到Failed to connect to host 的错误(即executor lost)。</li></ul><p>shuffle过程中，各个节点会通过shuffle write过程将相同key都会先写入本地磁盘文件中，然后其他节点的shuffle read过程通过网络传输拉取各个节点上的磁盘文件中的相同key。这其中大量数据交换涉及到的网络传输和文件读写操作是shuffle操作十分耗时的根本原因</p><p><strong>3.4 spark的shuffle类型</strong></p><p>参数spark.shuffle.manager用于设置ShuffleManager的类型。Spark1.5以后，该参数有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2以前的默认值，Spark1.2之后的默认值都是SortShuffleManager。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</p><p>由于SortShuffleManager默认会对数据进行排序，因此如果业务需求中需要排序的话，使用默认的SortShuffleManager就可以；但如果不需要排序，可以通过bypass机制或设置HashShuffleManager避免排序，同时也能提供较好的磁盘读写性能。</p><p>HashShuffleManager流程：</p><p><img src="https://pic3.zhimg.com/80/v2-ac4da726d0fd470fa4a058b750d79ba6_1440w.jpg" alt="img"></p><p>SortShuffleManager流程：</p><p><img src="https://pic2.zhimg.com/80/v2-6e277c1ca522102acf693b2fe6e36775_1440w.jpg" alt="img"></p><p><strong>3.5 如何开启bypass机制</strong></p><p>bypass机制通过参数spark.shuffle.sort.bypassMergeThreshold设置，默认值是200，表示当ShuffleManager是SortShuffleManager时，若shuffle read task的数量小于这个阈值（默认200）时，则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式写数据，但最后会将每个task产生的所有临时磁盘文件合并成一个文件，并创建索引文件。</p><p>这里给出的调优建议是，当使用SortShuffleManager时，如果的确不需要排序，可以将这个参数值调大一些，大于shuffle read task的数量。那么此时就会自动开启bypass机制，map-side就不会进行排序了，减少排序的性能开销，提升shuffle操作效率。但这种方式并没有减少shuffle write过程产生的磁盘文件数量，所以写的性能没有改变。</p><p><strong>3.6 HashShuffleManager优化建议</strong></p><p>如果使用HashShuffleManager，可以设置spark.shuffle.consolidateFiles参数。该参数默认为false，只有当使用HashShuffleManager且该参数设置为True时，才会开启consolidate机制，大幅度合并shuffle write过程产生的输出文件，对于shuffle read task 数量特别多的情况下，可以极大地减少磁盘IO开销，提升shuffle性能。参考社区同学给出的数据，consolidate性能比开启bypass机制的SortShuffleManager高出10% ~ 30%。</p><p><strong>3.7 shuffle调优建议</strong></p><p>除了上述的几个参数调优，shuffle过程还有一些参数可以提高性能：</p><pre class="line-numbers language-text"><code class="language-text">- spark.shuffle.file.buffer : 默认32M，shuffle Write阶段写文件时的buffer大小，若内存资源比较充足，可适当将其值调大一些（如64M），减少executor的IO读写次数，提高shuffle性能- spark.shuffle.io.maxRetries ： 默认3次，Shuffle Read阶段取数据的重试次数，若shuffle处理的数据量很大，可适当将该参数调大。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3.8 shuffle操作过程中的常见错误</p><p>SparkSQL中的shuffle错误：</p><pre class="line-numbers language-text"><code class="language-text">org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0org.apache.spark.shuffle.FetchFailedException:Failed to connect to hostname/192.168.xx.xxx:50268<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>RDD中的shuffle错误：</p><pre class="line-numbers language-text"><code class="language-text">WARN TaskSetManager: Lost task 17.1 in stage 4.1 (TID 1386, spark050013): java.io.FileNotFoundException: /data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/2f/temp_shuffle_e22e013a-5392-4edb-9874-a196a1dad97c (没有那个文件或目录)FetchFailed(BlockManagerId(6083b277-119a-49e8-8a49-3539690a2a3f-S155, spark050013, 8533), shuffleId=1, mapId=143, reduceId=3, message=org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/0e/shuffle_1_143_0.data, offset=997061, length=112503}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>处理shuffle类操作的注意事项：</p><ul><li>减少shuffle数据量：在shuffle前过滤掉不必要的数据，只选取需要的字段处理</li><li>针对SparkSQL和DataFrame的join、group by等操作：可以通过 spark.sql.shuffle.partitions控制分区数，默认设置为200，可根据shuffle的量以及计算的复杂度提高这个值，如2000等</li><li>RDD的join、group by、reduceByKey等操作：通过spark.default.parallelism控制shuffle read与reduce处理的分区数，默认为运行任务的core总数，官方建议为设置成运行任务的core的2~3倍</li><li>提高executor的内存：即spark.executor.memory的值</li><li>分析数据验证是否存在数据倾斜的问题：如空值如何处理，异常数据（某个key对应的数据量特别大）时是否可以单独处理，可以考虑自定义数据分区规则，如何自定义可以参考下面的join优化环节</li></ul><h4 id="4-join性能优化"><a href="#4-join性能优化" class="headerlink" title="4) join性能优化"></a>4) join性能优化</h4><p>Spark所有的操作中，join操作是最复杂、代价最大的操作，也是大部分业务场景的性能瓶颈所在。所以针对join操作的优化是使用spark必须要学会的技能。</p><p>spark的join操作也分为Spark SQL的join和Spark RDD的join。</p><p><strong>4.1 Spark SQL 的join操作</strong></p><p>4.1.1 Hash Join</p><p>Hash Join的执行方式是先将小表映射成Hash Table的方式，再将大表使用相同方式映射到Hash Table，在同一个hash分区内做join匹配。</p><p>hash join又分为broadcast hash join和shuffle hash join两种。其中Broadcast hash join，顾名思义，就是把小表广播到每一个节点上的内存中，大表按Key保存到各个分区中，小表和每个分区的大表做join匹配。这种情况适合一个小表和一个大表做join且小表能够在内存中保存的情况。如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-e738483e795e49c5c58a943c0d914e9f_1440w.jpg" alt="img"></p><p>当Hash Join不能适用的场景就需要Shuffle Hash Join了，Shuffle Hash Join的原理是按照join Key分区，key相同的数据必然分配到同一分区中，将大表join分而治之，变成小表的join，可以提高并行度。执行过程也分为两个阶段：</p><ul><li>shuffle阶段：分别将两个表按照join key进行分区，将相同的join key数据重分区到同一节点</li><li>hash join阶段：每个分区节点上的数据单独执行单机hash join算法</li></ul><p>Shuffle Hash Join的过程如下图所示：</p><p><img src="https://pic2.zhimg.com/80/v2-f1b93fcb9091521f24aa2c26ff31c771_1440w.jpg" alt="img"></p><p>4.1.2 Sort-Merge Join</p><p>SparkSQL针对两张大表join的情况提供了全新的算法——Sort-merge join，整个过程分为三个步骤：</p><ul><li>Shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式进行处理</li><li>sort阶段：对单个分区节点的两表数据，分别进行排序</li><li>merge阶段：对排好序的两张分区表数据执行join操作。分别遍历两个有序序列，遇到相同的join key就merge输出，否则继续取更小一边的key，即合并两个有序列表的方式。</li></ul><p>sort-merge join流程如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-63765c1a2359fce326413ac546cefd3e_1440w.jpg" alt="img"></p><p><strong>4.2 Spark RDD的join操作</strong></p><p>Spark的RDD join没有上面这么多的分类，但是面临的业务需求是一样的。如果是大表join小表的情况，则可以将小表声明为broadcast变量，使用map操作快速实现join功能，但又不必执行Spark core中的join操作。</p><p>如果是两个大表join，则必须依赖Spark Core中的join操作了。Spark RDD Join的过程可以自行阅读源码了解，这里只做一个大概的讲解。</p><p>spark的join过程中最核心的函数是cogroup方法，这个方法中会判断join的两个RDD所使用的partitioner是否一样，如果分区相同，即存在OneToOneDependency依赖，不用进行hash分区，可直接join；如果要关联的RDD和当前RDD的分区不一致时，就要对RDD进行重新hash分区，分到正确的分区中，即存在ShuffleDependency，需要先进行shuffle操作再join。因此提升join效率的一个思路就是使得两个RDD具有相同的partitioners。</p><p>所以针对Spark RDD的join操作的优化建议是：</p><ul><li>如果需要join的其中一个RDD比较小，可以直接将其存入内存，使用broadcast hash join</li><li>在对两个RDD进行join操作之前，使其使用同一个partitioners，避免join操作的shuffle过程</li><li>如果两个RDD其一存在重复的key也会导致join操作性能变低，因此最好先进行key值的去重处理</li></ul><p><strong>4.3 数据倾斜优化</strong></p><p>均匀数据分布的情况下，前面所说的优化建议就足够了。但存在数据倾斜时，仍然会有性能问题。主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。</p><p>shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。</p><p>4.3.1 分析数据分布</p><p>如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。</p><p>4.3.2 数据倾斜的解决方案</p><p>这里参考美团技术博客中给出的几个方案。</p><p>1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。</p><p>2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key</p><p>3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量</p><p>4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。</p><p>5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。</p><p>6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作</p><h4 id="5-其他错误总结"><a href="#5-其他错误总结" class="headerlink" title="5) 其他错误总结"></a>5) 其他错误总结</h4><p>(1) 报错信息</p><pre class="line-numbers language-text"><code class="language-text">java.lang.OutOfMemory, unable to create new native thread Caused by: java.lang.OutOfMemoryError: unable to create new native thread         at java.lang.Thread.start0(Native Method)         at java.lang.Thread.start(Thread.java:640) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>解决方案：</p><p>上面这段错误提示的本质是Linux操作系统无法创建更多进程，导致出错，并不是系统的内存不足。因此要解决这个问题需要修改Linux允许创建更多的进程，就需要修改Linux最大进程数</p><p>（2）报错信息</p><p>由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实就是将/tmp目录挂载到内存当中, 那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误<br>No Space Left on the device（Shuffle临时文件过多）</p><p>解决方案：</p><p>修改配置文件spark-env.sh,把临时文件引入到一个自定义的目录中去, 即:</p><p>export SPARK_LOCAL_DIRS=/home/utoken/datadir/spark/tmp</p><p>（3）报错信息</p><p>Worker节点中的work目录占用许多磁盘空间, 这些是Driver上传到worker的文件, 会占用许多磁盘空间</p><p>解决方案：</p><p>需要定时做手工清理work目录</p><p>（4）spark-shell提交Spark Application如何解决依赖库</p><p>解决方案：</p><p>利用–driver-class-path选项来指定所依赖的jar文件，注意的是–driver-class-path后如果需要跟着多个jar文件的话，jar文件之间使用冒号:来分割。</p><p>（5）内存不足或数据倾斜导致Executor Lost，shuffle fetch失败，Task重试失败等（spark-submit提交）</p><pre class="line-numbers language-text"><code class="language-text">TaskSetManager: Lost task 1.0 in stage 6.0 (TID 100, 192.168.10.37): java.lang.OutOfMemoryError: Java heap spaceINFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.37:57139 (size: 42.0 KB, free: 24.2 MB)INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.38:53816 (size: 42.0 KB, free: 24.2 MB)INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 102, 192.168.10.37, ANY, 2152 bytes)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>解决方案：</p><p>增加worker内存，或者相同资源下增加partition数目，这样每个task要处理的数据变少，占用内存变少</p><p>如果存在shuffle过程，设置shuffle read阶段的并行数</p><h2 id="2-SparkSQL"><a href="#2-SparkSQL" class="headerlink" title="2. SparkSQL"></a>2. SparkSQL</h2><h3 id="2-1-Spark-SQL-的原理和运行机制"><a href="#2-1-Spark-SQL-的原理和运行机制" class="headerlink" title="2.1 Spark SQL 的原理和运行机制"></a>2.1 Spark SQL 的原理和运行机制</h3><img src="https://pic2.zhimg.com/80/v2-9338f9e4a1d5ce568f47097f5b56f285_1440w.jpg" alt="img" style="zoom:50%;"><p>从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作</p><ul><li>Parser 解析 SQL，生成 Unresolved Logical Plan</li><li>由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan</li><li>Optimizer根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan</li><li>Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan</li><li>CBO 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan</li><li>Spark 以 DAG 的方法执行上述 Physical Plan</li><li>在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率</li></ul><p><strong>Parser</strong></p><p>Spark SQL 使用 Antlr 进行记法和语法解析，并生成 UnresolvedPlan。</p><p>当用户使用 SparkSession.sql(sqlText : String) 提交 SQL 时，SparkSession 最终会调用 SparkSqlParser 的 parsePlan 方法。该方法分两步</p><ul><li>使用 Antlr 生成的 SqlBaseLexer 对 SQL 进行词法分析，生成 CommonTokenStream</li><li>使用 Antlr 生成的 SqlBaseParser 进行语法分析，得到 LogicalPlan</li></ul><p><strong>Analyzer</strong></p><p>从 Analyzer 的构造方法可见</p><ul><li><p>Analyzer 持有一个 SessionCatalog 对象的引用</p></li><li><p>Analyzer 继承自 RuleExecutor[LogicalPlan]，因此可对 LogicalPlan 进行转换</p><p><strong>Optimizer</strong></p></li></ul><p>Spark SQL 目前的优化主要是基于规则的优化，即 RBO （Rule-based optimization）</p><ul><li>每个优化以 Rule 的形式存在，每条 Rule 都是对 Analyzed Plan 的等价转换</li><li>RBO 设计良好，易于扩展，新的规则可以非常方便地嵌入进 Optimizer</li><li>RBO 目前已经足够好，但仍然需要更多规则来 cover 更多的场景</li><li>优化思路主要是减少参与计算的数据量以及计算本身的代价</li></ul><p><strong>PushdownPredicate</strong><br>PushdownPredicate 是最常见的用于减少参与计算的数据量的方法。</p><p><strong>SparkPlanner</strong></p><p>得到优化后的 LogicalPlan 后，SparkPlanner 将其转化为 SparkPlan 即物理计划。</p><p>本例中由于 score 表数据量较小，Spark 使用了 BroadcastJoin。因此 score 表经过 Filter 后直接使用 BroadcastExchangeExec 将数据广播出去，然后结合广播数据对 people 表使用 BroadcastHashJoinExec 进行 Join。再经过 Project 后使用 HashAggregateExec 进行分组聚合。</p><p>至此，一条 SQL 从提交到<strong>解析</strong>、<strong>分析</strong>、<strong>优化</strong>以及执行的完整过程就介绍完毕。</p><h3 id="2-3-Spark-SQL-的优化策略"><a href="#2-3-Spark-SQL-的优化策略" class="headerlink" title="2.3 Spark SQL 的优化策略"></a>2.3 Spark SQL 的优化策略</h3><p><strong>1）内存列式存储与内存缓存表</strong><br> Spark SQL可以通过cacheTable将数据存储转换为列式存储，同时将数据加载到内存缓存。cacheTable相当于在分布式集群的内存物化视图，将数据缓存，这样迭代的或者交互式的查询不用再从HDFS读数据，直接从内存读取数据大大减少了I/O开销。列式存储的优势在于Spark SQL只需要读出用户需要的列，而不需要像行存储那样每次都将所有列读出，从而大大减少内存缓存数据量，更高效地利用内存数据缓存，同时减少网络传输和I/O开销。数据按照列式存储，由于是数据类型相同的数据连续存储，所以能够利用序列化和压缩减少内存空间的占用。</p><p> <strong>2）列存储压缩</strong><br> 为了减少内存和硬盘空间占用，Spark SQL采用了一些压缩策略对内存列存储数据进行压缩。Spark SQL的压缩方式要比Shark丰富很多，如它支持PassThrough、RunLengthEncoding、DictionaryEncoding、BooleanBitSet、IntDelta、LongDelta等多种压缩方式，这样能够大幅度减少内存空间占用、网络传输和I/O开销。</p><p> <strong>3）逻辑查询优化</strong><br> SparkSQL在逻辑查询优化（见图8-4）上支持列剪枝、谓词下压、属性合并等逻辑查询优化方法。列剪枝为了减少读取不必要的属性列、减少数据传输和计算开销，在查询优化器进行转换的过程中会优化列剪枝。<br> 下面介绍一个逻辑优化的例子。<br> SELECT Class FROM （SELECT ID，Name，Class  FROM STUDENT ） S WHERE S.ID=1</p><p>Catalyst将原有查询通过谓词下压，将选择操作ID=1优先执行，这样过滤大部分数据，通过属性合并将最后的投影只做一次，最终保留Class属性列。<br> <strong>4）Join优化</strong><br> Spark SQL深度借鉴传统数据库的查询优化技术的精髓，同时在分布式环境下调整和创新特定的优化策略。现在Spark SQL对Join进行了优化，支持多种连接算法，现在的连接算法已经比Shark丰富，而且很多原来Shark的元素也逐步迁移过来，如BroadcastHashJoin、BroadcastNestedLoopJoin、HashJoin、LeftSemiJoin，等等。<br> 下面介绍其中的一个Join算法。<br> BroadcastHashJoin将小表转化为广播变量进行广播，这样避免Shuffle开销，最后在分区内做Hash连接。这里使用的就是Hive中Map Side Join的思想，同时使用DBMS中的Hash连接算法做连接。 随着Spark SQL的发展，未来会有更多的查询优化策略加入进来，同时后续Spark SQL会支持像Shark Server一样的服务端和JDBC接口，兼容更多的持久化层，如NoSQL、传统的DBMS等。一个强有力的结构化大数据查询引擎正在崛起。</p><h2 id="3-SparkStreaming"><a href="#3-SparkStreaming" class="headerlink" title="3. SparkStreaming"></a>3. SparkStreaming</h2><p>3.1 原理剖析（源码级别）和运行机制</p><p>3.2 Spark Dstream 及其 API 操作</p><p>3.3 Spark Streaming 消费 Kafka 的两种方式</p><p>3.4 Spark 消费 Kafka 消息的 Offset 处理</p><p>3.5 窗口操作</p><h2 id="4-SparkMlib"><a href="#4-SparkMlib" class="headerlink" title="4. SparkMlib"></a>4. SparkMlib</h2><p>可实现聚类、分类、推荐等算法</p><h1 id="三-Flink"><a href="#三-Flink" class="headerlink" title="三. Flink"></a>三. Flink</h1><ul><li>Flink 集群的搭建</li><li>Flink 的架构原理</li><li>Flink 的编程模型</li><li>Flink 集群的 HA 配置</li><li>Flink DataSet 和 DataSteam API</li><li>序列化</li><li>Flink 累加器</li><li>状态 State 的管理和恢复</li><li>窗口和时间</li><li>并行度</li><li>Flink 和消息中间件 Kafka 的结合</li><li>Flink Table 和 SQL 的原理和用法</li></ul><h1 id="四-Kafka"><a href="#四-Kafka" class="headerlink" title="四. Kafka"></a>四. Kafka</h1><h2 id="1-Kafka-的设计"><a href="#1-Kafka-的设计" class="headerlink" title="1. Kafka 的设计"></a>1. Kafka 的设计</h2><p>Kafka 将消息以 topic 为单位进行归纳</p><p>将向 Kafka topic 发布消息的程序成为 producers.</p><p>将预订 topics 并消费消息的程序成为 consumer.</p><p>Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker.</p><p>producers 通过网络将消息发送到 Kafka 集群，集群向消费者提供消息</p><h2 id="2-数据传输的三种事务定义"><a href="#2-数据传输的三种事务定义" class="headerlink" title="2. 数据传输的三种事务定义"></a>2. 数据传输的三种事务定义</h2><p>数据传输的事务定义通常有以下三种级别：</p><p>（1）最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输</p><p>（2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.</p><p>（3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而</p><p>且仅仅被传输一次，这是大家所期望的</p><h2 id="3-Kafka-判断一个节点是否活着两大条件"><a href="#3-Kafka-判断一个节点是否活着两大条件" class="headerlink" title="3. Kafka 判断一个节点是否活着两大条件"></a>3. Kafka 判断一个节点是否活着两大条件</h2><p>（1）节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连</p><p>接</p><p>（2）如果节点是个 follower,他必须能及时的同步 leader 的写操作，延时不能太久</p><h2 id="4-Kafa-consumer-是否可以消费指定分区消息？"><a href="#4-Kafa-consumer-是否可以消费指定分区消息？" class="headerlink" title="4. Kafa consumer 是否可以消费指定分区消息？"></a>4. Kafa consumer 是否可以消费指定分区消息？</h2><p>​      Kafa consumer 消费消息时，向 broker 发出”fetch”请求去消费特定分区的消息，consumer</p><p>指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer 拥有</p><p>了 offset 的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的</p><h2 id="5-Kafka-消息是采用-Pull-模式or-Push-模式？"><a href="#5-Kafka-消息是采用-Pull-模式or-Push-模式？" class="headerlink" title="5. Kafka 消息是采用 Pull 模式or Push 模式？"></a>5. Kafka 消息是采用 Pull 模式or Push 模式？</h2><p>​          Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到</p><p>consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统</p><p>的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息</p><p>一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的</p><p>consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的</p><p>consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消</p><p>息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，</p><p>consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式</p><p>​          Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push</p><p>模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还</p><p>是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一</p><p>次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决</p><p>定这些策略</p><p>​          Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，</p><p>直到新消息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达</p><p>(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发</p><h2 id="6-Kafka-存储在硬盘上的消息格式是什么？"><a href="#6-Kafka-存储在硬盘上的消息格式是什么？" class="headerlink" title="6. Kafka 存储在硬盘上的消息格式是什么？"></a>6. Kafka 存储在硬盘上的消息格式是什么？</h2><p>消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和 CRC32</p><p>校验码。</p><ul><li>消息长度: 4 bytes (value: 1+4+n)</li><li>版本号: 1 byte</li><li>CRC 校验码: 4 bytes</li><li>具体的消息: n bytes</li></ul><h2 id="7-Kafka-高效文件存储设计特点"><a href="#7-Kafka-高效文件存储设计特点" class="headerlink" title="7. Kafka 高效文件存储设计特点"></a>7. Kafka 高效文件存储设计特点</h2><p>(1).Kafka 把 topic 中一个 parition 大文件分成多个小文件段，通过多个小文件段，就容易定</p><p>期清除或删除已经消费完文件，减少磁盘占用。</p><p>(2).通过索引信息可以快速定位 message 和确定 response 的最大大小。</p><p>(3).通过 index 元数据全部映射到 memory，可以避免 segment file 的 IO 磁盘操作。</p><p>(4).通过索引文件稀疏存储，可以大幅降低 index 文件元数据占用空间大小。</p><h2 id="8-Kafka-与传统消息系统之间有三个关键区别"><a href="#8-Kafka-与传统消息系统之间有三个关键区别" class="headerlink" title="8. Kafka 与传统消息系统之间有三个关键区别"></a>8. Kafka 与传统消息系统之间有三个关键区别</h2><p>(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留</p><p>(2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据</p><p>提升容错能力和高可用性</p><p>(3).Kafka 支持实时的流式处理</p><h2 id="9-Kafka-创建-Topic-时如何将分区放置到不同的-Broker-中"><a href="#9-Kafka-创建-Topic-时如何将分区放置到不同的-Broker-中" class="headerlink" title="9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中"></a>9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中</h2><ul><li>副本因子不能大于 Broker 的个数；</li><li>第一个分区（编号为 0）的第一个副本放置位置是随机从 brokerList 选择的；</li><li>其他分区的第一个副本放置位置相对于第 0 个分区依次往后移。也就是如果我们有 5 个Broker，5 个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个Broker 上，依次类推；</li><li>剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的</li></ul><h2 id="10-Kafka-新建的分区会在哪个目录下创建"><a href="#10-Kafka-新建的分区会在哪个目录下创建" class="headerlink" title="10. Kafka 新建的分区会在哪个目录下创建"></a>10. Kafka 新建的分区会在哪个目录下创建</h2><p>在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，</p><p>这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘</p><p>上用于提高读写性能。</p><p>当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。</p><p>如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个</p><p>目录下创建文件夹用于存放数据。</p><p>但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？</p><p>答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic</p><p>名+分区 ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就</p><p>是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁</p><p>盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。</p><h2 id="11-partition-的数据如何保存到硬盘"><a href="#11-partition-的数据如何保存到硬盘" class="headerlink" title="11. partition 的数据如何保存到硬盘"></a>11. partition 的数据如何保存到硬盘</h2><p>topic 中的多个 partition 以文件夹的形式保存到 broker，每个分区序号从 0 递增，</p><p>且消息有序</p><p>Partition 文件下有多个 segment（xxx.index，xxx.log）</p><p>segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为 1g</p><p>如果大小大于 1g 时，会滚动一个新的 segment 并且以上一个 segment 最后一条消息的偏移</p><p>量命名</p><h2 id="12-kafka-的-ack-机制"><a href="#12-kafka-的-ack-机制" class="headerlink" title="12. kafka 的 ack 机制"></a>12. kafka 的 ack 机制</h2><p>request.required.acks 有三个值 0 1 -1</p><p>0:生产者不会等待 broker 的 ack，这个延迟最低但是存储的保证最弱当 server 挂掉的时候</p><p>就会丢数据</p><p>1：服务端会等待 ack 值 leader 副本确认接收到消息后发送 ack 但是如果 leader 挂掉后他</p><p>不确保是否复制完成新 leader 也会导致数据丢失</p><p>-1：同样在 1 的基础上 服务端会等所有的 follower 的副本受到数据后才会受到 leader 发出</p><p>的 ack，这样数据不会丢失</p><h2 id="13-Kafka-的消费者如何消费数据"><a href="#13-Kafka-的消费者如何消费数据" class="headerlink" title="13. Kafka 的消费者如何消费数据"></a>13. Kafka 的消费者如何消费数据</h2><p>​       消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置</p><p>等到下次消费时，他会接着上次位置继续消费</p><h2 id="14-消费者负载均衡策略"><a href="#14-消费者负载均衡策略" class="headerlink" title="14. 消费者负载均衡策略"></a>14. 消费者负载均衡策略</h2><p>​       一个消费者组中的一个分片对应一个消费者成员，他能保证每个消费者成员都能访问，如</p><p>果组中成员太多会有空闲的成员</p><h2 id="15-数据有序"><a href="#15-数据有序" class="headerlink" title="15. 数据有序"></a>15. 数据有序</h2><p>​       一个消费者组里它的内部是有序的</p><p>​       消费者组与消费者组之间是无序的</p><h2 id="16-kafaka-生产数据时数据的分组策略"><a href="#16-kafaka-生产数据时数据的分组策略" class="headerlink" title="16. kafaka 生产数据时数据的分组策略"></a>16. kafaka 生产数据时数据的分组策略</h2><p>​        生产者决定数据产生到集群的哪个 partition 中</p><p>​        每一条消息都是以（key，value）格式</p><p>​        Key 是由生产者发送数据传入</p><p>​        所以生产者（key）决定了数据产生到集群的哪个 partition</p><h1 id="五-数据仓库"><a href="#五-数据仓库" class="headerlink" title="五. 数据仓库"></a>五. 数据仓库</h1><h2 id="5-1-数仓概念相关"><a href="#5-1-数仓概念相关" class="headerlink" title="5.1 数仓概念相关"></a>5.1 数仓概念相关</h2><h3 id="1-数据仓库、数据集市、数据库之间的区别"><a href="#1-数据仓库、数据集市、数据库之间的区别" class="headerlink" title="1. 数据仓库、数据集市、数据库之间的区别"></a>1. 数据仓库、数据集市、数据库之间的区别</h3><ul><li><p><strong>数据仓库 ：</strong>数据仓库是一个面向主题的、集成的、随时间变化的、但信息本身相对稳定的数据集合，用于对管理决策过程的支持。是企业级的，能为整个企业各个部门的运行提供决策支持手段；</p></li><li><p><strong>数据集市：</strong>则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。</p></li><li><p><strong>数据库：</strong>是一种软件，用来实现数据库逻辑过程，属于物理层；</p><blockquote><p>数据仓库是数据库概念的升级，从数据量来说，数据仓库要比数据库更庞大德多，主要用于数据挖掘和数据分析，辅助领导做决策</p><p>只是数据库内的数据时限要远远的长于操作型环境中的数据时限。在操作型环境中一般只保存有60<del>90天的数据，而在数据仓库中则要需要保存较长时限的数据（例如：5</del>10年），以适应DSS进行趋势分析的要求。</p></blockquote></li></ul><h3 id="2-OLAP、OLTP概念及用途"><a href="#2-OLAP、OLTP概念及用途" class="headerlink" title="2. OLAP、OLTP概念及用途"></a>2. OLAP、OLTP概念及用途</h3><ul><li><p><strong>OLAP:</strong>  即<code>On-Line Analysis Processing</code>在线分析处理。</p><blockquote><p><strong>OLAP的特点</strong>：联机分析处理的主要特点，是直接仿照用户的多角度思考模式，预先为用户组建<strong>多维</strong>的数据模型，维指的是用户的分析角度。</p></blockquote></li><li><p><strong>OLTP:</strong>  即<code>On-Line Transaction Processing</code>联机事务处理过程(OLTP)</p><blockquote><p>OLTP的特点：结构复杂、实时性要求高。</p></blockquote><p><strong>OLAP和OLTP区别</strong></p><blockquote><p>1、<strong>基本含义不同</strong>：OLTP是传统的关系型数据库的bai主要应用，主要是基本的、日常的事务处理，记du录即时的增、删、改、查，比如在银行存取一笔款，就是一个事务交易。OLAP即联机分析处理，是数据仓库的核心部心，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。典型的应用就是复杂的动态报表系统。</p><p>2、<strong>实时性要求不同</strong>：OLTP实时性要求高，OLTP 数据库旨在使事务应用程序仅写入所需的数据，以便尽快处理单个事务。OLAP的实时性要求不是很高，很多应用顶多是每天更新一下数据。</p><p>3、<strong>数据量不同</strong>：OLTP数据量不是很大，一般只读/写数十条记录，处理简单的事务。OLAP数据量大，因为OLAP支持的是动态查询，所以用户也许要通过将很多数据的统计后才能得到想要知道的信息，例如时间序列分析等等，所以处理的数据量很大。</p><p>4、<strong>用户和系统的面向性不同</strong>：OLTP是面向顾客的,用于事务和查询处理。OLAP是面向市场的,用于数据分析。</p><p>5、<strong>数据库设计不同</strong>：OLTP采用实体-联系ER模型和面向应用的数据库设计。OLAP采用星型或雪花模型和面向主题的数据库设计。</p></blockquote></li></ul><p><img src="https://iknow-pic.cdn.bcebos.com/80cb39dbb6fd526618590e1fa618972bd407364e?x-bce-process=image/resize,m_lfit,w_600,h_800,limit_1" alt="OLAP和OLTP区别"></p><h3 id="3-事实表、维度表、拉链表概念及区别"><a href="#3-事实表、维度表、拉链表概念及区别" class="headerlink" title="3. 事实表、维度表、拉链表概念及区别"></a>3. 事实表、维度表、拉链表概念及区别</h3><ul><li><strong>事实表：</strong>事实表其实质就是通过各种维度和一些指标值得组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。</li><li><strong>维度表：</strong>维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。</li><li><strong>拉链表：</strong>拉链表，它是一种维护<strong>历史状态</strong>，以及<strong>最新状态数据</strong>的一种表。拉链表也是分区表，有些不变的数据或者是已经达到状态终点的数据就会把它放在分区里面，分区字段一般为开始时间：start_date和结束时间：end_date。一般在该天有效的数据，它的end_date是大于等于该天的日期的。获取某一天全量的数据，可以通过表中的start_date和end_date来做筛选，选出固定某一天的数据。例如我想取截止到20190813的全量数据，其where过滤条件就是where start_date&lt;=’20190813’ and end_date&gt;=20190813。</li></ul><h3 id="4-全量表、增量表、快照表概念及区别"><a href="#4-全量表、增量表、快照表概念及区别" class="headerlink" title="4. 全量表、增量表、快照表概念及区别"></a>4. 全量表、增量表、快照表概念及区别</h3><ul><li><strong>全量表：</strong>全量表没有分区，表中的数据是前一天的所有数据，比如说今天是24号，那么全量表里面拥有的数据是23号的所有数据，每次往全量表里面写数据都会覆盖之前的数据，所以<strong>全量表不能记录历史的数据情况，只有截止到当前最新的、全量的数据</strong>。</li><li><strong>增量表：</strong>增量表，就是<strong>记录每天新增数据的表</strong>，比如说，从24号到25号新增了那些数据，改变了哪些数据，这些都会存储在增量表的25号分区里面。上面说的快照表的25号分区和24号分区（都是t+1，实际时间分别对应26号和25号），它两的数据相减就是实际时间25号到26号有变化的、增加的数据，也就相当于增量表里面25号分区的数据。</li><li><strong>快照表：</strong>那么要能查到历史数据情况又该怎么办呢？这个时候快照表就派上用途了，快照表是有时间分区的，每个分区里面的数据都是分区时间对应的前一天的所有全量数据，比如说当前数据表有3个分区，24号，25号，26号。其中，24号分区里面的数据就是从历史到23号的所有数据，25号分区里面的数据就是从历史到24号的所有数据，以此类推。</li></ul><h3 id="4-什么叫维度和度量值"><a href="#4-什么叫维度和度量值" class="headerlink" title="4. 什么叫维度和度量值"></a>4. 什么叫维度和度量值</h3><ul><li><p><strong>维度</strong>：说明数据，维度是指可指定不同值的对象的描述性<strong>属性或特征</strong>。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。 </p></li><li><p><strong>度量</strong>：事实表和维度交叉汇聚的点，度量和维度构成OLAP的主要概念，这里面对于在事实表或者一个多维立方体里面存放的数值型的、连续的字段，就是度量。这符合上面的意思，有标准，一个度量字段肯定是统一单位，例如元、户数。如果一个度量字段，其中的度量值可能是欧元又有可能是美元，那这个度量可没法汇总。在统一计量单位下，对不同维度的描述。</p></li></ul><h3 id="5-什么叫缓慢维度变化（Slowly-Changing-Dimensions，SCD"><a href="#5-什么叫缓慢维度变化（Slowly-Changing-Dimensions，SCD" class="headerlink" title="5. 什么叫缓慢维度变化（Slowly Changing Dimensions，SCD)"></a>5. 什么叫缓慢维度变化（Slowly Changing Dimensions，SCD)</h3><p>​       维度建模的数据仓库中，有一个概念叫Slowly Changing Dimensions，中文一般翻译成缓慢变化维，经常被简写为SCD。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随时间发生变化的维度我们一般称之为缓慢变化维，并且把处理维度表的历史变化信息的问题称为处理缓慢变化维的问题，有时也简称为处理SCD的问题。</p><p><strong>处理缓慢变化维的方法通常分为三种方式：</strong></p><ul><li><strong>第一种方式是直接覆盖原值</strong>。这样处理，最容易实现，但是没有保留历史数据，无法分析历史变化信息。第一种方式通常简称为“TYPE 1”。</li><li><strong>第二种方式是添加维度行</strong>。这样处理，需要<strong>代理键</strong>的支持。实现方式是当有维度属性发生变化时，生成一条新的维度记录，主键是新分配的代理键，通过自然键可以和原维度记录保持关联。第二种方式通常简称为“TYPE 2”。</li><li><strong>第三种方式是添加属性列</strong>。这种处理的实现方式是对于需要分析历史信息的属性添加一列，来记录该属性变化前的值，而本属性字段使用TYPE 1来直接覆盖。这种方式的优点是可以同时分析当前及前一次变化的属性值，缺点是只保留了最后一次变化信息。第三种方式通常简称为“TYPE 3”。</li></ul><h2 id="5-2-数仓分层设计"><a href="#5-2-数仓分层设计" class="headerlink" title="5.2 数仓分层设计"></a>5.2 数仓分层设计</h2><h3 id="1-数据仓库分为4层："><a href="#1-数据仓库分为4层：" class="headerlink" title="1. 数据仓库分为4层："></a>1. 数据仓库分为4层：</h3><ul><li>ODS层 （原始数据层）  BDM</li><li>DWD层 （明细数据层） FDM</li><li>DWS层 （服务数据层） GDM ADM</li><li>ADS层 （数据应用层）  APP</li></ul><h3 id="2-各层主要负责职责"><a href="#2-各层主要负责职责" class="headerlink" title="2. 各层主要负责职责"></a>2. 各层主要负责职责</h3><p><strong>ODS层（原始数据层</strong>）：存放原始数据，直接加载原始日志、数据，数据保存原貌不做处理。</p><p><strong>DWD层（明细数据层）</strong>：结构与粒度原始表保持一致，对ODS层数据进行清洗（去除空值、脏数据、超过极限范围的数据）</p><p><strong>DWS层 （服务数据层）</strong>：以DWD为基础，进行轻度汇总</p><p><strong>ADS层 （数据应用层）</strong>：为各种统计报表提供数据</p><h3 id="3-为什么要分层？"><a href="#3-为什么要分层？" class="headerlink" title="3. **为什么要分层？**"></a><strong>3. **为什么要分层</strong>？**</h3><ul><li><p><strong>空间换时间：</strong>通过建设多层次的数据模型供用户使用，避免用户直接使用操作型数据，可以更高效的访问数据</p></li><li><p><strong>把复杂问题简单化：一</strong>个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复</p></li><li><p><strong>便于处理业务的变化：</strong>随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知</p></li></ul><h3 id="4-数仓中每层表的建模？怎么建模？"><a href="#4-数仓中每层表的建模？怎么建模？" class="headerlink" title="4. 数仓中每层表的建模？怎么建模？"></a>4. 数仓中每层表的建模？怎么建模？</h3><p><strong>（1）ODS：</strong> 特点是保持原始数据的原貌，不作修改！</p><p>原始数据怎么建模，ODS就怎么建模！举例： 用户行为数据特征是一条记录就是一行！</p><p>ODS层表(line string) 业务数据，参考Sqoop导入的数据类型进行建模！</p><p>（2）<strong>DWD层</strong>：特点从ODS层，将数据进行ETL（清洗），轻度聚合，再展开明细！</p><ul><li>在展开明细时，对部分维度表进行降维操作</li></ul><blockquote><p>例如：将商品一二三级分类表，sku商品表，spu商品表，商品品牌表合并汇总为一张维度表！</p></blockquote><ul><li>对事实表，参考星型模型的建模策略，按照<strong>选择业务过程→声明粒度→确认维度→确认事实</strong>思路进行建模</li></ul><blockquote><p><strong>选择业务过程</strong>： 选择感兴趣的事实表<br><strong>声明粒度</strong>： 选择最细的粒度！可以由最细的粒度通过聚合的方式得到粗粒度！<br><strong>确认维度</strong>： 根据3w原则确认维度，挑选自己感兴趣的维度<br><strong>确认事实</strong>： 挑选感兴趣的度量字段，一般是从事实表中选取！</p></blockquote><ul><li>DWS层： 根据业务需求进行分主题建模！一般是建宽表！</li><li>DWT层： 根据业务需求进行分主题建模！一般是建宽表！</li><li>ADS层： 根据业务需求进行建模！</li></ul><h2 id="5-3-数仓建模"><a href="#5-3-数仓建模" class="headerlink" title="5.3 数仓建模"></a>5.3 数仓建模</h2><h3 id="1-维度建模概念、类型、过程"><a href="#1-维度建模概念、类型、过程" class="headerlink" title="1. 维度建模概念、类型、过程"></a>1. 维度建模概念、类型、过程</h3><p><strong>维度建模：</strong>维度建模是一种将数据结构化的逻辑设计方法，它将客观世界划分为<strong>度量</strong>和<strong>上下文</strong>。度量是常常是以数值形式出现，事实周围有上下文包围着，这种上下文被直观地分成独立的逻辑块，称之为<strong>维度</strong>。它与实体-关系建模有很大的区别，实体-关系建模是面向应用，遵循第三范式，以消除数据冗余为目标的设计技术。维度建模是面向分析，为了提高查询性能可以增加数据冗余，反规范化的设计技术。</p><p><strong>维度建模过程：</strong>确定业务流程-&gt;确定粒度-&gt;确定纬度-&gt;确定事实</p><blockquote><p>建模四步走：</p><p><strong>1.选取要建模的业务处理流程</strong></p><p>　　　　关注业务处理流程，而不是业务部门！</p><p><strong>2.定义业务处理的粒度</strong></p><p>　　　　“如何描述事实表的单个行？”</p><p><strong>3.选定用于每个事实表行的维度</strong></p><p>　　　　常见维度包括日期、产品等</p><p><strong>4.确定用于形成每个事实表行的数字型事实</strong></p><p>　　　　典型的事实包括订货量、支出额这样的可加性数据</p></blockquote><h3 id="2-星型模型和雪花模型概念、区别"><a href="#2-星型模型和雪花模型概念、区别" class="headerlink" title="2. 星型模型和雪花模型概念、区别"></a>2. 星型模型和雪花模型概念、区别</h3><p>​      在多维分析的商业智能解决方案中，<strong>根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。</strong>在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。</p><p>当所有维表都<strong>直接</strong>连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型，</p><img src="https://images2018.cnblogs.com/blog/1135580/201802/1135580-20180228173453995-1193173201.png" alt="img" style="zoom: 80%;"><p><strong>星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余</strong>，</p><p>　　如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。</p><p>　　当有<strong>一个或多个维表没有直接连接到事实表上</strong>，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。</p><p><img src="https://images2018.cnblogs.com/blog/1135580/201802/1135580-20180228173658849-1541605786.png" alt="img"></p><p>雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。</p><p>　　它的优点是 : <strong>通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。</strong></p><p>　　<strong>此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率（空间换易用与效率）。</strong></p><p><strong>1.数据优化</strong></p><p>　　雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。</p><p>　　相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。</p><p>　　<strong>2.业务模型</strong></p><p>　　主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。</p><p>　　在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。</p><p>　　<strong>3.性能</strong></p><p>　　第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。</p><p>而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。</p><p>　　<strong>4.ETL</strong></p><p>　　雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。</p><p>　　星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。</p><p>　　<strong>总结</strong></p><p>　　通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。</p><h2 id="5-4-数仓使用经验"><a href="#5-4-数仓使用经验" class="headerlink" title="5.4 数仓使用经验"></a>5.4 数仓使用经验</h2><h3 id="3-数据仓库系统的数据质量如何保证？方案？"><a href="#3-数据仓库系统的数据质量如何保证？方案？" class="headerlink" title="3. 数据仓库系统的数据质量如何保证？方案？"></a>3. 数据仓库系统的数据质量如何保证？方案？</h3><p>数据质量评估</p><ol><li><p>完整性</p></li><li><p>准确性</p></li><li><p>及时性</p></li><li><p>一致性</p></li></ol><h3 id="4-如何实现增量抽取？"><a href="#4-如何实现增量抽取？" class="headerlink" title="4. 如何实现增量抽取？"></a>4. 如何实现增量抽取？</h3><p>(主要采用时间戳方式，提供数据抽取和处理的性能)</p><h3 id="5-常见的数据治理方案"><a href="#5-常见的数据治理方案" class="headerlink" title="5. 常见的数据治理方案"></a>5. 常见的数据治理方案</h3><p>1）数据压缩</p><p>2）小文件合并</p><p>3）冷数据处理</p><h1 id="六-数据库"><a href="#六-数据库" class="headerlink" title="六. 数据库"></a>六. 数据库</h1><h2 id="6-1-基本概念"><a href="#6-1-基本概念" class="headerlink" title="6.1 基本概念"></a>6.1 基本概念</h2><h4 id="1-主键、外键、超键、候选键"><a href="#1-主键、外键、超键、候选键" class="headerlink" title="1. 主键、外键、超键、候选键"></a>1. 主键、外键、超键、候选键</h4><blockquote><p><strong>超键</strong>：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。</p><p><strong>候选键</strong>：是最小超键，即没有冗余元素的超键。</p><p><strong>主键</strong>：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。</p><p><strong>外键</strong>：在一个表中存在的另一个表的主键称此表的外键。</p></blockquote><h4 id="2-为什么用自增列作为主键"><a href="#2-为什么用自增列作为主键" class="headerlink" title="2. 为什么用自增列作为主键"></a>2. 为什么用自增列作为主键</h4><blockquote><p>如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引、</p><p>如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引、</p><p>如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的)。</p><p>数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）</p><p>如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页</p><p>如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p></blockquote><h4 id="3-触发器的作用？"><a href="#3-触发器的作用？" class="headerlink" title="3. 触发器的作用？"></a>3. 触发器的作用？</h4><blockquote><p>触发器是一种特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。</p></blockquote><h4 id="4-什么是存储过程？用什么来调用？"><a href="#4-什么是存储过程？用什么来调用？" class="headerlink" title="4. 什么是存储过程？用什么来调用？"></a>4. 什么是存储过程？用什么来调用？</h4><blockquote><p>存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。</p><p><strong>调用：</strong></p><p>1）可以用一个命令对象来调用存储过程。</p><p>2）可以供外部程序调用，比如：java程序。</p></blockquote><h4 id="5-存储过程的优缺点？"><a href="#5-存储过程的优缺点？" class="headerlink" title="5. 存储过程的优缺点？"></a>5. 存储过程的优缺点？</h4><blockquote><p><strong>优点：</strong></p><p>1）存储过程是预编译过的，执行效率高。</p><p>2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。</p><p>3）安全性高，执行存储过程需要有一定权限的用户。</p><p>4）存储过程可以重复使用，可减少数据库开发人员的工作量。</p><p><strong>缺点：</strong>移植性差</p></blockquote><h4 id="6-存储过程与函数的区别"><a href="#6-存储过程与函数的区别" class="headerlink" title="6. 存储过程与函数的区别"></a>6. 存储过程与函数的区别</h4><p><img src="https://i.imgur.com/ymE9HPJ.png" alt="img"></p><h4 id="7-什么叫视图？游标是什么？"><a href="#7-什么叫视图？游标是什么？" class="headerlink" title="7. 什么叫视图？游标是什么？"></a>7. 什么叫视图？游标是什么？</h4><blockquote><p><strong>视图：</strong></p><p>是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改会影响基本表。它使得我们获取数据更容易，相比多表查询。</p><p><strong>游标：</strong></p><p>是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。</p></blockquote><h4 id="8-视图的优缺点"><a href="#8-视图的优缺点" class="headerlink" title="8. 视图的优缺点"></a>8. 视图的优缺点</h4><blockquote><p><strong>优点：</strong></p><p>1对数据库的访问，因为视图可以有选择性的选取数据库里的一部分。</p><p>2)用户通过简单的查询可以从复杂查询中得到结果。</p><p>3)维护数据的独立性，试图可从多个表检索数据。</p><p>4)对于相同的数据可产生不同的视图。</p><p><strong>缺点：</strong></p><p>性能：查询视图时，必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，那么就无法更改数据</p></blockquote><h4 id="9-drop、truncate、-delete区别"><a href="#9-drop、truncate、-delete区别" class="headerlink" title="9. drop、truncate、 delete区别"></a>9. drop、truncate、 delete区别</h4><blockquote><p><strong>最基本：</strong></p><ul><li>drop直接删掉表。</li><li>truncate删除表中数据，再插入时自增长id又从1开始。</li><li>delete删除表中数据，可以加where字句。</li></ul><p>（1） DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。</p><p>（2） 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。</p><p>（3） 一般而言，drop &gt; truncate &gt; delete</p><p>（4） 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view</p><p>（5） TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。</p><p>（6） truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。</p><p>（7） delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。</p><p>（8） truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚。</p><p>（9） 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老师想触发trigger,还是用delete。</p><p>（10） Truncate table 表名 速度快,而且效率高,因为:?truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。</p><p>（11） TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。</p><p>（12） 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。</p></blockquote><h4 id="10-什么是临时表，临时表什么时候删除"><a href="#10-什么是临时表，临时表什么时候删除" class="headerlink" title="10. 什么是临时表，临时表什么时候删除?"></a>10. 什么是临时表，临时表什么时候删除?</h4><blockquote><p><strong>临时表可以手动删除：</strong><br>DROP TEMPORARY TABLE IF EXISTS temp_tb;</p><p><strong>临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。因此在不同的连接中可以创建同名的临时表，并且操作属于本连接的临时表。<br>创建临时表的语法与创建表语法类似，不同之处是增加关键字TEMPORARY，</strong></p><p>如：</p><p>CREATE TEMPORARY TABLE tmp_table (</p><p>NAME VARCHAR (10) NOT NULL,</p><p>time date NOT NULL<br>);</p><p>select * from tmp_table;</p></blockquote><h4 id="11-非关系型数据库和关系型数据库区别，优势比较"><a href="#11-非关系型数据库和关系型数据库区别，优势比较" class="headerlink" title="11. 非关系型数据库和关系型数据库区别，优势比较?"></a>11. 非关系型数据库和关系型数据库区别，优势比较?</h4><blockquote><p><strong>非关系型数据库的优势：</strong></p><ul><li><strong>性能：</strong>NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。</li><li><strong>可扩展性：</strong>同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。</li></ul><p><strong>关系型数据库的优势：</strong></p><ul><li><strong>复杂查询：</strong>可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。</li><li><strong>事务支持：</strong>使得对于安全性能很高的数据访问要求得以实现。</li></ul><p><strong>其他：</strong></p><p><strong>1.</strong>对于这两类数据库，对方的优势就是自己的弱势，反之亦然。</p><p><strong>2.</strong>NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能，比如MongoDB。</p><p><strong>3.</strong>对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国，比如Redis set nx。</p></blockquote><h4 id="12-数据库范式，根据某个场景设计数据表"><a href="#12-数据库范式，根据某个场景设计数据表" class="headerlink" title="12. 数据库范式，根据某个场景设计数据表?"></a>12. 数据库范式，根据某个场景设计数据表?</h4><blockquote><p><strong>第一范式:</strong>(确保每列保持原子性)所有字段值都是不可分解的原子值。</p><p>第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。<br>第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。<br>上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。</p><p><strong>第二范式:</strong>(确保表中的每列都和主键相关)在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。</p><p>第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。<br>比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键。</p><p><strong>第三范式:</strong>(确保每列都和主键列直接相关,而不是间接相关) 数据表中的每一列数据都和主键直接相关，而不能间接相关。</p><p>第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。<br>比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。</p><p><strong>BCNF:</strong>符合3NF，并且，主属性不依赖于主属性。</p><p>若关系模式属于第二范式，且每个属性都不传递依赖于键码，则R属于BC范式。<br>通常BC范式的条件有多种等价的表述：每个非平凡依赖的左边必须包含键码；每个决定因素必须包含键码。<br>BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。<br>还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。<br>一般，一个数据库设计符合3NF或BCNF就可以了。</p><p><strong>第四范式:</strong>要求把同一表内的多对多关系删除。</p><p><strong>第五范式:</strong>从最终结构重新建立原始结构。</p></blockquote><h4 id="13-什么是-内连接、外连接、交叉连接、笛卡尔积等"><a href="#13-什么是-内连接、外连接、交叉连接、笛卡尔积等" class="headerlink" title="13. 什么是 内连接、外连接、交叉连接、笛卡尔积等?"></a>13. 什么是 内连接、外连接、交叉连接、笛卡尔积等?</h4><blockquote><p><strong>内连接:</strong> 只连接匹配的行</p><p><strong>左外连接:</strong> 包含左边表的全部行（不管右边的表中是否存在与它们匹配的行），以及右边表中全部匹配的行</p><p><strong>右外连接:</strong> 包含右边表的全部行（不管左边的表中是否存在与它们匹配的行），以及左边表中全部匹配的行</p><p>例如1：<br>SELECT a.<em>,b.</em> FROM luntan LEFT JOIN usertable as b ON a.username=b.username</p><p>例如2：<br>SELECT a.<em>,b.</em> FROM city as a FULL OUTER JOIN user as b ON a.username=b.username</p><p><strong>全外连接:</strong> 包含左、右两个表的全部行，不管另外一边的表中是否存在与它们匹配的行。</p><p><strong>交叉连接:</strong> 生成笛卡尔积－它不使用任何匹配或者选取条件，而是直接将一个数据源中的每个行与另一个数据源的每个行都一一匹配</p><p>例如：<br>SELECT type,pub_name FROM titles CROSS JOIN publishers ORDER BY type</p><p><strong>注意：</strong></p><p>很多公司都只是考察是否知道其概念，但是也有很多公司需要不仅仅知道概念，还需要动手写sql,一般都是简单的连接查询，具体关于连接查询的sql练习，参见以下链接：</p><p><a href="https://www.nowcoder.com/ta/sql" target="_blank" rel="noopener">牛客网数据库SQL实战</a></p><p><a href="https://leetcode-cn.com/problemset/database/" target="_blank" rel="noopener">leetcode中文网站数据库练习</a></p><p><a href="http://www.baidu.com/" target="_blank" rel="noopener">我的另一篇文章，常用sql练习50题</a></p></blockquote><h4 id="14-varchar和char的使用场景"><a href="#14-varchar和char的使用场景" class="headerlink" title="14. varchar和char的使用场景?"></a>14. varchar和char的使用场景?</h4><blockquote></blockquote><blockquote><p><strong>1.</strong>char的长度是不可变的，而varchar的长度是可变的。</p><p>定义一个char[10]和varchar[10]。<br>如果存进去的是‘csdn’,那么char所占的长度依然为10，除了字符‘csdn’外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。</p><p><strong>2.</strong>char的存取数度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。<br>char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。<br>varchar是以空间效率为首位。</p><p><strong>3.</strong>char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。<br>varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。</p><p><strong>4.</strong>两者的存储数据都非unicode的字符数据。</p></blockquote><h4 id="15-SQL语言分类"><a href="#15-SQL语言分类" class="headerlink" title="15. SQL语言分类"></a>15. SQL语言分类</h4><blockquote><p><strong>SQL语言共分为四大类：</strong></p><ul><li>数据查询语言DQL</li><li>数据操纵语言DML</li><li>数据定义语言DDL</li><li>数据控制语言DCL。</li></ul><p><strong>1. 数据查询语言DQL</strong></p><p>数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：</p><p>SELECT<br>FROM<br>WHERE</p><p><strong>2 .数据操纵语言DML</strong></p><p>数据操纵语言DML主要有三种形式：</p><ol><li>插入：INSERT</li><li>更新：UPDATE</li><li>删除：DELETE</li></ol><p><strong>3. 数据定义语言DDL</strong></p><p>数据定义语言DDL用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等如：<br>CREATE TABLE/VIEW/INDEX/SYN/CLUSTER</p><p>表 视图 索引 同义词 簇</p><p>DDL操作是隐性提交的！不能rollback</p><p><strong>4. 数据控制语言DCL</strong></p><p>数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如：</p><ol><li>GRANT：授权。</li><li>ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚—ROLLBACK；回滚命令使数据库状态回到上次最后提交的状态。其格式为：<br>SQL&gt;ROLLBACK;</li><li>COMMIT [WORK]：提交。</li></ol><p>在数据库的插入、删除和修改操作时，只有当事务在提交到数据<br>库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看<br>到所做的事情，别人只有在最后提交完成后才可以看到。<br>提交数据有三种类型：显式提交、隐式提交及自动提交。下面分<br>别说明这三种类型。</p><p>(1) 显式提交<br>用COMMIT命令直接完成的提交为显式提交。其格式为：<br>SQL&gt;COMMIT；</p><p>(2) 隐式提交<br>用SQL命令间接完成的提交为隐式提交。这些命令是：<br>ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP，<br>EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。</p><p>(3) 自动提交<br>若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后，<br>系统将自动进行提交，这就是自动提交。其格式为：<br>SQL&gt;SET AUTOCOMMIT ON；</p><p>参考文章：<br><a href="https://www.cnblogs.com/study-s/p/5287529.html" target="_blank" rel="noopener">https://www.cnblogs.com/study-s/p/5287529.html</a></p></blockquote><h4 id="16-like-和-的区别"><a href="#16-like-和-的区别" class="headerlink" title="16. like %和-的区别"></a>16. like %和-的区别</h4><blockquote><p><strong>通配符的分类:</strong></p><p><strong>%百分号通配符:</strong>表示任何字符出现任意次数(可以是0次).</p><p><strong>_下划线通配符:</strong>表示只能匹配单个字符,不能多也不能少,就是一个字符.</p><p><strong>like操作符:</strong> LIKE作用是指示mysql后面的搜索模式是利用通配符而不是直接相等匹配进行比较.</p><p><strong>注意:</strong> 如果在使用like操作符时,后面的没有使用通用匹配符效果是和=一致的,SELECT * FROM products WHERE products.prod_name like ‘1000’;<br>只能匹配的结果为1000,而不能匹配像JetPack 1000这样的结果.</p><ul><li>%通配符使用: 匹配以”yves”开头的记录:(包括记录”yves”) SELECT <em>FROM products WHERE products.prod_name like ‘yves%’;<br>匹配包含”yves”的记录(包括记录”yves”) SELECT</em> FROM products WHERE products.prod_name like ‘%yves%’;<br>匹配以”yves”结尾的记录(包括记录”yves”,不包括记录”yves “,也就是yves后面有空格的记录,这里需要注意) SELECT * FROM products WHERE products.prod_name like ‘%yves’;</li><li><em>通配符使用: SELECT *FROM products WHERE products.prod_name like ‘_yves’; 匹配结果为: 像”yyves”这样记录.<br>SELECT\</em> FROM products WHERE products.prod*name like ‘yves**’; 匹配结果为: 像”yvesHe”这样的记录.(一个下划线只能匹配一个字符,不能多也不能少)</li></ul><p><strong>注意事项:</strong></p><ul><li>注意大小写,在使用模糊匹配时,也就是匹配文本时,mysql是可能区分大小的,也可能是不区分大小写的,这个结果是取决于用户对MySQL的配置方式.如果是区分大小写,那么像YvesHe这样记录是不能被”yves__”这样的匹配条件匹配的.</li><li>注意尾部空格,”%yves”是不能匹配”heyves “这样的记录的.</li><li>注意NULL,%通配符可以匹配任意字符,但是不能匹配NULL,也就是说SELECT * FROM products WHERE products.prod_name like ‘%;是匹配不到products.prod_name为NULL的的记录.</li></ul><p><strong>技巧与建议:</strong></p><p>正如所见， MySQL的通配符很有用。但这种功能是有代价的：通配符搜索的处理一般要比前面讨论的其他搜索所花时间更长。这里给出一些使用通配符要记住的技巧。</p><ul><li>不要过度使用通配符。如果其他操作符能达到相同的目的，应该 使用其他操作符。</li><li>在确实需要使用通配符时，除非绝对有必要，否则不要把它们用 在搜索模式的开始处。把通配符置于搜索模式的开始处，搜索起 来是最慢的。</li><li>仔细注意通配符的位置。如果放错地方，可能不会返回想要的数.</li></ul></blockquote><p>参考博文：<a href="https://blog.csdn.net/u011479200/article/details/78513632" target="_blank" rel="noopener">https://blog.csdn.net/u011479200/article/details/78513632</a></p><h4 id="17-count-、count-1-、count-column-的区别"><a href="#17-count-、count-1-、count-column-的区别" class="headerlink" title="17. count(*)、count(1)、count(column)的区别"></a>17. count(*)、count(1)、count(column)的区别</h4><blockquote><ul><li>count(*)对行的数目进行计算,包含NULL</li><li>count(column)对特定的列的值具有的行数进行计算,不包含NULL值。</li><li>count()还有一种使用方式,count(1)这个用法和count(*)的结果是一样的。</li></ul><p><strong>性能问题:</strong></p><p>1.任何情况下SELECT COUNT(*) FROM tablename是最优选择;</p><p>2.尽量减少SELECT COUNT(*) FROM tablename WHERE COL = ‘value’ 这种查询;</p><p>3.杜绝SELECT COUNT(COL) FROM tablename WHERE COL2 = ‘value’ 的出现。</p><ul><li>如果表没有主键,那么count(1)比count(*)快。</li><li>如果有主键,那么count(主键,联合主键)比count(*)快。</li><li>如果表只有一个字段,count(*)最快。</li></ul><p>count(1)跟count(主键)一样,只扫描主键。count(*)跟count(非主键)一样,扫描整个表。明显前者更快一些。</p></blockquote><h4 id="18-最左前缀原则"><a href="#18-最左前缀原则" class="headerlink" title="18. 最左前缀原则"></a>18. 最左前缀原则</h4><blockquote><p><strong>多列索引：</strong></p><p>ALTER TABLE people ADD INDEX lname_fname_age (lame,fname,age);</p><p>为了提高搜索效率，我们需要考虑运用多列索引,由于索引文件以B－Tree格式保存，所以我们不用扫描任何记录，即可得到最终结果。</p><p>注：在mysql中执行查询时，只能使用一个索引，如果我们在lname,fname,age上分别建索引,执行查询时，只能使用一个索引，mysql会选择一个最严格(获得结果集记录数最少)的索引。</p><p><strong>最左前缀原则：</strong>顾名思义，就是最左优先，上例中我们创建了lname_fname_age多列索引,相当于创建了(lname)单列索引，(lname,fname)组合索引以及(lname,fname,age)组合索引。</p></blockquote><h2 id="6-2-索引"><a href="#6-2-索引" class="headerlink" title="6.2 索引"></a>6.2 索引</h2><h4 id="1-什么是索引？"><a href="#1-什么是索引？" class="headerlink" title="1. 什么是索引？"></a>1. 什么是索引？</h4><blockquote><p><strong>何为索引：</strong></p><p>数据库索引，是数据库管理系统中一个排序的数据结构，索引的实现通常使用B树及其变种B+树。</p><p>在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。</p></blockquote><h4 id="2-索引的作用？它的优点缺点是什么？"><a href="#2-索引的作用？它的优点缺点是什么？" class="headerlink" title="2. 索引的作用？它的优点缺点是什么？"></a>2. 索引的作用？它的优点缺点是什么？</h4><blockquote><p><strong>索引作用：</strong></p><p>协助快速查询、更新数据库表中数据。</p><p>为表设置索引要付出代价的：</p><ul><li><p>一是增加了数据库的存储空间</p></li><li><p>二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动)。</p><h4 id="3-索引的优缺点？"><a href="#3-索引的优缺点？" class="headerlink" title="3.索引的优缺点？"></a><strong>3.索引的优缺点？</strong></h4></li></ul><p><strong>创建索引可以大大提高系统的性能（优点）：</strong></p><p>1.通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。</p><p>2.可以大大加快数据的检索速度，这也是创建索引的最主要的原因。</p><p>3.可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。</p><p>4.在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。</p><p>5.通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。</p><p><strong>增加索引也有许多不利的方面(缺点)：</strong></p><p>1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。</p><p>2.索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。</p><p>3.当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。</p></blockquote><h4 id="4-哪些列适合建立索引、哪些不适合建索引？"><a href="#4-哪些列适合建立索引、哪些不适合建索引？" class="headerlink" title="4. 哪些列适合建立索引、哪些不适合建索引？"></a>4. 哪些列适合建立索引、哪些不适合建索引？</h4><blockquote><p>索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。</p><p><strong>一般来说，应该在这些列上创建索引：</strong></p><p>（1）在经常需要搜索的列上，可以加快搜索的速度；</p><p>（2）在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构；</p><p>（3）在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度；</p><p>（4）在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的；</p><p>（5）在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；</p><p>（6）在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。</p><p><strong>对于有些列不应该创建索引：</strong></p><p>（1）对于那些在查询中很少使用或者参考的列不应该创建索引。</p><p>这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。</p><p>（2）对于那些只有很少数据值的列也不应该增加索引。</p><p>这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。</p><p>（3）对于那些定义为text, image和bit数据类型的列不应该增加索引。</p><p>这是因为，这些列的数据量要么相当大，要么取值很少。</p><p>(4)当修改性能远远大于检索性能时，不应该创建索引。</p><p>这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。</p></blockquote><h4 id="5-什么样的字段适合建索引"><a href="#5-什么样的字段适合建索引" class="headerlink" title="5. 什么样的字段适合建索引"></a>5. 什么样的字段适合建索引</h4><blockquote><p>唯一、不为空、经常被查询的字段</p><h4 id="6-MySQL-B-Tree索引和Hash索引的区别"><a href="#6-MySQL-B-Tree索引和Hash索引的区别" class="headerlink" title="6.MySQL B+Tree索引和Hash索引的区别?"></a><strong>6.MySQL B+Tree索引和Hash索引的区别?</strong></h4><p><strong>Hash索引和B+树索引的特点：</strong></p><ul><li>Hash索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位;</li><li>B+树索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问;</li></ul><p><strong>为什么不都用Hash索引而使用B+树索引？</strong></p><ol><li>Hash索引仅仅能满足”=”,”IN”和””查询，不能使用范围查询,因为经过相应的Hash算法处理之后的Hash值的大小关系，并不能保证和Hash运算前完全一样；</li><li>Hash索引无法被用来避免数据的排序操作，因为Hash值的大小关系并不一定和Hash运算前的键值完全一样；</li><li>Hash索引不能利用部分索引键查询，对于组合索引，Hash索引在计算Hash值的时候是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用；</li><li>Hash索引在任何时候都不能避免表扫描，由于不同索引键存在相同Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要回表查询数据；</li><li>Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。</li></ol><p><strong>补充：</strong></p><p>1.MySQL中，只有HEAP/MEMORY引擎才显示支持Hash索引。</p><p>2.常用的InnoDB引擎中默认使用的是B+树索引，它会实时监控表上索引的使用情况，如果认为建立哈希索引可以提高查询效率，则自动在内存中的“自适应哈希索引缓冲区”建立哈希索引（在InnoDB中默认开启自适应哈希索引），通过观察搜索模式，MySQL会利用index key的前缀建立哈希索引，如果一个表几乎大部分都在缓冲池中，那么建立一个哈希索引能够加快等值查询。<br>B+树索引和哈希索引的明显区别是：</p><p>3.如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据；</p><p>4.如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索；<br>同理，哈希索引没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）；</p><p>5.哈希索引也不支持多列联合索引的最左匹配规则；</p><p>6.B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。</p><p>7.在大多数场景下，都会有范围查询、排序、分组等查询特征，用B+树索引就可以了。</p></blockquote><h4 id="7-B树和B-树的区别"><a href="#7-B树和B-树的区别" class="headerlink" title="7. B树和B+树的区别"></a>7. B树和B+树的区别</h4><blockquote><ol><li>B树，每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为nul，叶子结点不包含任何关键字信息。<br><img src="https://i.imgur.com/RbzI0R8.jpg" alt="img"></li><li>B+树，所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大的顺序链接，所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (而B 树的非终节点也包含需要查找的有效信息)<br><img src="https://i.imgur.com/9VbnDME.jpg" alt="img"></li></ol></blockquote><h4 id="8-为什么说B-比B树更适合实际应用中操作系统的文件索引和数据库索引？"><a href="#8-为什么说B-比B树更适合实际应用中操作系统的文件索引和数据库索引？" class="headerlink" title="8. 为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？"></a>8. 为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？</h4><blockquote><p><strong>1.B+的磁盘读写代价更低</strong></p><p>B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。</p><p><strong>2.B+tree的查询效率更加稳定</strong></p><p>由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</p></blockquote><h4 id="9-聚集索引和非聚集索引区别"><a href="#9-聚集索引和非聚集索引区别" class="headerlink" title="9. 聚集索引和非聚集索引区别?"></a>9. 聚集索引和非聚集索引区别?</h4><blockquote><p><strong>聚合索引(clustered index):</strong></p><p>聚集索引<strong>表记录的排列顺序和索引的排列顺序一致，所以查询效率快，</strong>只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。<br>聚集索引类似于新华字典中用拼音去查找汉字，拼音检索表于书记顺序都是按照a~z排列的，就像相同的逻辑顺序于物理顺序一样，当你需要查找a,ai两个读音的字，或是想一次寻找多个傻(sha)的同音字时，也许向后翻几页，或紧接着下一行就得到结果了。</p><p><strong>非聚合索引(nonclustered index):</strong></p><p>非聚集索引<strong>指定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，</strong>两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。<br>非聚集索引类似在新华字典上通过偏旁部首来查询汉字，检索表也许是按照横、竖、撇来排列的，但是由于正文中是a~z的拼音顺序，所以就类似于逻辑地址于物理地址的不对应。同时适用的情况就在于分组，大数目的不同值，频繁更新的列中，这些情况即不适合聚集索引。</p><p><strong>根本区别：</strong></p><p>聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。</p></blockquote><h2 id="6-3-事务"><a href="#6-3-事务" class="headerlink" title="6.3 事务"></a>6.3 事务</h2><h4 id="1-什么是事务？"><a href="#1-什么是事务？" class="headerlink" title="1. 什么是事务？"></a>1. 什么是事务？</h4><blockquote><p>事务是对数据库中一系列操作进行统一的回滚或者提交的操作，主要用来保证数据的完整性和一致性。</p></blockquote><h4 id="2-事务四大特性（ACID）原子性、一致性、隔离性、持久性"><a href="#2-事务四大特性（ACID）原子性、一致性、隔离性、持久性" class="headerlink" title="2. 事务四大特性（ACID）原子性、一致性、隔离性、持久性?"></a>2. 事务四大特性（ACID）原子性、一致性、隔离性、持久性?</h4><blockquote><p><strong>原子性（Atomicity）:</strong><br>原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。</p><p><strong>一致性（Consistency）:</strong><br>事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。</p><p><strong>隔离性（Isolation）:</strong><br>隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。</p><p><strong>持久性（Durability）:</strong><br>持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</p></blockquote><h4 id="3-事务的并发-事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别"><a href="#3-事务的并发-事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别" class="headerlink" title="3. 事务的并发?事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别?"></a>3. 事务的并发?事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别?</h4><blockquote><p>从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题，然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行， 在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行， 事务的隔离级别可以通过隔离事务属性指定。<br><strong>事务的并发问题</strong></p><p><strong>1、脏读：</strong>事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据</p><p><strong>2、不可重复读：</strong>事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。</p><p><strong>3、幻读：</strong>幻读解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。</p><p>例如：事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作 这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。 而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有跟没有修改一样，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。<br><strong>小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。</strong></p><p><strong>事务的隔离级别</strong></p><p><img src="https://i.imgur.com/xAeWTSp.png" alt="img"></p><p><strong>读未提交：</strong>另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据脏读</p><p><strong>不可重复读：</strong>事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。</p><p><strong>可重复读：</strong>在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。但是，会有幻读现象</p><p><strong>串行化：</strong>最高的隔离级别，在这个隔离级别下，不会产生任何异常。并发的事务，就像事务是在一个个按照顺序执行一样</p></blockquote><p><strong>特别注意：</strong></p><blockquote><p>MySQL默认的事务隔离级别为repeatable-read</p><p>MySQL 支持 4 中事务隔离级别.</p><p>事务的隔离级别要得到底层数据库引擎的支持, 而不是应用程序或者框架的支持.</p><p>Oracle 支持的 2 种事务隔离级别：READ_COMMITED , SERIALIZABLE</p><p>SQL规范所规定的标准，不同的数据库具体的实现可能会有些差异</p><p><strong>MySQL中默认事务隔离级别是“可重复读”时并不会锁住读取到的行</strong></p><p><strong>事务隔离级别：</strong>未提交读时，写数据只会锁住相应的行。</p><p><strong>事务隔离级别为：</strong>可重复读时，写数据会锁住整张表。</p><p><strong>事务隔离级别为：</strong>串行化时，读写数据都会锁住整张表。</p><p>隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，鱼和熊掌不可兼得啊。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed，它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。</p></blockquote><h4 id="4-事务传播行为"><a href="#4-事务传播行为" class="headerlink" title="4. 事务传播行为"></a>4. 事务传播行为</h4><blockquote><p><strong>1.PROPAGATION_REQUIRED：</strong>如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。</p><p><strong>2.PROPAGATION_SUPPORTS：</strong>支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。</p><p><strong>3.PROPAGATION_MANDATORY：</strong>支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。</p><p><strong>4.PROPAGATION_REQUIRES_NEW：</strong>创建新事务，无论当前存不存在事务，都创建新事务。</p><p><strong>5.PROPAGATION_NOT_SUPPORTED：</strong>以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。</p><p><strong>6.PROPAGATION_NEVER：</strong>以非事务方式执行，如果当前存在事务，则抛出异常。</p><p><strong>7.PROPAGATION_NESTED：</strong>如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。</p></blockquote><h4 id="5-嵌套事务"><a href="#5-嵌套事务" class="headerlink" title="5. 嵌套事务"></a>5. 嵌套事务</h4><blockquote><p><strong>什么是嵌套事务？</strong></p><p>嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点就在于那个save point。看几个问题就明了了：</p><p><strong>如果子事务回滚，会发生什么？</strong></p><p>父事务会回滚到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。</p><p><strong>如果父事务回滚，会发生什么？</strong></p><p>父事务回滚，子事务也会跟着回滚！为什么呢，因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理。那么：</p><p><strong>事务的提交，是什么情况？</strong></p><p>是父事务先提交，然后子事务提交，还是子事务先提交，父事务再提交？答案是第二种情况，还是那句话，子事务是父事务的一部分，由父事务统一提交。</p></blockquote><p>参考文章：<a href="https://blog.csdn.net/liangxw1/article/details/51197560" target="_blank" rel="noopener">https://blog.csdn.net/liangxw1/article/details/51197560</a></p><h2 id="6-4-存储引擎"><a href="#6-4-存储引擎" class="headerlink" title="6.4 存储引擎"></a>6.4 存储引擎</h2><h4 id="1-MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别"><a href="#1-MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别" class="headerlink" title="1. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别?"></a>1. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别?</h4><blockquote><p><strong>两种存储引擎的大致区别表现在：</strong></p><p>1.<strong>InnoDB支持事务，MyISAM不支持，</strong> <strong>这一点是非常之重要。</strong>事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而MyISAM就不可以了。</p><p>2.MyISAM适合查询以及插入为主的应用。</p><p>3.InnoDB适合频繁修改以及涉及到安全性较高的应用。</p><p>4.InnoDB支持外键，MyISAM不支持。</p><p>5.从MySQL5.5.5以后，InnoDB是默认引擎。</p><p>6.InnoDB不支持FULLTEXT类型的索引。</p><p>7.InnoDB中不保存表的行数，如select count(<em>) from table时，InnoDB需要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count(</em>)语句包含where条件时MyISAM也需要扫描整个表。</p><p>8.对于自增长的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中可以和其他字段一起建立联合索引。</p><p>9.DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除，效率非常慢。MyISAM则会重建表。</p><p>10.InnoDB支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’。</p></blockquote><h4 id="2-MySQL存储引擎MyISAM与InnoDB如何选择"><a href="#2-MySQL存储引擎MyISAM与InnoDB如何选择" class="headerlink" title="2. MySQL存储引擎MyISAM与InnoDB如何选择"></a>2. MySQL存储引擎MyISAM与InnoDB如何选择</h4><blockquote><p>MySQL有多种存储引擎，每种存储引擎有各自的优缺点，可以择优选择使用：MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE。</p><p>虽然MySQL里的存储引擎不只是MyISAM与InnoDB这两个，但常用的就是两个。<br>关于MySQL数据库提供的两种存储引擎，MyISAM与InnoDB选择使用：</p></blockquote><ul><li>1.INNODB会支持一些关系数据库的高级功能，如事务功能和行级锁，MyISAM不支持。</li><li>2.MyISAM的性能更优，占用的存储空间少，所以，选择何种存储引擎，视具体应用而定。</li></ul><blockquote><p>如果你的应用程序一定要使用事务，毫无疑问你要选择INNODB引擎。但要注意，INNODB的行级锁是有条件的。在where条件没有使用主键时，照样会锁全表。比如DELETE FROM mytable这样的删除语句。</p><p>如果你的应用程序对查询性能要求较高，就要使用MyISAM了。MyISAM索引和数据是分开的，而且其索引是压缩的，可以更好地利用内存。所以它的查询性能明显优于INNODB。压缩后的索引也能节约一些磁盘空间。MyISAM拥有全文索引的功能，这可以极大地优化LIKE查询的效率。</p><p>有人说MyISAM只能用于小型应用，其实这只是一种偏见。如果数据量比较大，这是需要通过升级架构来解决，比如分表分库，而不是单纯地依赖存储引擎。</p><p>现在一般都是选用innodb了，主要是MyISAM的全表锁，读写串行问题，并发效率锁表，效率低，MyISAM对于读写密集型应用一般是不会去选用的。<br>MEMORY存储引擎</p><p>MEMORY是MySQL中一类特殊的存储引擎。它使用存储在内存中的内容来创建表，而且数据全部放在内存中。这些特性与前面的两个很不同。<br>每个基于MEMORY存储引擎的表实际对应一个磁盘文件。该文件的文件名与表名相同，类型为frm类型。该文件中只存储表的结构。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。值得注意的是，服务器需要有足够的内存来维持MEMORY存储引擎的表的使用。如果不需要了，可以释放内存，甚至删除不需要的表。</p><p>MEMORY默认使用哈希索引。速度比使用B型树索引快。当然如果你想用B型树索引，可以在创建索引时指定。</p><p>注意，MEMORY用到的很少，因为它是把数据存到内存中，如果内存出现异常就会影响数据。如果重启或者关机，所有数据都会消失。因此，基于MEMORY的表的生命周期很短，一般是一次性的。</p></blockquote><h4 id="3-MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景"><a href="#3-MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景" class="headerlink" title="3. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景?"></a>3. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景?</h4><blockquote><p><strong>事务处理上方面</strong></p></blockquote><ul><li>MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。</li><li>InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</li></ul><blockquote><p><strong>锁级别</strong></p></blockquote><ul><li>MyISAM：只支持表级锁，用户在操作MyISAM表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。</li><li>InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。</li></ul><blockquote><p><strong>关于存储引擎MyISAM和InnoDB的其他参考资料如下：</strong></p><p><a href="http://blog.csdn.net/lc0817/article/details/52757194" target="_blank" rel="noopener">MySQL存储引擎中的MyISAM和InnoDB区别详解</a></p><p><a href="https://www.cnblogs.com/kevingrace/p/5685355.html" target="_blank" rel="noopener">MySQL存储引擎之MyISAM和Innodb总结性梳理</a></p></blockquote><h2 id="6-5-优化"><a href="#6-5-优化" class="headerlink" title="6.5 优化"></a>6.5 优化</h2><h4 id="1-查询语句不同元素（where、jion、limit、group-by、having等等）执行先后顺序"><a href="#1-查询语句不同元素（where、jion、limit、group-by、having等等）执行先后顺序" class="headerlink" title="1. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序?"></a>1. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序?</h4><ul><li>1.查询中用到的关键词主要包含<strong>六个</strong>，并且他们的顺序依次为 <strong>select–from–where–group by–having–order by</strong></li></ul><blockquote><p><strong>其中select和from是必须的，其他关键词是可选的，这六个关键词的执行顺序 与sql语句的书写顺序并不是一样的，而是按照下面的顺序来执行</strong></p><p><strong>from:</strong>需要从哪个数据表检索数据</p></blockquote><blockquote><p><strong>where:</strong>过滤表中数据的条件</p><p><strong>group by:</strong>如何将上面过滤出的数据分组</p><p><strong>having:</strong>对上面已经分组的数据进行过滤的条件</p><p><strong>select:</strong>查看结果集中的哪个列，或列的计算结果</p><p><strong>order by :</strong>按照什么样的顺序来查看返回的数据</p></blockquote><ul><li>2.<strong>from后面的表关联，是自右向左解析 而where条件的解析顺序是自下而上的。</strong></li></ul><blockquote><p>也就是说，在写SQL语句的时候，尽量把数据量小的表放在最右边来进行关联（用小表去匹配大表），而把能筛选出小量数据的条件放在where语句的最左边 （用小表去匹配大表）</p><p>其他参考资源：<br><a href="http://www.cnblogs.com/huminxxl/p/3149097.html" target="_blank" rel="noopener">http://www.cnblogs.com/huminxxl/p/3149097.html</a></p></blockquote><h4 id="2-使用explain优化sql和索引"><a href="#2-使用explain优化sql和索引" class="headerlink" title="2. 使用explain优化sql和索引?"></a>2. 使用explain优化sql和索引?</h4><blockquote><p><strong>对于复杂、效率低的sql语句，我们通常是使用explain sql 来分析sql语句，这个语句可以打印出，语句的执行。这样方便我们分析，进行优化</strong></p><p><strong>table：</strong>显示这一行的数据是关于哪张表的</p><p><strong>type：</strong>这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、index和ALL</p><p><strong>all:</strong>full table scan ;MySQL将遍历全表以找到匹配的行；</p><p><strong>index:</strong> index scan; index 和 all的区别在于index类型只遍历索引；</p><p><strong>range：</strong>索引范围扫描，对索引的扫描开始于某一点，返回匹配值的行，常见与between ，等查询；</p><p><strong>ref：</strong>非唯一性索引扫描，返回匹配某个单独值的所有行，常见于使用非唯一索引即唯一索引的非唯一前缀进行查找；</p><p><strong>eq_ref：</strong>唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常用于主键或者唯一索引扫描；</p><p><strong>const，system：</strong>当MySQL对某查询某部分进行优化，并转为一个常量时，使用这些访问类型。如果将主键置于where列表中，MySQL就能将该查询转化为一个常量。</p><p><strong>possible_keys：</strong>显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句</p><p><strong>key：</strong> 实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MySQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MySQL忽略索引</p><p><strong>key_len：</strong>使用的索引的长度。在不损失精确性的情况下，长度越短越好</p><p><strong>ref：</strong>显示索引的哪一列被使用了，如果可能的话，是一个常数</p><p><strong>rows：</strong>MySQL认为必须检查的用来返回请求数据的行数</p><p><strong>Extra：</strong>关于MySQL如何解析查询的额外信息。将在表4.3中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思MySQL根本不能使用索引，结果是检索会很慢。</p></blockquote><h4 id="3-MySQL慢查询怎么解决"><a href="#3-MySQL慢查询怎么解决" class="headerlink" title="3. MySQL慢查询怎么解决?"></a>3. MySQL慢查询怎么解决?</h4><blockquote><ul><li>slow_query_log 慢查询开启状态。</li><li>slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）。</li><li>long_query_time 查询超过多少秒才记录。</li></ul></blockquote><h2 id="6-6-数据库锁"><a href="#6-6-数据库锁" class="headerlink" title="6.6 数据库锁"></a>6.6 数据库锁</h2><h4 id="1-mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决"><a href="#1-mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决" class="headerlink" title="1. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决?"></a>1. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决?</h4><blockquote><p><strong>MySQL有三种锁的级别：</strong>页级、表级、行级。</p><ul><li><strong>表级锁：</strong>开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。</li><li><strong>行级锁：</strong>开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。</li><li><strong>页面锁：</strong>开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般<br><strong>什么情况下会造成死锁?</strong></li></ul><p><strong>什么是死锁？</strong></p><p><strong>死锁:</strong> 是指两个或两个以上的进程在执行过程中。因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程。</p><p>表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。</p><p><strong>死锁的关键在于：</strong>两个(或以上)的Session加锁的顺序不一致。</p><p>那么对应的解决死锁问题的关键就是：让不同的session加锁有次序。</p><p><strong>死锁的解决办法?</strong></p><p>1.查出的线程杀死 kill<br>SELECT trx_MySQL_thread_id FROM information_schema.INNODB_TRX;</p><p>2.设置锁的超时时间<br>Innodb 行锁的等待时间，单位秒。可在会话级别设置，RDS 实例该参数的默认值为 50（秒）。</p><p>生产环境不推荐使用过大的 innodb_lock_wait_timeout参数值<br>该参数支持在会话级别修改，方便应用在会话级别单独设置某些特殊操作的行锁等待超时时间，如下：<br>set innodb_lock_wait_timeout=1000; —设置当前会话 Innodb 行锁等待超时时间，单位秒。</p><p>3.指定获取锁的顺序</p></blockquote><h4 id="2-有哪些锁（乐观锁悲观锁），select-时怎么加排它锁"><a href="#2-有哪些锁（乐观锁悲观锁），select-时怎么加排它锁" class="headerlink" title="2. 有哪些锁（乐观锁悲观锁），select 时怎么加排它锁?"></a>2. 有哪些锁（乐观锁悲观锁），select 时怎么加排它锁?</h4><blockquote><p><strong>悲观锁（Pessimistic Lock）:</strong></p><p><strong>悲观锁特点:</strong>先获取锁，再进行业务操作。</p><p>即“悲观”的认为获取锁是非常有可能失败的，因此要先确保获取锁成功再进行业务操作。通常所说的<strong>“一锁二查三更新”即指的是使用悲观锁。</strong>通常来讲在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select for update时会获取被select中的数据行的行锁，因此其他并发执行的select for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。</p><p><strong>补充：</strong><br>不同的数据库对select for update的实现和支持都是有所区别的，</p><ul><li>oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，MySQL就没有no wait这个选项。</li><li>MySQL还有个问题是select for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此如果在MySQL中用悲观锁务必要确定走了索引，而不是全表扫描。</li></ul><p><strong>乐观锁（Optimistic Lock）:</strong></p><p><strong>1.</strong>乐观锁，也叫乐观并发控制，它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，那么当前正在提交的事务会进行回滚。</p><p><strong>2.\</strong>*<em>乐观锁的特点先进行业务操作，不到万不得已不去拿锁。*</em>即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。<br>乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。</p><p><strong>3.</strong>一般的做法是<strong>在需要锁的数据上增加一个版本号，或者时间戳</strong>，</p><p><strong>实现方式举例如下：</strong></p><p><strong>乐观锁（给表加一个版本号字段）</strong> 这个并不是乐观锁的定义，给表加版本号，是<strong>数据库实现乐观锁的一种方式</strong>。</p><ol><li>SELECT data AS old_data, version AS old_version FROM …;</li><li>根据获取的数据进行业务操作，得到new_data和new_version</li><li>UPDATE SET data = new_data, version = new_version WHERE version = old_version</li></ol><p>if (updated row &gt; 0) {</p><p>// 乐观锁获取成功，操作完成</p><p>} else {</p><p>// 乐观锁获取失败，回滚并重试</p><p>}</p><p><strong>注意：</strong></p><ul><li>乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能</li><li>乐观锁还适用于一些比较特殊的场景，例如在业务操作过程中无法和数据库保持连接等悲观锁无法适用的地方。</li></ul><p><strong>总结：</strong><br>悲观锁和乐观锁是数据库用来保证数据并发安全防止更新丢失的两种方法，例子在select … for update前加个事务就可以防止更新丢失。悲观锁和乐观锁大部分场景下差异不大，一些独特场景下有一些差别，一般我们可以从如下几个方面来判断。</p><ul><li><strong>响应速度：</strong> 如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁。’</li><li><strong>冲突频率：</strong> 如果冲突频率非常高，建议采用悲观锁，保证成功率，如果冲突频率大，乐观锁会需要多次重试才能成功，代价比较大。</li><li><strong>重试代价：</strong> 如果重试代价大，建议采用悲观锁。</li></ul></blockquote><h1 id="七-操作系统"><a href="#七-操作系统" class="headerlink" title="七. 操作系统"></a>七. 操作系统</h1><h2 id="7-1-Linux"><a href="#7-1-Linux" class="headerlink" title="7.1 Linux"></a>7.1 Linux</h2><h3 id="1-文件管理"><a href="#1-文件管理" class="headerlink" title="1. 文件管理"></a>1. 文件管理</h3><h4 id="1-cat-命令：连接文件"><a href="#1-cat-命令：连接文件" class="headerlink" title="1. cat 命令：连接文件"></a>1. cat 命令：连接文件</h4><ul><li><p><strong>描述：</strong>cat 命令用于连接文件并打印到标准输出设备上。</p></li><li><p><strong>功能：</strong></p><pre class="line-numbers language-shell"><code class="language-shell"># 1.一次显示整个文件: cat filename# 2.从键盘创建一个文件:cat > filename# 3.将几个文件合并为一个文件:cat file1 file2 > file<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>示例：</strong></p><p>（1）把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里</p><pre class="line-numbers language-shell"><code class="language-shell">cat -n log2012.log log2013.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里</p><pre class="line-numbers language-shell"><code class="language-shell">cat -b log2012.log log2013.log log.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）使用 here doc 生成新文件</p><pre class="line-numbers language-shell"><code class="language-shell">cat >log.txt <<EOF>Hello>World>PWD=$(pwd)>EOFls -l log.txtcat log.txtHelloWorldPWD=/opt/soft/test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）反向列示</p><pre class="line-numbers language-shell"><code class="language-shell">tac log.txtPWD=/opt/soft/testWorldHello<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="2-chmod-命令-权限控制"><a href="#2-chmod-命令-权限控制" class="headerlink" title="2. chmod 命令: 权限控制"></a>2. chmod 命令: 权限控制</h4><p>Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。</p><p>用于改变 linux 系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。</p><p>每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。</p><p>以文件 log2012.log 为例：</p><pre class="line-numbers language-shell"><code class="language-shell">-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。</p><p>常用参数：</p><ul><li>-c 当发生改变时，报告处理信息</li><li>-R 处理指定目录以及其子目录下所有文件</li></ul><p>权限范围：</p><ul><li>u ：目录或者文件的当前的用户</li><li>g ：目录或者文件的当前的群组</li><li>o ：除了目录或者文件的当前用户或群组之外的用户或者群组</li><li>a ：所有的用户及群组</li></ul><p>权限代号：</p><ul><li>r ：读权限，用数字4表示</li><li>w ：写权限，用数字2表示</li><li>x ：执行权限，用数字1表示</li><li>- ：删除权限，用数字0表示</li><li>s ：特殊权限</li></ul><p>实例：</p><p>（1）增加文件 t.log 所有用户可执行权限</p><pre class="line-numbers language-shell"><code class="language-shell">chmod a+x t.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息</p><pre class="line-numbers language-shell"><code class="language-shell">chmod u=r t.log -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限</p><pre class="line-numbers language-shell"><code class="language-shell">chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）将 test 目录及其子目录所有文件添加可读权限</p><pre class="line-numbers language-shell"><code class="language-shell">chmod u+r,g+r,o+r -R text/ -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-chown-命令：更改拥有者权限"><a href="#3-chown-命令：更改拥有者权限" class="headerlink" title="3. chown 命令：更改拥有者权限"></a>3. chown 命令：更改拥有者权限</h4><p>chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。</p><ul><li>-c 显示更改的部分的信息</li><li>-R 处理指定目录及子目录</li></ul><p>实例：</p><p>（1）改变拥有者和群组 并显示改变信息</p><pre class="line-numbers language-shell"><code class="language-shell">chown -c mail:mail log2012.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）改变文件群组</p><pre class="line-numbers language-shell"><code class="language-shell">chown -c :mail t.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）改变文件夹及子文件目录属主及属组为 mail</p><pre class="line-numbers language-shell"><code class="language-shell">chown -cR mail: test/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-cp-命令：复制文件"><a href="#4-cp-命令：复制文件" class="headerlink" title="4. cp 命令：复制文件"></a>4. cp 命令：复制文件</h4><p>将源文件复制至目标文件，或将多个源文件复制至目标目录。</p><p>注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！</p><ul><li>-i 提示</li><li>-r 复制目录及目录内所有项目</li><li>-a 复制的文件与原文件时间一样</li></ul><p>实例：</p><p>（1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。</p><pre class="line-numbers language-shell"><code class="language-shell">cp -ai a.txt test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）为 a.txt 建议一个链接（快捷方式）</p><pre class="line-numbers language-shell"><code class="language-shell">cp -s a.txt link_a.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-find-命令：查找文件"><a href="#5-find-命令：查找文件" class="headerlink" title="5. find 命令：查找文件"></a>5. find 命令：查找文件</h4><p>用于在文件树中查找文件，并作出相应的处理。</p><p>命令格式：</p><pre class="line-numbers language-shell"><code class="language-shell">find pathname -options [-print -exec -ok ...]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>命令参数：</p><ul><li>pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。</li><li>-print： find命令将匹配的文件输出到标准输出。</li><li>-exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为’command’ { } ;，注意{ }和\；之间的空格。</li><li>-ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。</li></ul><p>命令选项：</p><ul><li>-name 按照文件名查找文件</li><li>-perm 按文件权限查找文件</li><li>-user 按文件属主查找文件</li><li>-group 按照文件所属的组来查找文件。</li><li>-type 查找某一类型的文件，诸如：</li></ul><ol><li>b - 块设备文件</li><li>d - 目录</li><li>c - 字符设备文件</li><li>l - 符号链接文件</li><li>p - 管道文件</li><li>f - 普通文件</li></ol><p>实例：</p><p>（1）查找 48 小时内修改过的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find -atime -2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在当前目录查找 以 .log 结尾的文件。 . 代表当前目录</p><pre class="line-numbers language-shell"><code class="language-shell">find ./ -name '*.log'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查找 /opt 目录下 权限为 777 的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find /opt -perm 777<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）查找大于 1K 的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find -size +1000c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查找等于 1000 字符的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find -size 1000c <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>-exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。</p><h4 id="6-head-命令：显示文件开头"><a href="#6-head-命令：显示文件开头" class="headerlink" title="6. head 命令：显示文件开头"></a>6. head 命令：显示文件开头</h4><p>head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。</p><p>常用参数：</p><pre class="line-numbers language-shell"><code class="language-shell">-n<行数> 显示的行数（行数为复数表示从最后向前数）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例：</p><p>（1）显示 1.log 文件中前 20 行</p><pre class="line-numbers language-shell"><code class="language-shell">head 1.log -n 20<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）显示 1.log 文件前 20 字节</p><pre class="line-numbers language-shell"><code class="language-shell">head -c 20 log2014.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）显示 t.log最后 10 行</p><pre class="line-numbers language-shell"><code class="language-shell">head -n -10 t.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="7-less-命令：文件浏览"><a href="#7-less-命令：文件浏览" class="headerlink" title="7. less 命令：文件浏览"></a>7. less 命令：文件浏览</h4><p>less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。</p><p>常用命令参数：</p><ul><li>-i 忽略搜索时的大小写</li><li>-N 显示每行的行号</li><li>-o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来</li><li>-s 显示连续空行为一行</li><li>/字符串：向下搜索“字符串”的功能</li><li>?字符串：向上搜索“字符串”的功能</li><li>n：重复前一个搜索（与 / 或 ? 有关）</li><li>N：反向重复前一个搜索（与 / 或 ? 有关）</li><li>-x &lt;数字&gt; 将“tab”键显示为规定的数字空格</li><li>b 向后翻一页</li><li>d 向后翻半页</li><li>h 显示帮助界面</li><li>Q 退出less 命令</li><li>u 向前滚动半页</li><li>y 向前滚动一行</li><li>空格键 滚动一行</li><li>回车键 滚动一页</li><li>[pagedown]： 向下翻动一页</li><li>[pageup]： 向上翻动一页</li></ul><p>实例：</p><p>（1）ps 查看进程信息并通过 less 分页显示</p><pre class="line-numbers language-shell"><code class="language-shell">ps -aux | less -N<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看多个文件</p><pre class="line-numbers language-shell"><code class="language-shell">less 1.log 2.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以使用 n 查看下一个，使用 p 查看前一个。</p><h4 id="8-ln-命令：文件同步链接"><a href="#8-ln-命令：文件同步链接" class="headerlink" title="8. ln 命令：文件同步链接"></a>8. ln 命令：文件同步链接</h4><p>功能是为文件在另外一个位置建立一个同步的链接，当在不同目录需要该问题时，就不需要为每一个目录创建同样的文件，通过 ln 创建的链接（link）减少磁盘占用量。</p><p>链接分类：软件链接及硬链接</p><p>软链接：</p><p>1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式</p><p>2.软链接可以 跨文件系统 ，硬链接不可以</p><p>3.软链接可以对一个不存在的文件名进行链接</p><p>4.软链接可以对目录进行链接</p><p>硬链接:</p><p>1.硬链接，以文件副本的形式存在。但不占用实际空间。</p><p>2.不允许给目录创建硬链接</p><p>3.硬链接只有在同一个文件系统中才能创建</p><p>需要注意：</p><p>第一：ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化；</p><p>第二：ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。</p><p>第三：ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。</p><p>常用参数：</p><ol><li>-b 删除，覆盖以前建立的链接</li><li>-s 软链接（符号链接）</li><li>-v 显示详细处理过程</li></ol><p>实例：</p><p>（1）给文件创建软链接，并显示操作信息</p><pre class="line-numbers language-shell"><code class="language-shell">ln -sv source.log link.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）给文件创建硬链接，并显示操作信息</p><pre class="line-numbers language-shell"><code class="language-shell">ln -v source.log link1.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）给目录创建软链接</p><pre class="line-numbers language-shell"><code class="language-shell">ln -sv /opt/soft/test/test3 /opt/soft/test/test5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="9-locate-命令：快速查找档案"><a href="#9-locate-命令：快速查找档案" class="headerlink" title="9. locate 命令：快速查找档案"></a>9. locate 命令：快速查找档案</h4><p>​       locate 通过搜寻系统内建文档数据库达到快速找到档案，数据库由 updatedb 程序来更新，updatedb 是由 cron daemon 周期性调用的。默认情况下 locate 命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是 locate 所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb 每天会跑一次，可以由修改 crontab 来更新设定值 (etc/crontab)。</p><p>locate 与 find 命令相似，可以使用如 *、? 等进行正则匹配查找</p><p>常用参数：</p><ol><li>-l num（要显示的行数）</li><li>-f 将特定的档案系统排除在外，如将proc排除在外</li><li>-r 使用正则运算式做为寻找条件</li></ol><p>实例：</p><p>（1）查找和 pwd 相关的所有文件(文件名中包含 pwd）</p><pre class="line-numbers language-shell"><code class="language-shell">locate pwd<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）搜索 etc 目录下所有以 sh 开头的文件</p><pre class="line-numbers language-shell"><code class="language-shell">locate /etc/sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查找 /var 目录下，以 reason 结尾的文件</p><pre class="line-numbers language-shell"><code class="language-shell">locate -r '^/var.*reason$'（其中.表示一个字符，*表示任务多个；.*表示任意多个字符）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="10-more-命令：文件分页浏览"><a href="#10-more-命令：文件分页浏览" class="headerlink" title="10. more 命令：文件分页浏览"></a>10. more 命令：文件分页浏览</h4><p>​      功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。</p><p>命令参数：</p><ul><li>+n 从笫 n 行开始显示</li><li>-n 定义屏幕大小为n行</li><li>+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示</li><li>-c 从顶部清屏，然后显示</li><li>-d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能</li><li>-l 忽略Ctrl+l（换页）字符</li><li>-p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似</li><li>-s 把连续的多个空行显示为一行</li><li>-u 把文件内容中的下画线去掉</li></ul><p>常用操作命令：</p><ul><li>Enter 向下 n 行，需要定义。默认为 1 行</li><li>Ctrl+F 向下滚动一屏</li><li>空格键 向下滚动一屏</li><li>Ctrl+B 返回上一屏</li><li>= 输出当前行的行号</li><li>:f 输出文件名和当前行的行号</li><li>V 调用vi编辑器</li><li>!命令 调用Shell，并执行命令</li><li>q 退出more</li></ul><p>实例：</p><p>（1）显示文件中从第3行起的内容</p><pre class="line-numbers language-shell"><code class="language-shell">more +3 text.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在所列出文件目录详细信息，借助管道使每次显示 5 行</p><pre class="line-numbers language-shell"><code class="language-shell">ls -l | more -5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>按空格显示下 5 行。</p><h4 id="11-mv-命令：文件移动"><a href="#11-mv-命令：文件移动" class="headerlink" title="11. mv 命令：文件移动"></a>11. mv 命令：文件移动</h4><p>移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。</p><p>当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。</p><p>实例：</p><p>（1）将文件 test.log 重命名为 test1.txt</p><pre class="line-numbers language-shell"><code class="language-shell">mv test.log test1.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中</p><pre class="line-numbers language-shell"><code class="language-shell">mv llog1.txt log2.txt log3.txt /test3<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖</p><pre class="line-numbers language-shell"><code class="language-shell">mv -i log1.txt log2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）移动当前文件夹下的所有文件到上一级目录</p><pre class="line-numbers language-shell"><code class="language-shell">mv * ../<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="12-rm-命令：文件删除"><a href="#12-rm-命令：文件删除" class="headerlink" title="12. rm 命令：文件删除"></a>12. rm 命令：文件删除</h4><p>​     删除一个目录中的一个或多个文件或目录，如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。</p><pre class="line-numbers language-shell"><code class="language-shell">rm [选项] 文件…<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例：</p><p>（1）删除任何 .log 文件，删除前逐一询问确认：</p><pre class="line-numbers language-shell"><code class="language-shell">rm -i *.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）删除 test 子目录及子目录中所有档案删除，并且不用一一确认：</p><pre class="line-numbers language-shell"><code class="language-shell">rm -rf test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）删除以 -f 开头的文件</p><pre class="line-numbers language-shell"><code class="language-shell">rm -- -f*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="13-tail-命令：显示文件末尾"><a href="#13-tail-命令：显示文件末尾" class="headerlink" title="13. tail 命令：显示文件末尾"></a>13. tail 命令：显示文件末尾</h4><p>用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。</p><p>常用参数：</p><ul><li>-f 循环读取（常用于查看递增的日志文件）</li><li>-n&lt;行数&gt; 显示行数（从后向前）</li></ul><p>（1）循环读取逐渐增加的文件内容</p><pre class="line-numbers language-shell"><code class="language-shell">ping 127.0.0.1 > ping.log &<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。</p><pre class="line-numbers language-shell"><code class="language-shell">tail -f ping.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（查看日志）</p><h4 id="14-touch-命令：修改时间属性"><a href="#14-touch-命令：修改时间属性" class="headerlink" title="14. touch 命令：修改时间属性"></a>14. touch 命令：修改时间属性</h4><p>Linux touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。</p><p>ls -l 可以显示档案的时间记录。</p><p>语法</p><pre class="line-numbers language-shell"><code class="language-shell">touch [-acfm][-d<日期时间>][-r<参考文件或目录>] [-t<日期时间>][--help][--version][文件或目录…]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><ul><li>a 改变档案的读取时间记录。</li><li>m 改变档案的修改时间记录。</li><li>c 假如目的档案不存在，不会建立新的档案。与 –no-create 的效果一样。</li><li>f 不使用，是为了与其他 unix 系统的相容性而保留。</li><li>r 使用参考档的时间记录，与 –file 的效果一样。</li><li>d 设定时间与日期，可以使用各种不同的格式。</li><li>t 设定档案的时间记录，格式与 date 指令相同。</li><li>–no-create 不会建立新档案。</li><li>–help 列出指令格式。</li><li>–version 列出版本讯息。</li></ul><p>实例</p><p>使用指令”touch”修改文件”testfile”的时间属性为当前系统时间，输入如下命令：</p><pre class="line-numbers language-shell"><code class="language-shell">$ touch testfile                #修改文件的时间属性 <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>首先，使用ls命令查看testfile文件的属性，如下所示：</p><pre class="line-numbers language-shell"><code class="language-shell">$ ls -l testfile                #查看文件的时间属性  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>#原来文件的修改时间为16:09</p><pre class="line-numbers language-shell"><code class="language-shell">-rw-r--r-- 1 hdd hdd 55 2011-08-22 16:09 testfile  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行指令”touch”修改文件属性以后，并再次查看该文件的时间属性，如下所示：</p><pre class="line-numbers language-shell"><code class="language-shell">$ touch testfile                #修改文件时间属性为当前系统时间  $ ls -l testfile                #查看文件的时间属性  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>#修改后文件的时间属性为当前系统时间</p><pre class="line-numbers language-shell"><code class="language-shell">-rw-r--r-- 1 hdd hdd 55 2011-08-22 19:53 testfile  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用指令”touch”时，如果指定的文件不存在，则将创建一个新的空白文件。例如，在当前目录下，使用该指令创建一个空白文件”file”，输入如下命令：</p><pre class="line-numbers language-shell"><code class="language-shell">$ touch file            #创建一个名为“file”的新的空白文件 <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="15-vim-命令：文本编辑器"><a href="#15-vim-命令：文本编辑器" class="headerlink" title="15. vim 命令：文本编辑器"></a>15. vim 命令：文本编辑器</h4><p>Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。</p><p>打开文件并跳到第 10 行：vim +10 filename.txt 。</p><p>打开文件跳到第一个匹配的行：vim +/search-term filename.txt 。</p><p>以只读模式打开文件：vim -R /etc/passwd 。</p><p>基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。</p><p>简单的说，我们可以将这三个模式想成底下的图标来表示：</p><img src="https://pic3.zhimg.com/80/v2-3545dde7a3b6e7d389b95f327b7508f2_1440w.jpg" alt="img" style="zoom:50%;"><h4 id="16-whereis-命令：搜索程序名"><a href="#16-whereis-命令：搜索程序名" class="headerlink" title="16. whereis 命令：搜索程序名"></a>16. whereis 命令：搜索程序名</h4><p>whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis 及 locate 都是基于系统内建的数据库进行搜索，因此效率很高，而find则是遍历硬盘查找文件。</p><p>常用参数：</p><ul><li>-b 定位可执行文件。</li><li>-m 定位帮助文件。</li><li>-s 定位源代码文件。</li><li>-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。</li></ul><p>实例：</p><p>（1）查找 locate 程序相关文件</p><pre class="line-numbers language-shell"><code class="language-shell">whereis locate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查找 locate 的源码文件</p><pre class="line-numbers language-shell"><code class="language-shell">whereis -s locate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查找 lcoate 的帮助文件</p><pre class="line-numbers language-shell"><code class="language-shell">whereis -m locate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="17-which-命令：查找文件"><a href="#17-which-命令：查找文件" class="headerlink" title="17. which 命令：查找文件"></a>17. which 命令：查找文件</h4><p>在 linux 要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索：</p><ul><li>which 查看可执行文件的位置。</li><li>whereis 查看文件的位置。</li><li>locate 配合数据库查看文件位置。</li><li>find 实际搜寻硬盘查询文件名称。</li></ul><p>which 是在 PATH 就是指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果。使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。</p><p>常用参数：</p><pre class="line-numbers language-shell"><code class="language-shell">-n 　指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例：</p><p>（1）查看 ls 命令是否存在，执行哪个</p><pre class="line-numbers language-shell"><code class="language-shell">which ls<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看 which</p><pre class="line-numbers language-shell"><code class="language-shell">which which<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查看 cd</p><pre class="line-numbers language-shell"><code class="language-shell">which cd（显示不存在，因为 cd 是内建命令，而 which 查找显示是 PATH 中的命令）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看当前 PATH 配置：</p><pre class="line-numbers language-shell"><code class="language-shell">echo $PATH<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>或使用 env 查看所有环境变量及对应值</p><h3 id="2-文档编辑"><a href="#2-文档编辑" class="headerlink" title="2. 文档编辑"></a>2. 文档编辑</h3><h4 id="1-grep-命令：文本搜索"><a href="#1-grep-命令：文本搜索" class="headerlink" title="1. grep 命令：文本搜索"></a>1. grep 命令：文本搜索</h4><p>强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。</p><p>grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。</p><p>命令格式：</p><pre class="line-numbers language-shell"><code class="language-shell">grep [option] pattern file|dir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>常用参数：</p><ul><li>-A n –after-context显示匹配字符后n行</li><li>-B n –before-context显示匹配字符前n行</li><li>-C n –context 显示匹配字符前后n行</li><li>-c –count 计算符合样式的列数</li><li>-i 忽略大小写</li><li>-l 只列出文件内容符合指定的样式的文件名称</li><li>-f 从文件中读取关键词</li><li>-n 显示匹配内容的所在文件中行数</li><li>-R 递归查找文件夹</li></ul><p>grep 的规则表达式:</p><ul><li>^  #锚定行的开始 如：’^grep’匹配所有以grep开头的行。</li><li>$ #锚定行的结束 如：’grep’匹配所有以grep结尾的行。</li><li>. #匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。</li><li>* #匹配零个或多个先前字符 如：’*grep’匹配所有一个或多个空格后紧跟grep的行。</li><li>.* #一起用代表任意字符。</li><li>[] #匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。</li><li>[^] #匹配一个不在指定范围内的字符，如：’[^A-FH-Z]rep’匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。</li><li>(..) #标记匹配字符，如’(love)‘，love被标记为1。</li><li>&lt; #锚定单词的开始，如:’&lt;grep’匹配包含以grep开头的单词的行。</li><li>&gt; #锚定单词的结束，如’grep&gt;‘匹配包含以grep结尾的单词的行。</li><li>x{m} #重复字符x，m次，如：’0{5}‘匹配包含5个o的行。</li><li>x{m,} #重复字符x,至少m次，如：’o{5,}‘匹配至少有5个o的行。</li><li>x{m,n} #重复字符x，至少m次，不多于n次，如：’o{5,10}‘匹配5–10个o的行。</li><li>\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：’G\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p。</li><li>\W #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。</li><li>\b #单词锁定符，如: ‘\bgrep\b’只匹配grep。</li></ul><p>实例：</p><p>（1）查找指定进程</p><pre class="line-numbers language-shell"><code class="language-shell">ps -ef | grep svn<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查找指定进程个数</p><pre class="line-numbers language-shell"><code class="language-shell">ps -ef | grep svn -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）从文件中读取关键词</p><pre class="line-numbers language-shell"><code class="language-shell">cat test1.txt | grep -f key.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）从文件夹中递归查找以grep开头的行，并只列出文件</p><pre class="line-numbers language-shell"><code class="language-shell">grep -lR '^grep' /tmp<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）查找非x开关的行内容</p><pre class="line-numbers language-shell"><code class="language-shell">grep '^[^x]' test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（6）显示包含 ed 或者 at 字符的内容行</p><pre class="line-numbers language-shell"><code class="language-shell">grep -E 'ed|at' test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-wc-命令：文件统计"><a href="#2-wc-命令：文件统计" class="headerlink" title="2. wc 命令：文件统计"></a>2. wc 命令：文件统计</h4><p>wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出</p><p>命令格式：</p><pre class="line-numbers language-shell"><code class="language-shell">wc [option] file..<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>命令参数：</p><ul><li>-c 统计字节数</li><li>-l 统计行数</li><li>-m 统计字符数</li><li>-w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串</li></ul><p>实例：</p><p>（1）查找文件的 行数 单词数 字节数 文件名</p><pre class="line-numbers language-shell"><code class="language-shell">wc text.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-shell"><code class="language-shell">7     8     70     test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）统计输出结果的行数</p><pre class="line-numbers language-shell"><code class="language-shell">cat test.txt | wc -l<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-磁盘管理"><a href="#3-磁盘管理" class="headerlink" title="3. 磁盘管理"></a>3. 磁盘管理</h3><h4 id="1-cd-命令：切换目录"><a href="#1-cd-命令：切换目录" class="headerlink" title="1. cd 命令：切换目录"></a>1. cd 命令：切换目录</h4><p>cd(changeDirectory) 命令语法：</p><pre class="line-numbers language-shell"><code class="language-shell">cd [目录名]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>说明：切换当前目录至 dirName。</p><p>实例：</p><p>（1）进入要目录</p><pre class="line-numbers language-shell"><code class="language-shell">cd /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）进入 “home” 目录</p><pre class="line-numbers language-text"><code class="language-text">cd ~<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）进入上一次工作路径</p><pre class="line-numbers language-text"><code class="language-text">cd -<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）把上个命令的参数作为cd参数使用。</p><pre class="line-numbers language-shell"><code class="language-shell">cd !$<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-df-命令：显示磁盘空间"><a href="#2-df-命令：显示磁盘空间" class="headerlink" title="2. df 命令：显示磁盘空间"></a>2. df 命令：显示磁盘空间</h4><p>显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示：</p><ol><li>-a 全部文件系统列表</li><li>-h 以方便阅读的方式显示信息</li><li>-i 显示inode信息</li><li>-k 区块为1024字节</li><li>-l 只显示本地磁盘</li><li>-T 列出文件系统类型</li></ol><p>实例：</p><p>（1）显示磁盘使用情况</p><pre class="line-numbers language-shell"><code class="language-shell">df -l<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）以易读方式列出所有文件系统及其类型</p><pre class="line-numbers language-shell"><code class="language-shell">df -haT<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-du-命令：查看文件使用空间"><a href="#3-du-命令：查看文件使用空间" class="headerlink" title="3. du 命令：查看文件使用空间"></a>3. du 命令：查看文件使用空间</h4><p>du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看：</p><p>命令格式：</p><pre class="line-numbers language-text"><code class="language-text">du [选项] [文件]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>常用参数：</p><ul><li>-a 显示目录中所有文件大小</li><li>-k 以KB为单位显示文件大小</li><li>-m 以MB为单位显示文件大小</li><li>-g 以GB为单位显示文件大小</li><li>-h 以易读方式显示文件大小</li><li>-s 仅显示总计</li><li>-c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和</li></ul><p>实例：</p><p>（1）以易读方式显示文件夹内及子文件夹大小</p><pre class="line-numbers language-text"><code class="language-text">du -h scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）以易读方式显示文件夹内所有文件大小</p><pre class="line-numbers language-text"><code class="language-text">du -ah scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和</p><pre class="line-numbers language-text"><code class="language-text">du -hc test/ scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）输出当前目录下各个子目录所使用的空间</p><pre class="line-numbers language-text"><code class="language-text">du -hc --max-depth=1 scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-ls命令：查看文件夹文件"><a href="#4-ls命令：查看文件夹文件" class="headerlink" title="4. ls命令：查看文件夹文件"></a>4. ls命令：查看文件夹文件</h4><p>就是 list 的缩写，通过 ls 命令不仅可以查看 linux 文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)查看目录信息等等。</p><p>常用参数搭配：</p><ul><li>ls -a 列出目录所有文件，包含以.开始的隐藏文件</li><li>ls -A 列出除.及..的其它文件</li><li>ls -r 反序排列</li><li>ls -t 以文件修改时间排序</li><li>ls -S 以文件大小排序</li><li>ls -h 以易读大小显示</li><li>ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来</li></ul><p>实例：</p><p>(1) 按易读方式按时间反序排序，并显示文件详细信息</p><pre class="line-numbers language-text"><code class="language-text">ls -lhrt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(2) 按大小反序显示文件详细信息</p><pre class="line-numbers language-text"><code class="language-text">ls -lrS<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(3)列出当前目录中所有以”t”开头的目录的详细内容</p><pre class="line-numbers language-text"><code class="language-text">ls -l t*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(4) 列出文件绝对路径（不包含隐藏文件）</p><pre class="line-numbers language-text"><code class="language-text">ls | sed "s:^:`pwd`/:"<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(5) 列出文件绝对路径（包含隐藏文件）</p><pre class="line-numbers language-text"><code class="language-text">find $pwd -maxdepth 1 | xargs ls -ld<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-mkdir-命令：创建文件夹"><a href="#5-mkdir-命令：创建文件夹" class="headerlink" title="5. mkdir 命令：创建文件夹"></a>5. mkdir 命令：创建文件夹</h4><p>mkdir 命令用于创建文件夹。</p><p>可用选项：</p><ul><li>-m: 对新建目录设置存取权限，也可以用 chmod 命令设置;</li><li>-p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。</li></ul><p>实例：</p><p>（1）当前工作目录下创建名为 t的文件夹</p><pre class="line-numbers language-text"><code class="language-text">mkdir t<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建：</p><pre class="line-numbers language-text"><code class="language-text">mkdir -p /tmp/test/t1/t<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="6-pwd-命令：查看当前目录"><a href="#6-pwd-命令：查看当前目录" class="headerlink" title="6. pwd 命令：查看当前目录"></a>6. pwd 命令：查看当前目录</h4><p>pwd 命令用于查看当前工作目录路径。</p><p>实例：</p><p>（1）查看当前路径</p><pre class="line-numbers language-shell"><code class="language-shell">pw<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看软链接的实际路径</p><pre class="line-numbers language-shell"><code class="language-shell">pwd -P<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="7-rmdir-命令：删除空目录"><a href="#7-rmdir-命令：删除空目录" class="headerlink" title="7. rmdir 命令：删除空目录"></a>7. rmdir 命令：删除空目录</h4><p>从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。</p><p>注意：不能删除非空目录</p><p>实例：</p><p>（1）当 parent 子目录被删除后使它也成为空目录的话，则顺便一并删除：</p><pre class="line-numbers language-text"><code class="language-text">rmdir -p parent/child/child11<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-网络通讯"><a href="#4-网络通讯" class="headerlink" title="4. 网络通讯"></a>4. 网络通讯</h3><h4 id="1-ifconfig-命令：查看网络端口"><a href="#1-ifconfig-命令：查看网络端口" class="headerlink" title="1. ifconfig 命令：查看网络端口"></a>1. ifconfig 命令：查看网络端口</h4><p>ifconfig 用于查看和配置 Linux 系统的网络接口。</p><p>查看所有网络接口及其状态：ifconfig -a 。</p><p>使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。</p><h4 id="2-iptables-命令：开放端口"><a href="#2-iptables-命令：开放端口" class="headerlink" title="2. iptables 命令：开放端口"></a>2. iptables 命令：开放端口</h4><p>iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如</p><p>把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp –dport 80 -j REJECT 。</p><p>开启 80 端口，因为web对外都是这个端口</p><pre class="line-numbers language-text"><code class="language-text">iptables -A INPUT -p tcp --dport 80 -j ACCEP<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。</p><h4 id="3-netstat-命令：查看网络连接状态"><a href="#3-netstat-命令：查看网络连接状态" class="headerlink" title="3. netstat 命令：查看网络连接状态"></a>3. netstat 命令：查看网络连接状态</h4><p>Linux netstat命令用于显示网络状态。</p><p>利用netstat指令可让你得知整个Linux系统的网络情况。</p><p>语法</p><pre class="line-numbers language-text"><code class="language-text">netstat [-acCeFghilMnNoprstuvVwx][-A<网络类型>][--ip]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><ul><li>-a或–all 显示所有连线中的Socket。</li><li>-A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。</li><li>-c或–continuous 持续列出网络状态。</li><li>-C或–cache 显示路由器配置的快取信息。</li><li>-e或–extend 显示网络其他相关信息。</li><li>-F或–fib 显示FIB。</li><li>-g或–groups 显示多重广播功能群组组员名单。</li><li>-h或–help 在线帮助。</li><li>-i或–interfaces 显示网络界面信息表单。</li><li>-l或–listening 显示监控中的服务器的Socket。</li><li>-M或–masquerade 显示伪装的网络连线。</li><li>-n或–numeric 直接使用IP地址，而不通过域名服务器。</li><li>-N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。</li><li>-o或–timers 显示计时器。</li><li>-p或–programs 显示正在使用Socket的程序识别码和程序名称。</li><li>-r或–route 显示Routing Table。</li><li>-s或–statistice 显示网络工作信息统计表。</li><li>-t或–tcp 显示TCP传输协议的连线状况。</li><li>-u或–udp 显示UDP传输协议的连线状况。</li><li>-v或–verbose 显示指令执行过程。</li><li>-V或–version 显示版本信息。</li><li>-w或–raw 显示RAW传输协议的连线状况。</li><li>-x或–unix 此参数的效果和指定”-A unix”参数相同。</li><li>–ip或–inet 此参数的效果和指定”-A inet”参数相同。</li></ul><p><strong>实例</strong></p><p><strong>1）如何查看系统都开启了哪些端口？</strong></p><p><img src="https://pic3.zhimg.com/80/v2-df1430b4718867e521b68d27abfb700e_1440w.jpg" alt="img"></p><p><strong>2）如何查看网络连接状况？</strong></p><p><img src="https://pic2.zhimg.com/80/v2-5adefdeefa70d35ca5b59b8a9cc9acf5_1440w.jpg" alt="img"></p><p><strong>3）如何统计系统当前进程连接数？</strong></p><p>输入命令 <code>netstat -an | grep ESTABLISHED | wc -l</code> 。</p><p>输出结果 177 。一共有 177 连接数。</p><p><strong>4）用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？</strong></p><p>严格来说，这个题目考验的是对 awk 的使用。</p><p>首先，使用 <code>netstat -an|grep ESTABLISHED</code> 命令。结果如下：</p><p><img src="https://pic4.zhimg.com/80/v2-f7058b4652870858da60759619b9870b_1440w.jpg" alt="img"></p><h4 id="4-ping-命令：检测主机网络连接"><a href="#4-ping-命令：检测主机网络连接" class="headerlink" title="4. ping 命令：检测主机网络连接"></a>4. ping 命令：检测主机网络连接</h4><p>Linux ping命令用于检测主机。</p><p>执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。</p><p>指定接收包的次数</p><pre class="line-numbers language-text"><code class="language-text">ping -c 2 百度一下，你就知道<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-telnet-命令：远端登陆"><a href="#5-telnet-命令：远端登陆" class="headerlink" title="5.telnet 命令：远端登陆"></a>5.telnet 命令：远端登陆</h4><p>Linux telnet命令用于远端登入。</p><p>执行telnet指令开启终端机阶段作业，并登入远端主机。</p><p>语法</p><pre class="line-numbers language-text"><code class="language-text">telnet [-8acdEfFKLrx][-b<主机别名>][-e<脱离字符>][-k<域名>][-l<用户名称>][-n<记录文件>][-S<服务类型>][-X<认证形态>][主机名称或IP地址<通信端口>]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><pre class="line-numbers language-text"><code class="language-text">-8 允许使用8位字符资料，包括输入与输出。-a 尝试自动登入远端系统。-b<主机别名> 使用别名指定远端主机名称。-c 不读取用户专属目录里的.telnetrc文件。-d 启动排错模式。-e<脱离字符> 设置脱离字符。-E 滤除脱离字符。-f 此参数的效果和指定"-F"参数相同。-F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。-k<域名> 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。-K 不自动登入远端主机。-l<用户名称> 指定要登入远端主机的用户名称。-L 允许输出8位字符资料。-n<记录文件> 指定文件记录相关信息。-r 使用类似rlogin指令的用户界面。-S<服务类型> 设置telnet连线所需的IP TOS信息。-x 假设主机有支持数据加密的功能，就使用它。-X<认证形态> 关闭指定的认证形态。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实例</p><p>登录远程主机</p><pre class="line-numbers language-shell"><code class="language-shell"># 登录IP为 192.168.0.5 的远程主机telnet 192.168.0.5 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="5-系统管理"><a href="#5-系统管理" class="headerlink" title="5. 系统管理"></a>5. 系统管理</h3><h4 id="1-date-命令：显示系统日期"><a href="#1-date-命令：显示系统日期" class="headerlink" title="1. date 命令：显示系统日期"></a>1. date 命令：显示系统日期</h4><p>显示或设定系统的日期与时间。</p><p>命令参数：</p><ul><li>-d&lt;字符串&gt; 　显示字符串所指的日期与时间。字符串前后必须加上双引号。</li><li>-s&lt;字符串&gt; 　根据字符串来设置日期与时间。字符串前后必须加上双引号。</li></ul><p>-u 　显示GMT。</p><p>%H 小时(00-23)</p><p>%I 小时(00-12)</p><p>%M 分钟(以00-59来表示)</p><p>%s 总秒数。起算时间为1970-01-01 00:00:00 UTC。</p><p>%S 秒(以本地的惯用法来表示)</p><p>%a 星期的缩写。</p><p>%A 星期的完整名称。</p><p>%d 日期(以01-31来表示)。</p><p>%D 日期(含年月日)。</p><p>%m 月份(以01-12来表示)。</p><p>%y 年份(以00-99来表示)。</p><p>%Y 年份(以四位数来表示)。</p><p>实例：</p><p>（1）显示下一天</p><pre class="line-numbers language-text"><code class="language-text">date +%Y%m%d --date="+1 day"  //显示下一天的日期<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）-d参数使用</p><pre class="line-numbers language-text"><code class="language-text">date -d "nov 22"  今年的 11 月 22 日是星期三date -d '2 weeks' 2周后的日期date -d 'next monday' (下周一的日期)date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%ddate -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%ddate -d last-month +%Y%m(上个月是几月)date -d next-month +%Y%m(下个月是几月)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-free-命令：显示内存使用情况"><a href="#2-free-命令：显示内存使用情况" class="headerlink" title="2. free 命令：显示内存使用情况"></a>2. free 命令：显示内存使用情况</h4><p>显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。</p><p>命令参数：</p><ul><li>-b 以Byte显示内存使用情况</li><li>-k 以kb为单位显示内存使用情况</li><li>-m 以mb为单位显示内存使用情况</li><li>-g 以gb为单位显示内存使用情况</li><li>-s&lt;间隔秒数&gt; 持续显示内存</li><li>-t 显示内存使用总合</li></ul><p>实例：</p><p>（1）显示内存使用情况</p><pre class="line-numbers language-text"><code class="language-text">freefree -kfree -m<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）以总和的形式显示内存的使用信息</p><pre class="line-numbers language-text"><code class="language-text">free -t<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）周期性查询内存使用情况</p><pre class="line-numbers language-text"><code class="language-text">free -s 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-kill-命令：杀掉指定进程"><a href="#3-kill-命令：杀掉指定进程" class="headerlink" title="3. kill 命令：杀掉指定进程"></a>3. kill 命令：杀掉指定进程</h4><p>发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用”-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。</p><p>常用参数：</p><ul><li>-l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称</li><li>-a 当处理当前进程时，不限制命令名和进程号的对应关系</li><li>-p 指定kill 命令只打印相关进程的进程号，而不发送任何信号</li><li>-s 指定发送信号</li><li>-u 指定用户</li></ul><p>实例：</p><p>（1）先使用ps查找进程pro1，然后用kill杀掉</p><pre class="line-numbers language-text"><code class="language-text">kill -9 $(ps -ef | grep pro1)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-ps-命令：查看运行进程（快照）"><a href="#4-ps-命令：查看运行进程（快照）" class="headerlink" title="4. ps 命令：查看运行进程（快照）"></a>4. ps 命令：查看运行进程（快照）</h4><p>ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top</p><p>linux上进程有5种状态:</p><ol><li>运行(正在运行或在运行队列中等待)</li><li>中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号)</li><li>不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生)</li><li>僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放)</li><li>停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行)</li></ol><p>ps 工具标识进程的5种状态码:</p><ol><li>D 不可中断 uninterruptible sleep (usually IO)</li><li>R 运行 runnable (on run queue)</li><li>S 中断 sleeping</li><li>T 停止 traced or stopped</li><li>Z 僵死 a defunct (”zombie”) process</li></ol><p>命令参数：</p><ul><li>-A 显示所有进程</li><li>a 显示所有进程</li><li>-a 显示同一终端下所有进程</li><li>c 显示进程真实名称</li><li>e 显示环境变量</li><li>f 显示进程间的关系</li><li>r 显示当前终端运行的进程</li><li>-aux 显示所有包含其它使用的进程</li></ul><p>实例：</p><p>（1）显示当前所有进程环境变量及进程间关系</p><pre class="line-numbers language-text"><code class="language-text">ps -ef<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）显示当前所有进程</p><pre class="line-numbers language-text"><code class="language-text">ps -A<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）与grep联用查找某进程</p><pre class="line-numbers language-text"><code class="language-text">ps -aux | grep apache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）找出与 cron 与 syslog 这两个服务有关的 PID 号码</p><pre class="line-numbers language-text"><code class="language-text">ps aux | grep '(cron|syslog)'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-rpm-命令：管理套件"><a href="#5-rpm-命令：管理套件" class="headerlink" title="5. rpm 命令：管理套件"></a>5. rpm 命令：管理套件</h4><p>Linux rpm 命令用于管理套件。</p><p>rpm(redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。</p><p># 查看系统自带jdk</p><pre class="line-numbers language-text"><code class="language-text">rpm -qa | grep jdk<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 删除系统自带jdk</p><pre class="line-numbers language-text"><code class="language-text">rpm -e --nodeps 查看jdk显示的数据<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 安装jdk</p><pre class="line-numbers language-text"><code class="language-text">rpm -ivh jdk-7u80-linux-x64.rpm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="6-top-命令：显示当前进程信息（动态）"><a href="#6-top-命令：显示当前进程信息（动态）" class="headerlink" title="6. top 命令：显示当前进程信息（动态）"></a>6. top 命令：显示当前进程信息（动态）</h4><p>显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等</p><p>常用参数：</p><ul><li>-c 显示完整的进程命令</li><li>-s 保密模式</li><li>-p &lt;进程号&gt; 指定进程显示</li><li>-n &lt;次数&gt;循环显示次数</li></ul><p>实例：</p><p><img src="https://pic1.zhimg.com/80/v2-a229fdfac92fa6b7402522898d55899c_1440w.jpg" alt="img"></p><blockquote><p>前五行是当前系统情况整体的统计信息区。</p><p><strong>第一行，任务队列信息，同 uptime 命令的执行结果</strong>，具体参数说明情况如下：</p><p>14:06:23 — 当前系统时间</p><p>up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！）</p><p>2 users — 当前有2个用户登录系统</p><p>load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。</p><p>load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。</p><p><strong>第二行，Tasks — 任务（进程）</strong>，具体信息说明如下：</p><p>系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。</p><p><strong>第三行，cpu状态信息</strong>，具体属性说明如下：</p><ul><li>5.9%us — 用户空间占用CPU的百分比。</li><li>3.4% sy — 内核空间占用CPU的百分比。</li><li>0.0% ni — 改变过优先级的进程占用CPU的百分比</li><li>90.4% id — 空闲CPU百分比</li><li>0.0% wa — IO等待占用CPU的百分比</li><li>0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比</li><li>0.2% si — 软中断（Software Interrupts）占用CPU的百分比</li></ul><p>备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！</p><p><strong>第四行，内存状态</strong>，具体信息如下：</p><p>32949016k total — 物理内存总量（32GB）</p><p>14411180k used — 使用中的内存总量（14GB）</p><p>18537836k free — 空闲内存总量（18GB）</p><p>169884k buffers — 缓存的内存量 （169M）</p><p><strong>第五行，swap交换分区信息</strong>，具体信息说明如下：</p><p>32764556k total — 交换区总量（32GB）</p><p>0k used — 使用的交换区总量（0K）</p><p>32764556k free — 空闲交换区总量（32GB）</p><p>3612636k cached — 缓冲的交换区总量（3.6GB）</p><p><strong>第六行，空行</strong>。</p><p>第七行以下：各进程（任务）的状态监控，项目列信息说明如下：</p><p>PID — 进程id</p><p>USER — 进程所有者</p><p>PR — 进程优先级</p><p>NI — nice值。负值表示高优先级，正值表示低优先级</p><p>VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES</p><p>RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA</p><p>SHR — 共享内存大小，单位kb</p><p>S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程</p><p>%CPU — 上次更新到现在的CPU时间占用百分比</p><p>%MEM — 进程使用的物理内存百分比</p><p>TIME+ — 进程使用的CPU时间总计，单位1/100秒</p><p>COMMAND — 进程名称（命令名/命令行）</p></blockquote><h4 id="7-top-交互命令："><a href="#7-top-交互命令：" class="headerlink" title="7. top 交互命令："></a>7. top 交互命令：</h4><ul><li>h 显示top交互命令帮助信息</li><li>c 切换显示命令名称和完整命令行</li><li>m 以内存使用率排序</li><li>P 根据CPU使用百分比大小进行排序</li><li>T 根据时间/累计时间进行排序</li><li>W 将当前设置写入~/.toprc文件中</li><li>o或者O 改变显示项目的顺序</li></ul><h4 id="8-yum-命令：软件包管理器"><a href="#8-yum-命令：软件包管理器" class="headerlink" title="8. yum 命令：软件包管理器"></a>8. yum 命令：软件包管理器</h4><p>yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。</p><p>基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。</p><p>yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。</p><p>1.列出所有可更新的软件清单命令：yum check-update</p><p>2.更新所有软件命令：yum update</p><p>3.仅安装指定的软件命令：yum install <package_name></package_name></p><p>4.仅更新指定的软件命令：yum update <package_name></package_name></p><p>5.列出所有可安裝的软件清单命令：yum list</p><p>6.删除软件包命令：yum remove <package_name></package_name></p><p>7.查找软件包 命令：yum search</p><p>8.清除缓存命令:</p><p>yum clean packages: 清除缓存目录下的软件包</p><p>yum clean headers: 清除缓存目录下的 headers</p><p>yum clean oldheaders: 清除缓存目录下旧的 headers</p><p>yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers</p><p>实例</p><p>安装 pam-devel</p><pre class="line-numbers language-text"><code class="language-text">[root@www ~]# yum install pam-devel<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="6-备份压缩"><a href="#6-备份压缩" class="headerlink" title="6. 备份压缩"></a>6. 备份压缩</h3><h4 id="1-bzip2-命令：bz2文件压缩解压"><a href="#1-bzip2-命令：bz2文件压缩解压" class="headerlink" title="1. bzip2 命令：bz2文件压缩解压"></a>1. bzip2 命令：bz2文件压缩解压</h4><pre class="line-numbers language-text"><code class="language-text">创建 *.bz2 压缩文件：bzip2 test.txt 。解压 *.bz2 文件：bzip2 -d test.txt.bz2 。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="2-gzip-命令：gz文件压缩解压"><a href="#2-gzip-命令：gz文件压缩解压" class="headerlink" title="2. gzip 命令：gz文件压缩解压"></a>2. gzip 命令：gz文件压缩解压</h4><pre class="line-numbers language-text"><code class="language-text">创建一个 *.gz 的压缩文件：gzip test.txt 。解压 *.gz 文件：gzip -d test.txt.gz 。显示压缩的比率：gzip -l *.gz 。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="3-tar-命令：文件打包"><a href="#3-tar-命令：文件打包" class="headerlink" title="3. tar 命令：文件打包"></a>3. tar 命令：文件打包</h4><p>用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。</p><p>弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件</p><p>常用参数：</p><ol><li>-c 建立新的压缩文件</li><li>-f 指定压缩文件</li><li>-r 添加文件到已经压缩文件包中</li><li>-u 添加改了和现有的文件到压缩包中</li><li>-x 从压缩包中抽取文件</li><li>-t 显示压缩文件中的内容</li><li>-z 支持gzip压缩</li><li>-j 支持bzip2压缩</li><li>-Z 支持compress解压文件</li><li>-v 显示操作过程</li></ol><p><strong>有关 gzip 及 bzip2 压缩:</strong></p><p><strong>gzip 实例</strong>：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz</p><p>对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz</p><p><strong>bz2实例</strong>：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2</p><p>对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2</p><p>实例：</p><p>（1）将文件全部打包成 tar 包</p><pre class="line-numbers language-shell"><code class="language-shell">tar -cvf log.tar 1.log,2.log 或tar -cvf log.*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩</p><pre class="line-numbers language-shell"><code class="language-shell">tar -zcvf /tmp/etc.tar.gz /etc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的）</p><pre class="line-numbers language-shell"><code class="language-shell">tar -ztvf /tmp/etc.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）要压缩打包 /home, /etc ，但不要 /home/dmtsai</p><pre class="line-numbers language-shell"><code class="language-shell">tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-unzip-命令"><a href="#4-unzip-命令" class="headerlink" title="4. unzip 命令"></a>4. unzip 命令</h4><pre class="line-numbers language-text"><code class="language-text">解压 *.zip 文件：unzip test.zip 。查看 *.zip 文件的内容：unzip -l jasper.zip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="7-2-操作系统理论"><a href="#7-2-操作系统理论" class="headerlink" title="7.2 操作系统理论"></a>7.2 操作系统理论</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h4 id="基本特征"><a href="#基本特征" class="headerlink" title="基本特征"></a>基本特征</h4><h5 id="1-并发"><a href="#1-并发" class="headerlink" title="1. 并发"></a>1. 并发</h5><p>并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。</p><p>并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。</p><p>操作系统通过引入进程和线程，使得程序能够并发运行。</p><h5 id="2-共享"><a href="#2-共享" class="headerlink" title="2. 共享"></a>2. 共享</h5><p>共享是指系统中的资源可以被多个并发进程共同使用。</p><p>有两种共享方式：互斥共享和同时共享。</p><p>互斥共享的资源称为临界资源，例如打印机等，在同一时间只允许一个进程访问，需要用同步机制来实现对临界资源的访问。</p><h5 id="3-虚拟"><a href="#3-虚拟" class="headerlink" title="3. 虚拟"></a><strong>3. <strong>虚拟</strong></strong></h5><p>虚拟技术把一个物理实体转换为多个逻辑实体。</p><p>主要有两种虚拟技术：时分复用技术和空分复用技术。</p><p>多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片并快速切换。</p><p>虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页置换到内存中。</p><h5 id="4-异步"><a href="#4-异步" class="headerlink" title="4. 异步"></a>4. 异步</h5><p>异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。</p><h4 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h4><h5 id="1-进程管理"><a href="#1-进程管理" class="headerlink" title="1. 进程管理"></a>1. 进程管理</h5><p>进程控制、进程同步、进程通信、死锁处理、处理机调度等。</p><h5 id="2-内存管理"><a href="#2-内存管理" class="headerlink" title="2. 内存管理"></a><strong>2. <strong>内存管理</strong></strong></h5><p>内存分配、地址映射、内存保护与共享、虚拟内存等。</p><h5 id="3-文件管理"><a href="#3-文件管理" class="headerlink" title="3. 文件管理"></a><strong>3. <strong>文件管理</strong></strong></h5><p><a href="https://cloud.tencent.com/product/cfs?from=10680" target="_blank" rel="noopener">文件存储</a>空间的管理、目录管理、文件读写管理和保护等。</p><h5 id="4-设备管理"><a href="#4-设备管理" class="headerlink" title="4. 设备管理"></a><strong>4. <strong>设备管理</strong></strong></h5><p>完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。</p><p>主要包括缓冲管理、设备分配、设备处理、虛拟设备等。</p><h4 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h4><p>如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/pmt2abktte.jpeg?imageView2/2/w/1620" alt="img"></p><p>Linux 的系统调用主要有以下这些：</p><table><thead><tr><th align="left">Task</th><th align="left">Commands</th></tr></thead><tbody><tr><td align="left">进程控制</td><td align="left">fork(); exit(); wait();</td></tr><tr><td align="left">进程通信</td><td align="left">pipe(); shmget(); mmap();</td></tr><tr><td align="left">文件操作</td><td align="left">open(); read(); write();</td></tr><tr><td align="left">设备操作</td><td align="left">ioctl(); read(); write();</td></tr><tr><td align="left">信息维护</td><td align="left">getpid(); alarm(); sleep();</td></tr><tr><td align="left">安全</td><td align="left">chmod(); umask(); chown();</td></tr></tbody></table><h4 id="大内核和微内核"><a href="#大内核和微内核" class="headerlink" title="大内核和微内核"></a>大内核和微内核</h4><h5 id="1-大内核"><a href="#1-大内核" class="headerlink" title="1. 大内核"></a>1. 大内核</h5><p>大内核是将操作系统功能作为一个紧密结合的整体放到内核。</p><p>由于各模块共享信息，因此有很高的性能。</p><h5 id="2-微内核"><a href="#2-微内核" class="headerlink" title="2. 微内核"></a>2. 微内核</h5><p>由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。</p><p>在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。</p><p>因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/od2ypvw5gb.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="中断分类"><a href="#中断分类" class="headerlink" title="中断分类"></a>中断分类</h4><h5 id="1-外中断"><a href="#1-外中断" class="headerlink" title="1. 外中断"></a>1. 外中断</h5><p>由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。</p><h5 id="2-异常"><a href="#2-异常" class="headerlink" title="2. 异常"></a>2. 异常</h5><p>由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。</p><h5 id="3-陷入"><a href="#3-陷入" class="headerlink" title="3. 陷入"></a>3. 陷入</h5><p>在用户程序中使用系统调用。</p><h3 id="2-进程管理"><a href="#2-进程管理" class="headerlink" title="2. 进程管理"></a>2. 进程管理</h3><p><strong>进程与线程</strong></p><h5 id="1-进程"><a href="#1-进程" class="headerlink" title="1. 进程"></a>1. 进程</h5><p>进程是资源分配的基本单位。</p><p>进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。</p><p>下图显示了 4 个程序创建了 4 个进程，这 4 个进程可以并发地执行。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/hu0tt1z1tj.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="2-线程"><a href="#2-线程" class="headerlink" title="*2. *线程"></a>*<em>2. *</em>线程</h5><p>线程是独立调度的基本单位。</p><p>一个进程中可以有多个线程，它们共享进程资源。</p><p>QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/xagt3l9ulw.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="3-区别"><a href="#3-区别" class="headerlink" title="3. 区别"></a>3. 区别</h5><p>Ⅰ 拥有资源</p><p>进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。</p><p>Ⅱ 调度</p><p>线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。</p><p>Ⅲ 系统开销</p><p>由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。</p><p>Ⅳ 通信方面</p><p>线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。</p><p><strong>进程状态的切换</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/07gn4qy6s2.jpeg?imageView2/2/w/1620" alt="img"></p><ul><li>就绪状态（ready）：等待被调度</li><li>运行状态（running）</li><li>阻塞状态（waiting）：等待资源</li></ul><p>应该注意以下内容：</p><ul><li>只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。</li><li>阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。</li></ul><h4 id="进程调度算法"><a href="#进程调度算法" class="headerlink" title="进程调度算法"></a>进程调度算法</h4><p>不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。</p><h5 id="1-批处理系统"><a href="#1-批处理系统" class="headerlink" title="1. 批处理系统"></a>1. 批处理系统</h5><p>批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。</p><p><strong>1.1 先来先服务 first-come first-serverd（FCFS）</strong></p><p>按照请求的顺序进行调度。</p><p>有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。</p><p><strong>1.2 短作业优先 shortest job first（SJF）</strong></p><p>按估计运行时间最短的顺序进行调度。</p><p>长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。</p><p><strong>1.3 最短剩余时间优先 shortest remaining time next（SRTN）</strong></p><p>按估计剩余时间最短的顺序进行调度。</p><h5 id="2-交互式系统"><a href="#2-交互式系统" class="headerlink" title="2. 交互式系统"></a>2. 交互式系统</h5><p>交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。</p><p>2.1 时间片轮转</p><p>将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。</p><p>时间片轮转算法的效率和时间片的大小有很大关系：</p><ul><li>因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。</li><li>而如果时间片过长，那么实时性就不能得到保证。</li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/rczmx9j6f0.jpeg?imageView2/2/w/1620" alt="img"></p><p>2.2 优先级调度</p><p>为每个进程分配一个优先级，按优先级进行调度。</p><p>为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。</p><p>2.3 多级反馈队列</p><p>一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。</p><p>多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。</p><p>每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。</p><p>可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/55iy3nvaqt.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="3-实时系统"><a href="#3-实时系统" class="headerlink" title="3. 实时系统"></a>3. 实时系统</h5><p>实时系统要求一个请求在一个确定时间内得到响应。</p><p>分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。</p><h4 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h4><h5 id="1-临界区"><a href="#1-临界区" class="headerlink" title="1. 临界区"></a>1. 临界区</h5><p>对临界资源进行访问的那段代码称为临界区。</p><p>为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token comment" spellcheck="true">// entry section</span><span class="token comment" spellcheck="true">// critical section;</span><span class="token comment" spellcheck="true">// exit section</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h5 id="2-同步与互斥"><a href="#2-同步与互斥" class="headerlink" title="2. 同步与互斥"></a>2. 同步与互斥</h5><ul><li>同步：多个进程按一定顺序执行；</li><li>互斥：多个进程在同一时刻只有一个进程能进入临界区。</li></ul><h5 id="3-信号量"><a href="#3-信号量" class="headerlink" title="3. 信号量"></a>3. 信号量</h5><p>信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。</p><ul><li><strong>down</strong>  : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0；</li><li><strong>up</strong> ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。</li></ul><p>down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。</p><p>如果信号量的取值只能为 0 或者 1，那么就成为了  <strong>互斥量（Mutex）</strong> ，0 表示临界区已经加锁，1 表示临界区解锁。</p><pre class="line-numbers language-javascript"><code class="language-javascript">typedef int semaphore<span class="token punctuation">;</span>semaphore mutex <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">P1</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 临界区</span>    <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">P2</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 临界区</span>    <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> <strong>使用信号量实现生产者-消费者问题</strong>  </p><p>问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。</p><p>因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。</p><p>为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。</p><p>注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。</p><pre class="line-numbers language-javascript"><code class="language-javascript">#define N <span class="token number">100</span>typedef int semaphore<span class="token punctuation">;</span>semaphore mutex <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>semaphore empty <span class="token operator">=</span> N<span class="token punctuation">;</span>semaphore full <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">producer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>TRUE<span class="token punctuation">)</span> <span class="token punctuation">{</span>        int item <span class="token operator">=</span> <span class="token function">produce_item</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">insert_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">consumer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>TRUE<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        int item <span class="token operator">=</span> <span class="token function">remove_item</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">consume_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="4-管程"><a href="#4-管程" class="headerlink" title="4. 管程"></a>4. 管程</h5><p>使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易。</p><p>c 语言不支持管程，下面的示例代码使用了类 Pascal 语言来描述管程。示例代码的管程提供了 insert() 和 remove() 方法，客户端代码通过调用这两个方法来解决生产者-消费者问题。</p><pre class="line-numbers language-javascript"><code class="language-javascript">monitor ProducerConsumer    integer i<span class="token punctuation">;</span>    condition c<span class="token punctuation">;</span>    procedure <span class="token function">insert</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    begin        <span class="token comment" spellcheck="true">// ...</span>    end<span class="token punctuation">;</span>    procedure <span class="token function">remove</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    begin        <span class="token comment" spellcheck="true">// ...</span>    end<span class="token punctuation">;</span>end monitor<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>管程有一个重要特性：在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否者其它进程永远不能使用管程。</p><p>管程引入了  <strong>条件变量</strong>  以及相关的操作：<strong>wait()</strong> 和 <strong>signal()</strong> 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。</p><p><strong>使用管程实现生产者-消费者问题</strong> </p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token comment" spellcheck="true">// 管程</span>monitor ProducerConsumer    condition full<span class="token punctuation">,</span> empty<span class="token punctuation">;</span>    integer count <span class="token punctuation">:</span><span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    condition c<span class="token punctuation">;</span>    procedure <span class="token function">insert</span><span class="token punctuation">(</span>item<span class="token punctuation">:</span> integer<span class="token punctuation">)</span><span class="token punctuation">;</span>    begin        <span class="token keyword">if</span> count <span class="token operator">=</span> N then <span class="token function">wait</span><span class="token punctuation">(</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">insert_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>        count <span class="token punctuation">:</span><span class="token operator">=</span> count <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> count <span class="token operator">=</span> <span class="token number">1</span> then <span class="token function">signal</span><span class="token punctuation">(</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>    end<span class="token punctuation">;</span>    <span class="token keyword">function</span> remove<span class="token punctuation">:</span> integer<span class="token punctuation">;</span>    begin        <span class="token keyword">if</span> count <span class="token operator">=</span> <span class="token number">0</span> then <span class="token function">wait</span><span class="token punctuation">(</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>        remove <span class="token operator">=</span> remove_item<span class="token punctuation">;</span>        count <span class="token punctuation">:</span><span class="token operator">=</span> count <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> count <span class="token operator">=</span> N <span class="token operator">-</span><span class="token number">1</span> then <span class="token function">signal</span><span class="token punctuation">(</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>    end<span class="token punctuation">;</span>end monitor<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 生产者客户端</span>procedure producerbegin    <span class="token keyword">while</span> <span class="token boolean">true</span> <span class="token keyword">do</span>    begin        item <span class="token operator">=</span> produce_item<span class="token punctuation">;</span>        ProducerConsumer<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>    endend<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 消费者客户端</span>procedure consumerbegin    <span class="token keyword">while</span> <span class="token boolean">true</span> <span class="token keyword">do</span>    begin        item <span class="token operator">=</span> ProducerConsumer<span class="token punctuation">.</span>remove<span class="token punctuation">;</span>        <span class="token function">consume_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>    endend<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信"></a>进程通信</h4><p>进程同步与进程通信很容易混淆，它们的区别在于：</p><ul><li>进程同步：控制多个进程按一定顺序执行；</li><li>进程通信：进程间传输信息。</li></ul><p>进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传输一些进程同步所需要的信息。</p><h5 id="1-管道"><a href="#1-管道" class="headerlink" title="1. 管道"></a>1. 管道</h5><p>管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。</p><pre class="line-numbers language-javascript"><code class="language-javascript">#include <span class="token operator">&lt;</span>unistd<span class="token punctuation">.</span>h<span class="token operator">></span>int <span class="token function">pipe</span><span class="token punctuation">(</span>int fd<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>它具有以下限制：</p><ul><li>只支持半双工通信（单向交替传输）；</li><li>只能在父子进程中使用。</li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/hx8g1045y3.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="2-FIFO"><a href="#2-FIFO" class="headerlink" title="2. FIFO"></a>2. FIFO</h5><p>也称为命名管道，去除了管道只能在父子进程中使用的限制。</p><pre class="line-numbers language-javascript"><code class="language-javascript">#include <span class="token operator">&lt;</span>sys<span class="token operator">/</span>stat<span class="token punctuation">.</span>h<span class="token operator">></span>int <span class="token function">mkfifo</span><span class="token punctuation">(</span><span class="token keyword">const</span> char <span class="token operator">*</span>path<span class="token punctuation">,</span> mode_t mode<span class="token punctuation">)</span><span class="token punctuation">;</span>int <span class="token function">mkfifoat</span><span class="token punctuation">(</span>int fd<span class="token punctuation">,</span> <span class="token keyword">const</span> char <span class="token operator">*</span>path<span class="token punctuation">,</span> mode_t mode<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/i73d40mmih.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="3-消息队列"><a href="#3-消息队列" class="headerlink" title="3. 消息队列"></a>3. <a href="https://cloud.tencent.com/product/cmq?from=10680" target="_blank" rel="noopener">消息队列</a></h5><p>相比于 FIFO，消息队列具有以下优点：</p><ul><li>消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难；</li><li>避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法；</li><li>读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。</li></ul><h5 id="4-信号量"><a href="#4-信号量" class="headerlink" title="4. 信号量"></a>4. 信号量</h5><p>它是一个计数器，用于为多个进程提供对共享数据对象的访问。</p><h5 id="5-共享存储"><a href="#5-共享存储" class="headerlink" title="5. 共享存储"></a>5. 共享存储</h5><p>允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。</p><p>需要使用信号量用来同步对共享存储的访问。</p><p>多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用使用内存的匿名段。</p><h5 id="6-套接字"><a href="#6-套接字" class="headerlink" title="6. 套接字"></a>6. 套接字</h5><p>与其它通信机制不同的是，它可用于不同机器间的进程通信。</p><h3 id="3-死锁"><a href="#3-死锁" class="headerlink" title="3. 死锁"></a>3. 死锁</h3><h5 id="必要条件"><a href="#必要条件" class="headerlink" title="必要条件"></a>必要条件</h5><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/t65ue2s1ya.jpeg?imageView2/2/w/1620" alt="img"></p><ul><li>互斥：每个资源要么已经分配给了一个进程，要么就是可用的。</li><li>占有和等待：已经得到了某个资源的进程可以再请求新的资源。</li><li>不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。</li><li>环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。</li></ul><h5 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h5><p>主要有以下四种方法：</p><ul><li>鸵鸟策略</li><li>死锁检测与死锁恢复</li><li>死锁预防</li><li>死锁避免</li></ul><h5 id="鸵鸟策略"><a href="#鸵鸟策略" class="headerlink" title="鸵鸟策略"></a>鸵鸟策略</h5><p>把头埋在沙子里，假装根本没发生问题。</p><p>因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。</p><p>当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。</p><p>大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。</p><h5 id="死锁检测与死锁恢复"><a href="#死锁检测与死锁恢复" class="headerlink" title="死锁检测与死锁恢复"></a>死锁检测与死锁恢复</h5><p>不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。</p><p><strong>1. 每种类型一个资源的死锁检测</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/pij0ss6sqm.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。</p><p>图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。</p><p>每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。</p><p><strong>2. 每种类型多个资源的死锁检测</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/ijj77i1mx1.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图中，有三个进程四个资源，每个数据代表的含义如下：</p><ul><li>E 向量：资源总量</li><li>A 向量：资源剩余量</li><li>C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量</li><li>R 矩阵：每个进程请求的资源数量</li></ul><p>进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。</p><p>算法总结如下：</p><p>每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。</p><ol><li>寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。</li><li>如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。</li><li>如果没有这样一个进程，算法终止。</li></ol><h6 id="3-死锁恢复"><a href="#3-死锁恢复" class="headerlink" title="3. 死锁恢复"></a>3. 死锁恢复</h6><ul><li>利用抢占恢复</li><li>利用回滚恢复</li><li>通过杀死进程恢复</li></ul><h5 id="死锁预防"><a href="#死锁预防" class="headerlink" title="死锁预防"></a>死锁预防</h5><p>在程序运行之前预防发生死锁。</p><p><strong>1. 破坏互斥条件</strong></p><p>例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。</p><p><strong>2. 破坏占有和等待条件</strong></p><p>一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。</p><p><strong>3. 破坏不可抢占条件</strong></p><p><strong>4. 破坏环路等待</strong></p><p>给资源统一编号，进程只能按编号顺序来请求资源。</p><h5 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h5><p>在程序运行时避免发生死锁。</p><p><strong>1. 安全状态</strong></p><p>图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。</p><p>定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。</p><p>安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。</p><p><strong>2. 单个资源的银行家算法</strong></p><p>一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/wxp7hahob2.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。</p><p><strong>3. 多个资源的银行家算法</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/ree0vdeeyi.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。</p><p>检查一个状态是否安全的算法如下：</p><ul><li>查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。</li><li>假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。</li><li>重复以上两步，直到所有进程都标记为终止，则状态时安全的。</li></ul><p>如果一个状态不是安全的，需要拒绝进入这个状态。</p><h3 id="4-内存管理"><a href="#4-内存管理" class="headerlink" title="4. 内存管理"></a>4. 内存管理</h3><h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。</p><p>为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。</p><p>从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/k7jtx5iwmc.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="分页系统地址映射"><a href="#分页系统地址映射" class="headerlink" title="分页系统地址映射"></a>分页系统地址映射</h4><p>内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。</p><p>一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。</p><p>下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/c2dnokrvmc.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="页面置换算法"><a href="#页面置换算法" class="headerlink" title="页面置换算法"></a>页面置换算法</h4><p>在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。</p><p>页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，当有新的缓存到达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据。</p><p>页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。</p><p><strong>1. 最佳</strong></p><blockquote><p>OPT, Optimal replacement algorithm</p></blockquote><p>所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。</p><p>是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。</p><p>举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列：</p><p>开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。</p><p><strong>2. 最近最久未使用</strong></p><blockquote><p>LRU, Least Recently Used</p></blockquote><p>虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。</p><p>为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。</p><p>因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/2x6e053pb1.jpeg?imageView2/2/w/1620" alt="img"></p><p><strong>3. 最近未使用</strong></p><blockquote><p>NRU, Not Recently Used</p></blockquote><p>每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类：</p><ul><li>R=0，M=0</li><li>R=0，M=1</li><li>R=1，M=0</li><li>R=1，M=1</li></ul><p>当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。</p><p>NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。</p><p><strong>4. 先进先出</strong></p><blockquote><p>FIFO, First In First Out</p></blockquote><p>选择换出的页面是最先进入的页面。</p><p>该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。</p><p><strong>5. 第二次机会算法</strong></p><p>FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改：</p><p>当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/tnlcns5fh1.jpeg?imageView2/2/w/1620" alt="img"></p><p><strong>6. 时钟</strong></p><blockquote><p>Clock</p></blockquote><p>第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面连接起来，再使用一个指针指向最老的页面。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/o6tw921hf2.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="分段"><a href="#分段" class="headerlink" title="分段"></a>分段</h4><p>虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。</p><p>下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/zfur3waatz.jpeg?imageView2/2/w/1620" alt="img"></p><p>分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/bjksrn7rbd.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="段页式"><a href="#段页式" class="headerlink" title="段页式"></a>段页式</h4><p>程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统的共享和保护，又拥有分页系统的虚拟内存功能。</p><h4 id="分页与分段的比较"><a href="#分页与分段的比较" class="headerlink" title="分页与分段的比较"></a>分页与分段的比较</h4><ul><li>对程序员的透明性：分页透明，但是分段需要程序员显示划分每个段。</li><li>地址空间的维度：分页是一维地址空间，分段是二维的。</li><li>大小是否可以改变：页的大小不可变，段的大小可以动态改变。</li><li>出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。</li></ul><h1 id="八-计算机网络"><a href="#八-计算机网络" class="headerlink" title="八. 计算机网络"></a>八. 计算机网络</h1><h2 id="8-1-TCP和UDP的区别"><a href="#8-1-TCP和UDP的区别" class="headerlink" title="8.1 TCP和UDP的区别"></a>8.1 TCP和UDP的区别</h2><p>TCP<strong>面向连接（三次握手机制），通信前需要先建立连接；UDP面向无连接</strong>，通信前不需要建立连接；</p><p>TCP保障可靠传输（按序、无差错、不丢失、不重复）；UDP<strong>不保障可靠</strong>传输，使用最大努力交付；</p><p>TCP面向字节流的传输，UDP面向数据报的传输。</p><h2 id="8-2-TCP协议的三次握手-连接-和四次挥手-关闭"><a href="#8-2-TCP协议的三次握手-连接-和四次挥手-关闭" class="headerlink" title="8.2 TCP协议的三次握手(连接)和四次挥手(关闭)"></a>8.2 TCP协议的三次握手(连接)和四次挥手(关闭)</h2><p><strong>1. 三次握手过程</strong></p><p>形象理解：<br> 客户机：【how are you ?】<br> 服务器：【fine.And you?】<br> 客户机：【Fine.】</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cclip_image002.jpg" alt="IMG_256"></td></tr></tbody></table><p>具体过程：（SYN, (SYN+ACK), ACK) </p><p>（1）第一次握手：建立连接时，客户端A发送SYN包（SYN=j）到服务器B，并进入SYN_SEND状态，等待服务器B确认。</p><p>（2）第二次握手：服务器B收到SYN包，必须确认客户A的SYN（ACK=j+1），同时自己也发送一个SYN包（SYN=k），即SYN+ACK包，此时服务器B进入SYN_RECV状态。</p><p>（3）第三次握手：客户端A收到服务器B的SYN＋ACK包，向服务器B发送确认包ACK（ACK=k+1），此包发送完毕，客户端A和服务器B进入ESTABLISHED状态，完成三次握手。</p><p>完成三次握手，客户端与服务器开始传送数据。</p><p>确认号：其数值等于发送方的发送序号 +1(即接收方期望接收的下一个序列号)。</p><p><strong>2.**</strong>四次挥手过程**</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cclip_image004.jpg" alt="IMG_257"></td></tr></tbody></table><p>由于TCP连接是全双工的，一个TCP连接存在双向的读写通道，因此每个方向都必须单独进行关闭。TCP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。 </p><p>简单说来是 “先关读，后关写”，一共需要四个阶段。以客户机发起关闭连接为例：<br> 1.服务器读通道关闭<br> 2.客户机写通道关闭<br> 3.客户机读通道关闭<br> 4.服务器写通道关闭</p><p>关闭行为是在发起方数据发送完毕之后，给对方发出一个FIN（finish）数据段。直到接收到对方发送的FIN，且对方收到了接收确认ACK之后，双方的数据通信完全结束，过程中每次接收都需要返回确认数据段ACK。<br> 详细过程：<br> 第一阶段 客户机发送完数据之后，向服务器发送一个FIN数据段，序列号为i；<br> 1.服务器收到FIN(i)后，返回确认段ACK，序列号为i+1，关闭服务器读通道；<br> 2.客户机收到ACK(i+1)后，关闭客户机写通道；<br> （此时，客户机仍能通过读通道读取服务器的数据，服务器仍能通过写通道写数据）<br> 第二阶段 服务器发送完数据之后，向客户机发送一个FIN数据段，序列号为j；<br> 3.客户机收到FIN(j)后，返回确认段ACK，序列号为j+1，关闭客户机读通道；<br> 4.服务器收到ACK(j+1)后，关闭服务器写通道。</p><p>这是标准的TCP关闭两个阶段，服务器和客户机都可以发起关闭，完全对称。<br> FIN标识是通过发送最后一块数据时设置的，标准的例子中，服务器还在发送数据，所以要等到发送完的时候，设置FIN（此时可称为TCP连接处于半关闭状态，因为数据仍可从被动关闭一方向主动关闭方传送）。如果在服务器收到FIN(i)时，已经没有数据需要发送，可以在返回ACK(i+1)的时候就设置FIN(j)标识，这样就相当于可以合并第二步和第三步。</p><h2 id="8-3-TCP协议的通信状态"><a href="#8-3-TCP协议的通信状态" class="headerlink" title="8.3 TCP协议的通信状态"></a>8.3 TCP协议的通信状态</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="D:\2.我的工作\8.社招求职\2019JD\3.面试梳理\clip_image006.jpg" alt="IMG_258" style="zoom: 80%;"></td></tr></tbody></table><p>整个通信状态如图</p><p>说明：客户端和服务器均有6个状态。其中经常问到的两个状态是TIME_WAIT和CLOSE_WAIT.<br> 从图上可以发现，</p><p>TIME_WAIT状态是客户端【发起主动关闭的一方】<em>在四次挥手第二阶段</em>完成时，进入的状态。<br> CLOSE_WAIT状态是服务端【收到被动关闭的一方】<em>在四次挥手的第一阶段</em>完成时，进入的状态。</p><p>TIME_WAIT状态将持续2个MSL(Max Segment Lifetime),在Windows下默认为4分钟，即240秒。TIME_WAIT状态下的socket不能被回收使用. 具体现象是对于一个处理大量短连接的服务器,如果是由服务器主动关闭客户端的连接，将导致服务器端存在大量的处于TIME_WAIT状态的socket， 甚至比处于Established状态下的socket多的多,严重影响服务器的处理能力，甚至耗尽可用的socket，停止服务。</p><p>为什么需要TIME_WAIT？【保证Server最后收到了ACK】</p><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cclip_image009.gif" alt="img"></p><p>原因有二：<br> <strong>一、保证TCP协议的全双工连接能够可靠关闭<br> 二、保证这次连接的重复数据段从网络中消失</strong></p><p>先说第一点，如果Client直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。</p><p>再说第二点，如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。</p><h2 id="8-4-网络编程时的同步、异步、阻塞、非阻塞"><a href="#8-4-网络编程时的同步、异步、阻塞、非阻塞" class="headerlink" title="8.4 网络编程时的同步、异步、阻塞、非阻塞"></a>8.4 网络编程时的同步、异步、阻塞、非阻塞</h2><p><strong>同步/异步主要针对C端:</strong><br> <strong>同步：</strong><br> 所谓同步，就是在c端发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。<br> 例如普通B/S模式（同步）：提交请求-&gt;等待服务器处理-&gt;处理完毕返回 这个期间客户端浏览器不能干任何事</p><p><strong>异步：</strong><br> 异步的概念和同步相对。当c端一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。<br> 例如 ajax请求（异步）: 请求通过事件触发-&gt;服务器处理（这是浏览器仍然可以作其他事情）-&gt;处理完毕</p><p><strong>阻塞/非阻塞主要针对S端:</strong><br> <strong>阻塞</strong><br> 阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。</p><p>有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。 例如，我们在socket中调用recv函数，如果缓冲区中没有数据，这个函数就会一直等待，直到有数据才返回。而此时，当前线程还会继续处理各种各样的消息。<br> 快递的例子：比如到你某个时候到A楼一层（假如是内核缓冲区）取快递，但是你不知道快递什么时候过来，你又不能干别的事，只能死等着。但你可以睡觉（进程处于休眠状态），因为你知道快递把货送来时一定会给你打个电话（假定一定能叫醒你）。</p><p><strong>非阻塞</strong><br> 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。<br> 还是等快递的例子：如果用忙轮询的方法，每隔5分钟到A楼一层(内核缓冲区）去看快递来了没有。如果没来，立即返回。而快递来了，就放在A楼一层，等你去取。</p><p>对象的阻塞模式和阻塞函数调用<br> 对象是否处于阻塞模式和函数是不是阻塞调用有很强的相关性，但是并不是一一对应的。阻塞对象上可以有非阻塞的调用方式，我们可以通过一定的API去轮询状 态，在适当的时候调用阻塞函数，就可以避免阻塞。而对于非阻塞对象，调用特殊的函数也可以进入阻塞调用。函数select就是这样的一个例子。</p><p>总结几句话就是：<br> <strong>同步/异步主要针对C端</strong><br> \1. 同步，就是我客户端（c端调用者）调用一个功能，该功能没有结束前，我（c端调用者）死等结果。<br> \2. 异步，就是我（c端调用者）调用一个功能，不需要知道该功能结果，该功能有结果后通知我（c端调用者）即回调通知。<br> 阻塞/非阻塞主要针对S端<br> \3. 阻塞， 就是调用我（s端被调用者，函数），我（s端被调用者，函数）没有接收完数据或者没有得到结果之前，我不会返回。<br> \4. 非阻塞， 就是调用我（s端被调用者，函数），我（s端被调用者，函数）立即返回，通过select通知调用者</p><h2 id="8-5-进程间通信方式"><a href="#8-5-进程间通信方式" class="headerlink" title="8.5 进程间通信方式"></a>8.5 进程间通信方式</h2><p><strong>1.**</strong>什么是进程间通信？**</p><p>由于不同的进程运行在各自不同的内存空间中,其中一个进程对于变量的修改另一方是无法感知的,因此,进程之间的消息传递不能通过变量或其他数据结构直接进行,只能通过进程间通信来完成.<br> 进程间通信是指不同进程间进行数据共享和数据交换.</p><p><strong>2.**</strong>进程通信的分类**</p><p>根据进程通信时信息量大小的不同,可以将进程通信划分为两大类型:<br> 控制信息的通信(低级通信)和大批数据信息的通信(高级通信).</p><p>低级通信主要用于进程之间的同步,互斥,终止和挂起等等控制信息的传递.</p><p>高级通信主要用于进程间数据块数据的交换和共享,常见的高级通信有管道,消息队列,共享内存等.</p><p><strong>进程通信的方式</strong></p><p>1)<strong>管道</strong>【管道分为有名管道和无名管道】</p><p>管道是一种半双工的通信方式,数据只能单向流动,而且只能在具有亲缘关系的进程间使用.进程的亲缘关系一般指的是父子关系.管道一般用于两个不同进程之间的通信.当一个进程创建了一个管道,并调用fork创建自己的一个子进程后,父进程关闭读管道端,子进程关闭写管道端,这样提供了两个进程之间数据流动的一种方式.</p><p>2)<strong>信号量</strong></p><p>信号量是一个计数器,可以用来控制多个线程对共享资源的访问.,它不是用于交换大批数据,而用于多线程之间的同步.它常作为一种锁机制,防止某进程在访问资源时其它进程也访问该资源.因此,主要作为进程间以及同一个进程内不同线程之间的同步手段.</p><p>3)<strong>信号</strong></p><p>信号是一种比较复杂的通信方式,用于通知接收进程某个事件已经发生.</p><p>4)<strong>消息队列</strong></p><p>消息队列是消息的链表,存放在内核中并由消息队列标识符标识.消息队列克服了信号传递信息少,管道只能承载无格式字节流以及缓冲区大小受限等特点.消息队列是UNIX下不同进程之间可实现共享资源的一种机制,UNIX允许不同进程将格式化的数据流以消息队列形式发送给任意进程.对消息队列具有操作权限的进程都可以使用msget完成对消息队列的操作控制.通过使用消息类型,进程可以按任何顺序读信息,或为消息安排优先级顺序.</p><p>5)<strong>共享内存</strong></p><p>共享内存就是映射一段能被其他进程所访问的内存,这段共享内存由一个进程创建,但多个进程都可以访问.共享内存是最快的IPC(进程间通信)方式,它是针对其它进程间通信方式运行效率低而专门设计的.它往往与其他通信机制,如信号量,配合使用,来实现进程间的同步与通信.</p><p>6)<strong>套接字(socket)</strong></p><p>套接口也是一种进程间通信机制,与其他通信机制不同的是,它可用于不同进程及其间进程的通信.</p><p><strong>3.**</strong>各种通信方式的优缺点**</p><p>无名管道简单方便．但局限于单向通信的工作方式．并且只能在创建它的进程及其子孙进程之间实现管道的共享：有名管道虽然可以提供给任意关系的进程使用．但是由于其长期存在于系统之中，使用不当容易出错．所以普通用户一般不建议使用。</p><p>消息队列可以不再局限于父子进程．而允许任意进程通过共享消息队列来实现进程间通信．并由系统调用函数来实现消息发送和接收之间的同步．从而使得用户在使用消息缓冲进行通信时不再需要考虑同步问题．使用方便，但是消息队列中信息的复制需要额外消耗CPU的时间．不适宜于信息量大或操作频繁的场合。<br> 因此．对于不同的应用问题，要根据问题本身的情况来选择进程间的通信方式。</p><h2 id="8-6-TCP的流量控制和拥塞控制"><a href="#8-6-TCP的流量控制和拥塞控制" class="headerlink" title="8.6 TCP的流量控制和拥塞控制"></a>8.6 TCP的流量控制和拥塞控制</h2><p><strong>流量控制【点对点】</strong><br> 所谓的流量控制就是让发送方的发送速率不要太快，让接收方来得及接受。利用<strong>滑动窗口</strong>机制可以很方便的在TCP连接上实现对发送方的流量控制。</p><p><strong>拥塞控制【网络资源】</strong><br> 在某段时间，若对网络中的某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变化，这种情况叫做拥塞。</p><p>所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能承受现有的网络负荷。<br> 流量控制往往指的是点对点通信量的控制，是个端到端的问题。流量控制所要做的就是控制发送端发送数据的速率，以便使接收端来得及接受。<br> 拥塞控制的四种算法，即<strong>慢开始（Slow-start)，拥塞避免（Congestion Avoidance)快重传（Fast Restrangsmit)和快回复（Fast Recovery）</strong></p><h1 id="九-大数据算法"><a href="#九-大数据算法" class="headerlink" title="九. 大数据算法"></a>九. 大数据算法</h1><h2 id="9-1-两个超大文件找共同出现的单词"><a href="#9-1-两个超大文件找共同出现的单词" class="headerlink" title="9.1 两个超大文件找共同出现的单词"></a>9.1 两个超大文件找共同出现的单词</h2><p><strong>1.题目描述</strong></p><p>给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?</p><p><strong>2.思考过程</strong> </p><p>（1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。</p><p>（2）但上述方法有一个明显问题，加载一个文件的数据需要50亿*64bytes = 320G远远大于4G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。</p><p>（3）针对上述问题，我们分治算法的思想。</p><blockquote><p><strong>step1</strong>：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,…,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计）</p><p><strong>step2</strong>：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,…,b999)（为什么要这样做? 文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中）</p><p>所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url）</p><p><strong>step3：</strong>因为每个hash大约300M，所以我们再可以采用（1）中的想法</p><p>最后对两个新的url文件做hadoop计数，reduce的结果中count为2的即是重复项。</p><p>也可用其他方法。</p></blockquote><h2 id="9-2-海量数据求-TopN"><a href="#9-2-海量数据求-TopN" class="headerlink" title="9.2 海量数据求 TopN"></a>9.2 海量数据求 TopN</h2><p><strong>问题描述</strong></p><p>有1亿个浮点数，如果找出期中最大的10000个？</p><p><strong>解题思路</strong></p><ul><li><p><strong>最容易想到的方法是将数据全部排序</strong>，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。</p></li><li><p><strong>第二种方法为局部淘汰法</strong>，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即10000。</p></li><li><p><strong>第三种方法是分治法</strong>，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100*10000个数据里面找出最大的10000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^64=4MB，一共需要101次这样的比较。</p></li><li><p><strong>第四种方法是Hash法</strong>。如果这1亿个数里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。</p></li><li><p><strong>第五种方法采用最小堆</strong>。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。</p></li></ul><h2 id="9-3-海量数据找出不重复的（整数）数据-分治-位图法"><a href="#9-3-海量数据找出不重复的（整数）数据-分治-位图法" class="headerlink" title="9.3 海量数据找出不重复的（整数）数据(分治+位图法)"></a>9.3 海量数据找出不重复的（整数）数据(分治+位图法)</h2><p><strong>题目描述</strong></p><p>在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。</p><p><strong>解答思路</strong></p><p><strong>方法一：分治法</strong></p><p>​      与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。</p><p><strong>方法二：位图法（bit-map）</strong></p><p>​      位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。</p><p>​       位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。</p><p>​       假设我们要对 [0,7] 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：</p><p><code>0 0 0 0 0 0 0 0</code><br>然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：</p><p>0 0 0 0 1 0 1 0<br>依次遍历，结束后，位数组是这样的：</p><p>0 1 1 0 1 1 1 0<br>每个为 1 的位，它的下标都表示了一个数：</p><p>for i in range(8):<br>    if bits[i] == 1:<br>        print(i)<br>1<br>2<br>3<br>这样我们其实就已经实现了排序。</p><p>对于<strong>整数相关的算法</strong>的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。</p><p>那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：</p><ul><li><p>00 表示这个数字没出现过；<br>01 表示这个数字出现过一次（即为题目所找的不重复整数）；<br>10 表示这个数字出现了多次。</p><p>那么这 232 个整数，总共所需内存为 <code>232*2b=1GB</code>。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：</p></li></ul><p>遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。</p><p>方法总结<br><strong>判断数字是否重复的问题，位图法是一种非常高效的方法。</strong></p><h2 id="9-4-布隆过滤器"><a href="#9-4-布隆过滤器" class="headerlink" title="9.4 布隆过滤器"></a>9.4 布隆过滤器</h2><p><strong>什么是布隆过滤器</strong></p><p>本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。</p><p>相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。</p><p><strong>实现原理</strong></p><h4 id="HashMap-的问题"><a href="#HashMap-的问题" class="headerlink" title="HashMap 的问题"></a>HashMap 的问题</h4><p>讲述布隆过滤器的原理之前，我们先思考一下，通常你判断某个元素是否存在用的是什么？应该蛮多人回答 HashMap 吧，确实可以将值映射到 HashMap 的 Key，然后可以在 O(1) 的时间复杂度内返回结果，效率奇高。但是 HashMap 的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了。</p><p>还比如说你的数据集存储在远程服务器上，本地服务接受输入，而数据集非常大不可能一次性读进内存构建 HashMap 的时候，也会存在问题。</p><p><strong>布隆过滤器数据结构</strong></p><p>布隆过滤器是一个 bit 向量或者说 bit 数组，长这样：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-07e149c32a2608fa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>​       如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并对每个生成的哈希值指向的 bit 位置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成了哈希值 1、4、7，则上图转变为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-12449becdb038afd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>Ok，我们现在再存一个值 “tencent”，如果哈希函数返回 3、4、8 的话，图继续变为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-802577f6332d76b4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>​        值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit 位，因此它被覆盖了。现在我们如果想查询 “dianping” 这个值是否存在，哈希函数返回了 1、5、8三个值，结果我们发现 5 这个 bit 位上的值为 0，说明没有任何一个值映射到这个 bit 位上，因此我们可以很确定地说 “dianping” 这个值不存在。而当我们需要查询 “baidu” 这个值是否存在的话，那么哈希函数必然会返回 1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。</p><p>这是为什么呢？答案跟简单，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值 “taobao” 即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断 “taobao” 这个值存在。</p><h4 id="支持删除么"><a href="#支持删除么" class="headerlink" title="支持删除么"></a>支持删除么</h4><p>​     传统的布隆过滤器并不支持删除操作。但是名为 Counting Bloom filter 的变种可以用来测试元素计数个数是否绝对小于某个阈值，它支持元素删除。可以参考文章 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fcloud.tencent.com%2Fdeveloper%2Farticle%2F1136056" target="_blank" rel="noopener">Counting Bloom Filter 的原理和实现</a></p><h4 id="如何选择哈希函数个数和布隆过滤器长度"><a href="#如何选择哈希函数个数和布隆过滤器长度" class="headerlink" title="如何选择哈希函数个数和布隆过滤器长度"></a>如何选择哈希函数个数和布隆过滤器长度</h4><p>​       很显然，过小的布隆过滤器很快所有的 bit 位均为 1，那么查询任何值都会返回“可能存在”，起不到过滤的目的了。布隆过滤器的长度会直接影响误报率，布隆过滤器越长其误报率越小。</p><p>​        另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置位 1 的速度越快，且布隆过滤器的效率越低；但是如果太少的话，那我们的误报率会变高。</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-76dccfbdc9d7bdb1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>如何选择适合业务的 k 和 m 值呢，这里直接贴一个公式：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-675967d74620371f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/349/format/webp" alt="img"></p><p>​       k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。<br> 至于如何推导这个公式，我在知乎发布的<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F43263751" target="_blank" rel="noopener">文章</a>有涉及，感兴趣可以看看，不感兴趣的话记住上面这个公式就行了。</p><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><p>​       常见的适用常见有，利用布隆过滤器减少磁盘 IO 或者网络请求，因为一旦一个值必定不存在的话，我们可以不用进行后续昂贵的查询请求。</p><p>​       另外，既然你使用布隆过滤器来加速查找和判断是否存在，那么性能很低的哈希函数不是个好选择，推荐 MurmurHash、Fnv 这些。</p><h4 id="大Value拆分"><a href="#大Value拆分" class="headerlink" title="大Value拆分"></a>大Value拆分</h4><p>​      Redis 因其支持 setbit 和 getbit 操作，且纯内存性能高等特点，因此天然就可以作为布隆过滤器来使用。但是布隆过滤器的不当使用极易产生大 Value，增加 Redis 阻塞风险，因此生成环境中建议对体积庞大的布隆过滤器进行拆分。</p><p>​      拆分的形式方法多种多样，但是本质是不要将 Hash(Key) 之后的请求分散在多个节点的多个小 bitmap 上，而是应该拆分成多个小 bitmap 之后，对一个 Key 的所有哈希函数都落在这一个小 bitmap 上</p><h2 id="9-5-bit-map"><a href="#9-5-bit-map" class="headerlink" title="9.5 bit-map"></a>9.5 bit-map</h2><p><strong>Bit-map的基本思想</strong></p><p>　　32位机器上，对于一个整型数，比如int a=1 在内存中占32bit位，这是为了方便计算机的运算。但是对于某些应用场景而言，这属于一种巨大的浪费，因为我们可以用对应的32bit位对应存储十进制的0-31个数，而这就是Bit-map的基本思想。<strong>Bit-map算法利用这种思想处理大量数据的排序、查询以及去重。Bitmap在用户群做交集和并集运算的时候也有极大的便利</strong>。</p><blockquote><p>在此我用一个简单的例子来详细介绍BitMap算法的原理。</p><p>假设我们要对0-7内的5个元素(4,7,2,5,3)进行排序(这里假设元素没有重复)。我们可以使用BitMap算法达到排序目的。要表示8个数，我们需要8个byte。</p><p>1.首先我们开辟一个字节(8byte)的空间，将这些空间的所有的byte位都设置为0</p><p>2.然后便利这5个元素，第一个元素是4，因为下边从0开始，因此我们把第五个字节的值设置为1</p><p>3.然后再处理剩下的四个元素，最终8个字节的状态如下图</p><p>　             　<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI3MTA3My8yMDE3MTEvMTI3MTA3My0yMDE3MTEyMzE2MTUyNTgyMS03MTg5MTg0ODIucG5n?x-oss-process=image/format,png" alt="img"></p><p>4.现在我们遍历一次bytes区域，把值为1的byte的位置输出(2,3,4,5,7)，这样便达到了排序的目的 </p><p>从上面的例子我们可以看出，BitMap算法的思想还是比较简单的，关键的问题是如何确定10进制的数到2进制的映射图</p></blockquote><p><strong>MAP映射：</strong></p><p>　　假设需要排序或则查找的数的总数N=100000000，BitMap中1bit代表一个数字，1个int = 4Bytes = 4*8bit = 32 bit,那么N个数需要N/32 int空间。所以我们需要申请内存空间的大小为int a[1 + N/32]，其中：a[0]在内存中占32为可以对应十进制数0-31，依次类推：</p><p>　　a[0]—————————–&gt; 0-31</p><p>　　a[1]——————————&gt; 32-63</p><p>　　a[2]——————————-&gt; 64-95</p><p>　　a[3]——————————–&gt; 96-127</p><p>　　………………………………………………</p><p>　　那么十进制数如何转换为对应的bit位，下面介绍用位移将十进制数转换为对应的bit位:</p><p>　　<strong>1.求十进制数在对应数组a中的下标</strong></p><p>　　十进制数0-31，对应在数组a[0]中，32-63对应在数组a[1]中，64-95对应在数组a[2]中………，使用数学归纳分析得出结论：对于一个十进制数n，其在数组a中的下标为：a[n/32]</p><p>　　<strong>2.求出十进制数在对应数a[i]中的下标</strong></p><p>　　例如十进制数1在a[0]的下标为1，十进制数31在a[0]中下标为31，十进制数32在a[1]中下标为0。 在十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32求得在对应数组a[i]中的下标。</p><p>　　<strong>3.位移</strong></p><p>　　对于一个十进制数n,对应在数组a[n/32][n%32]中，但数组a毕竟不是一个二维数组，我们通过移位操作实现置1</p><p>　　a[n/32] |= 1 &lt;&lt; n % 32<br>　　移位操作：<br>　　a[n&gt;&gt;5] |= 1 &lt;&lt; (n &amp; 0x1F)</p><p>　　n &amp; 0x1F 保留n的后五位 相当于 n % 32 求十进制数在数组a[i]中的下标。</p><h2 id="9-6-字典树"><a href="#9-6-字典树" class="headerlink" title="9.6 字典树"></a>9.6 字典树</h2><p>Trie树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。字典树（Trie）可以保存一些 字符串 -&gt; 值 的对应关系。基本上，它跟 Java 的 HashMap 功能相同，都是 key-value 映射，只不过 Trie 的 key 只能是字符串。</p><p><strong>Trie的核心思想是空间换时间</strong>。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。</p><p>Trie 的强大之处就在于它的时间复杂度。它的插入和查询时间复杂度都为 O(k) ，其中 k 为 key 的长度，与 Trie 中保存了多少个元素无关。Hash 表号称是 O(1) 的，但在计算 hash 的时候就肯定会是 O(k) ，而且还有碰撞之类的问题；</p><p><strong>Trie 的缺点是空间消耗很高</strong>。</p><p>典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：<strong>最大限度地减少无谓的字符串比较，查询效率比哈希表高</strong>。</p><p><strong>Trie树的基本性质：</strong></p><p>（1）根节点不包含字符，除根节点意外每个节点只包含一个字符。<br>（2）从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。<br>（3）每个节点的所有子节点包含的字符串不相同。<br>（4）如果字符的种数为n，则每个结点的出度为n，这也是空间换时间的体现，浪费了很多的空间。<br>（5）插入查找的复杂度为O(n)，n为字符串长度。</p><p><strong>基本思想（以字母树为例）：</strong></p><p><strong>1、插入过程</strong></p><p>对于一个单词，从根开始，沿着单词的各个字母所对应的树中的节点分支向下走，直到单词遍历完，将最后的节点标记为红色，表示该单词已插入Trie树。</p><p><strong>2、查询过程</strong></p><p>同样的，从根开始按照单词的字母顺序向下遍历trie树，一旦发现某个节点标记不存在或者单词遍历完成而最后的节点未标记为红色，则表示该单词不存在，若最后的节点标记为红色，表示该单词存在。</p><p><strong>字典树的数据结构</strong></p><p>一般可以按下面步骤构建：</p><p>利用串构建一个字典树，这个字典树保存了串的公共前缀信息，因此可以降低查询操作的复杂度。</p><p>下面以英文单词构建的字典树为例，这棵Trie树中每个结点包括26个孩子结点，因为总共有26个英文字母(假设单词都是小写字母组成)。</p><p>则可声明包含Trie树的结点信息的结构体:</p><pre class="line-numbers language-c++"><code class="language-c++">typedef struct Trie_node{    int count;                    // 统计单词前缀出现的次数    struct Trie_node* next[26];   // 指向各个子树的指针    bool exist;                   // 标记该结点处是否构成单词  }TrieNode , *Trie;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中next是一个指针数组，存放着指向各个孩子结点的指针。</p><p>其中next是一个指针数组，存放着指向各个孩子结点的指针。</p><p>如给出字符串”abc”,”ab”,”bd”,”dda”，根据该字符串序列构建一棵Trie树。则构建的树如下:</p><h2 id="9-7-倒排索引"><a href="#9-7-倒排索引" class="headerlink" title="9.7 倒排索引"></a>9.7 倒排索引</h2><p>倒排索引是目前搜索引擎公司对搜索引擎最常用的存储方式，也是搜索引擎的核心内容，在搜索引擎的实际应用中，有时需要按照关键字的某些值查找记录，所以是按照关键字建立索引，这个索引就被称为倒排索引。</p><p>首先你要明确，索引这东西，一般是用于提高查询效率的。举个最简单的例子，已知有5个文本文件，需要我们去查某个单词位于哪个文本文件中，最直观的做法就是挨个加载每个文本文件中的单词到内存中，然后用for循环遍历一遍数组，直到找到这个单词。这种做法就是正向索引的思路。</p><p>正向索引的这种查询效率也不需要我多吐槽了。倒排索引的思路其实也并不难。再举一个例子，有两段文本</p><p>D1：Hello, conan!</p><p>D2：Hello, hattori!</p><p>第一步，找到所有的单词</p><p>Hello、conan、hattori</p><p>第二步，找到包含这些单词的文本位置</p><p>Hello（D1，D2）</p><p>conan（D1）</p><p>hattori（D2）</p><p>我们将单词作为Hash表的Key，将所在的文本位置作为Hash表的Value保存起来。</p><p>当我们要查询某个单词的所在位置时，只需要根据这张Hash表就可以迅速的找到目标文档。</p><p>结合之前的说的正向索引，不难发现。正向索引是通过文档去查找单词，反向索引则是通过单词去查找文档。</p><p>倒排索引的优点还包括在处理复杂的多关键字查询时，可在倒排表中先完成查询的并、交等逻辑运算，得到结果后再对记录进行存取，这样把对文档的查询转换为地址集合的运算，从而提高查找速度。</p><h1 id="十-数据结构和算法"><a href="#十-数据结构和算法" class="headerlink" title="十. 数据结构和算法"></a>十. 数据结构和算法</h1><h2 id="10-1-数组"><a href="#10-1-数组" class="headerlink" title="10.1 数组"></a>10.1 数组</h2><p>连续子数组的最大和</p><p>调整数组顺序使奇数位于偶数前面</p><h2 id="10-2-链表"><a href="#10-2-链表" class="headerlink" title="10.2 链表"></a>10.2 链表</h2><h4 id="1-链表删除"><a href="#1-链表删除" class="headerlink" title="1. 链表删除"></a>1. 链表删除</h4><h5 id="删除链表中的节点"><a href="#删除链表中的节点" class="headerlink" title="删除链表中的节点"></a><a href="https://leetcode-cn.com/problems/delete-node-in-a-linked-list/" target="_blank" rel="noopener">删除链表中的节点</a></h5><p>核心代码：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">deleteNode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>               node<span class="token punctuation">.</span>val <span class="token operator">=</span> node<span class="token punctuation">.</span>next<span class="token punctuation">.</span>val        node<span class="token punctuation">.</span>next <span class="token operator">=</span> node<span class="token punctuation">.</span>next<span class="token punctuation">.</span>next<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>[python 垃圾回收机制</p><blockquote><p><strong>引用计数</strong></p><p>Python语言默认采用的垃圾收集机制是『引用计数法 Reference Counting』，该算法最早George E. Collins在1960的时候首次提出，50年后的今天，该算法依然被很多编程语言使用，『引用计数法』的原理是：每个对象维护一个<code>ob_ref</code>字段，用来记录该对象当前被引用的次数，每当新的引用指向该对象时，它的引用计数ob_ref加1，每当该对象的引用失效时计数ob_ref减1，一旦对象的引用计数为0，该对象立即被回收，对象占用的内存空间将被释放。它的缺点是需要额外的空间维护引用计数，这个问题是其次的，不过最主要的问题是它不能解决对象的“循环引用”，因此，也有很多语言比如Java并没有采用该算法做来垃圾的收集机制。</p><p><strong>标记清除</strong></p><p>『标记清除（Mark—Sweep）』算法是一种基于<strong>追踪回收</strong>（tracing GC）技术实现的垃圾回收算法。它分为两个阶段：第一阶段是<strong>标记阶段</strong>，GC会把所有的『活动对象』打上标记，<strong>第二阶段是把那些没有标记的对象『非活动对象』进行回收</strong>。那么GC又是如何判断哪些是活动对象哪些是非活动对象的呢？</p><p>对象之间通过引用（指针）连在一起，构成一个<strong>有向图</strong>，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。</p><p><img src="https://foofish.net/images/mark-sweep.svg" alt="img"></p><p>在上图中，我们把小黑圈视为全局变量，也就是把它作为root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而4和5不可达，那么1、2、3就是活动对象，4和5是非活动对象会被GC回收。</p><p>标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些<strong>容器对象</strong>，比如<strong>list</strong>、<strong>dict</strong>、<strong>tuple</strong>，<strong>instance</strong>等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的缺点：<strong>清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象。</strong></p><h3 id="分代回收"><a href="#分代回收" class="headerlink" title="分代回收"></a>分代回收</h3><p><strong>分代回收是一种以空间换时间的操作方式</strong>，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象</p></blockquote></li></ul><hr><h5 id="奇偶链表"><a href="#奇偶链表" class="headerlink" title="奇偶链表"></a>奇偶链表</h5><p>核心代码</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">oddEvenList</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> ListNode<span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token operator">not</span> head<span class="token punctuation">:</span><span class="token keyword">return</span> head        odd <span class="token operator">=</span> head        even_head <span class="token operator">=</span> even <span class="token operator">=</span> head<span class="token punctuation">.</span>next        <span class="token keyword">while</span> odd<span class="token punctuation">.</span>next <span class="token operator">and</span> even<span class="token punctuation">.</span>next<span class="token punctuation">:</span>            odd<span class="token punctuation">.</span>next <span class="token operator">=</span> odd<span class="token punctuation">.</span>next<span class="token punctuation">.</span>next            even<span class="token punctuation">.</span>next <span class="token operator">=</span> even<span class="token punctuation">.</span>next<span class="token punctuation">.</span>next            odd<span class="token punctuation">,</span>even <span class="token operator">=</span> odd<span class="token punctuation">.</span>next<span class="token punctuation">,</span>even<span class="token punctuation">.</span>next        odd<span class="token punctuation">.</span>next <span class="token operator">=</span> even_head        <span class="token keyword">return</span> head<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>知识点：</p><h4 id="2-链表合并"><a href="#2-链表合并" class="headerlink" title="2. 链表合并"></a>2. 链表合并</h4><h5 id="1-2-1-合并两个有序链表"><a href="#1-2-1-合并两个有序链表" class="headerlink" title="1.2.1 合并两个有序链表"></a>1.2.1 <a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/" target="_blank" rel="noopener">合并两个有序链表</a></h5><p>核心代码</p><pre class="line-numbers language-python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-链表反转"><a href="#3-链表反转" class="headerlink" title="3. 链表反转"></a>3. 链表反转</h4><h5 id="1-3-1-K-个一组翻转链表"><a href="#1-3-1-K-个一组翻转链表" class="headerlink" title="1.3.1  K 个一组翻转链表"></a><a href="https://leetcode-cn.com/problems/reverse-nodes-in-k-group/" target="_blank" rel="noopener">1.3.1  K 个一组翻转链表</a></h5><p><strong>解题答案</strong>：</p><p>​     <strong>Python</strong>版</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#官方题解：</span><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 翻转一个子链表，并且返回新的头与尾</span>    <span class="token keyword">def</span> <span class="token function">reverse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">,</span> tail<span class="token punctuation">:</span> ListNode<span class="token punctuation">)</span><span class="token punctuation">:</span>        prev <span class="token operator">=</span> tail<span class="token punctuation">.</span>next        p <span class="token operator">=</span> head        <span class="token keyword">while</span> prev <span class="token operator">!=</span> tail<span class="token punctuation">:</span>            nex <span class="token operator">=</span> p<span class="token punctuation">.</span>next            p<span class="token punctuation">.</span>next <span class="token operator">=</span> prev            prev <span class="token operator">=</span> p            p <span class="token operator">=</span> nex        <span class="token keyword">return</span> tail<span class="token punctuation">,</span> head    <span class="token keyword">def</span> <span class="token function">reverseKGroup</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">,</span> k<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> ListNode<span class="token punctuation">:</span>        hair <span class="token operator">=</span> ListNode<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        hair<span class="token punctuation">.</span>next <span class="token operator">=</span> head        pre <span class="token operator">=</span> hair        <span class="token keyword">while</span> head<span class="token punctuation">:</span>            tail <span class="token operator">=</span> pre            <span class="token comment" spellcheck="true"># 查看剩余部分长度是否大于等于 k</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>                tail <span class="token operator">=</span> tail<span class="token punctuation">.</span>next                <span class="token keyword">if</span> <span class="token operator">not</span> tail<span class="token punctuation">:</span>                    <span class="token keyword">return</span> hair<span class="token punctuation">.</span>next            nex <span class="token operator">=</span> tail<span class="token punctuation">.</span>next            head<span class="token punctuation">,</span> tail <span class="token operator">=</span> self<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span>head<span class="token punctuation">,</span> tail<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 把子链表重新接回原链表</span>            pre<span class="token punctuation">.</span>next <span class="token operator">=</span> head            tail<span class="token punctuation">.</span>next <span class="token operator">=</span> nex            pre <span class="token operator">=</span> tail            head <span class="token operator">=</span> tail<span class="token punctuation">.</span>next        <span class="token keyword">return</span> hair<span class="token punctuation">.</span>next<span class="token comment" spellcheck="true">#复杂度分析</span>时间复杂度：O<span class="token punctuation">(</span>n<span class="token punctuation">)</span>，其中 n为链表的长度。head 指针会在O<span class="token punctuation">(</span>N<span class="token operator">/</span>K<span class="token punctuation">)</span>个结点上停留，每次停留需要进行一次 O<span class="token punctuation">(</span>k<span class="token punctuation">)</span>的翻转操作。空间复杂度：O<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>，我们只需要建立常数个变量。<span class="token comment" spellcheck="true">#网友解法：</span>复杂度分析    时间复杂度：O<span class="token punctuation">(</span>N<span class="token punctuation">)</span>。    空间复杂度：O<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#时间最优解法</span><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">reverseKGroup</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">,</span> k<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> ListNode<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 判断链表长度是否大于K</span>        cur <span class="token operator">=</span> head        <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token operator">not</span> cur<span class="token punctuation">:</span> <span class="token keyword">return</span> head            cur <span class="token operator">=</span> cur<span class="token punctuation">.</span>next        <span class="token comment" spellcheck="true"># 反转头部的长度为k的子链表</span>        pre <span class="token operator">=</span> head        itera <span class="token operator">=</span> head<span class="token punctuation">.</span>next        <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>k<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            next <span class="token operator">=</span> itera<span class="token punctuation">.</span>next            itera<span class="token punctuation">.</span>next <span class="token operator">=</span> pre            pre <span class="token operator">=</span> itera            itera <span class="token operator">=</span> next        <span class="token comment" spellcheck="true"># 反转链表的剩余部分</span>        head<span class="token punctuation">.</span>next <span class="token operator">=</span> self<span class="token punctuation">.</span>reverseKGroup<span class="token punctuation">(</span>cur<span class="token punctuation">,</span> k<span class="token punctuation">)</span>        <span class="token keyword">return</span> pre   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h5><ul><li><p>反转链表操作</p><pre class="line-numbers language-python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h4 id="4-链表相交"><a href="#4-链表相交" class="headerlink" title="4. 链表相交"></a>4. 链表相交</h4><h5 id="1-4-1-相交链表"><a href="#1-4-1-相交链表" class="headerlink" title="1.4.1 相交链表"></a><a href="https://leetcode-cn.com/problems/intersection-of-two-linked-lists/" target="_blank" rel="noopener">1.4.1 相交链表</a></h5><p>核心代码</p><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">-</span><span class="token operator">-</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-链表旋转"><a href="#5-链表旋转" class="headerlink" title="5. 链表旋转"></a>5. 链表旋转</h4><h5 id="1-5-1-旋转链表"><a href="#1-5-1-旋转链表" class="headerlink" title="1.5.1 旋转链表"></a><a href="https://leetcode-cn.com/problems/rotate-list/" target="_blank" rel="noopener">1.5.1 旋转链表</a></h5><p>核心代码</p><h2 id="10-3-字符串"><a href="#10-3-字符串" class="headerlink" title="10.3 字符串"></a>10.3 字符串</h2><h4 id="1-字符串比较"><a href="#1-字符串比较" class="headerlink" title="1. 字符串比较"></a>1. 字符串比较</h4><h5 id="比较字符串最小字母出现频次"><a href="#比较字符串最小字母出现频次" class="headerlink" title="比较字符串最小字母出现频次"></a><a href="https://leetcode-cn.com/problems/compare-strings-by-frequency-of-the-smallest-character/" target="_blank" rel="noopener">比较字符串最小字母出现频次</a></h5><h5 id="比较版本号"><a href="#比较版本号" class="headerlink" title="比较版本号"></a><a href="https://leetcode-cn.com/problems/compare-version-numbers/" target="_blank" rel="noopener">比较版本号</a></h5><p>核心代码：</p><pre class="line-numbers language-python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-字符串连接"><a href="#2-字符串连接" class="headerlink" title="2. 字符串连接"></a>2. 字符串连接</h4><h5 id=""><a href="#" class="headerlink" title=""></a></h5><h4 id="3-字符串匹配"><a href="#3-字符串匹配" class="headerlink" title="3. 字符串匹配"></a>3. 字符串匹配</h4><h5 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a><a href="https://leetcode-cn.com/problems/pattern-matching-lcci/" target="_blank" rel="noopener">模式匹配</a></h5><h5 id="数组中的字符串匹配"><a href="#数组中的字符串匹配" class="headerlink" title="数组中的字符串匹配"></a><a href="https://leetcode-cn.com/problems/string-matching-in-an-array/" target="_blank" rel="noopener">数组中的字符串匹配</a></h5><h5 id="通配符匹配"><a href="#通配符匹配" class="headerlink" title="通配符匹配"></a><a href="https://leetcode-cn.com/problems/wildcard-matching/" target="_blank" rel="noopener">通配符匹配</a></h5><h5 id="驼峰式匹配"><a href="#驼峰式匹配" class="headerlink" title="驼峰式匹配"></a><a href="https://leetcode-cn.com/problems/camelcase-matching/" target="_blank" rel="noopener">驼峰式匹配</a></h5><h5 id="正则表达式匹配"><a href="#正则表达式匹配" class="headerlink" title="正则表达式匹配"></a><a href="https://leetcode-cn.com/problems/regular-expression-matching/" target="_blank" rel="noopener">正则表达式匹配</a></h5><h5 id="重复叠加字符串匹配"><a href="#重复叠加字符串匹配" class="headerlink" title="重复叠加字符串匹配"></a><a href="https://leetcode-cn.com/problems/repeated-string-match/" target="_blank" rel="noopener">重复叠加字符串匹配</a></h5><h4 id="4-字符串反转"><a href="#4-字符串反转" class="headerlink" title="4. 字符串反转"></a>4. 字符串反转</h4><h5 id="反转字符串中的单词-III"><a href="#反转字符串中的单词-III" class="headerlink" title="反转字符串中的单词 III"></a><a href="https://leetcode-cn.com/problems/reverse-words-in-a-string-iii/" target="_blank" rel="noopener">反转字符串中的单词 III</a></h5><h5 id="仅仅反转字母"><a href="#仅仅反转字母" class="headerlink" title="仅仅反转字母"></a><a href="https://leetcode-cn.com/problems/reverse-only-letters/" target="_blank" rel="noopener">仅仅反转字母</a></h5><h5 id="反转字符串中的元音字母"><a href="#反转字符串中的元音字母" class="headerlink" title="反转字符串中的元音字母"></a><a href="https://leetcode-cn.com/problems/reverse-vowels-of-a-string/" target="_blank" rel="noopener">反转字符串中的元音字母</a></h5><h5 id="反转字符串"><a href="#反转字符串" class="headerlink" title="反转字符串"></a><a href="https://leetcode-cn.com/problems/reverse-string/" target="_blank" rel="noopener">反转字符串</a></h5><h5 id="反转字符串-II"><a href="#反转字符串-II" class="headerlink" title="反转字符串 II"></a><a href="https://leetcode-cn.com/problems/reverse-string-ii/" target="_blank" rel="noopener">反转字符串 II</a></h5><h4 id="5-字符串旋转"><a href="#5-字符串旋转" class="headerlink" title="5. 字符串旋转"></a>5. 字符串旋转</h4><h5 id="旋转数字"><a href="#旋转数字" class="headerlink" title="旋转数字"></a><a href="https://leetcode-cn.com/problems/rotated-digits/" target="_blank" rel="noopener">旋转数字</a></h5><h5 id="II-左旋转字符串"><a href="#II-左旋转字符串" class="headerlink" title="II. 左旋转字符串"></a><a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/" target="_blank" rel="noopener">II. 左旋转字符串</a></h5><h4 id="6-字符串子串"><a href="#6-字符串子串" class="headerlink" title="6. 字符串子串"></a>6. 字符串子串</h4><h5 id="子串能表示从-1-到-N-数字的二进制串"><a href="#子串能表示从-1-到-N-数字的二进制串" class="headerlink" title="子串能表示从 1 到 N 数字的二进制串"></a><a href="https://leetcode-cn.com/problems/binary-string-with-substrings-representing-1-to-n/" target="_blank" rel="noopener">子串能表示从 1 到 N 数字的二进制串</a></h5><h5 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">最长回文子串</a></h5><h5 id="定长子串中元音的最大数目"><a href="#定长子串中元音的最大数目" class="headerlink" title="定长子串中元音的最大数目"></a><a href="https://leetcode-cn.com/problems/maximum-number-of-vowels-in-a-substring-of-given-length/" target="_blank" rel="noopener">定长子串中元音的最大数目</a></h5><h5 id="串联所有单词的子串"><a href="#串联所有单词的子串" class="headerlink" title=" 串联所有单词的子串"></a><a href="https://leetcode-cn.com/problems/substring-with-concatenation-of-all-words/" target="_blank" rel="noopener"> 串联所有单词的子串</a></h5><h5 id="单字符重复子串的最大长度"><a href="#单字符重复子串的最大长度" class="headerlink" title="单字符重复子串的最大长度"></a><a href="https://leetcode-cn.com/problems/swap-for-longest-repeated-character-substring/" target="_blank" rel="noopener">单字符重复子串的最大长度</a></h5><h5 id="子串的最大出现次数"><a href="#子串的最大出现次数" class="headerlink" title="子串的最大出现次数"></a><a href="https://leetcode-cn.com/problems/maximum-number-of-occurrences-of-a-substring/" target="_blank" rel="noopener">子串的最大出现次数</a></h5><h2 id="10-4-二叉树"><a href="#10-4-二叉树" class="headerlink" title="10.4 二叉树"></a>10.4 二叉树</h2><h4 id="1-二叉树遍历"><a href="#1-二叉树遍历" class="headerlink" title="1. 二叉树遍历"></a>1. 二叉树遍历</h4><h5 id="二叉树的层序遍历"><a href="#二叉树的层序遍历" class="headerlink" title="二叉树的层序遍历"></a><a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/" target="_blank" rel="noopener">二叉树的层序遍历</a></h5><h4 id="2-二叉树路径"><a href="#2-二叉树路径" class="headerlink" title="2. 二叉树路径"></a>2. 二叉树路径</h4><h5 id="二叉树中的最大路径和"><a href="#二叉树中的最大路径和" class="headerlink" title="二叉树中的最大路径和"></a><a href="https://leetcode-cn.com/problems/binary-tree-maximum-path-sum/" target="_blank" rel="noopener">二叉树中的最大路径和</a></h5><h4 id="3-二叉树翻转"><a href="#3-二叉树翻转" class="headerlink" title="3. 二叉树翻转"></a>3. 二叉树翻转</h4><h5 id="翻转等价二叉树"><a href="#翻转等价二叉树" class="headerlink" title=" 翻转等价二叉树"></a><a href="https://leetcode-cn.com/problems/flip-equivalent-binary-trees/" target="_blank" rel="noopener"> 翻转等价二叉树</a></h5><h4 id="4-二叉树搜索"><a href="#4-二叉树搜索" class="headerlink" title="4. 二叉树搜索"></a>4. 二叉树搜索</h4><h5 id="二叉树的最大深度"><a href="#二叉树的最大深度" class="headerlink" title="二叉树的最大深度"></a><a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/" target="_blank" rel="noopener">二叉树的最大深度</a></h5><h5 id="二叉搜索树中的搜索"><a href="#二叉搜索树中的搜索" class="headerlink" title="二叉搜索树中的搜索"></a><a href="https://leetcode-cn.com/problems/search-in-a-binary-search-tree/" target="_blank" rel="noopener">二叉搜索树中的搜索</a></h5><h4 id="5-二叉树子树"><a href="#5-二叉树子树" class="headerlink" title="5. 二叉树子树"></a>5. 二叉树子树</h4><h5 id="出现次数最多的子树元素和"><a href="#出现次数最多的子树元素和" class="headerlink" title="出现次数最多的子树元素和"></a><a href="https://leetcode-cn.com/problems/most-frequent-subtree-sum/" target="_blank" rel="noopener">出现次数最多的子树元素和</a></h5><h2 id="10-5-堆"><a href="#10-5-堆" class="headerlink" title="10.5 堆"></a>10.5 堆</h2><h2 id="10-6-动态规划"><a href="#10-6-动态规划" class="headerlink" title="10.6 动态规划"></a>10.6 动态规划</h2><h4 id="1-连续子数组最值"><a href="#1-连续子数组最值" class="headerlink" title="1. 连续子数组最值"></a>1. 连续子数组最值</h4><h5 id="连续子数组的最大和"><a href="#连续子数组的最大和" class="headerlink" title="连续子数组的最大和"></a><a href="https://leetcode-cn.com/problems/lian-xu-zi-shu-zu-de-zui-da-he-lcof/" target="_blank" rel="noopener">连续子数组的最大和</a></h5><h5 id="最长重复子数组"><a href="#最长重复子数组" class="headerlink" title="最长重复子数组"></a><a href="https://leetcode-cn.com/problems/maximum-length-of-repeated-subarray/" target="_blank" rel="noopener">最长重复子数组</a></h5><h5 id="K个逆序对数组"><a href="#K个逆序对数组" class="headerlink" title="K个逆序对数组"></a><a href="https://leetcode-cn.com/problems/k-inverse-pairs-array/" target="_blank" rel="noopener">K个逆序对数组</a></h5><h5 id="最长递增子序列的个数"><a href="#最长递增子序列的个数" class="headerlink" title="最长递增子序列的个数"></a><a href="https://leetcode-cn.com/problems/number-of-longest-increasing-subsequence/" target="_blank" rel="noopener">最长递增子序列的个数</a></h5><h5 id="最长的斐波那契子序列的长度"><a href="#最长的斐波那契子序列的长度" class="headerlink" title="[最长的斐波那契子序列的长度]"></a>[最长的斐波那契子序列的长度]</h5><h2 id="10-7-二分查找"><a href="#10-7-二分查找" class="headerlink" title="10.7 二分查找"></a>10.7 二分查找</h2><h4 id="1-数组二分查找"><a href="#1-数组二分查找" class="headerlink" title="1. 数组二分查找"></a>1. 数组二分查找</h4><h5 id="分割数组的最大值"><a href="#分割数组的最大值" class="headerlink" title="分割数组的最大值"></a><a href="https://leetcode-cn.com/problems/split-array-largest-sum/" target="_blank" rel="noopener">分割数组的最大值</a></h5><h5 id="两个数组的交集"><a href="#两个数组的交集" class="headerlink" title="两个数组的交集"></a><a href="https://leetcode-cn.com/problems/intersection-of-two-arrays/" target="_blank" rel="noopener">两个数组的交集</a></h5><h5 id="寻找两个正序数组的中位数"><a href="#寻找两个正序数组的中位数" class="headerlink" title="寻找两个正序数组的中位数"></a><a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/" target="_blank" rel="noopener">寻找两个正序数组的中位数</a></h5><h5 id="最长重复子数组-1"><a href="#最长重复子数组-1" class="headerlink" title="最长重复子数组"></a><a href="https://leetcode-cn.com/problems/maximum-length-of-repeated-subarray/" target="_blank" rel="noopener">最长重复子数组</a></h5><h5 id="长度最小的子数组"><a href="#长度最小的子数组" class="headerlink" title="长度最小的子数组"></a><a href="https://leetcode-cn.com/problems/minimum-size-subarray-sum/" target="_blank" rel="noopener">长度最小的子数组</a></h5><h4 id="2-堆二分查找"><a href="#2-堆二分查找" class="headerlink" title="2. 堆二分查找"></a>2. 堆二分查找</h4><h5 id="有序矩阵中第K小的元素"><a href="#有序矩阵中第K小的元素" class="headerlink" title="有序矩阵中第K小的元素"></a><a href="https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/" target="_blank" rel="noopener">有序矩阵中第K小的元素</a></h5><h5 id="第-K-个最小的素数分数"><a href="#第-K-个最小的素数分数" class="headerlink" title="第 K 个最小的素数分数"></a><a href="https://leetcode-cn.com/problems/k-th-smallest-prime-fraction/" target="_blank" rel="noopener">第 K 个最小的素数分数</a></h5><h5 id="找出第-k-小的距离对"><a href="#找出第-k-小的距离对" class="headerlink" title="找出第 k 小的距离对"></a><a href="https://leetcode-cn.com/problems/find-k-th-smallest-pair-distance/" target="_blank" rel="noopener">找出第 k 小的距离对</a></h5><h4 id="3-二叉树二分查找"><a href="#3-二叉树二分查找" class="headerlink" title="3. 二叉树二分查找"></a>3. 二叉树二分查找</h4><h5 id="二叉搜索树中第K小的元素"><a href="#二叉搜索树中第K小的元素" class="headerlink" title="二叉搜索树中第K小的元素"></a><a href="https://leetcode-cn.com/problems/kth-smallest-element-in-a-bst/" target="_blank" rel="noopener">二叉搜索树中第K小的元素</a></h5><h5 id="完全二叉树的节点个数"><a href="#完全二叉树的节点个数" class="headerlink" title="[完全二叉树的节点个数]("></a>[完全二叉树的节点个数](</h5><h2 id="10-8-排序"><a href="#10-8-排序" class="headerlink" title="10.8 排序"></a>10.8 排序</h2><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;一-Hadoop篇&quot;&gt;&lt;a href=&quot;#一-Hadoop篇&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="Job" scheme="https://dataquaner.github.io/categories/Job/"/>
    
    
      <category term="Job" scheme="https://dataquaner.github.io/tags/Job/"/>
    
  </entry>
  
  <entry>
    <title>Spark面试问题梳理：选择题</title>
    <link href="https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/"/>
    <id>https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/</id>
    <published>2020-06-21T06:35:00.000Z</published>
    <updated>2020-06-21T12:27:36.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Spark-的四大组件下面哪个不是-D"><a href="#1-Spark-的四大组件下面哪个不是-D" class="headerlink" title="1. Spark 的四大组件下面哪个不是 (D )"></a>1. <strong>Spark 的四大组件下面哪个不是 (D )</strong></h2><p>A.Spark Streaming    B. Mlib </p><p>C Graphx    D.Spark R</p><h2 id="2-下面哪个端口不是-spark-自带服务的端口-C"><a href="#2-下面哪个端口不是-spark-自带服务的端口-C" class="headerlink" title="2. 下面哪个端口不是 spark 自带服务的端口 (C )"></a>2. 下面哪个端口不是 spark 自带服务的端口 (C )</h2><p>A.8080 B.4040 C.8090 D.18080</p><p>备注：8080：spark集群web ui端口，4040：sparkjob监控端口，18080：jobhistory端口</p><h2 id="3-spark-1-4-版本的最大变化-B"><a href="#3-spark-1-4-版本的最大变化-B" class="headerlink" title="3. spark 1.4 版本的最大变化 (B )"></a><strong>3. spark 1.4 版本的最大变化 (B )</strong></h2><p>A spark sql Release 版本  B .引入 Spark R </p><p>C DataFrame D.支持动态资源分配</p><h2 id="4-Spark-Job-默认的调度模式-A"><a href="#4-Spark-Job-默认的调度模式-A" class="headerlink" title="4. Spark Job 默认的调度模式 (A )"></a>4. Spark Job 默认的调度模式 (A )</h2><p>A FIFO   B FAIR   </p><p>C 无   D 运行时指定</p><blockquote><p>备注：Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。</p></blockquote><h2 id="5-哪个不是本地模式运行的条件-D"><a href="#5-哪个不是本地模式运行的条件-D" class="headerlink" title="5.哪个不是本地模式运行的条件 ( D)"></a>5.哪个不是本地模式运行的条件 ( D)</h2><p>A spark.localExecution.enabled=true  </p><p>B 显式指定本地运行</p><p>C finalStage 无父 Stage</p><p>D partition默认值</p><blockquote><p>备注：【问题】Spark在windows能跑集群模式吗？</p><p>我认为是可以的，但是需要详细了解cmd命令行的写法。目前win下跑spark的单机模式是没有问题的。</p></blockquote><blockquote><p>【关键点】spark启动机制容易被windows的命令行cmd坑</p><p>　　1、带空格、奇怪字符的安装路径，cmd不能识别。最典型的坑就是安装在Program Files文件夹下的程序，因为Program和Files之间有个空格，所以cmd竟不能识别。之前就把JDK安装在了Program Files下面，然后启动spark的时候，总是提示我找不到JDK。我明明配置了环境变量了啊？这就是所谓了《已经配置环境变量，spark 仍然找不到Java》的错误问题。至于奇怪的字符，如感叹号!，我经常喜欢用来将重要的文件夹排在最前面，但cmd命令提示符不能识别。</p><p>　　2、是否需要配置hadoop的路径的问题——答案是需要用HDFS或者yarn就配，不需要用则不需配置。目前大多数的应用场景里面，Spark大规模集群基本安装在Linux服务器上，而自己用windows跑spark的情景，则大多基于学习或者实验性质，如果我们所要读取的数据文件从本地windows系统的硬盘读取（比如说d:\data\ml.txt），基本上不需要配置hadoop路径。我们都知道，在编spark程序的时候，可以指定spark的启动模式，而启动模式有这么三中（以python代码举例）：</p><p>　　　（2.1）本地情况，conf = SparkConf().setMaster(“local[*]”) ——&gt;也就是拿本机的spark来跑程序</p><p>　　　（2.2）远程情况，conf = SparkConf().setMaster(“spark://remotehost:7077”) ——&gt;远程spark主机</p><p>　　　（2.3）yarn情况，conf = SparkConf().setMaster(“yarn-client”) ——&gt;远程或本地 yarn集群代理spark</p><p>针对这3种情况，配置hadoop安装路径都有什么作用呢？（2.1）本地的情况，直接拿本机安装的spark来运行spark程序（比如d:\spark-1.6.2），则配不配制hadoop路径取决于是否需要使用hdfs。java程序的情况就更为简单，只需要导入相应的hadoop的jar包即可，是否配置hadoop路径并不重要。（2.2）的情况大体跟（2.1）的情况相同，虽然使用的远程spark，但如果使用本地数据，则运算的元数据也是从本地上传到远程spark集群的，无需配置hdfs。而（2.3）的情况就大不相同，经过我搜遍baidu、google、bing引擎，均没找到SparkConf直接配置远程yarn地址的方法，唯一的一个帖子介绍可以使用yarn://remote:8032的形式，则会报错“无法解析 地址”。查看Spark的官方说明，Spark其实是通过hadoop路径下的etc\hadoop文件夹中的配置文件来寻找yarn集群的。因此，需要使用yarn来运行spark的情况，在spark那配置好hadoop的目录就尤为重要。后期经过虚拟机的验证，表明，只要windows本地配置的host地址等信息与linux服务器端相同（注意应更改hadoop-2/etc/hadoop 下各种文件夹的配置路径，使其与windows本地一致），是可以直接在win下用yarn-client提交spark任务到远程集群的。</p><p>3、是否需要配置环境变量的问题，若初次配置，可以考虑在IDE里面配置，或者在程序本身用setProperty函数进行配置。因为配置windows下的hadoop、spark环境是个非常头疼的问题，有可能路径不对而导致无法找到相应要调用的程序。待实验多次成功率提高以后，再直接配置windows的全局环境变量不迟。</p><p>　　4、使用Netbeans这个IDE的时候，有遇到Netbeans不能清理构建的问题。原因，极有可能是导入了重复的库，spark里面含有hadoop包，记得检查冲突。同时，在清理构建之前，记得重新编译一遍程序，再进行清理并构建。</p><p>　　５、经常遇到WARN YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources资源不足无法运行的问题，添加conf.set(“spark.executor.memory”, “512m”);语句进行资源限制。先前在虚拟机跑spark，由于本身机子性能不高，给虚拟机设置的内存仅仅2G，导致hadoop和spark双开之后系统资源严重不足。因此可以缩小每个executor的运算规模。其他资源缺乏问题的解决方法参考<a href="http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html</a></p></blockquote><h2 id="6-下面哪个不是-RDD-的特点-C"><a href="#6-下面哪个不是-RDD-的特点-C" class="headerlink" title="6.下面哪个不是 RDD 的特点 (C )"></a>6.下面哪个不是 RDD 的特点 (C )</h2><p>A. 可分区   B 可序列化   C 可修改   D 可持久化</p><h2 id="7-关于广播变量，下面哪个是错误的-D"><a href="#7-关于广播变量，下面哪个是错误的-D" class="headerlink" title="7. 关于广播变量，下面哪个是错误的 (D )"></a>7. 关于广播变量，下面哪个是错误的 (D )</h2><p>A 任何函数调用    B 是只读的  </p><p>C 存储在各个节点    D 存储在磁盘或 HDFS</p><h2 id="8-关于累加器，下面哪个是错误的-D"><a href="#8-关于累加器，下面哪个是错误的-D" class="headerlink" title="8. 关于累加器，下面哪个是错误的 (D )"></a>8. 关于累加器，下面哪个是错误的 (D )</h2><p>A 支持加法 B 支持数值类型 </p><p>C 可并行 D 不支持自定义类型</p><h2 id="9-Spark-支持的分布式部署方式中哪个是错误的-D"><a href="#9-Spark-支持的分布式部署方式中哪个是错误的-D" class="headerlink" title="9.Spark 支持的分布式部署方式中哪个是错误的 (D )"></a>9.Spark 支持的分布式部署方式中哪个是错误的 (D )</h2><p>A standalone B spark on mesos  </p><p>C spark on YARN D Spark on local</p><h2 id="10-Stage-的-Task-的数量由什么决定-A"><a href="#10-Stage-的-Task-的数量由什么决定-A" class="headerlink" title="10.Stage 的 Task 的数量由什么决定 (A )"></a>10.Stage 的 Task 的数量由什么决定 (A )</h2><p>A Partition B Job C Stage D TaskScheduler</p><h2 id="11-下面哪个操作是窄依赖-B"><a href="#11-下面哪个操作是窄依赖-B" class="headerlink" title="11.下面哪个操作是窄依赖 (B )"></a>11.下面哪个操作是窄依赖 (B )</h2><p>A join B filter </p><p>C group D sort</p><h2 id="12-下面哪个操作肯定是宽依赖-C"><a href="#12-下面哪个操作肯定是宽依赖-C" class="headerlink" title="12.下面哪个操作肯定是宽依赖 (C )"></a>12.下面哪个操作肯定是宽依赖 (C )</h2><p>A map B flatMap </p><p>C reduceByKey D sample</p><h2 id="13-spark-的-master-和-worker-通过什么方式进行通信的？-D"><a href="#13-spark-的-master-和-worker-通过什么方式进行通信的？-D" class="headerlink" title="13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )"></a>13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )</h2><p>A http B nio C netty D Akka</p><blockquote><p>备注：从spark1.3.1之后，netty完全代替 了akka</p><p>一直以来，基于Akka实现的RPC通信框架是Spark引以为豪的主要特性，也是与Hadoop等分布式计算框架对比过程中一大亮点，但是时代和技术都在演化，从Spark1.3.1版本开始，为了解决大数据块（如shuffle）的传输问题，Spark引入了Netty通信框架，到了1.6.0版本，Netty居然完全取代了Akka，承担Spark内部所有的RPC通信以及数据流传输。</p><p>那么Akka又是什么东西？从Akka出现背景来说，它是基于Actor的RPC通信系统，它的核心概念也是Message，它是基于协程的，性能不容置疑；基于scala的偏函数，易用性也没有话说，但是它毕竟只是RPC通信，无法适用大的package/stream的数据传输，这也是Spark早期引入Netty的原因。</p><p>那么Netty为什么可以取代Akka？首先不容置疑的是Akka可以做到的，Netty也可以做到，但是Netty可以做到，Akka却无法做到，原因是啥？在软件栈中，Akka相比Netty要Higher一点，它专门针对RPC做了很多事情，而Netty相比更加基础一点，可以为不同的应用层通信协议（RPC，FTP，HTTP等）提供支持，在早期的Akka版本，底层的NIO通信就是用的Netty；其次一个优雅的工程师是不会允许一个系统中容纳两套通信框架，恶心！最后，虽然Netty没有Akka协程级的性能优势，但是Netty内部高效的Reactor线程模型，无锁化的串行设计，高效的序列化，零拷贝，内存池等特性也保证了Netty不会存在性能问题。</p><p>那么Spark是怎么用Netty来取代Akka呢？一句话，利用偏函数的特性，基于Netty“仿造”出一个简约版本的Actor模型！！</p></blockquote><h2 id="14-默认的存储级别-A"><a href="#14-默认的存储级别-A" class="headerlink" title="14. 默认的存储级别 (A )"></a>14. 默认的存储级别 (A )</h2><p>A MEMORY_ONLY B MEMORY_ONLY_SER</p><p>C MEMORY_AND_DISK D MEMORY_AND_DISK_SER</p><pre class="line-numbers language-scala"><code class="language-scala">备注：<span class="token comment" spellcheck="true">//不会保存任务数据 </span><span class="token keyword">val</span> NONE <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//直接将RDD的partition保存在该节点的Disk上 </span><span class="token keyword">val</span> DISK_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//直接将RDD的partition保存在该节点的Disk上,在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> DISK_ONLY_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition对应的原生的Java Object保存在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //那么这些partition将不会缓存,并且需要的时候被重新计算,默认缓存的级别 </span><span class="token keyword">val</span> MEMORY_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition对应的原生的Java Object保存在JVM中,在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> MEMORY_ONLY_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_ONLY_SER <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_ONLY_SER_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition反序列化后的对象存储在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //超出的partition将被保存在Disk上,并且在需要时读取 </span><span class="token keyword">val</span> MEMORY_AND_DISK <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> MEMORY_AND_DISK_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_AND_DISK_SER <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_AND_DISK_SER_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition序列化后存储在Tachyon中 </span><span class="token keyword">val</span> OFF_HEAP <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="15-spark-deploy-recoveryMode-不支持那种-D"><a href="#15-spark-deploy-recoveryMode-不支持那种-D" class="headerlink" title="15 spark.deploy.recoveryMode 不支持那种 (D )"></a>15 spark.deploy.recoveryMode 不支持那种 (D )</h2><p>A.ZooKeeper B. FileSystem </p><p>D NONE D Hadoop</p><h2 id="16-下列哪个不是-RDD-的缓存方法-C"><a href="#16-下列哪个不是-RDD-的缓存方法-C" class="headerlink" title="16.下列哪个不是 RDD 的缓存方法 (C )"></a>16.下列哪个不是 RDD 的缓存方法 (C )</h2><p>A persist() B Cache() </p><p>C Memory()</p><h2 id="17-Task-运行在下来哪里个选项中-Executor-上的工作单元-C"><a href="#17-Task-运行在下来哪里个选项中-Executor-上的工作单元-C" class="headerlink" title="17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )"></a>17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )</h2><p>A Driver program B. spark master </p><p>C.worker node D Cluster manager</p><h2 id="18-hive-的元数据存储在-derby-和-MySQL-中有什么区别-B"><a href="#18-hive-的元数据存储在-derby-和-MySQL-中有什么区别-B" class="headerlink" title="18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )"></a>18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )</h2><p>A.没区别 B.多会话</p><p>C.支持网络环境 D数据库的区别</p><pre class="line-numbers language-sql"><code class="language-sql">备注：  Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。内置的derby主要问题是并发性能很差，可以理解为单线程操作。Derby还有一个特性。更换目录执行操作，会找不到相关表等<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="19-DataFrame-和-RDD-最大的区别-B"><a href="#19-DataFrame-和-RDD-最大的区别-B" class="headerlink" title="19.DataFrame 和 RDD 最大的区别 (B )"></a>19.DataFrame 和 RDD 最大的区别 (B )</h2><p>A.科学统计支持 B.多了 schema </p><p>C.存储方式不一样 D.外部数据源支持</p><blockquote><p>备注：</p><p><strong>上图直观体现了RDD与DataFrame的区别</strong>：左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><p><strong>提升执行效率</strong>： RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。</p><p><strong>减少数据读取</strong>：分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。</p></blockquote><blockquote><p>上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。</p><p>  对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过（例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200）。</p><p>  此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。</p><p> 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><p>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。</p><p>对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。</p></blockquote><ul><li><p><strong>RDD和Dataset</strong></p><p>​    DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。</p><p>​    DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为Spark</p><p>SQl类型，然而RDD依赖于运行时反射机制。</p></li><li><p><strong>DataFrame和Dataset</strong></p><p>​    Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。因此具有如下三个特点：</p><p>​    DataSet可以在编译时检查类型</p><p>并且是面向对象的编程接口。</p></li></ul><h2 id="20-Master-的-ElectedLeader-事件后做了哪些操作-D"><a href="#20-Master-的-ElectedLeader-事件后做了哪些操作-D" class="headerlink" title="20.Master 的 ElectedLeader 事件后做了哪些操作 (D )"></a>20.Master 的 ElectedLeader 事件后做了哪些操作 (D )</h2><p>A. 通知 driver B.通知 worker </p><p>C.注册 application D.直接 ALIVE</p><h2 id="34-cache后面能不能接其他算子-它是不是action操作？"><a href="#34-cache后面能不能接其他算子-它是不是action操作？" class="headerlink" title="34.cache后面能不能接其他算子,它是不是action操作？"></a>34.cache后面能不能接其他算子,它是不是action操作？</h2><p>答：cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。</p><p>cache不是action操作</p><h2 id="35-reduceByKey是不是action？"><a href="#35-reduceByKey是不是action？" class="headerlink" title="35.reduceByKey是不是action？"></a>35.reduceByKey是不是action？</h2><p>答：不是，很多人都会以为是action，reduce rdd是action</p><h2 id="36-数据本地性是在哪个环节确定的？"><a href="#36-数据本地性是在哪个环节确定的？" class="headerlink" title="36.数据本地性是在哪个环节确定的？"></a>36.数据本地性是在哪个环节确定的？</h2><p>具体的task运行在那他机器上，dag划分stage的时候确定的</p><h2 id="37-RDD的弹性表现在哪几点？"><a href="#37-RDD的弹性表现在哪几点？" class="headerlink" title="37.RDD的弹性表现在哪几点？"></a>37.RDD的弹性表现在哪几点？</h2><p>1）自动的进行内存和磁盘的存储切换；</p><p>2）基于Lingage的高效容错；</p><p>3）task如果失败会自动进行特定次数的重试；</p><p>4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；</p><p>5）checkpoint和persist，数据计算之后持久化缓存</p><p>6）数据调度弹性，DAG TASK调度和资源无关</p><p>7）数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par</p><h2 id="38-常规的容错方式有哪几种类型？"><a href="#38-常规的容错方式有哪几种类型？" class="headerlink" title="38.常规的容错方式有哪几种类型？"></a>38.常规的容错方式有哪几种类型？</h2><p>1）.数据检查点,会发生拷贝，浪费资源</p><p>2）.记录数据的更新，每次更新都会记录下来，比较复杂且比较消耗性能</p><h2 id="39-RDD通过Linage（记录数据更新）的方式为何很高效？"><a href="#39-RDD通过Linage（记录数据更新）的方式为何很高效？" class="headerlink" title="39.RDD通过Linage（记录数据更新）的方式为何很高效？"></a>39.RDD通过Linage（记录数据更新）的方式为何很高效？</h2><p>1）lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且rDD</p><p>之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就</p><p>产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条</p><p>将复杂计算链条存储下来，计算的时候从后往前回溯</p><p>900步是上一个stage的结束，要么就checkpoint</p><p>2）记录原数据，是每次修改都记录，代价很大</p><p>如果修改一个集合，代价就很小，官方说rdd是</p><p>粗粒度的操作，是为了效率，为了简化，每次都是</p><p>操作数据集合，写或者修改操作，都是基于集合的</p><p>rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的</p><p>也可以是细粒度，读可以读其中的一条条的记录。</p><p>3）简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景</p><p>如网络爬虫，现实世界中，大多数写是粗粒度的场景</p><h2 id="40-RDD有哪些缺陷？"><a href="#40-RDD有哪些缺陷？" class="headerlink" title="40.RDD有哪些缺陷？"></a>40.RDD有哪些缺陷？</h2><p>1）不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的</p><p>所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是</p><p>说可以一条条的读</p><p>2）不支持增量迭代计算，Flink支持</p><h2 id="41-说一说Spark程序编写的一般步骤？"><a href="#41-说一说Spark程序编写的一般步骤？" class="headerlink" title="41.说一说Spark程序编写的一般步骤？"></a>41.说一说Spark程序编写的一般步骤？</h2><p>答：初始化，资源，数据源，并行化，rdd转化，action算子打印输出结果或者也可以存至相应的数据存储介质，具体的可看下图：</p><p>file:///E:/%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6/qq19B99AF2399E52F466CC3CF7E3B24ED5/069fa7b471f54e038440faf63233acce/640.webp</p><h2 id="42-Spark有哪两种算子？"><a href="#42-Spark有哪两种算子？" class="headerlink" title="42. Spark有哪两种算子？"></a>42. Spark有哪两种算子？</h2><p>答：Transformation（转化）算子和Action（执行）算子。</p><h2 id="43-Spark提交你的jar包时所用的命令是什么？"><a href="#43-Spark提交你的jar包时所用的命令是什么？" class="headerlink" title="43. Spark提交你的jar包时所用的命令是什么？"></a>43. Spark提交你的jar包时所用的命令是什么？</h2><p>答：spark-submit。</p><h2 id="44-Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？"><a href="#44-Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？" class="headerlink" title="44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？"></a>44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？</h2><p>答：在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h2 id="45-你所理解的Spark的shuffle过程？"><a href="#45-你所理解的Spark的shuffle过程？" class="headerlink" title="45. 你所理解的Spark的shuffle过程？"></a>45. 你所理解的Spark的shuffle过程？</h2><p>答：从下面三点去展开</p><p>1）shuffle过程的划分</p><p>2）shuffle的中间结果如何存储</p><p>3）shuffle的数据如何拉取过来</p><p>可以参考这篇博文：<a href="http://www.cnblogs.com/jxhd1/p/6528540.html" target="_blank" rel="noopener">http://www.cnblogs.com/jxhd1/p/6528540.html</a></p><blockquote><p><strong>Shuffle后续优化方向</strong>：通过上面的介绍，我们了解到，Shuffle过程的主要存储介质是磁盘，尽量的减少IO是Shuffle的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，Shuffle过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有“土豪”内存节点，减少Shuffle IO的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW计算引擎运行地更好。</p><p><strong>MapReduce Shuffle后续优化方向</strong>：压缩：对数据进行压缩，减少写读数据量；</p><p>减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；<br>内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；<br>网络框架：netty的性能据说要占优了；<br><strong>本节点上的数据不走网络框架</strong>：对于本节点上的Map输出，Reduce直接去读吧，不需要绕道网络框架。<br>Spark Shuffle后续优化方向：Spark作为MapReduce的进阶架构，对于Shuffle过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是Spark的Shuffle对于我们来说在一些方面还是需要优化的。</p><p>压缩：对数据进行压缩，减少写读数据量；<br><strong>内存化</strong>：Spark历史版本中是有这样设计的：Map写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。</p></blockquote><h2 id="46-你如何从Kafka中获取数据？"><a href="#46-你如何从Kafka中获取数据？" class="headerlink" title="46. 你如何从Kafka中获取数据？"></a>46. 你如何从Kafka中获取数据？</h2><p><strong>1) 基于Receiver的方式</strong></p><p>这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。</p><p><strong>2) 基于Direct的方式</strong></p><p>这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据</p><h2 id="47-对于Spark中的数据倾斜问题你有什么好的方案？"><a href="#47-对于Spark中的数据倾斜问题你有什么好的方案？" class="headerlink" title="47. 对于Spark中的数据倾斜问题你有什么好的方案？"></a>47. 对于Spark中的数据倾斜问题你有什么好的方案？</h2><p>1）前提是定位数据倾斜，是OOM了，还是任务执行缓慢，看日志，看WebUI</p><p>2)解决方法，有多个方面</p><p>· 避免不必要的shuffle，如使用广播小表的方式，将reduce-side-join提升为map-side-join</p><p>·分拆发生数据倾斜的记录，分成几个部分进行，然后合并join后的结果</p><p>·改变并行度，可能并行度太少了，导致个别task数据压力大</p><p>·两阶段聚合，先局部聚合，再全局聚合</p><p>·自定义paritioner，分散key的分布，使其更加均匀</p><p>详细解决方案参考博文《Spark数据倾斜优化方法》</p><h2 id="48-RDD创建有哪几种方式？"><a href="#48-RDD创建有哪几种方式？" class="headerlink" title="48.RDD创建有哪几种方式？"></a>48.RDD创建有哪几种方式？</h2><p>1).使用程序中的集合创建rdd</p><p>2).使用本地文件系统创建rdd</p><p>3).使用hdfs创建rdd，</p><p>4).基于数据库db创建rdd</p><p>5).基于Nosql创建rdd，如hbase</p><p>6).基于s3创建rdd，</p><p>7).基于数据流，如socket创建rdd</p><p>如果只回答了前面三种，是不够的，只能说明你的水平还是入门级的，实践过程中有很多种创建方式。</p><h2 id="49-Spark并行度怎么设置比较合适"><a href="#49-Spark并行度怎么设置比较合适" class="headerlink" title="49.Spark并行度怎么设置比较合适"></a>49.Spark并行度怎么设置比较合适</h2><p>答：spark并行度，每个core承载2<del>4个partition,如，32个core，那么64</del>128之间的并行度，也就是</p><p>设置64~128个partion，并行读和数据规模无关，只和内存使用量和cpu使用</p><p>时间有关</p><h2 id="50-Spark中数据的位置是被谁管理的？"><a href="#50-Spark中数据的位置是被谁管理的？" class="headerlink" title="50.Spark中数据的位置是被谁管理的？"></a>50.Spark中数据的位置是被谁管理的？</h2><p>答：每个数据分片都对应具体物理位置，数据的位置是被blockManager，无论</p><p>数据是在磁盘，内存还是tacyan，都是由blockManager管理</p><p>答：Spark中的数据本地性有三种：</p><p>a.PROCESS_LOCAL是指读取缓存在本地节点的数据</p><p>b.NODE_LOCAL是指读取本地节点硬盘数据</p><p>c.ANY是指读取非本地节点数据</p><p>通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。</p><h2 id="52-rdd有几种操作类型？"><a href="#52-rdd有几种操作类型？" class="headerlink" title="52.rdd有几种操作类型？"></a>52.rdd有几种操作类型？</h2><p>1）transformation，rdd由一种转为另一种rdd</p><p>2）action，</p><p>3）cronroller，crontroller是控制算子,cache,persist，对性能和效率的有很好的支持</p><p>三种类型，不要回答只有2中操作</p><h2 id="53-Spark如何处理不能被序列化的对象？"><a href="#53-Spark如何处理不能被序列化的对象？" class="headerlink" title="53.Spark如何处理不能被序列化的对象？"></a>53.Spark如何处理不能被序列化的对象？</h2><p>将不能序列化的内容封装成object</p><h2 id="54-collect功能是什么，其底层是怎么实现的？"><a href="#54-collect功能是什么，其底层是怎么实现的？" class="headerlink" title="54.collect功能是什么，其底层是怎么实现的？"></a>54.collect功能是什么，其底层是怎么实现的？</h2><p>答：driver通过collect把集群中各个节点的内容收集过来汇总成结果，collect返回结果是Array类型的，collect把各个节点上的数据抓过来，抓过来数据是Array型，collect对Array抓过来的结果进行合并，合并后Array中只有一个元素，是tuple类型（KV类型的）的。</p><h2 id="55-Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"><a href="#55-Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？" class="headerlink" title="55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"></a>55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？</h2><p>答：</p><p>1）因为输入数据有很多task，尤其是有很多小文件的时候，有多少个输入block就会有多少个task启动；</p><p>2）spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。</p><p>3）参数可以通过spark_home/conf/spark-default.conf配置文件设置:</p><p>spark.sql.shuffle.partitions 50 spark.default.parallelism 10</p><p>第一个是针对spark sql的task数量</p><p>第二个是非spark sql程序设置生效</p><h2 id="56-为什么Spark-Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生"><a href="#56-为什么Spark-Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生" class="headerlink" title="56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?"></a>56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?</h2><p>答：会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑</p><p>否则很容易出现长时间分配不到资源，job一直不能运行的情况。</p><h2 id="57-map与flatMap的区别"><a href="#57-map与flatMap的区别" class="headerlink" title="57.map与flatMap的区别"></a>57.map与flatMap的区别</h2><blockquote><p>map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象</p><p>flatMap：对RDD每个元素转换，然后再扁平化</p><p>将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组</p><p>对象，会抛弃值为null的值</p></blockquote><h2 id="58-列举你常用的action？"><a href="#58-列举你常用的action？" class="headerlink" title="58.列举你常用的action？"></a>58.列举你常用的action？</h2><p>collect，reduce,take,count,saveAsTextFile等</p><h2 id="59-Spark为什么要持久化，一般什么场景下要进行persist操作？"><a href="#59-Spark为什么要持久化，一般什么场景下要进行persist操作？" class="headerlink" title="59.Spark为什么要持久化，一般什么场景下要进行persist操作？"></a>59.Spark为什么要持久化，一般什么场景下要进行persist操作？</h2><blockquote><p>为什么要进行持久化？</p><p>spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤</p><p>只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。</p></blockquote><blockquote><p>以下场景会使用persist</p><p>1）某个步骤计算非常耗时，需要进行persist持久化</p><p>2）计算链条非常长，重新恢复要算很多步骤，很好使，persist</p><p>3）checkpoint所在的rdd要持久化persist，</p><p>lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前</p><p>要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。</p><p>4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</p><p>5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</p><p>60.为什么要进行序列化</p><p>序列化可以减少数据的体积，减少存储空间，高效存储和传输数据，不好的是使用的时候要反序列化，非常消耗CPU</p></blockquote><h2 id="61-介绍一下join操作优化经验？"><a href="#61-介绍一下join操作优化经验？" class="headerlink" title="61.介绍一下join操作优化经验？"></a>61.介绍一下join操作优化经验？</h2><blockquote><p>答：join其实常见的就分为两类： map-side join 和  reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。</p><p>备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。</p></blockquote><h2 id="62-介绍一下cogroup-rdd实现原理，你在什么场景下用过这个rdd？"><a href="#62-介绍一下cogroup-rdd实现原理，你在什么场景下用过这个rdd？" class="headerlink" title="62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？"></a>62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？</h2><blockquote><p>答：cogroup的函数实现:这个实现根据两个要进行合并的两个RDD操作,生成一个CoGroupedRDD的实例,这个RDD的返回结果是把相同的key中两个RDD分别进行合并操作,最后返回的RDD的value是一个Pair的实例,这个实例包含两个Iterable的值,第一个值表示的是RDD1中相同KEY的值,第二个值表示的是RDD2中相同key的值.由于做cogroup的操作,需要通过partitioner进行重新分区的操作,因此,执行这个流程时,需要执行一次shuffle的操作(如果要进行合并的两个RDD的都已经是shuffle后的rdd,同时他们对应的partitioner相同时,就不需要执行shuffle,)，</p><p>场景：表关联查询</p></blockquote><p>63下面这段代码输出结果是什么？</p><hr><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">def</span> joinRdd<span class="token punctuation">(</span>sc<span class="token operator">:</span>SparkContext<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token keyword">val</span> name<span class="token operator">=</span> Array<span class="token punctuation">(</span>Tuple2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">"spark"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">"tachyon"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token string">"hadoop"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">val</span> score<span class="token operator">=</span> Array<span class="token punctuation">(</span>Tuple2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">val</span> namerdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">val</span> scorerdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>score<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">val</span> result <span class="token operator">=</span> namerdd<span class="token punctuation">.</span>join<span class="token punctuation">(</span>scorerdd<span class="token punctuation">)</span><span class="token punctuation">;</span>result <span class="token punctuation">.</span>collect<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>答案<span class="token operator">:</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>Spark<span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">(</span>tachyon<span class="token punctuation">,</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">(</span>hadoop<span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Spark-的四大组件下面哪个不是-D&quot;&gt;&lt;a href=&quot;#1-Spark-的四大组件下面哪个不是-D&quot; class=&quot;headerlink&quot; title=&quot;1. Spark 的四大组件下面哪个不是 (D )&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;Spark 的四
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark面试问题梳理</title>
    <link href="https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/"/>
    <id>https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/</id>
    <published>2020-06-21T06:35:00.000Z</published>
    <updated>2020-06-21T12:25:46.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题一：Spark中的RDD是什么，有哪些特性？"><a href="#问题一：Spark中的RDD是什么，有哪些特性？" class="headerlink" title="问题一：Spark中的RDD是什么，有哪些特性？"></a><strong>问题一：Spark中的RDD是什么，有哪些特性？</strong></h2><h3 id="1-RDD是什么？"><a href="#1-RDD是什么？" class="headerlink" title="1.RDD是什么？"></a><strong>1.RDD是什么？</strong></h3><blockquote><p><strong>RDD</strong>（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合</p></blockquote><ul><li><strong>Dataset</strong>：就是一个集合，用于存放数据的</li><li><strong>Destributed</strong>：分布式，可以并行在集群计算</li><li><strong>Resilient</strong>：表示弹性的，弹性表示<ul><li>RDD中的数据可以存储在内存或者磁盘中；</li><li>RDD中的分区是可以改变的；</li></ul></li></ul><h3 id="2-五大特性："><a href="#2-五大特性：" class="headerlink" title="2. 五大特性："></a><strong>2. 五大特性：</strong></h3><ol><li><strong>A list of partitions</strong>：一个分区列表，RDD中的数据都存储在一个分区列表中</li><li><strong>A function for computing each split</strong>：作用在每一个分区中的函数</li><li><strong>A list of dependencies on other RDDs</strong>：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的</li><li><strong>Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)</strong>：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向</li><li>可选项，数据本地性，数据位置最优</li></ol><h2 id="问题二：-概述一下spark中的常用算子区别（map-mapPartitions，foreach，foreachPatition）"><a href="#问题二：-概述一下spark中的常用算子区别（map-mapPartitions，foreach，foreachPatition）" class="headerlink" title="问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）"></a>问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）</h2><p>常用算子：</p><ul><li><strong>map</strong>：用于遍历RDD，将函数应用于每一个元素，返回新的RDD（transformation算子）</li><li><strong>foreach</strong>：用于遍历RDD，将函数应用于每一个元素，无返回值（action算子）</li><li><strong>mapPatitions</strong>：用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子）</li><li><strong>foreachPatition</strong>：用于遍历操作RDD中的每一个分区，无返回值（action算子）</li></ul><p><strong>总结</strong>：一般使用<strong>mapPatitions</strong>和<strong>foreachPatition</strong>算子比map和foreach更加高效，推荐使用</p><h2 id="问题三：-谈谈spark中的宽窄依赖："><a href="#问题三：-谈谈spark中的宽窄依赖：" class="headerlink" title="问题三：.谈谈spark中的宽窄依赖："></a><strong>问题三：.谈谈spark中的宽窄依赖</strong>：</h2><p>答：RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题四：spark中如何划分stage："><a href="#问题四：spark中如何划分stage：" class="headerlink" title="问题四：spark中如何划分stage："></a>问题四：spark中如何划分stage：</h2><ul><li><strong>概念</strong>：</li></ul><blockquote><p>​       Spark任务会根据<strong>RDD</strong>之间的依赖关系，形成一个<strong>DAG有向无环图</strong>，<strong>DAG</strong>会提交给<strong>DAGScheduler</strong>，<strong>DAGScheduler</strong>会把<strong>DAG</strong>划分相互依赖的多个<strong>stage</strong>，划分依据就是<strong>宽窄依赖</strong>，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以<strong>taskSet</strong>的形式提交给<strong>TaskScheduler</strong>运行，stage是由一组并行的task组成。</p><p>​       Spark程序中可以因为不同的action触发众多的job，一个程序中可以有很多的job，每一个job是由一个或者多个stage构成的，后面的stage依赖于前面的stage，也就是说只有前面依赖的stage计算完毕后，后面的stage才会运行；</p><p>​        stage 的划分标准就是宽依赖：何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；</p><p>​       切割规则：从后往前，遇到宽依赖就切割stage；</p></blockquote><ul><li><strong>图解</strong>：</li></ul><p><img src="https://img-blog.csdn.net/20180909161915933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><ul><li><strong>计算格式</strong>：pipeline管道计算模式，piepeline只是一种计算思想，一种模式。</li></ul><blockquote><p>​        spark的pipeline管道计算模式相当于执行了一个高阶函数，也就是说来一条数据然后计算一条数据，会把所有的逻辑走完，然后落地，而MapReduce是1+1=2，2+1=3这样的计算模式，也就是计算完落地，然后再计算，然后再落地到磁盘或者内存，最后数据是落在计算节点上，按reduce的hash分区落地。管道计算模式完全基于内存计算，所以比MapReduce快的原因。</p><p>管道中的RDD何时落地：shuffle write的时候，对RDD进行持久化的时候。</p><p>​    stage的task的并行度是由stage的最后一个RDD的分区数来决定的，一般来说，一个partition对应一个task，但最后reduce的时候可以手动改变reduce的个数，也就是改变最后一个RDD的分区数，也就改变了并行度。例如：reduceByKey(<em>+</em>,3)</p></blockquote><ul><li><strong>优化</strong>：提高stage的并行度：reduceByKey(<em>+</em>,patition的个数) ，join(<em>+</em>,patition的个数)</li></ul><h2 id="问题五：DAGScheduler分析："><a href="#问题五：DAGScheduler分析：" class="headerlink" title="问题五：DAGScheduler分析："></a>问题五：DAGScheduler分析：</h2><p>答：</p><ul><li><p><strong>概述</strong>：<strong>DAGScheduler</strong>是一个面向stage 的调度器；</p></li><li><p><strong>主要入参</strong>：</p><ul><li>dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,resultHandler, localProperties.get)</li><li>rdd： final RDD；</li><li>cleanedFunc： 计算每个分区的函数；</li><li>resultHander： 结果侦听器；</li></ul></li><li><p><strong>主要功能</strong>：</p><ol><li><p>接受用户提交的job；</p></li><li><p>将job根据类型划分为不同的stage，记录那些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</p></li><li><p>决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给<strong>TaskScheduler</strong>；</p></li><li><p>重新提交<strong>shuffle</strong>输出丢失的stage给taskScheduler；</p></li></ol></li></ul><p>注：一个stage内部的错误不是由shuffle输出丢失造成的，DAGScheduler是不管的，由TaskScheduler负责尝试重新提交task执行。</p><h2 id="问题六：Job的生成："><a href="#问题六：Job的生成：" class="headerlink" title="问题六：Job的生成："></a><strong>问题六：Job的生成：</strong></h2><p>​        一旦driver程序中出现action，就会生成一个job，比如count等，向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。</p><p>​        每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是<strong>宽依赖</strong>，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p><h2 id="问题七：有向无环图："><a href="#问题七：有向无环图：" class="headerlink" title="问题七：有向无环图："></a><strong>问题七：有向无环图：</strong></h2><p>​    <strong>DAG</strong>，有向无环图，简单的来说，就是一个由顶点和有方向性的边构成的图中，从任意一个顶点出发，没有任意一条路径会将其带回到出发点的顶点位置，为每个spark job计算具有依赖关系的多个stage任务阶段，通常根据shuffle来划分stage，如reduceByKey,groupByKey等涉及到shuffle的transformation就会产生新的stage ，然后将每个stage划分为具体的一组任务，以TaskSets的形式提交给底层的任务调度模块来执行，其中不同stage之前的RDD为宽依赖关系，TaskScheduler任务调度模块负责具体启动任务，监控和汇报任务运行情况。</p><h2 id="问题八：RDD是什么以及它的分类："><a href="#问题八：RDD是什么以及它的分类：" class="headerlink" title="问题八：RDD是什么以及它的分类："></a><strong>问题八：RDD是什么以及它的分类：</strong></h2><p><img src="https://img-blog.csdn.net/20180909162040887?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题九：RDD的操作"><a href="#问题九：RDD的操作" class="headerlink" title="问题九：RDD的操作"></a><strong>问题九：RDD的操作</strong></h2><p><img src="https://img-blog.csdn.net/20180909162041368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/2018090916204272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162101528?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162110376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/2018090916212499?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162136156?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163215275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163232304?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163244483?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163257291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题十-RDD缓存："><a href="#问题十-RDD缓存：" class="headerlink" title="问题十: RDD缓存："></a><strong>问题十: RDD缓存</strong>：</h2><p>​        Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 <strong>transformation</strong>自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。</p><p>​         Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。</p><p>​          Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li><p>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</p></li><li><p>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p></li><li><p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p></li><li><p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p></li><li><p>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</p></li><li><p>在不会使用cached RDD的时候，及时使用unpersist方法来释放它。</p></li></ul><h2 id="问题十一：RDD共享变量："><a href="#问题十一：RDD共享变量：" class="headerlink" title="问题十一：RDD共享变量："></a>问题十一：RDD共享变量：</h2><p>​       在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。</p><p>(1). <strong>广播变量（Broadcast Variables）</strong></p><p>– 广播变量缓存到各个节点的内存中，而不是每个 Task</p><p>– 广播变量被创建后，能在集群中运行的任何函数调用</p><p>– 广播变量是只读的，不能在被广播后修改</p><p>– 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本</p><p>val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量<br>(2). <strong>累加器</strong></p><p>​    累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值</p><h2 id="问题十二：spark-submit的时候如何引入外部jar包："><a href="#问题十二：spark-submit的时候如何引入外部jar包：" class="headerlink" title="问题十二：spark-submit的时候如何引入外部jar包："></a>问题十二：spark-submit的时候如何引入外部jar包：</h2><p>在通过spark-submit提交任务时，可以通过添加配置参数来指定 </p><p>–driver-class-path 外部jar包<br>–jars 外部jar包</p><h2 id="问题十三：spark如何防止内存溢出："><a href="#问题十三：spark如何防止内存溢出：" class="headerlink" title="问题十三：spark如何防止内存溢出："></a>问题十三：spark如何防止内存溢出：</h2><ul><li><p>driver端的内存溢出 </p><ul><li><strong>可以增大driver的内存参数</strong>：<code>spark.driver.memory (default 1g)</code><br>这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。</li><li><strong>map过程产生大量对象导致内存溢出</strong><br>这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。<br>面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区， 不会有shuffle的过程。</li></ul></li><li><p>数据不平衡导致内存溢出 </p><pre><code> 数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。</code></pre></li><li><p>shuffle后内存溢出 </p><pre><code>  shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。</code></pre></li><li><p>standalone模式下资源分配不均匀导致内存溢出</p><pre><code>  在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</code></pre><p>​      这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()<br>rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。</p></li></ul><h2 id="问题十四：spark中cache和persist的区别："><a href="#问题十四：spark中cache和persist的区别：" class="headerlink" title="问题十四：spark中cache和persist的区别："></a>问题十四：spark中cache和persist的区别：</h2><p><strong>cache</strong>：缓存数据，默认是缓存在内存中，其本质还是调用persist<br><strong>persist</strong>: 缓存数据，有丰富的数据缓存策略。数据可以保存在内存也可以保存在磁盘中，使用的时候指定对应的缓存级别就可以了。</p><h2 id="问题十五：spark中的数据倾斜的现象，原因，后果："><a href="#问题十五：spark中的数据倾斜的现象，原因，后果：" class="headerlink" title="问题十五：spark中的数据倾斜的现象，原因，后果："></a>问题十五：spark中的数据倾斜的现象，原因，后果：</h2><p><strong>(1)、数据倾斜的现象</strong><br>        多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。<br><strong>(2)、数据倾斜的原因</strong> </p><ul><li><p>数据问题<br>1、key本身分布不均衡（包括大量的key为空）<br>2、key的设置不合理</p></li><li><p>spark使用问题<br>1、shuffle时的并发度不够<br>2、计算方式有误</p></li></ul><p><strong>(3)、数据倾斜的后果</strong> </p><ol><li><p>spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）</p></li><li><p>过多的数据在同一个task中运行，将会把executor撑爆。</p></li></ol><h2 id="问题十六：spark数据倾斜的处理："><a href="#问题十六：spark数据倾斜的处理：" class="headerlink" title="问题十六：spark数据倾斜的处理："></a>问题十六：spark数据倾斜的处理：</h2><p>发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首<strong>先要检查数据本身</strong>，是否存在异常数据。</p><ol><li><p><strong>数据问题造成的数据倾斜</strong><br><strong>找出异常的key</strong><br>如果任务长时间卡在最后最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个。<br>比如:</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>“key”<span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span><span class="token operator">+</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><p>   如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况: </p><p>1、null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。<br>2、无效数据，大量重复的测试数据或是对结果影响不大的有效数据。<br>3、有效数据，业务导致的正常数据分布。</p><p><strong>解决办法</strong> </p><p>第1，2种情况，直接对数据进行过滤即可（因为该数据对当前业务不会产生影响）。<br>第3种情况则需要进行一些特殊操作，常见的有以下几种做法<br>(1) <strong>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作</strong>。<br>(2) <strong>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</strong><br>(3) 使用reduceByKey 代替 groupByKey(reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义.)<br>(4) 使用<strong>map join</strong>。</p><p><strong>案例</strong> </p><p>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:<br>(1) 将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>(2) 对数据进行 reduceByKey(func)<br>(3) 将 key + 随机值 转成 key<br>(4) 再对数据进行 reduceByKey(func)</p><p><strong>案例操作流程分析</strong>： </p><p>假设说有倾斜的Key，我们给所有的Key加上一个随机数，然后进行reduceByKey操作；此时同一个Key会有不同的随机数前缀，在进行reduceByKey操作的时候原来的一个非常大的倾斜的Key就分而治之变成若干个更小的Key，不过此时结果和原来不一样，怎么破？进行map操作，目的是把随机数前缀去掉，然后再次进行reduceByKey操作。（当然，如果你很无聊，可以再次做随机数前缀），这样我们就可以把原本倾斜的Key通过分而治之方案分散开来，最后又进行了全局聚合<br>注意1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。<br>注意2: 单独处理异常数据时，可以配合使用Map Join解决。</p><p>2.<strong>spark使用不当造成的数据倾斜</strong></p><ul><li><p><strong>提高shuffle并行度</strong><br>dataFrame和sparkSql可以设置spark.sql.shuffle.partitions参数控制shuffle的并发度，默认为200。<br>rdd操作可以设置spark.default.parallelism控制并发度，默认参数由不同的Cluster Manager控制。</p><ul><li><strong>局限性:</strong> 只是让每个task执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些key的大      小非常大，即使一个task单独执行它，也会受到数据倾斜的困扰。</li></ul></li><li><p><strong>使用map join 代替reduce join</strong></p><pre><code>  在小表不是特别大(取决于你的executor大小)的情况下使用，可以使程序避免shuffle的过程，自然也就没有数据倾斜的困扰了.（详细见http://blog.csdn.net/lsshlsw/article/details/50834858、http://blog.csdn.net/lsshlsw/article/details/48694893）</code></pre><ul><li><p><strong>局限性</strong>: 因为是先将小数据发送到每个executor上，所以数据量不能太大。</p><p>​    </p><h2 id="问题十七：spark中map-side-join关联优化："><a href="#问题十七：spark中map-side-join关联优化：" class="headerlink" title="问题十七：spark中map-side-join关联优化："></a>问题十七：spark中map-side-join关联优化：</h2></li></ul></li></ul><p>​      将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。</p><p>如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。</p><p>何时使用：<strong>在海量数据中匹配少量特定数据</strong></p><p>原理：reduce-side-join 的缺陷在于会将key相同的数据发送到同一个partition中进行运算，大数据集的传输需要长时间的IO，同时任务并发度收到限制，还可能造成数据倾斜。</p><p>reduce-side-join 运行图如下</p><p><img src="https://img-blog.csdn.net/20180909162441742?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>map-side-join 运行图如下：</p><p><img src="https://img-blog.csdn.net/20180909162409221?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>将少量的数据转化为Map进行广播，广播会将此 Map 发送到每个节点中，如果不进行广播，每个task执行时都会去获取该Map数据，造成了性能浪费。对大数据进行遍历，使用mapPartition而不是map，因为mapPartition是在每个partition中进行操作，因此可以减少遍历时新建broadCastMap.value对象的空间消耗，同时匹配不到的数据也不会返回。</p><h2 id="问题十八：kafka整合sparkStreaming问题："><a href="#问题十八：kafka整合sparkStreaming问题：" class="headerlink" title="问题十八：kafka整合sparkStreaming问题："></a>问题十八：kafka整合sparkStreaming问题：</h2><p><strong>(1)、如何实现sparkStreaming读取kafka中的数据</strong><br>可以这样说：在kafka0.10版本之前有二种方式与sparkStreaming整合，一种是基于<strong>receiver</strong>，一种是<strong>direct</strong>,然后分别阐述这2种方式分别是什么 </p><ul><li><p><strong>receiver</strong>：是采用了kafka高级api,利用receiver接收器来接受kafka topic中的数据，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据，kafka中topic的偏移量是保存在zk中的。<br>基本使用：还有几个需要注意的点： </p><p>​       在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度.<br>​         对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。<br>在默认配置下，这种方式可能会因为底层的失败而丢失数据. 因为receiver一直在接收数据,在其已经通知zookeeper数据接收完成但是还没有处理的时候,executor突然挂掉(或是driver挂掉通知executor关闭),缓存在其中的数据就会丢失. 如果希望做到高可靠, 让数据零丢失,如果我们启用了Write Ahead Logs(spark.streaming.receiver.writeAheadLog.enable=true）该机制会同步地将接收到的Kafka数据写入分布式文件系统(比如HDFS)上的预写日志中. 所以, 即使底层节点出现了失败, 也可以使用预写日志中的数据进行恢复. 复制到文件系统如HDFS，那么storage level需要设置成 StorageLevel.MEMORY_AND_DISK_SER，也就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER)</p></li></ul><ul><li><strong>direct</strong>: 在spark1.3之后，引入了Direct方式。不同于Receiver的方式，Direct方式没有receiver这一层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，之后根据设定的maxRatePerPartition来处理每个batch。（设置spark.streaming.kafka.maxRatePerPartition=10000。限制每秒钟从topic的每个partition最多消费的消息条数）</li></ul><p><strong>(2) 对比这2中方式的优缺点：</strong></p><ul><li><p>采用receiver方式：这种方式可以保证数据不丢失，但是无法保证数据只被处理一次，WAL实现的是At-least-once语义（至少被处理一次），如果在写入到外部存储的数据还没有将offset更新到zookeeper就挂掉,这些数据将会被反复消费. 同时,降低了程序的吞吐量。</p></li><li><p>采用direct方式:    相比Receiver模式而言能够确保机制更加健壮. 区别于使用Receiver来被动接收数据, Direct模式会周期性地主动查询Kafka, 来获得每个topic+partition的最新的offset, 从而定义每个batch的offset的范围. 当处理数据的job启动时, 就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 </p><pre><code>**优点**： **1、简化并行读取** 如果要读取多个partition, 不需要创建多个输入DStream然后对它们进行union操作. Spark会创建跟Kafka partition一样多的RDD partition, 并且会并行从Kafka中读取数据. 所以在Kafka partition和RDD partition之间, 有一个一对一的映射关系.**2、高性能** 如果要保证零数据丢失, 在基于receiver的方式中, 需要开启WAL机制. 这种方式其实效率低下, 因为数据实际上被复制了两份, Kafka自己本身就有高可靠的机制, 会对数据复制一份, 而这里又会复制一份到WAL中. 而基于direct的方式, 不依赖Receiver, 不需要开启WAL机制, 只要Kafka中作了数据的复制, 那么就可以通过Kafka的副本进行恢复.**3、一次且仅一次的事务机制** 基于receiver的方式, 是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的. 这是消费Kafka数据的传统方式. 这种方式配合着WAL机制可以保证数据零丢失的高可靠性, 但是却无法保证数据被处理一次且仅一次, 可能会处理两次. 因为Spark和ZooKeeper之间可能是不同步的. 基于direct的方式, 使用kafka的简单api, Spark Streaming自己就负责追踪消费的offset, 并保存在checkpoint中. Spark自己一定是同步的, 因此可以保证数据是消费一次且仅消费一次。不过需要自己完成将offset写入zk的过程,在官方文档中都有相应介绍. </code></pre><pre class="line-numbers language-scala"><code class="language-scala"><span class="token operator">-</span><span class="token operator">*</span>简单代码实例： messages<span class="token punctuation">.</span>foreachRDD<span class="token punctuation">(</span>rdd<span class="token keyword">=></span><span class="token punctuation">{</span> <span class="token keyword">val</span> message <span class="token operator">=</span> rdd<span class="token punctuation">.</span>map<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//对数据进行一些操作 </span>message<span class="token punctuation">.</span>map<span class="token punctuation">(</span>method<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//更新zk上的offset (自己实现) </span>updateZKOffsets<span class="token punctuation">(</span>rdd<span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>sparkStreaming程序自己消费完成后，自己主动去更新zk上面的偏移量。也可以将zk中的偏移量保存在mysql或者redis数据库中，下次重启的时候，直接读取mysql或者redis中的偏移量，获取到上次消费的偏移量，接着读取数据。</p></li></ul><h2 id="问题十九：利用scala语言进行排序"><a href="#问题十九：利用scala语言进行排序" class="headerlink" title="问题十九：利用scala语言进行排序"></a>问题十九：利用scala语言进行排序</h2><p><strong>1.冒泡</strong>：</p><p><img src="https://img-blog.csdn.net/20180909162545316?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><strong>2.快读排序</strong>：</p><p><img src="https://img-blog.csdn.net/20180909162606931?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162624563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题二十：spark-master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？"><a href="#问题二十：spark-master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？" class="headerlink" title="问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？"></a>问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？</h2><p>​        Spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括worker,master,application,executors.standby节点要从zk中获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的，另外，master切换需要注意两点：</p><p>​    1. 在master切换的过程中，所有的已经在运行的程序皆正常运行，因为spark application在运行前就已经通过cluster manager获得了计算资源，所以在运行时job本身的调度和处理master是没有任何关系的；</p><p>​    2. 在master的切换过程中唯一的影响是不能提交新的job，一方面不能提交新的应用程序给集群，因为只有Active master才能接受新的程序的提交请求，另外一方面，已经运行的程序也不能action操作触发新的job提交请求。</p><p><strong>问题二十一：spark master HA主从切换过程不会影响集群已有的作业运行，为什么？</strong></p><p>答：因为程序在运行之前，已经向集群申请过资源，这些资源已经提交给driver了，也就是说已经分配好资源了，这是粗粒度分配，一次性分配好资源后不需要再关心资源分配，在运行时让driver和executor自动交互，弊端是如果资源分配太多，任务运行完不会很快释放，造成资源浪费，这里不适用细粒度分配的原因是因为任务提交太慢。</p><h2 id="问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？"><a href="#问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？" class="headerlink" title="问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？"></a>问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？</h2><p><strong>1.粗粒度</strong>：启动时就分配好资源，程序启动，后续具体使用就使用分配好的资源，不需要再分配资源。好处：作业特别多时，资源复用率较高，使用粗粒度。缺点：容易资源浪费，如果一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度。如果有999个资源闲置在那里，会造成资源大量浪费。</p><p><strong>2.细粒度</strong>：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。</p><h2 id="问题二十三：driver的功能是什么："><a href="#问题二十三：driver的功能是什么：" class="headerlink" title="问题二十三：driver的功能是什么："></a><strong>问题二十三：driver的功能是什么</strong>：</h2><p>1.一个spark作业运行时包括一个driver进程，也就是作业的主进程，具有main函数，并且有sparkContext的实例，是程序的入口；</p><p><strong>2.功能</strong>：负责向集群申请资源，向master注册信息，负责了作业的调度，负责了作业的解析，生成stage并调度task到executor上，包括DAGScheduler，TaskScheduler。</p><h2 id="问题二十四：spark的有几种部署模式，每种模式特点？"><a href="#问题二十四：spark的有几种部署模式，每种模式特点？" class="headerlink" title="问题二十四：spark的有几种部署模式，每种模式特点？"></a>问题二十四：spark的有几种部署模式，每种模式特点？</h2><p><strong>1）本地模式</strong></p><p>Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类</p><p>·  local：只启动一个executor</p><p>·  local[k]:启动k个executor</p><p>·  local：启动跟cpu数目相同的 executor</p><p><strong>2)standalone模式</strong></p><p>分布式部署集群， 自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础，</p><p><strong>3)Spark on yarn模式</strong></p><p>分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端</p><p><strong>4）Spark On Mesos模式。</strong>官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序：</p><p><strong>1)   粗粒度模式</strong>（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p><p><strong>2)   细粒度模式（Fine-grained Mode）</strong>：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。</p><h2 id="问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a><strong>问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</strong></h2><p><strong>1）Spark core</strong>：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。</p><p><strong>2）SparkStreaming</strong>是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。</p><p><strong>3）Spark sql</strong>：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析</p><p><strong>4）BlinkDB</strong> ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。</p><p><strong>5）MLBase</strong>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib，MLI、ML Optimizer和MLRuntime。</p><p><strong>6）GraphX</strong>是Spark中用于图和图并行计算</p><h2 id="问题二十六：spark中worker-的主要工作是什么？"><a href="#问题二十六：spark中worker-的主要工作是什么？" class="headerlink" title="问题二十六：spark中worker 的主要工作是什么？"></a><strong>问题二十六：spark中worker 的主要工作是什么？</strong></h2><p><strong>主要功能</strong>：管理当前节点内存，CPU的使用情况，接受master发送过来的资源指令，通过executorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务，需要注意的是：</p><p><strong>1.worker</strong>会不会汇报当前信息给master？worker心跳给master主要只有workid，不会以心跳的方式发送资源信息给master，这样master就知道worker是否存活，只有故障的时候才会发送资源信息；</p><p><strong>2.worker</strong>不会运行代码，具体运行的是executor，可以运行具体application斜的业务逻辑代码，操作代码的节点，不会去运行代码。</p><h2 id="问题二十七：简单说一下hadoop和spark的shuffle相同和差异？"><a href="#问题二十七：简单说一下hadoop和spark的shuffle相同和差异？" class="headerlink" title="问题二十七：简单说一下hadoop和spark的shuffle相同和差异？"></a><strong>问题二十七：简单说一下hadoop和spark的shuffle相同和差异？</strong></h2><p><strong>1）从 high-level 的角度来看，两者并没有大的差别</strong>。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</p><p><strong>2）从 low-level 的角度来看，两者差别不小。</strong> Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。</p><p><strong>3）从实现角度来看，两者也有不少差别。</strong> Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。</p><p>如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？ </p><p>Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</p><h2 id="问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别"><a href="#问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别" class="headerlink" title="问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别"></a>问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别</h2><p><strong>两者都是用mr模型来进行并行计算:</strong></p><p>1) hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。 </p><p>2) spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 </p><p>3) hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 </p><p>spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。</p><h2 id="问题二十九：RDD机制？"><a href="#问题二十九：RDD机制？" class="headerlink" title="问题二十九：RDD机制？"></a>问题二十九：RDD机制？</h2><p>rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 </p><p>所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。 </p><p>rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。</p><h2 id="问题三十：spark有哪些组件？"><a href="#问题三十：spark有哪些组件？" class="headerlink" title="问题三十：spark有哪些组件？"></a><strong>问题三十：spark有哪些组件？</strong></h2><p>答：主要有如下组件：</p><p>1）master：管理集群和节点，不参与计算。 </p><p>2）worker：计算节点，进程本身不参与计算，和master汇报。 </p><p>3）Driver：运行程序的main方法，创建spark context对象。 </p><p>4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 </p><p>5）client：用户提交程序的入口。</p><h2 id="问题三十一：spark工作机制？"><a href="#问题三十一：spark工作机制？" class="headerlink" title="问题三十一：spark工作机制？"></a><strong>问题三十一：spark工作机制？</strong></h2><p>答：用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 </p><p>执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。 task scheduler会将stage划分为task set分发到各个节点的executor中执行。</p><h2 id="问题三十二：spark的优化怎么做？"><a href="#问题三十二：spark的优化怎么做？" class="headerlink" title="问题三十二：spark的优化怎么做？"></a><strong>问题三十二：spark的优化怎么做？</strong></h2><p>答： spark调优比较复杂，但是大体可以分为三个方面来进行，</p><p><strong>1）平台层面的调优：</strong>防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet，</p><p><strong>2）应用程序层面的调优</strong>：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等，</p><p><strong>3）JVM层面的调优</strong>：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等</p><p>​       序列化在分布式系统中扮演着重要的角色，优化Spark程序时，首当其冲的就是对序列化方式的优化。Spark为使用者提供两种序列化方式：</p><ul><li>Java serialization: 默认的序列化方式。</li></ul><ul><li>Kryo serialization: 相较于 Java serialization 的方式，速度更快，空间占用更小，但并不支持所有的序列化格式，同时使用的时候需要注册class。spark-sql中默认使用的是kyro的序列化方式。<br>可以在spark-default.conf设置全局参数，也可以代码中初始化时对SparkConf设置 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) ，该参数会同时作用于机器之间数据的shuffle操作以及序列化rdd到磁盘，内存。<br>Spark不将Kyro设置成默认的序列化方式是因为它需要对类进行注册，官方强烈建议在一些网络数据传输很大的应用中使用kyro序列化。</li></ul><p>如果你要序列化的对象比较大，可以增加参数spark.kryoserializer.buffer所设置的值。</p><p>如果你没有注册需要序列化的class，Kyro依然可以照常工作，但会存储每个对象的全类名(full class name)，这样的使用方式往往比默认的 Java serialization 还要浪费更多的空间。</p><p>可以设置 spark.kryo.registrationRequired 参数为 true，使用kyro时如果在应用中有类没有进行注册则会报错：</p><p>如上这个错误需要添加</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题一：Spark中的RDD是什么，有哪些特性？&quot;&gt;&lt;a href=&quot;#问题一：Spark中的RDD是什么，有哪些特性？&quot; class=&quot;headerlink&quot; title=&quot;问题一：Spark中的RDD是什么，有哪些特性？&quot;&gt;&lt;/a&gt;&lt;strong&gt;问题一：Sp
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>HiveSQL优化 hive参数版总结</title>
    <link href="https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/"/>
    <id>https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/</id>
    <published>2020-06-11T06:50:00.000Z</published>
    <updated>2020-06-11T10:22:08.086Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。</strong></p></blockquote><h2 id="1-Hive优化目标"><a href="#1-Hive优化目标" class="headerlink" title="1. Hive优化目标"></a><strong>1. Hive优化目标</strong></h2><ul><li><p>在有限的资源下，执行效率更高</p></li><li><p>常见问题</p><ul><li><p>数据倾斜</p></li><li><p>map数设置</p></li><li><p>reduce数设置</p></li><li><p>其他</p></li></ul></li></ul><h2 id="2-Hive执行优化"><a href="#2-Hive执行优化" class="headerlink" title="2. Hive执行优化"></a><strong>2. Hive执行优化</strong></h2><ul><li><p><code>HQL --&gt; Job --&gt; Map/Reduce</code></p></li><li><p>执行计划<code>explain [extended] hql</code></p><p>样例</p><p><code>select col,count(1) from test2 group by col;</code></p><p><code>explain select col,count(1) from test2 group by col;</code></p></li></ul><h2 id="3-Hive表优化"><a href="#3-Hive表优化" class="headerlink" title="3. Hive表优化"></a><strong>3. Hive表优化</strong></h2><ul><li><p>分区</p><p><code>set hive.exec.dynamic.partition=true;</code></p><p><code>set hive.exec.dynamic.partition.mode=nonstrict;</code></p><p>​    静态分区</p><p>​    动态分区</p></li><li><p>分桶</p><p>  <code>set hive.enforce.bucketing=true;</code></p><p>  <code>set hive.enforce.sorting=true;</code></p></li><li><p>数据</p><p>相同数据尽量聚集在一起</p></li></ul><h2 id="4-Hive-Job优化"><a href="#4-Hive-Job优化" class="headerlink" title="4. Hive Job优化"></a><strong>4. Hive Job优化</strong></h2><h3 id="并行化执行"><a href="#并行化执行" class="headerlink" title="并行化执行"></a>并行化执行</h3><p>每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间</p><ul><li><code>set hive.exec.parallel= true;</code></li><li><code>set hive.exec.parallel.thread.numbe=8;</code></li></ul><h3 id="本地化执行"><a href="#本地化执行" class="headerlink" title="本地化执行"></a>本地化执行</h3><ul><li>​    job的输入数据大小必须小于参数:<code>hive.exec.mode.local.auto.inputbytes.max(默认128MB)</code></li><li>​    job的map数必须小于参数:<code>hive.exec.mode.local.auto.tasks.max(默认4)</code></li><li>​    job的reduce数必须为0或者1</li><li>​    <code>set hive.exec.mode.local.auto=true;</code></li></ul><p>​       当一个job满足如上条件才能真正使用本地模式:</p><h3 id="job合并输入小文件"><a href="#job合并输入小文件" class="headerlink" title="job合并输入小文件"></a>job合并输入小文件</h3><ul><li><code>set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</code></li><li>合并文件数由mapred.max.split.size限制的大小决定</li></ul><h3 id="job合并输出小文件"><a href="#job合并输出小文件" class="headerlink" title="job合并输出小文件"></a>job合并输出小文件</h3><ul><li><p><code>set hive.merge.smallfiles.avgsize=256000000;</code>当输出文件平均小于该值，启动新job合并文件</p></li><li><p><code>set hive.merge.size.per.task=64000000;</code>合并之后的文件大小</p></li></ul><h3 id="JVM重利用"><a href="#JVM重利用" class="headerlink" title="JVM重利用"></a>JVM重利用</h3><ul><li><p><code>set mapred.job.reuse.jvm.num.tasks=20;</code></p><p>JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。</p></li></ul><h3 id="压缩数据"><a href="#压缩数据" class="headerlink" title="压缩数据"></a>压缩数据</h3><p><code>set hive.exec.compress.output=true;</code></p><p><code>set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec;</code></p><p><code>set mapred.output.compression.type=BLOCK;</code></p><p><code>set hive.exec.compress.intermediate=true;</code></p><p><code>set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</code></p><p><code>set hive.intermediate.compression.type=BLOCK;</code></p><p>中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式</p><p>hive查询最终的输出也可以压缩</p><h2 id="5-Hive-Map优化"><a href="#5-Hive-Map优化" class="headerlink" title="5. Hive Map优化"></a><strong>5. Hive Map优化</strong></h2><p><code>set mapred.map.tasks =10;</code> 无效</p><p> <strong>(1) 默认map个数</strong></p><p><code>default_num = total_size / block_size;</code></p><p>如果想增加map个数，则设置<code>mapred.map.tasks</code>为一个较大的值</p><p>如果想减小map个数，则设置<code>mapred.min.split.size</code>为一个较大的值</p><p>情况1：输入文件size巨大，但不是小文件</p><p>情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于<code>blockSize</code>。这种情况通过增大<code>mapred.min.split.size</code>不可行，需要使用<code>combineFileInputFormat</code>将多个input path合并成一个<code>InputSplit</code>送给mapper处理，从而减少<code>mapper</code>的数量。</p><h2 id="6-Hive-Shuffle优化"><a href="#6-Hive-Shuffle优化" class="headerlink" title="6. Hive Shuffle优化"></a><strong>6. Hive Shuffle优化</strong></h2><ul><li>Map端</li></ul><p>​    <code>io.sort.mb</code></p><p>​    <code>io.sort.spill.percent</code></p><p>​    <code>min.num.spill.for.combine</code></p><p>​    <code>io.sort.factor</code></p><p>​    <code>io.sort.record.percent</code></p><ul><li><p>Reduce端</p><p><code>mapred.reduce.parallel.copies</code></p><p><code>mapred.reduce.copy.backoff</code></p><p><code>io.sort.factor</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p></li></ul><h2 id="7-Hive-Reduce优化"><a href="#7-Hive-Reduce优化" class="headerlink" title="7. Hive Reduce优化"></a><strong>7. Hive Reduce优化</strong></h2><ul><li><p>需要reduce操作的查询</p><p>group by,</p><p>join,</p><p>distribute by,</p><p>cluster by…</p><p>order by 比较特殊,只需要一个reduce</p></li><li><p>sum,count,distinct…</p></li><li><p>聚合函数</p><p>高级查询</p></li><li><p>推测执行</p><p>mapred.reduce.tasks.speculative.execution</p><p>hive.mapred.reduce.tasks.speculative.execution</p></li><li><p>Reduce优化</p><p>numRTasks = min[maxReducers,input.size/perReducer]</p><p>maxReducers=hive.exec.reducers.max</p><p>perReducer = hive.exec.reducers.bytes.per.reducer</p><p>hive.exec.reducers.max 默认 ：999</p><p>hive.exec.reducers.bytes.per.reducer 默认:1G</p></li><li><p>set mapred.reduce.tasks=10;直接设置</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。&lt;/strong&gt;&lt;/p&gt;
&lt;/blo
      
    
    </summary>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>6.Hadoop面试系列之UDF</title>
    <link href="https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/"/>
    <id>https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/</id>
    <published>2020-06-10T14:57:19.767Z</published>
    <updated>2020-06-10T14:57:19.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-开发步骤"><a href="#1-开发步骤" class="headerlink" title="1. 开发步骤"></a>1. <strong>开发步骤</strong></h2><p>​       UDF简称自定义函数，它是Hive函数库的扩展，自定义函数UDF在MapReduce执行阶段发挥作用。开发步骤如下：</p><blockquote><p>1）  给hive.ql.exec.UDF包开发一个自定义函数类，从UDF继承。自定义函数类实现evaluate方法。</p><p>2）  在FunctionRegistry类中注册开发的自定义函数类。</p><p>3）  打包发布至Hive客户端。</p></blockquote><h3 id="1-1-开发工具"><a href="#1-1-开发工具" class="headerlink" title="1.1 开发工具"></a><strong>1.1 开发工具</strong></h3><p>​      Eclipse是一款开源的、基于Java的可扩展开发平台。Hadoop开发人员可通过在Eclipse上面开发UDF。</p><h3 id="1-2-UDF函数案例"><a href="#1-2-UDF函数案例" class="headerlink" title="1.2 UDF函数案例"></a>1.2 UDF函数案例</h3><h4 id="1）开发UDF函数类"><a href="#1）开发UDF函数类" class="headerlink" title="1）开发UDF函数类"></a>1）开发UDF函数类</h4><p>文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHelloWorld.java</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">.</span>UDF<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">UDFHelloWorld</span> <span class="token keyword">extends</span> <span class="token class-name">UDF</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> String <span class="token function">evaluate</span><span class="token punctuation">(</span>String str<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>str <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> null<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> <span class="token string">"HelloWorld "</span> <span class="token operator">+</span> str<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        helloUDF uf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">helloUDF</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//Text t = new Text("gfsg");</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>uf<span class="token punctuation">.</span><span class="token function">evaluate</span><span class="token punctuation">(</span><span class="token string">"nihao"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2）UDF类注册，注册方法"><a href="#2）UDF类注册，注册方法" class="headerlink" title="2）UDF类注册，注册方法"></a>2）UDF类注册，注册方法</h4><p>文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>UDFHelloWorld<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*** FunctionRegistry.*/</span><span class="token keyword">public</span> <span class="token keyword">final</span> <span class="token keyword">class</span> <span class="token class-name">FunctionRegistry</span> <span class="token punctuation">{</span>    <span class="token keyword">static</span> <span class="token punctuation">{</span>        <span class="token function">registerGenericUDF</span><span class="token punctuation">(</span><span class="token string">"concat"</span><span class="token punctuation">,</span> GenericUDFConcat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"substr"</span><span class="token punctuation">,</span> UDFSubstr<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"substring"</span><span class="token punctuation">,</span> UDFSubstr<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"space"</span><span class="token punctuation">,</span> UDFSpace<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"repeat"</span><span class="token punctuation">,</span> UDFRepeat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"ascii"</span><span class="token punctuation">,</span> UDFAscii<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"lpad"</span><span class="token punctuation">,</span> UDFLpad<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"rpad"</span><span class="token punctuation">,</span> UDFRpad<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"Hello"</span><span class="token punctuation">,</span> UDFHelloWorld<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerGenericUDF</span><span class="token punctuation">(</span><span class="token string">"size"</span><span class="token punctuation">,</span> GenericUDFSize<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3）Jar包发布路径"><a href="#3）Jar包发布路径" class="headerlink" title="3）Jar包发布路径"></a>3）Jar包发布路径</h4><p>发布路径：/opt/boh/hive/lib/hive-exec-0.12.0-cdh5.0.0.jar</p><p>上传至hadoop集群执行脚本的hive客户端。</p><h3 id="1-3Hive-UDF函数"><a href="#1-3Hive-UDF函数" class="headerlink" title="1.3Hive UDF函数"></a>1.3Hive UDF函数</h3><h4 id="1-3-1UDF函数列表"><a href="#1-3-1UDF函数列表" class="headerlink" title="1.3.1UDF函数列表"></a>1.3.1UDF函数列表</h4><ul><li>函数清单及其功能</li></ul><p><code>TO_DATE(string date,'format')</code></p><ul><li>格式化所需要的日期</li></ul><p><code>ADD_MONTHS(Timestamp date,int n)</code></p><ul><li>增加月数</li></ul><p><code>date_tostring(Timestamp date,'format')</code></p><ul><li>转换Date类型为指定格式字符串</li></ul><p><code>MONTHS_BETWEEN（Timestamp date1，Timestamp date2）</code></p><ul><li>返回两个日期之间的月数</li></ul><p><code>f_age(string identityId)</code></p><ul><li>验证身份证合法性并返回性别年龄</li></ul><p><code>f_checkidcard(string identityId)</code></p><ul><li>验证身份证合法性</li></ul><h4 id="1-3-2-UDF函数说明"><a href="#1-3-2-UDF函数说明" class="headerlink" title="1.3.2 UDF函数说明"></a>1.3.2 UDF函数说明</h4><ul><li>TO_DATE函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">Select</span>  to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">09</span> <span class="token number">11</span>:<span class="token number">11</span>:<span class="token number">11</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>ADD_MONTHS函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> add_months<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">10</span><span class="token operator">-</span><span class="token number">09</span> <span class="token number">11</span>:<span class="token number">11</span>:<span class="token number">11</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>date_tostring函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> date_tostring<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'YYYY-MM-DD'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">09</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>MONTHS_BETWEEN函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> MONTHS_BETWEEN<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>to_date<span class="token punctuation">(</span><span class="token string">'20140706111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span>返回结果：<span class="token number">2.096774193548387</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>f_age函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> f_age<span class="token punctuation">(</span><span class="token string">'511024198710148199'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">127</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>f_checkidcard函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> f_checkidcard<span class="token punctuation">(</span><span class="token string">'511024198710148199'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-开发步骤&quot;&gt;&lt;a href=&quot;#1-开发步骤&quot; class=&quot;headerlink&quot; title=&quot;1. 开发步骤&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;开发步骤&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;​       UDF简称自定义函数，它是Hive函数库的扩展，自定义
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hive开窗函数梳理</title>
    <link href="https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/"/>
    <id>https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/</id>
    <published>2020-06-10T06:35:00.000Z</published>
    <updated>2020-06-10T11:15:10.879Z</updated>
    
    <content type="html"><![CDATA[<p>本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。</p><p>假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。</p><p><img src="https://pic3.zhimg.com/80/v2-008682fd90478af4fb84e88bccd480ee_1440w.jpg" alt="img"></p><p><strong>1.SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">/*前三次贷款的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token number">3</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token keyword">CURRENT</span> <span class="token keyword">ROW</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv1<span class="token punctuation">,</span><span class="token comment" spellcheck="true">/*历史所有贷款 累加到下一次贷款 的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token keyword">UNBOUNDED</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token number">1</span> <span class="token keyword">FOLLOWING</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv2<span class="token keyword">FROM</span> loan <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。</p><p>窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用：</p><ul><li>n PRECEDING : 前n行    n preceding</li><li>n FOLLOWING：后n行</li><li>CURRENT ROW ： 当前行</li></ul><p>如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为:</p><p>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.</p><p>窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。</p><p><strong>2.NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#将数据按name切分成10区，并返回属于第几个分区</span>NTILE<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>ROW_NUMBER<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f3</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>DENSE_RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f4</span><span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中第一个函数<a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">NTILE(10)</a>是将数据按name切分成10区，并返回属于第几个分区。</p><blockquote><p>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p><p>   然后可以根据桶号，选取前或后 n分之几的数据。</p></blockquote><p>后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。</p><ul><li>ROW_NUMBER()返回的是一列连续的序号。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-0f2c6da71227f7840aea5257acf8d88b_1440w.png" alt="img"></p><ul><li>RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}.</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f0bd3985ddacd2f347151dacc24cce_1440w.png" alt="img"></p><ul><li>DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-ac60db4d8a9c658ddf17fb43f09f616d_1440w.png" alt="img"></p><p><strong>3.LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取上一笔贷款的日期,缺失默认填NULL</span>LAG<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> last_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取下一笔贷款的日期,缺失指定填'1970-1-1'</span>LEAD<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'1970-1-1'</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> next_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取最早一笔贷款的日期</span>FIRST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> first_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取新一笔贷款的日期</span>LAST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> latest_dt<span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/pelifymeng2/article/details/70313943" target="_blank" rel="noopener">LAG(n)</a>将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录</p><p><strong>4.GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdateGROUPING SETS<span class="token punctuation">(</span>substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> orderdate<span class="token punctuation">)</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。</p><p>比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5.</p><p>如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null.</p><pre class="line-numbers language-text"><code class="language-text">SELECT A,B,CFROM loan#分别按照月份和日进行分区GROUP BY substring(orderdate,1,7),orderdatewith CUBEORDER BY GROUPING__ID; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。</p><pre class="line-numbers language-text"><code class="language-text">SELECT A,B,CFROM loan#分别按照月份和日进行分区GROUP BY substring(orderdate,1,7),orderdatewith ROLLUPORDER BY GROUPING__ID; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。&lt;/p&gt;
&lt;p&gt;假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-008
      
    
    </summary>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
      <category term="开窗函数" scheme="https://dataquaner.github.io/tags/%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop核心知识之MapReduce原理</title>
    <link href="https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/"/>
    <id>https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/</id>
    <published>2020-06-08T13:14:00.000Z</published>
    <updated>2020-06-08T14:31:06.843Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>MapReduce是一个基于集群的计算<strong>平台</strong>，是一个简化分布式编程的计算<strong>框架</strong>，是一个将分布式计算抽象为Map和Reduce两个阶段的编程<strong>模型</strong>。<em>（这句话记住了是可以用来装逼的）</em></p></blockquote><h2 id="1-MapReduce工作流程"><a href="#1-MapReduce工作流程" class="headerlink" title="1.MapReduce工作流程"></a>1.MapReduce工作流程</h2><p>0)   用户提交任务 （含数据）</p><p>1)    集群首先对输入数据源进行<strong>切片</strong></p><p>2)    master 调度 worker 执行 map 任务</p><p>3)    worker 读取输入源片段</p><p>4)    worker 执行 map 任务，将任务输出保存在本地<br>5)    master 调度 worker 执行 reduce 任务，reduce worker 读取 map 任务的输出文件</p><p>6） 执行 reduce 任务，将任务输出保存到 HDFS</p><p><img src="https://img-blog.csdnimg.cn/20181215100217729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIzMTM3Mw==,size_16,color_FFFFFF,t_70" alt="MapReduce工作流程"></p><p>由上至下依次执行</p><table><thead><tr><th>过程</th><th>过程描述</th></tr></thead><tbody><tr><td></td><td>用户提交任务job 给集群</td></tr><tr><td>切片</td><td>集群查找源数据 对源数据做基本处理</td></tr><tr><td>分词(每行执行一次map函数)</td><td>集群(yarn的appliction)分配map任务节点worker</td></tr><tr><td>映射</td><td>其中间数据(存在本地)</td></tr><tr><td>分区(partition)</td><td>中间数据</td></tr><tr><td>排序 (或二次排序)</td><td>中间数据</td></tr><tr><td>聚合(combine有无key聚合后key无序)</td><td>中间数据  分组(group) 发生在排序后混洗前(个人理解)</td></tr><tr><td>混洗(shuffle后key有序)</td><td>混洗横跨mapper和reducer，其发生在mapper的输出和reducer的输入阶段</td></tr><tr><td>规约(reduce)</td><td>集群(yarn的appliction)分配reduce任务节点worker</td></tr></tbody></table><p><img src="https://upload-images.jianshu.io/upload_images/12652505-f3e65e7fc499b579.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1172/format/webp" alt="MapReduce工作原理"></p><p>下面针对具体过程详细介绍：</p><h2 id="2-切片split"><a href="#2-切片split" class="headerlink" title="2.切片split"></a>2.切片split</h2><p>​       HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</p><p><strong>Map任务的数量</strong>：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。<strong>大多数情况下，理想的分片大小是一个HDFS块</strong></p><p><strong>Reduce任务的数量：</strong> <strong>最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</strong> 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</p><h2 id="3-Map-阶段"><a href="#3-Map-阶段" class="headerlink" title="3.Map()阶段"></a>3.Map()阶段</h2><blockquote><ol><li><p>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数</p></li><li><p>重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出</p></li><li><p>对输出的key、value进行分区</p></li><li><p>对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中</p></li></ol></blockquote><h2 id="4-Reduce阶段"><a href="#4-Reduce阶段" class="headerlink" title="4. Reduce阶段"></a>4. Reduce阶段</h2><blockquote><p>多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上</p></blockquote><blockquote><p>对多个map的输出进行合并、排序。</p></blockquote><blockquote><p>重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出</p></blockquote><blockquote><p>把reduce的输出保存到文件中</p></blockquote><p>特别说明：</p><blockquote><p>切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理</p><p>分词，映射，分区，排序，聚合 都属map阶段</p><p>混洗  横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段</p><p>规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义</p><p>分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容</p></blockquote><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>执行步骤：</p><p>1）map任务处理——&gt;切片</p><ul><li>读取输入文件内容，解析成key、value对，输入文件的每一行，就是一个key、value对，对应调用一次map函数。</li><li>写自己的逻辑，对输入的key、value（k1,v1）处理，转换成新的key、value(k2,v2)输出。</li></ul><p>2）reduce任务处理——&gt;计算</p><ul><li>在reduce之前，有一个shuffle的过程对多个map任务的输出进行合并、排序、分组等操作。</li><li>写reduce函数自己的逻辑，对输入的key、value（k2,{v2,…}）处理，转换成新的key、value(k3,v3)输出。</li><li>把reduce的输出保存到文件中。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;MapReduce是一个基于集群的计算&lt;strong&gt;平台&lt;/strong&gt;，是一个简化分布式编程的计算&lt;strong&gt;框架&lt;/strong&gt;，是一个将分布式计算抽象为Map和Reduce两个阶段的编程&lt;strong&gt;模型&lt;/strong&gt;。&lt;em
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="https://dataquaner.github.io/tags/MapReduce/"/>
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>hadoop shell命令</title>
    <link href="https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/"/>
    <id>https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/</id>
    <published>2020-06-08T10:40:00.000Z</published>
    <updated>2020-06-08T11:13:27.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><p>FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 </p><p>所有的的FS shell命令使用URI路径作为参数。URI格式是<code>scheme://authority/path</code>。</p><p>对HDFS文件系统，scheme是hdfs，</p><p>对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。</p><p>一个HDFS文件或目录比如<code>/parent/child</code>可以表示成<code>hdfs://namenode:namenodeport/parent/child</code>，或者更简单的<code>/parent/child</code>（假设你配置文件中的默认值是<code>namenode:namenodeport</code>）。</p><p>大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。</p><h2 id="1-hadoop-fs-命令列表"><a href="#1-hadoop-fs-命令列表" class="headerlink" title="1. hadoop fs 命令列表"></a>1. hadoop fs 命令列表</h2><p><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#FS+Shell" target="_blank" rel="noopener">FS Shell</a></p><ul><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#cat" target="_blank" rel="noopener">cat</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chgrp" target="_blank" rel="noopener">chgrp</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chmod" target="_blank" rel="noopener">chmod</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chown" target="_blank" rel="noopener">chown</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#copyFromLocal" target="_blank" rel="noopener">copyFromLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#copyToLocal" target="_blank" rel="noopener">copyToLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#cp" target="_blank" rel="noopener">cp</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#du" target="_blank" rel="noopener">du</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#dus" target="_blank" rel="noopener">dus</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#expunge" target="_blank" rel="noopener">expunge</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#get" target="_blank" rel="noopener">get</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#getmerge" target="_blank" rel="noopener">getmerge</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#ls" target="_blank" rel="noopener">ls</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#lsr" target="_blank" rel="noopener">lsr</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#mkdir" target="_blank" rel="noopener">mkdir</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#movefromLocal" target="_blank" rel="noopener">movefromLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#mv" target="_blank" rel="noopener">mv</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#put" target="_blank" rel="noopener">put</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#rm" target="_blank" rel="noopener">rm</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#rmr" target="_blank" rel="noopener">rmr</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#setrep" target="_blank" rel="noopener">setrep</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#stat" target="_blank" rel="noopener">stat</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#tail" target="_blank" rel="noopener">tail</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test" target="_blank" rel="noopener">test</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#text" target="_blank" rel="noopener">text</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#touchz" target="_blank" rel="noopener">touchz</a></li></ul><p>特别说明： </p><blockquote><ul><li><code>hadoop fs</code>：通用的文件系统命令，针对任何系统，比如本地文件、HDFS文件、HFTP文件、S3文件系统等。</li><li><code>hadoop dfs</code>：特定针对HDFS的文件系统的相关操作，但是已经不推荐使用。</li><li><code>hdfs dfs</code>：与hadoop dfs类似，同样是针对HDFS文件系统的操作，替代hadoop dfs。</li></ul></blockquote><h2 id="2-各命令使用说明"><a href="#2-各命令使用说明" class="headerlink" title="2. 各命令使用说明"></a>2. 各命令使用说明</h2><ul><li><p><code>cat</code><br>使用方法：<code>hadoop fs -cat URI [URI …]</code></p><p>​    将路径指定文件的内容输出到<code>stdout</code>。</p><p>示例：    </p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2hadoop fs -cat file:///file3 /user/hadoop/file4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>返回值：</p><pre><code>  成功返回0，失败返回-1。</code></pre></li><li><p><code>chgrp</code><br>使用方法：<code>hadoop fs -chgrp [-R] GROUP URI [URI …]</code> </p><p>​        Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. </p><p>​        改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>chmod</code><br>使用方法：<code>hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …]</code></p><p>​      改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>chown</code><br>使用方法：<code>hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</code></p><p>​       改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>copyFromLocal</code><br>使用方法：<code>hadoop fs -copyFromLocal URI</code></p><p>​       除了限定源路径是一个本地文件外，和<code>put</code>命令相似。</p></li><li><p><code>copyToLocal</code><br>使用方法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI</code></p><p>​       除了限定目标路径是一个本地文件外，和get命令类似。</p></li><li><p><code>cp</code><br>使用方法：<code>hadoop fs -cp URI [URI …]</code></p><p>​     将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​     成功返回0，失败返回-1。</p><ul><li><p><code>du</code><br>使用方法：<code>hadoop fs -du URI [URI …]</code></p><p>​      显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>示例：    </p><pre class="line-numbers language-SJELL"><code class="language-SJELL">hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回值：</p><pre><code> 成功返回0，失败返回-1。</code></pre></li><li><p><code>dus</code><br>使用方法：<code>hadoop fs -dus</code></p><p>​    显示文件的大小。</p></li><li><p><code>expunge</code><br>使用方法：<code>hadoop fs -expunge</code></p></li><li><p>清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。</p></li><li><p><code>get</code><br>使用方法：<code>hadoop fs -get [-ignorecrc] [-crc]</code></p></li><li><p>​      复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。</p></li><li><p>示例：</p><pre class="line-numbers language-SHELL"><code class="language-SHELL">hadoop fs -get /user/hadoop/file localfilehadoop fs -get hdfs://host:port/user/hadoop/file localfile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​     成功返回0，失败返回-1。</p><ul><li><p>getmerge<br>使用方法：<code>hadoop fs -getmerge [addnl]</code></p></li><li><p>接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。</p></li><li><p>ls<br>使用方法：<code>hadoop fs -ls</code></p><p>​      如果是文件，则按照如下格式返回文件信息：<br>​          文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>​      如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>​         目录名</p><p>​         修改日期 修改时间 权限 用户ID 组ID<br>示例：       </p><pre class="line-numbers language-SHELL"><code class="language-SHELL">hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回值：</p><pre><code>    成功返回0，失败返回-1。</code></pre></li><li><p>mkdir<br>使用方法：hadoop fs -mkdir</p><p>​    接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。</p><p>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​    成功返回0，失败返回-1。</p><ul><li><p><code>movefromLocal</code><br>使用方法：<code>dfs -moveFromLocal</code></p><p> 输出一个”not implemented“信息。</p></li><li><p>mv<br>使用方法：<code>hadoop fs -mv URI [URI …]</code></p><p>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​    成功返回0，失败返回-1。</p><ul><li><p><code>put</code><br>使用方法：hadoop fs -put …</p><p>​      从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -put localfile /user/hadoop/hadoopfilehadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdirhadoop fs -put localfile hdfs://host:port/hadoop/hadoopfilehadoop fs -put - hdfs://host:port/hadoop/hadoopfile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>从标准输入中读取输入。<br>返回值：</p><p>​    成功返回0，失败返回-1。</p></li><li><p><code>rm</code><br>使用方法：<code>hadoop fs -rm URI [URI …]</code></p><p>​    删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  成功返回0，失败返回-1。</p><ul><li><p><code>rmr</code><br>使用方法：<code>hadoop fs -rmr URI [URI …]</code></p></li><li><p>delete的递归版本。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -rmr /user/hadoop/dirhadoop fs -rmr hdfs://host:port/user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  成功返回0，失败返回-1。</p><ul><li><p>setrep<br>使用方法：<code>hadoop fs -setrep [-R]</code></p><p>   改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。</p></li><li><p>示例：</p><p><code>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</code><br>返回值：</p><p>   成功返回0，失败返回-1。</p></li><li><p>stat<br>使用方法：hadoop fs -stat URI [URI …]</p><p>   返回指定路径的统计信息。</p></li><li><p>示例：</p></li><li><p><code>hadoop fs -stat path</code><br>返回值：<br>  成功返回0，失败返回-1。</p></li><li><p>tail<br>使用方法：<code>hadoop fs -tail [-f] URI</code></p><p>  将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。</p></li><li><p>示例：</p><p><code>hadoop fs -tail pathname</code><br>返回值：<br>   成功返回0，失败返回-1。</p></li><li><p>test<br>使用方法：hadoop fs -test -[ezd] URI</p><p>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。</p></li><li><p>示例：</p><p><code>hadoop fs -test -e filename</code><br>text<br>使用方法：hadoop fs -text</p><p>   将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</p></li><li><p>touchz<br>使用方法：<code>hadoop fs -touchz URI [URI …]</code></p><p>   创建一个0字节的空文件。</p></li><li><p>示例：</p><p><code>hadoop -touchz pathname</code><br>返回值：<br>成功返回0，失败返回-1。</p></li></ul><p>更多详细信息访问：<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;p&gt;FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 &lt;/p&gt;
&lt;p&gt;所有的的FS
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/tags/Hadoop/"/>
    
      <category term="Shell" scheme="https://dataquaner.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>【Hive日常问题】导入数据成功，查询显示NULL</title>
    <link href="https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/"/>
    <id>https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/</id>
    <published>2020-05-06T13:40:00.000Z</published>
    <updated>2020-05-06T11:17:26.029Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><strong>问题描述</strong></h2><p>hive导入数据成功，但是查询结果为NULL：</p><pre class="line-numbers language-powershell"><code class="language-powershell">load <span class="token keyword">data</span> local inpath <span class="token string">'/user/hive/student.txt'</span> into table hive_test<span class="token punctuation">.</span>students<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-powershell"><code class="language-powershell">Loading <span class="token keyword">data</span> to table hive_test<span class="token punctuation">.</span>studentsOK<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> hive_test<span class="token punctuation">.</span>students<span class="token punctuation">;</span>OK<span class="token boolean">NULL</span>    <span class="token boolean">NULL</span><span class="token boolean">NULL</span>    <span class="token boolean">NULL</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格：</p><p>OK<br>NULL    NULL<br>NULL    NULL<br>查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格：</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> students<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> students<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span> <span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">' '</span> <span class="token keyword">LINES</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">'\n'</span> STORED <span class="token keyword">AS</span> TEXTFILE<span class="token punctuation">;</span>OK<span class="token number">1</span>    sun<span class="token number">2</span>    lin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;hive导入数据成功，但是查询结果为NULL：&lt;/p&gt;
&lt;pre class=&quot;line-
      
    
    </summary>
    
    
      <category term="Data Question" scheme="https://dataquaner.github.io/categories/Data-Question/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Python高级特性之切片</title>
    <link href="https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/"/>
    <id>https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/</id>
    <published>2020-04-25T14:14:00.000Z</published>
    <updated>2020-04-25T10:10:47.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Python可切片对象的索引方式"><a href="#1-Python可切片对象的索引方式" class="headerlink" title="1. Python可切片对象的索引方式"></a>1. Python可切片对象的索引方式</h2><p>​       包括：正索引和负索引两部分，如下图所示，以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例：</p><p><img src="https:////upload-images.jianshu.io/upload_images/14029140-3da45bbfe1029df4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/464/format/webp" alt="img"></p><h2 id="2-Python切片操作的一般方式"><a href="#2-Python切片操作的一般方式" class="headerlink" title="2. Python切片操作的一般方式"></a>2. Python切片操作的一般方式</h2><p>​       一个完整的切片表达式包含两个“:”，用于分隔三个参数(start_index、end_index、step)。</p><ul><li>当只有一个“:”时，默认第三个参数step=1；</li><li>当一个“:”也没有时，start_index=end_index，表示切取start_index指定的那个元素。</li></ul><p>​       切片操作基本表达式：object[start_index:end_index:step]</p><ul><li>step：正负数均可，其绝对值大小决定了切取数据时的‘‘步长”，而正负号决定了“切取方向”，正表示“从左往右”取值，负表示“从右往左”取值。当step省略时，默认为1，即从左往右以步长1取值。“切取方向非常重要！”“切取方向非常重要！”“切取方向非常重要！”，重要的事情说三遍！</li><li>start_index：表示起始索引（包含该索引对应值）；该参数省略时，表示从对象“端点”开始取值，至于是从“起点”还是从“终点”开始，则由step参数的正负决定，step为正从“起点”开始，为负从“终点”开始。</li><li>end_index：表示终止索引（不包含该索引对应值）；该参数省略时，表示一直取到数据“端点”，至于是到“起点”还是到“终点”，同样由step参数的正负决定，step为正时直到“终点”，为负时直到“起点”。</li></ul><h2 id="3-Python切片操作详细例子"><a href="#3-Python切片操作详细例子" class="headerlink" title="3. Python切片操作详细例子"></a>3. Python切片操作详细例子</h2><p>​      以下示例均以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="1-切取单个元素"><a href="#1-切取单个元素" class="headerlink" title="1. 切取单个元素"></a>1. 切取单个元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">0</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">6</span>当索引只有一个数时，表示切取某一个元素。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-切取完整对象"><a href="#2-切取完整对象" class="headerlink" title="2. 切取完整对象"></a>2. 切取完整对象</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#从左往右</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#从左往右</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#从右往左</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-start-index和end-index全为正（-）索引的情况"><a href="#3-start-index和end-index全为正（-）索引的情况" class="headerlink" title="3. start_index和end_index全为正（+）索引的情况"></a>3. start_index和end_index全为正（+）索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，start_index<span class="token operator">=</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token number">6</span>同样表示从左往右取值。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>输出为空列表，说明没取到数据。step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，决定了从右往左取值，而start_index<span class="token operator">=</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token number">6</span>决定了从左往右取值，两者矛盾，所以为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>同样输出为空列表。step<span class="token operator">=</span><span class="token number">1</span>，决定了从左往右取值，而start_index<span class="token operator">=</span><span class="token number">6</span>到end_index<span class="token operator">=</span><span class="token number">2</span>决定了从右往左取值，两者矛盾，所以为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值<span class="token number">0</span>开始一直取到end_index<span class="token operator">=</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值<span class="token number">9</span>开始一直取到end_index<span class="token operator">=</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从start_index<span class="token operator">=</span><span class="token number">6</span>开始，一直取到“终点”值<span class="token number">9</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从start_index<span class="token operator">=</span><span class="token number">6</span>开始，一直取到“起点”<span class="token number">0</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-start-index和end-index全为负（-）索引的情况"><a href="#4-start-index和end-index全为负（-）索引的情况" class="headerlink" title="4. start_index和end_index全为负（-）索引的情况"></a>4. start_index和end_index全为负（-）索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，而start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>决定了从右往左取值，两者矛盾，所以为空。索引<span class="token operator">-</span><span class="token number">1</span>在<span class="token operator">-</span><span class="token number">6</span>的右边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>同样是从右往左取值。索引<span class="token operator">-</span><span class="token number">1</span>在<span class="token number">6</span>的右边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，而start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>同样是从左往右取值。索引<span class="token operator">-</span><span class="token number">6</span>在<span class="token operator">-</span><span class="token number">1</span>的左边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从“起点”开始一直取到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从“终点”开始一直取到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>开始，一直取到“终点”。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>开始，一直取到“起点”。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="5-start-index和end-index正（-）负（-）混合索引的情况"><a href="#5-start-index和end-index正（-）负（-）混合索引的情况" class="headerlink" title="5. start_index和end_index正（+）负（-）混合索引的情况"></a>5. start_index和end_index正（+）负（-）混合索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>的左边，因此从左往右取值，而step<span class="token operator">=</span><span class="token number">1</span>同样决定了从左往右取值，因此结果正确<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>的左边，因此从左往右取值，但step<span class="token operator">=</span><span class="token operator">-</span>则决定了从右往左取值，两者矛盾，因此为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token number">6</span>的右边，因此从右往左取值，但step<span class="token operator">=</span><span class="token number">1</span>则决定了从左往右取值，两者矛盾，因此为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token number">6</span>的右边，因此从右往左取值，而step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>同样决定了从右往左取值，因此结果正确。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="6-多层切片操作"><a href="#6-多层切片操作" class="headerlink" title="6. 多层切片操作"></a>6. 多层切片操作</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>相当于：a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>理论上可无限次多层切片操作，只要上一次返回的是非空可切片对象即可。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="7-切片操作的三个参数可以用表达式"><a href="#7-切片操作的三个参数可以用表达式" class="headerlink" title="7. 切片操作的三个参数可以用表达式"></a>7. 切片操作的三个参数可以用表达式</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>即：a<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="8-其他对象的切片操作"><a href="#8-其他对象的切片操作" class="headerlink" title="8. 其他对象的切片操作"></a>8. 其他对象的切片操作</h3><p>​       前面的切片操作以list对象为例进行说明，但实际上可进行切片操作的数据类型还有很多，包括元组、字符串等等。</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>元组的切片操作<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token string">'ABCDEFG'</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token string">'ACEG'</span>字符串的切片操作<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token function">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token function">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">87</span><span class="token number">90</span><span class="token number">93</span><span class="token number">96</span><span class="token number">99</span>就是利用<span class="token function">range</span><span class="token punctuation">(</span><span class="token punctuation">)</span>函数生成<span class="token number">1</span><span class="token operator">-</span><span class="token number">99</span>的整数，然后从start_index<span class="token operator">=</span><span class="token number">2</span>（即<span class="token number">3</span>）开始以step<span class="token operator">=</span><span class="token number">3</span>取值，直到终点，再在新序列中取最后五个数。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-常用切片操作"><a href="#4-常用切片操作" class="headerlink" title="4. 常用切片操作"></a>4. 常用切片操作</h2><h3 id="1-取偶数位置"><a href="#1-取偶数位置" class="headerlink" title="1.取偶数位置"></a>1.取偶数位置</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-取奇数位置"><a href="#2-取奇数位置" class="headerlink" title="2.取奇数位置"></a>2.取奇数位置</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-拷贝整个对象"><a href="#3-拷贝整个对象" class="headerlink" title="3.拷贝整个对象"></a>3.拷贝整个对象</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#41946376</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#41921864</span>或<span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">.</span><span class="token function">copy</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#39783752</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#39759176</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>需要注意的是：<strong>[:]和.copy()都属于“浅拷贝”，只拷贝最外层元素，内层嵌套元素则通过引用方式共享，而非独立分配内存</strong>，如果需要彻底拷贝则需采用“深拷贝”方式，如下例所示：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'a={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">9</span> <span class="token comment" spellcheck="true">#修改b的最外层元素，将1变成9</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'D'</span> <span class="token comment" spellcheck="true">#修改b的内嵌层元素</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'a={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'b={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'id(a)={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'id(b)={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#原始a</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#b修改内部元素A为D后，a中的A也变成了D，说明共享内部嵌套元素，但外部元素1没变。</span>b<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#修改后的b</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">38669128</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">38669192</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-修改单个元素"><a href="#4-修改单个元素" class="headerlink" title="4.修改单个元素"></a>4.修改单个元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="5-在某个位置插入元素"><a href="#5-在某个位置插入元素" class="headerlink" title="5.在某个位置插入元素"></a>5.在某个位置插入元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">,</span><span class="token string">'C'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-替换一部分元素"><a href="#6-替换一部分元素" class="headerlink" title="6.替换一部分元素"></a>6.替换一部分元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul><li>start_index、end_index、step三者可同为正、同为负，或正负混合。但必须遵循一个原则，即：当start_index表示的实际位置在end_index的左边时，从左往右取值，此时step必须是正数（同样表示从左往右）；当start_index表示的实际位置在end_index的右边时，表示从右往左取值，此时step必须是负数（同样表示从右往左），即两者的取值顺序必须相同。</li><li>当start_index或end_index省略时，取值的起始索引和终止索引由step的正负来决定，这种情况不会有取值方向矛盾（即不会返回空列表[]），但正和负取到的结果顺序是相反的，因为一个向左一个向右。</li><li>step的正负是必须要考虑的，尤其是当step省略时。比如a[-1:]，很容易就误认为是从“终点”开始一直取到“起点”，即a[-1:]= [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]，但实际上a[-1:]=[9]（注意不是9），原因在于step省略时step=1表示从左往右取值，而起始索引start_index=-1本身就是对象的最右边元素了，再往右已经没数据了，因此结果只含有9一个元素。</li><li>需要注意：“取单个元素（不带“:”）”时，返回的是对象的某个元素，其类型由元素本身的类型决定，而与母对象无关，如上面的a[0]=0、a[-4]=6，元素0和6都是“数值型”，而母对象a却是“list”型；“取连续切片（带“:”）”时，返回结果的类型与母对象相同，哪怕切取的连续切片只包含一个元素，如上面的a[-1:]=[9]，返回的是一个只包含元素“9”的list，而非数值型“9”。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Python可切片对象的索引方式&quot;&gt;&lt;a href=&quot;#1-Python可切片对象的索引方式&quot; class=&quot;headerlink&quot; title=&quot;1. Python可切片对象的索引方式&quot;&gt;&lt;/a&gt;1. Python可切片对象的索引方式&lt;/h2&gt;&lt;p&gt;​   
      
    
    </summary>
    
    
      <category term="Python" scheme="https://dataquaner.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://dataquaner.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode数组系列之#88：合并两个有序数组</title>
    <link href="https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/"/>
    <id>https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/</id>
    <published>2020-04-25T08:16:16.000Z</published>
    <updated>2020-04-25T11:02:08.816Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目：合并两个有序数组"><a href="#题目：合并两个有序数组" class="headerlink" title="题目：合并两个有序数组"></a>题目：合并两个有序数组</h3><h3 id="难度：Easy"><a href="#难度：Easy" class="headerlink" title="难度：Easy"></a>难度：Easy</h3><h3 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h3><blockquote><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>​        给你两个有序整数数组 nums1 和 nums2，请你将 nums2 合并到 nums1 中，使 nums1 成为一个有序数组。</p><h4 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h4><ul><li>初始化 nums1 和 nums2 的元素数量分别为 m 和 n 。</li><li>你可以假设 nums1 有足够的空间（空间大小大于或等于 m + n）来保存 nums2 中的元素。</li></ul><h4 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h4><p>​    输入:<br>​        nums1 = [1,2,3,0,0,0], m = 3<br>​        nums2 = [2,5,6],           n = 3</p><p>​     输出: </p><p>​        [1,2,2,3,5,6]</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/merge-sorted-array" target="_blank" rel="noopener">https://leetcode-cn.com/problems/merge-sorted-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="方法一：合并后排序"><a href="#方法一：合并后排序" class="headerlink" title="方法一：合并后排序"></a>方法一：合并后排序</h4><h5 id="直觉"><a href="#直觉" class="headerlink" title="直觉"></a>直觉</h5><p>​      最朴素的解法就是将两个数组合并之后再排序。</p><p>​      该算法只需要一行(Java是2行)，时间复杂度较差，为O((n + m)log(n + m))。这是由于这种方法没有利用两个数组本身已经有序这一点。</p><h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><h6 id="Python版"><a href="#Python版" class="headerlink" title="Python版"></a>Python版</h6><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> m<span class="token punctuation">:</span> int<span class="token punctuation">,</span> nums2<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Do not return anything, modify nums1 in-place instead.        """</span>         nums1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> sorted<span class="token punctuation">(</span>nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span> <span class="token operator">+</span> nums2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="Java版"><a href="#Java版" class="headerlink" title="Java版"></a>Java版</h5><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums1<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums2<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>nums2<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>    Arrays<span class="token punctuation">.</span><span class="token function">sort</span><span class="token punctuation">(</span>nums1<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="Scala版"><a href="#Scala版" class="headerlink" title="Scala版"></a>Scala版</h6><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">object</span> Solution <span class="token punctuation">{</span>    <span class="token keyword">def</span> merge<span class="token punctuation">(</span>nums1<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> m<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">,</span> nums2<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h5><ul><li>时间复杂度 : <em>O</em>((<em>n</em>+<em>m</em>)log(<em>n</em>+<em>m</em>))</li><li>空间复杂度 : O(1)</li></ul><h4 id="方法二：双指针-从前往后"><a href="#方法二：双指针-从前往后" class="headerlink" title="方法二：双指针 / 从前往后"></a>方法二：双指针 / 从前往后</h4><h5 id="直觉-1"><a href="#直觉-1" class="headerlink" title="直觉"></a>直觉</h5><p>​       一般而言，对于有序数组可以通过双指针法达到O(n+m)的时间复杂度。</p><p>最直接的算法实现是将指针p1 置为 nums1的开头， p2为 nums2的开头，在每一步将最小值放入输出数组中。</p><p>​      由于 nums1 是用于输出的数组，需要将nums1中的前m个元素放在其他地方，也就需要O(m) 的空间复杂度。</p><p><img src="https://pic.leetcode-cn.com/992f95361c37ad06deadb6f14a9970d0184fd47330365400dd1d6f7be239e0ff-image.png" alt="image.png"></p><h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><h5 id="Python版-1"><a href="#Python版-1" class="headerlink" title="Python版"></a>Python版</h5><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> nums2<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums1: List[int]        :type m: int        :type nums2: List[int]        :type n: int        :rtype: void Do not return anything, modify nums1 in-place instead.        """</span>        <span class="token comment" spellcheck="true"># Make a copy of nums1.</span>        nums1_copy <span class="token operator">=</span> nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span>         nums1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Two get pointers for nums1_copy and nums2.</span>        p1 <span class="token operator">=</span> <span class="token number">0</span>         p2 <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment" spellcheck="true"># Compare elements from nums1_copy and nums2</span>        <span class="token comment" spellcheck="true"># and add the smallest one into nums1.</span>        <span class="token keyword">while</span> p1 <span class="token operator">&lt;</span> m <span class="token operator">and</span> p2 <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>             <span class="token keyword">if</span> nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">:</span>                 nums1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">]</span><span class="token punctuation">)</span>                p1 <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">)</span>                p2 <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># if there are still elements to add</span>        <span class="token keyword">if</span> p1 <span class="token operator">&lt;</span> m<span class="token punctuation">:</span>             nums1<span class="token punctuation">[</span>p1 <span class="token operator">+</span> p2<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> p2 <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>            nums1<span class="token punctuation">[</span>p1 <span class="token operator">+</span> p2<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a><strong>复杂度分析</strong></h5><ul><li>时间复杂度 : O(n + m)</li><li>空间复杂度 : O(m)</li></ul><h4 id="方法三-双指针-从后往前"><a href="#方法三-双指针-从后往前" class="headerlink" title="方法三 : 双指针 / 从后往前"></a>方法三 : 双指针 / 从后往前</h4><h5 id="直觉-2"><a href="#直觉-2" class="headerlink" title="直觉"></a>直觉</h5><p>​       方法二已经取得了最优的时间复杂度O(n + m)，但需要使用额外空间。这是由于在从头改变nums1的值时，需要把nums1中的元素存放在其他位置。</p><p>​       如果我们从结尾开始改写 nums1 的值又会如何呢？这里没有信息，因此不需要额外空间。</p><p>这里的指针 p 用于追踪添加元素的位置。</p><p><img src="https://pic.leetcode-cn.com/57c1daae7dab21c175f0a3acc18e4535aecde350c5100832bd2fdb0e4279180e-image.png" alt="img"></p><p><img src="https://pic.leetcode-cn.com/bac9fc86e104b5fa65f144e0604e0f4ffe4585efac12c1942b618be1c70363ca-image.png" alt="img"></p><h5 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h5><p>Python版</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> nums2<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums1: List[int]        :type m: int        :type nums2: List[int]        :type n: int        :rtype: void Do not return anything, modify nums1 in-place instead.        """</span>        <span class="token comment" spellcheck="true"># two get pointers for nums1 and nums2</span>        p1 <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token number">1</span>        p2 <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># set pointer for nums1</span>        p <span class="token operator">=</span> m <span class="token operator">+</span> n <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># while there are still elements to compare</span>        <span class="token keyword">while</span> p1 <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">and</span> p2 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span>                p2 <span class="token operator">-=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span>  nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span>                p1 <span class="token operator">-=</span> <span class="token number">1</span>            p <span class="token operator">-=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># add missing elements from nums2</span>        nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span><span class="token punctuation">:</span>p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Java版</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums1<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums2<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// two get pointers for nums1 and nums2</span>    <span class="token keyword">int</span> p1 <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> p2 <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// set pointer for nums1</span>    <span class="token keyword">int</span> p <span class="token operator">=</span> m <span class="token operator">+</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// while there are still elements to compare</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>p1 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>p2 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">// compare two elements from nums1 and nums2 </span>      <span class="token comment" spellcheck="true">// and add the largest one in nums1 </span>      nums1<span class="token punctuation">[</span>p<span class="token operator">--</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">?</span> nums2<span class="token punctuation">[</span>p2<span class="token operator">--</span><span class="token punctuation">]</span> <span class="token operator">:</span> nums1<span class="token punctuation">[</span>p1<span class="token operator">--</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// add missing elements from nums2</span>    System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>nums2<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h5><ul><li>时间复杂度 : O(n + m)</li><li>空间复杂度 : O(1)</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;题目：合并两个有序数组&quot;&gt;&lt;a href=&quot;#题目：合并两个有序数组&quot; class=&quot;headerlink&quot; title=&quot;题目：合并两个有序数组&quot;&gt;&lt;/a&gt;题目：合并两个有序数组&lt;/h3&gt;&lt;h3 id=&quot;难度：Easy&quot;&gt;&lt;a href=&quot;#难度：Easy&quot; c
      
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://dataquaner.github.io/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://dataquaner.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>数据倾斜问题总结</title>
    <link href="https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/"/>
    <id>https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/</id>
    <published>2020-04-22T14:14:00.000Z</published>
    <updated>2020-04-22T14:35:55.073Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-什么是数据倾斜"><a href="#0-什么是数据倾斜" class="headerlink" title="0. 什么是数据倾斜"></a>0. 什么是数据倾斜</h2><blockquote><p>​        对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。</p><p>来源百度百科</p></blockquote><p>​        对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><h2 id="1-数据倾斜的现象"><a href="#1-数据倾斜的现象" class="headerlink" title="1. 数据倾斜的现象"></a>1. 数据倾斜的现象</h2><p>​       多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。</p><h2 id="2-数据倾斜的影响"><a href="#2-数据倾斜的影响" class="headerlink" title="2. 数据倾斜的影响"></a>2. 数据倾斜的影响</h2><p>1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：<strong>Out Of Memory</strong>。</p><p>2）<strong>运行速度慢</strong> ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。</p><p><img src="https://pic1.zhimg.com/80/v2-b26e15f4b1c3ce2f78fba64397b6fd60_1440w.jpg" alt="img"></p><p><strong><em>一个经验结论是：一般情况下，OOM的原因都是数据倾斜\</em></strong></p><h2 id="3-如何定位数据倾斜"><a href="#3-如何定位数据倾斜" class="headerlink" title="3. 如何定位数据倾斜"></a>3. 如何定位数据倾斜</h2><p>​         数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p><p><strong>原因</strong>： 查看任务-》查看Stage-》查看代码</p><p>​        某个task执行特别慢的情况</p><p>​        某个task莫名其妙内存溢出的情况</p><p>​        查看导致数据倾斜的key的数据分布情况</p><p><img src="https://pic1.zhimg.com/80/v2-b1b26a9b5e6a1d68d9aea4d1f2bc551c_1440w.jpg" alt="img"></p><p>也可从以下几种情况考虑：</p><p>1、是不是有OOM情况出现，一般是少数内存溢出的问题</p><p>2、是不是应用运行时间差异很大，总体时间很长</p><p>3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题</p><p>4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断</p><p>5、看看代码里面是否有一些导致Shuffle的算子出现</p><h2 id="4-数据倾斜的几种典型情况（重点）"><a href="#4-数据倾斜的几种典型情况（重点）" class="headerlink" title="4. 数据倾斜的几种典型情况（重点）"></a><strong>4. 数据倾斜的几种典型情况（重点）</strong></h2><ul><li>数据源中的数据分布不均匀，Spark需要频繁交互</li><li>数据集中的不同Key由于分区方式，导致数据倾斜</li><li>JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</li><li>聚合操作中，数据集中的数据分布不均匀（主要）</li><li>JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</li><li>JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</li><li>数据集中少数几个key数据量很大，不重要，其他数据均匀</li></ul><p>注意：</p><ul><li><p>需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题</p></li><li><p>只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以</p></li></ul><h2 id="5-数据倾斜的处理方法"><a href="#5-数据倾斜的处理方法" class="headerlink" title="5. 数据倾斜的处理方法"></a>5. 数据倾斜的处理方法</h2><p>​         发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。</p><h3 id="5-1-检查数据，找出异常的key"><a href="#5-1-检查数据，找出异常的key" class="headerlink" title="5.1 检查数据，找出异常的key"></a>5.1 检查数据，找出异常的key</h3><p>​          如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。</p><p>选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"key"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​        如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况:</p><ul><li><p>null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。</p></li><li><p>无效数据，大量重复的测试数据或是对结果影响不大的有效数据。</p></li><li><p>有效数据，业务导致的正常数据分布。</p></li></ul><p><strong>解决办法</strong><br>  第1，2种情况，直接对数据进行过滤即可。</p><p>  第3种情况则需要进行一些特殊操作，常见的有以下几种做法。</p><ul><li><p>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。</p></li><li><p>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</p></li><li><p>使用reduceByKey 代替 groupByKey</p></li><li><p>使用map join。</p><p><strong>举例</strong>：<br>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下：</p><p>将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>对数据进行 reduceByKey(func)<br>将 key + 随机值 转成 key<br>再对数据进行 reduceByKey(func)<br>tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。</p><p>tips2: 单独处理异常数据时，可以配合使用Map Join解决</p></li></ul><h4 id="5-1-1-数据源中的数据分布不均匀，Spark需要频繁交互"><a href="#5-1-1-数据源中的数据分布不均匀，Spark需要频繁交互" class="headerlink" title="5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互"></a><strong>5.1.1</strong> 数据源中的数据分布不均匀，Spark需要频繁交互</h4><p><strong>解决方案</strong>1：避免数据源的数据倾斜</p><p><strong>实现原理</strong>：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive或者Kafka中还是会发生数据倾斜。</p><p><strong>适用情况</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>总结</strong>：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题</p><h4 id="5-1-2-数据集中的不同Key由于分区方式，导致数据倾斜"><a href="#5-1-2-数据集中的不同Key由于分区方式，导致数据倾斜" class="headerlink" title="5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜"></a><strong>5.1.2</strong> 数据集中的不同Key由于分区方式，导致数据倾斜</h4><p><strong>解决方案1</strong>：调整并行度</p><p><strong>实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。</p><p><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。</p><p><img src="https://pic4.zhimg.com/80/v2-9a1722a9ceb6fe125f7b36715f6dcfff_1440w.jpg" alt="img"></p><p><strong>总结</strong>：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效</p><p><strong>解决方案2</strong>：</p><p><strong>缓解数据倾斜**</strong>（自定义Partitioner）**</p><p><strong>适用场景</strong>：大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p><p><strong>解决方案</strong>： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p><p><strong>优势</strong>： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong>： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><h3 id="5-2-检查Spark运行过程相关操作"><a href="#5-2-检查Spark运行过程相关操作" class="headerlink" title="5.2 检查Spark运行过程相关操作"></a>5.2 检查Spark运行过程相关操作</h3><h4 id="5-2-1-JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）"><a href="#5-2-1-JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）" class="headerlink" title="5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）"></a>5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</h4><p><strong>解决方案</strong>：Reduce side Join转变为Map side Join</p><p><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。</p><p><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。</p><h4 id="5-2-2-聚合操作中，数据集中的数据分布不均匀（主要）"><a href="#5-2-2-聚合操作中，数据集中的数据分布不均匀（主要）" class="headerlink" title="5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）"></a>5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）</h4><p><strong>解决方案</strong>：两阶段聚合（局部聚合+全局聚合）</p><p><strong>适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案</p><p><strong>实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案</p><p>将相同key的数据分拆处理</p><p><img src="https://pic3.zhimg.com/80/v2-495a5fed7eb38db37d2f0bd13c45a30e_1440w.jpg" alt="img"></p><h4 id="5-2-3-JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀"><a href="#5-2-3-JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀" class="headerlink" title="5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀"></a><strong>5.2.3</strong> JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</h4><p><strong>解决方案</strong>：为倾斜key增加随机前/后缀</p><p><strong>适用场景</strong>：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong>：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong>：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong>：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>注意</strong>：具有倾斜Key的RDD数据集中，key的数量比较少</p><p><img src="https://pic4.zhimg.com/80/v2-248b0cead5e9fb8a7b1cec840dd61b2f_1440w.jpg" alt="img"></p><h4 id="5-2-4-JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀"><a href="#5-2-4-JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀" class="headerlink" title="5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀"></a><strong>5.2.4</strong> JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</h4><p><strong>解决方案</strong>：随机前缀和扩容RDD进行join</p><p><strong>适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。</p><p><strong>实现思路</strong>：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><p>注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀）</p><p><img src="https://pic4.zhimg.com/80/v2-fa2211e3a343d7b68e83bfe83d67f0cb_1440w.jpg" alt="img"></p><h4 id="5-2-5-数据集中少数几个key数据量很大，不重要，其他数据均匀"><a href="#5-2-5-数据集中少数几个key数据量很大，不重要，其他数据均匀" class="headerlink" title="5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀"></a><strong>5.2.5</strong> 数据集中少数几个key数据量很大，不重要，其他数据均匀</h4><p><strong>解决方案</strong>：过滤少数倾斜Key</p><p><strong>适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-什么是数据倾斜&quot;&gt;&lt;a href=&quot;#0-什么是数据倾斜&quot; class=&quot;headerlink&quot; title=&quot;0. 什么是数据倾斜&quot;&gt;&lt;/a&gt;0. 什么是数据倾斜&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;​        对于集群系统，一般缓存是分布式的，即
      
    
    </summary>
    
    
      <category term="Data Question" scheme="https://dataquaner.github.io/categories/Data-Question/"/>
    
    
      <category term="数据倾斜" scheme="https://dataquaner.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
    
  </entry>
  
  <entry>
    <title>1.数据开发工程师面试题目必知必会</title>
    <link href="https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/"/>
    <id>https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/</id>
    <published>2020-04-21T10:59:38.680Z</published>
    <updated>2020-04-21T10:59:38.680Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面试题目梳理"><a href="#面试题目梳理" class="headerlink" title="面试题目梳理"></a>面试题目梳理</h2><h3 id="一-数据结构和算法-LeetCode"><a href="#一-数据结构和算法-LeetCode" class="headerlink" title="一. 数据结构和算法 LeetCode"></a>一. 数据结构和算法 <a href="https://leetcode-cn.com/problemset/all/" target="_blank" rel="noopener">LeetCode</a></h3><ol><li>合并数组</li><li>二元查找树转双向链表</li><li>二叉树层次遍历</li><li>堆 最小堆</li><li>排序算法</li><li>动态规划</li><li>青蛙跳台阶</li><li>贪心算法</li><li>字符串转换成整数</li><li>链表中倒数第K个结点</li><li><ol start="11"><li>二维数组中的查找</li></ol></li><li><ol start="12"><li>替换空格</li></ol></li><li><ol start="13"><li>从尾到头打印链表</li></ol></li><li><ol start="14"><li>重建二叉树</li></ol></li><li>用两个栈实现队列</li><li>斐波那契数列及变形题</li><li>二进制中1的个数</li><li>在O(1)时间删除链表结点</li><li>调整数组顺序使奇数位于偶数前面</li><li>反转链表</li><li>合并两个排序的链表</li><li>树的子结构</li><li>二叉树的镜像</li><li>顺时针打印矩阵</li><li>栈的压入、弹出序列</li><li>二叉搜索树的后序遍历序列</li><li>二叉树中和为某一值的路径</li><li>数组中出现次数超过一半的数字</li><li>最小的k个数</li><li>连续子数组的最大和</li><li>第一个只出现一次的字符</li><li>两个链表的第一个公共结点</li><li>链表中环的入口结点</li><li>二叉树的镜像</li><li>跳台阶</li><li>变态跳台阶</li><li>矩形覆盖</li><li>从上往下打印二叉树</li><li>二叉搜索树的第K个结点</li></ol><h3 id="二-计算平台"><a href="#二-计算平台" class="headerlink" title="二. 计算平台"></a>二. 计算平台</h3><h4 id="1-Hadoop"><a href="#1-Hadoop" class="headerlink" title="1. Hadoop"></a>1. Hadoop</h4><ol><li><p>数据倾斜问题</p></li><li><p>hive开窗函数</p></li><li><p>hive UDF UDAF<br>Mapreduce原理</p></li><li><p>MR的Shuffle过程</p></li><li><p>Yarn的工作机制，以及MR Job提交运行过程</p></li><li><p>MapReduce1的工作机制和过程</p></li><li><p>HDFS写入过程</p></li><li><p>Fsimage 与 EditLog定义及合并过程</p></li><li><p>HDFS读过程</p></li><li><p>HDFS简介</p></li><li><p>在向HDFS中写数据的时候，当写某一副本时出错怎么处理？</p></li><li><p>namenode的HA实现</p></li><li><p>简述联邦HDFS</p></li><li><p>HDFS源码解读–create()</p></li><li><p>NameNode高可用中editlog同步的过程</p></li><li><p>HDFS写入过程客户端奔溃怎么处理？（租约恢复）</p></li></ol><h4 id="2-Hive"><a href="#2-Hive" class="headerlink" title="2. Hive"></a>2. Hive</h4><ol><li>Hive内部表与外部表的区别</li><li>Hive与传统数据库的区别</li><li>Hiverc文件</li><li>Hive分区</li><li>Hive分区过多有何坏处以及分区时的注意事项</li><li>Hive中复杂数据类型的使用好处与坏处</li><li>hive分桶？</li><li>Hive元数据库是用来做什么的，存储哪些信息？</li><li>为何不使用Derby作为元数据库？</li><li>Hive什么情况下可以避免进行mapreduce？</li><li>Hive连接？</li><li>Hive MapJoin?</li><li>Hive的sort by, order by, distribute by, cluster by区别？</li><li>Hadoop计算框架特性</li><li>Hive优化常用手段</li><li>数据倾斜整理(转)</li><li>使用Hive如何进行抽样查询？</li></ol><h4 id="3-Spark"><a href="#3-Spark" class="headerlink" title="3. Spark"></a>3. Spark</h4><ol><li>Spark的运行模式</li><li>RDD是如何容错的？</li><li>Spark和MapReduce的区别</li><li>说一下Spark的RDD</li><li>自己实现一个RDD，需要实现哪些函数或者部分？</li><li>MapReduce和Spark的区别</li><li>Spark的Stage是怎么划分的？如何优化？</li><li>宽依赖与窄依赖区别</li><li>Spark性能调优</li><li>Flink、Storm与Spark Stream的区别（未）</li><li>说下spark中的transform和action</li><li>RDD、DataFrame和DataSet的区别</li><li>Spark执行任务流程（standalone、yarn）</li><li>Spark的数据容错机制</li><li>Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</li><li>Spark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？以及要注意的地方</li><li>driver的功能是什么？</li><li>spark端口</li><li>RDD有哪几种创建方式</li><li>map和flatmap的区别</li><li>Spark的基本工作流程</li></ol><h4 id="4-Flink"><a href="#4-Flink" class="headerlink" title="4. Flink"></a>4. Flink</h4><h5 id="4-1-Flink核心概念和基础"><a href="#4-1-Flink核心概念和基础" class="headerlink" title="4.1 Flink核心概念和基础"></a>4.1 Flink核心概念和基础</h5><blockquote><p>第一部分：Flink 中的核心概念和基础篇，包含了 Flink 的整体介绍、核心概念、算子等考察点。</p></blockquote><p>第二部分：Flink 进阶篇，包含了 Flink 中的数据传输、容错机制、序列化、数据热点、反压等实际生产环境中遇到的问题等考察点。</p><p>第三部分：Flink 源码篇，包含了 Flink 的核心代码实现、Job 提交流程、数据交换、分布式快照机制、Flink SQL 的原理等考察点。</p><h4 id="5-Storm："><a href="#5-Storm：" class="headerlink" title="5. Storm："></a><strong>5. Storm：</strong></h4><p>Storm的可靠性如何实现？包括spout和bolt两部分</p><p>怎么提高Storm的并发度？</p><p>Storm如何处理反压机制？</p><p>Storm中的Stream grouping有哪几种方式？</p><p>Storm的组件介绍</p><p>Storm怎么完成对单词的计数？</p><p>简述Strom的计算结构</p><h4 id="6-kafka："><a href="#6-kafka：" class="headerlink" title="6. kafka："></a><strong>6. kafka：</strong></h4><p>kafka介绍</p><p>Kafka与传统消息队列的区别？</p><p>kafka的零拷贝</p><p>kafka消息持久化和顺序读写？</p><h4 id="7-Kylin"><a href="#7-Kylin" class="headerlink" title="7. Kylin"></a>7. Kylin</h4><p>简介Kylin</p><p>Kylin的工作原理</p><p>Kylin的技术框架</p><p>Cube、Cuboid 和 Cube Segment</p><p>Kylin 对维度表的的要求</p><p>Cube的构建过程</p><p>全量构建和增量构建的区别</p><p>流式构建原理</p><h3 id="三-数据库"><a href="#三-数据库" class="headerlink" title="三. 数据库"></a>三. 数据库</h3><pre><code>     1）两大引擎Innodb引擎和MyIASM引擎，      2）mysql索引原理和底层实现BTREE、B+ TREE</code></pre><h3 id="四-数据仓库"><a href="#四-数据仓库" class="headerlink" title="四. 数据仓库"></a>四. 数据仓库</h3><p>​    1）拉链表<br>​    2）星型模型和雪花模型<br>​    3）维度建模过程</p><h3 id="五-操作系统"><a href="#五-操作系统" class="headerlink" title="五. 操作系统"></a>五. 操作系统</h3><p>   1）线程和进程，进程间的通信方式<br>   2）死锁<br>   3）内存分页<br>   4）同步异步阻塞</p><h3 id="六-计算机网络"><a href="#六-计算机网络" class="headerlink" title="六. 计算机网络"></a>六. 计算机网络</h3><ol><li>简述TCP和UDP的区别</li><li>七层协议每一层的任务及作用</li><li>简述http状态码</li><li>简述http协议与https协议</li><li>简述SSL协议</li><li>解析DNS过程</li><li>三次握手，四次挥手的过程？？为什么三握？</li></ol><h3 id="七-Linux"><a href="#七-Linux" class="headerlink" title="七. Linux"></a>七. Linux</h3><h4 id="1-比较常用Linux指令"><a href="#1-比较常用Linux指令" class="headerlink" title="1. 比较常用Linux指令"></a>1. 比较常用Linux指令</h4><p>　　1.1、ls/ll、cd、mkdir、rm-rf、cp、mv、ps -ef | grep xxx、kill、free-m、tar -xvf file.tar、（说那么十几二十来个估计差不多了）</p><h4 id="2-进程相关"><a href="#2-进程相关" class="headerlink" title="2. 进程相关"></a>2. 进程相关</h4><h5 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h5><p>　　2.1、ps -ef | grep xxx</p><p>　　2.2、ps -aux | grep xxx（-aux显示所有状态）</p><h5 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h5><p>　　3.1、kill -9[PID]  —(PID用查看进程的方式查找)</p><p>4、启动/停止服务</p><p>　　4.1、cd到bin目录cd/</p><p>　　4.2、./startup.sh  –打开（先确保有足够的权限）</p><p>　　4.3、./shutdown.sh —关闭</p><p>5、查看日志</p><p>　　5.1、cd到服务器的logs目录（里面有xx.out文件）</p><p>　　5.2、tail -f xx.out –此时屏幕上实时更新日志。ctr+c停止</p><p>　　5.3、查看最后100行日志 tail -100 xx.out </p><p>　　5.4、查看关键字附件的日志。如：cat filename | grep -C 5 ‘关键字’（关键字前后五行。B表示前，A表示后，C表示前后） —-使用不多**<br>**</p><p>　　5.5、还有vi查询啥的。用的也不多。</p><p>6、查看端口：（如查看某个端口是否被占用）</p><p>　　6.1、netstat -anp | grep 端口号（状态为LISTEN表示被占用）</p><p>7、查找文件</p><p>　　7.1、查找大小超过xx的文件： find . -type f -size +xxk —–(find . -type f -mtime -1 -size +100k -size-400k)–查区间大小的文件</p><p>　　7.2、通过文件名：find / -name xxxx  —整个硬盘查找</p><p>　　其余的基本上不常用</p><p>8、vim（vi）编辑器　　</p><p>　　有命令模式、输入模式、末行模式三种模式。<br>　　命令模式：查找内容(/abc、跳转到指定行(20gg)、跳转到尾行(G)、跳转到首行(gg)、删除行(dd)、插入行(o)、复制粘贴(yy,p)<br>　　输入模式：编辑文件内容<br>　　末行模式：保存退出(wq)、强制退出(q!)、显示文件行号(set number)<br>　　在命令模式下，输入a或i即可切换到输入模式，输入冒号(:)即可切换到末行模式；在输入模式和末行模式下，按esc键切换到命令模式</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;面试题目梳理&quot;&gt;&lt;a href=&quot;#面试题目梳理&quot; class=&quot;headerlink&quot; title=&quot;面试题目梳理&quot;&gt;&lt;/a&gt;面试题目梳理&lt;/h2&gt;&lt;h3 id=&quot;一-数据结构和算法-LeetCode&quot;&gt;&lt;a href=&quot;#一-数据结构和算法-LeetCode&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>IDEA环境下Git使用总结</title>
    <link href="https://dataquaner.github.io/2020/04/20/idea-xia-shi-yong-git-cao-zuo/"/>
    <id>https://dataquaner.github.io/2020/04/20/idea-xia-shi-yong-git-cao-zuo/</id>
    <published>2020-04-20T13:40:00.000Z</published>
    <updated>2020-04-20T04:13:12.538Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>工作中多人使用版本控制软件协作开发，常见的应用场景归纳如下：</p><p>假设小组中有两个人，组长小张，组员小袁</p><p>[TOC]</p><h2 id="场景一：小张创建项目并提交到远程Git仓库"><a href="#场景一：小张创建项目并提交到远程Git仓库" class="headerlink" title="场景一：小张创建项目并提交到远程Git仓库"></a>场景一：小张创建项目并提交到远程Git仓库</h2><p>创建好项目，选择VCS - &gt; Import into Version Control -&gt; Create Git Repository</p><p><img src="https://img-blog.csdn.net/20160912161234797" alt="img"></p><p>接下来指定本地仓库的位置，按个人习惯指定即可，例如这里选择了项目源代码同目录</p><p><img src="https://img-blog.csdn.net/20160912161334752" alt="img"></p><p>点击OK后创建完成本地仓库，注意，这里仅仅是本地的。下面把项目源码添加到本地仓库。</p><p>下图是Git与提交有关的三个命令对应的操作，Add命令是把文件从IDE的工作目录添加到本地仓库的stage区，Commit命令把stage区的暂存文件提交到当前分支的仓库，并清空stage区。Push命令把本地仓库的提交同步到远程仓库。</p><p><img src="https://img-blog.csdn.net/20160912164147415" alt="img"></p><p>IDEA中对操作做了一定的简化，Commit和Push可以在一步中完成。</p><p>具体操作，在项目上点击右键，选择Git菜单</p><p><img src="https://img-blog.csdn.net/20160912165901032" alt="img"></p><p><img src="https://img-blog.csdn.net/20160912165911954" alt="img"></p><p><img src="https://img-blog.csdn.net/20160912165921938" alt="img"></p><p>因为是第一次提交，Push前需要指定远程仓库的地址。如下图，点击Define remote后，在弹出的窗口中输入远程仓库地址。</p><p><img src="https://img-blog.csdn.net/20160912165942829" alt="img"></p><h2 id="场景二：小袁从远程Git仓库上获取项目源码"><a href="#场景二：小袁从远程Git仓库上获取项目源码" class="headerlink" title="场景二：小袁从远程Git仓库上获取项目源码"></a>场景二：小袁从远程Git仓库上获取项目源码</h2><p>即克隆项目，操作如下：</p><p><img src="https://img-blog.csdn.net/20160912170148207" alt="img"></p><p>输入小张Push时填写的远程仓库地址</p><p><img src="https://img-blog.csdn.net/20160912170214880" alt="img"></p><p>接下来按向导操作，即可把项目从远程仓库克隆到本地仓库和IDE工作区。</p><h2 id="场景三：小袁修改了部分源码，提交到远程仓库"><a href="#场景三：小袁修改了部分源码，提交到远程仓库" class="headerlink" title="场景三：小袁修改了部分源码，提交到远程仓库"></a>场景三：小袁修改了部分源码，提交到远程仓库</h2><p>这个操作和首次提交的流程基本一致，分别是 Add -&gt; Commit -&gt; Push。请参考场景一</p><h2 id="场景四：小张从远程仓库获取小袁的提交"><a href="#场景四：小张从远程仓库获取小袁的提交" class="headerlink" title="场景四：小张从远程仓库获取小袁的提交"></a>场景四：小张从远程仓库获取小袁的提交</h2><p>获取更新有两个命令：Fetch和Pull，Fetch是从远程仓库下载文件到本地的origin/master，然后可以手动对比修改决定是否合并到本地的master库。Push则是直接下载并合并。如果各成员在工作中都执行修改前先更新的规范，则可以直接使用Pull方式以简化操作。</p><p><img src="https://img-blog.csdn.net/20160912170628933" alt="img"></p><h2 id="场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发"><a href="#场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发" class="headerlink" title="场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发"></a>场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发</h2><p>建分支也是一个常用的操作，例如临时修改bug、开发不确定是否加入的功能等，都可以创建一个分支，再等待合适的时机合并到主干。</p><p>创建流程如下：</p><p><img src="https://img-blog.csdn.net/20160912171844429" alt="img"></p><p>选择New Branch并输入一个分支的名称</p><p><img src="https://img-blog.csdn.net/20160912171858663" alt="img"></p><p>创建完成后注意IDEA的右下角，如下图，Git: wangpangzi_branch表示已经自动切换到wangpangzi_branch分支，当前工作在这个分支上。</p><p>点击后弹出一个小窗口，在Local Branches中有其他可用的本地分支选项，点击后选择Checkout即可切换当前工作的分支。</p><p><img src="https://img-blog.csdn.net/20160912173123122" alt="img"></p><p>如下图，点击Checkout</p><p><img src="https://img-blog.csdn.net/20160912173307202" alt="img"></p><p>注意，这里创建的分支仅仅在本地仓库，如果想让组长小张获取到这个分支，还需要提交到远程仓库。</p><h2 id="场景六：小袁把分支提交到远程Git仓库"><a href="#场景六：小袁把分支提交到远程Git仓库" class="headerlink" title="场景六：小袁把分支提交到远程Git仓库"></a>场景六：小袁把分支提交到远程Git仓库</h2><p>切换到新建的分支，使用Push功能</p><p><img src="https://img-blog.csdn.net/20160912173718844" alt="img"></p><p><img src="https://img-blog.csdn.net/20160912174243815" alt="img"></p><h2 id="场景七：小张获取小袁提交的分支"><a href="#场景七：小张获取小袁提交的分支" class="headerlink" title="场景七：小张获取小袁提交的分支"></a>场景七：小张获取小袁提交的分支</h2><p>使用Pull功能打开更新窗口，点击Remote栏后面的刷新按钮，会在Branches to merge栏中刷新出新的分支。这里并不想做合并，所以不要选中任何分支，直接点击Pull按钮完成操作。</p><p><img src="https://img-blog.csdn.net/20160912174329143" alt="img"></p><p>更新后，再点击右下角，可以看到在Remote Branches区已经有了新的分支，点击后在弹出的子菜单中选择Checkout as new local branch，在本地仓库中创建该分支。完成后在Local Branches区也会出现该分支的选项，可以按上面的方法，点击后选择Checkout切换。</p><p><img src="https://img-blog.csdn.net/20160912174729488" alt="img"></p><h2 id="场景八：小张把分支合并到主干"><a href="#场景八：小张把分支合并到主干" class="headerlink" title="场景八：小张把分支合并到主干"></a>场景八：小张把分支合并到主干</h2><p>新功能开发完成，体验很好，项目组决定把该功能合并到主干上。</p><p>切换到master分支，选择Merge Changes</p><p><img src="https://img-blog.csdn.net/20160912175201306" alt="img"></p><p>选择要合并的分支，点击Merge完成</p><p><img src="https://img-blog.csdn.net/20160912175359903" alt="img"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;工作中多人使用版本控制软件协作开发，常见的应用场景归纳如下：&lt;/p&gt;
&lt;p&gt;假设小组中有两个人，组长小张，组员小袁&lt;/p&gt;
&lt;p&gt;[TOC]
      
    
    </summary>
    
    
      <category term="Git" scheme="https://dataquaner.github.io/categories/Git/"/>
    
    
      <category term="IDEA" scheme="https://dataquaner.github.io/tags/IDEA/"/>
    
      <category term="Git" scheme="https://dataquaner.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Flink面试问题梳理(基础+进阶+源码)</title>
    <link href="https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/"/>
    <id>https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/</id>
    <published>2020-04-18T13:40:00.000Z</published>
    <updated>2020-04-18T14:39:18.053Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一部分：Flink-面试基础篇"><a href="#第一部分：Flink-面试基础篇" class="headerlink" title="第一部分：Flink 面试基础篇"></a><strong>第一部分：Flink 面试基础篇</strong></h2><h3 id="1-简单介绍一下-Flink"><a href="#1-简单介绍一下-Flink" class="headerlink" title="1. 简单介绍一下 Flink"></a><strong>1. 简单介绍一下 Flink</strong></h3><p>​        Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。</p><p>​        Flink提供了诸多高抽象层的API以便用户编写分布式任务：</p><ul><li><p>DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。</p></li><li><p>DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。</p></li><li><p>Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。</p></li></ul><p>​      此外，Flink 还针对特定的应用领域提供了领域库，例如：Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。</p><p>根据官网的介绍，Flink 的特性包含：</p><blockquote><p>支持高吞吐、低延迟、高性能的流处理<br>支持带有事件时间的窗口 （Window） 操作<br>支持有状态计算的 Exactly-once 语义<br>支持高度灵活的窗口 （Window） 操作，支持基于 time、count、session 以及 data-driven 的窗口操作<br>支持具有 Backpressure 功能的持续流模型<br>支持基于轻量级分布式快照（Snapshot）实现的容错<br>一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理<br>Flink 在 JVM 内部实现了自己的内存管理<br>支持迭代计算<br>支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存</p></blockquote><h3 id="2-Flink-相比传统的-Spark-Streaming-有什么区别"><a href="#2-Flink-相比传统的-Spark-Streaming-有什么区别" class="headerlink" title="2. Flink 相比传统的 Spark Streaming 有什么区别?"></a><strong>2. Flink 相比传统的 Spark Streaming 有什么区别?</strong></h3><p>​       这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来：</p><blockquote><p><strong>Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。</strong></p></blockquote><p>下面我们就分几个方面介绍两个框架的主要区别：</p><h4 id="1-架构模型"><a href="#1-架构模型" class="headerlink" title="[1] 架构模型"></a><strong>[1] 架构模型</strong></h4><p>​      Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。</p><h4 id="2-任务调度"><a href="#2-任务调度" class="headerlink" title="[2] 任务调度"></a><strong>[2] 任务调度</strong></h4><p>​       Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。</p><p>​       Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。</p><h4 id="3-时间机制"><a href="#3-时间机制" class="headerlink" title="[3] 时间机制"></a><strong>[3] 时间机制</strong></h4><p>​        Spark Streaming 支持的时间机制有限，只支持<strong>处理时间</strong>。Flink 支持了流处理程序在时间上的三个定义：<strong>处理时间、事件时间、注入时间</strong>。同时也支持 <strong>watermark</strong> 机制来处理滞后数据。</p><h4 id="4-容错机制"><a href="#4-容错机制" class="headerlink" title="[4] 容错机制"></a><strong>[4] 容错机制</strong></h4><p>​        对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。</p><p>​         Flink 则使用两阶段提交协议来解决这个问题。</p><h3 id="3-Flink-的组件栈有哪些？"><a href="#3-Flink-的组件栈有哪些？" class="headerlink" title="3. Flink 的组件栈有哪些？"></a><strong>3. Flink 的组件栈有哪些？</strong></h3><p>​        根据 Flink 官网描述，Flink 是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。</p><p><img src="https://pic1.zhimg.com/80/v2-3d7d9b6e80a843212a4d168b500af1d8_1440w.jpg" alt="img"></p><p>图片来源于：<a href="https://link.zhihu.com/?target=https%3A//flink.apache.org">https://flink.apache.org</a></p><p>​         自下而上，每一层分别代表：</p><blockquote><ul><li>Deploy 层：该层主要涉及了Flink的部署模式，在上图中我们可以看出，Flink 支持包括local、Standalone、Cluster、Cloud等多种部署模式。</li><li>Runtime 层：Runtime层提供了支持 Flink 计算的核心实现，比如：支持分布式 Stream 处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。</li><li>API层：API 层主要实现了面向流（Stream）处理和批（Batch）处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API，后续版本，Flink有计划将DataStream和DataSet API进行统一。</li><li>Libraries层：该层称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。</li></ul></blockquote><h3 id="4-Flink-的运行必须依赖-Hadoop组件吗？"><a href="#4-Flink-的运行必须依赖-Hadoop组件吗？" class="headerlink" title="4. Flink 的运行必须依赖 Hadoop组件吗？"></a><strong>4. Flink 的运行必须依赖 Hadoop组件吗？</strong></h3><p>​        Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadoop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点。</p><h3 id="5-你们的Flink集群规模多大？"><a href="#5-你们的Flink集群规模多大？" class="headerlink" title="5. 你们的Flink集群规模多大？"></a><strong>5. 你们的Flink集群规模多大？</strong></h3><p>​      大家注意，这个问题看起来是问你实际应用中的Flink集群规模，其实还隐藏着另一个问题：Flink可以支持多少节点的集群规模？</p><p>​      在回答这个问题时候，可以将自己生产环节中的集群规模、节点、内存情况说明，同时说明部署模式（一般是Flink on Yarn），除此之外，用户也可以同时在小集群（少于5个节点）和拥有 TB 级别状态的上千个节点上运行 Flink 任务。</p><h3 id="6-Flink的基础编程模型了解吗？"><a href="#6-Flink的基础编程模型了解吗？" class="headerlink" title="6. Flink的基础编程模型了解吗？"></a><strong>6. Flink的基础编程模型了解吗？</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-2e5a594aaa7dd3efdcbb2c3f9e5a8fa9_1440w.jpg" alt="img"></p><p>​        上图是来自Flink官网的运行流程图。通过上图我们可以得知，Flink 程序的基本构建是数据输入来自一个 Source，Source 代表数据的输入端，经过 Transformation 进行转换，然后在一个或者多个Sink接收器中结束。数据流（stream）就是一组永远不会停止的数据记录流，而转换（transformation）是将一个或多个流作为输入，并生成一个或多个输出流的操作。执行时，Flink程序映射到 streaming dataflows，由流（streams）和转换操作（transformation operators）组成。</p><h3 id="7-Flink集群有哪些角色？各自有什么作用？"><a href="#7-Flink集群有哪些角色？各自有什么作用？" class="headerlink" title="7. Flink集群有哪些角色？各自有什么作用？"></a><strong>7. Flink集群有哪些角色？各自有什么作用？</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-a0c80153cec85fc9cd165d862b4d489e_1440w.jpg" alt="img"></p><p>​       Flink 程序在运行时主要有 TaskManager，JobManager，Client三种角色。其中JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。</p><p>​      TaskManager是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。</p><p>​      Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。</p><h3 id="8-说说-Flink-资源管理中-Task-Slot-的概念"><a href="#8-说说-Flink-资源管理中-Task-Slot-的概念" class="headerlink" title="8. 说说 Flink 资源管理中 Task Slot 的概念"></a><strong>8. 说说 Flink 资源管理中 Task Slot 的概念</strong></h3><p><img src="https://pic4.zhimg.com/80/v2-c692077d1cd718ad5671fa7d6128db93_1440w.jpg" alt="img"></p><p>​        在Flink架构角色中我们提到，TaskManager是实际负责执行计算的Worker，TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 Task Slot 的概念。</p><p>​       简单的说，TaskManager会将自己节点上管理的资源分为不同的Slot：固定大小的资源子集。这样就避免了不同Job的Task互相竞争内存资源，但是需要主要的是，Slot只会做内存的隔离。没有做CPU的隔离。</p><h3 id="9-说说-Flink-的常用算子？"><a href="#9-说说-Flink-的常用算子？" class="headerlink" title="9. 说说 Flink 的常用算子？"></a><strong>9. 说说 Flink 的常用算子？</strong></h3><p>​        Flink 最常用的常用算子包括：Map：DataStream → DataStream，输入一个参数产生一个参数，map的功能是对输入的参数进行转换操作。Filter：过滤掉指定条件的数据。KeyBy：按照指定的key进行分组。Reduce：用来进行结果汇总合并。Window：窗口函数，根据某些特性将每个key的数据进行分组（例如：在5s内到达的数据）</p><h3 id="10-说说你知道的Flink分区策略？"><a href="#10-说说你知道的Flink分区策略？" class="headerlink" title="10. 说说你知道的Flink分区策略？"></a><strong>10. 说说你知道的Flink分区策略？</strong></h3><p>​       什么要搞懂什么是分区策略。分区策略是用来决定数据如何发送至下游。目前 Flink 支持了8中分区策略的实现。</p><p><img src="https://pic2.zhimg.com/80/v2-135f2ee43f63fd4275f5e437fb21dda9_1440w.jpg" alt="img"></p><p>上图是整个Flink实现的分区策略继承图：</p><blockquote><ul><li><strong>GlobalPartitioner</strong>数据会被分发到下游算子的第一个实例中进行处理。</li><li><strong>ShufflePartitioner</strong>数据会被随机分发到下游算子的每一个实例中进行处理。</li><li><strong>RebalancePartitioner</strong>数据会被循环发送到下游的每一个实例中进行处理。</li><li><strong>RescalePartitioner</strong>这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为2，编号为A和B。下游并行度为4，编号为1，2，3，4。那么A则把数据循环发送给1和2，B则把数据循环发送给3和4。假设上游并行度为4，编号为A，B，C，D。下游并行度为2，编号为1，2。那么A和B则把数据发送给1，C和D则把数据发送给2。</li><li><strong>BroadcastPartitioner</strong>广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做join的场景。</li><li><strong>ForwardPartitioner</strong> 用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。</li><li><strong>KeyGroupStreamPartitioner</strong> Hash分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。</li><li><strong>CustomPartitionerWrapper</strong>用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑。</li></ul></blockquote><h3 id="11-Flink的并行度了解吗？Flink的并行度设置是怎样的？"><a href="#11-Flink的并行度了解吗？Flink的并行度设置是怎样的？" class="headerlink" title="11. Flink的并行度了解吗？Flink的并行度设置是怎样的？"></a><strong>11. Flink的并行度了解吗？Flink的并行度设置是怎样的？</strong></h3><p>​        Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。这些并行实例的数量被称为并行度。</p><p>​      我们在实际生产环境中可以从四个不同层面设置并行度：</p><ul><li><p>操作算子层面(Operator Level)</p></li><li><p>执行环境层面(Execution Environment Level)</p></li><li><p>客户端层面(Client Level)</p></li><li><p>系统层面(System Level)</p></li></ul><p>需要注意的优先级：算子层面&gt;环境层面&gt;客户端层面&gt;系统层面。</p><h3 id="12-Flink的Slot和parallelism有什么区别？"><a href="#12-Flink的Slot和parallelism有什么区别？" class="headerlink" title="12. Flink的Slot和parallelism有什么区别？"></a><strong>12. Flink的Slot和parallelism有什么区别？</strong></h3><p>官网上十分经典的图：</p><p><img src="https://pic2.zhimg.com/80/v2-d4ee22523e404ce88195bf5c4696f361_1440w.jpg" alt="img"></p><p>slot是指taskmanager的并发执行能力，假设我们将 taskmanager.numberOfTaskSlots 配置为3那么每一个 taskmanager 中分配3个 TaskSlot, 3个 taskmanager 一共有9个TaskSlot。</p><p><img src="https://pic4.zhimg.com/80/v2-855ff152aad46e75981864e13df3740b_1440w.jpg" alt="img"></p><p>parallelism是指taskmanager实际使用的并发能力。假设我们把 parallelism.default 设置为1，那么9个 TaskSlot 只能用1个，有8个空闲。</p><h3 id="13-Flink有没有重启策略？说说有哪几种？"><a href="#13-Flink有没有重启策略？说说有哪几种？" class="headerlink" title="13. Flink有没有重启策略？说说有哪几种？"></a><strong>13. Flink有没有重启策略？说说有哪几种？</strong></h3><p>Flink 实现了多种重启策略。</p><ul><li><p>固定延迟重启策略（Fixed Delay Restart Strategy）</p></li><li><p>故障率重启策略（Failure Rate Restart Strategy）</p></li><li><p>没有重启策略（No Restart Strategy）</p></li><li><p>Fallback重启策略（Fallback Restart Strategy）</p></li></ul><h3 id="14-用过Flink中的分布式缓存吗？如何使用？"><a href="#14-用过Flink中的分布式缓存吗？如何使用？" class="headerlink" title="14. 用过Flink中的分布式缓存吗？如何使用？"></a><strong>14. 用过Flink中的分布式缓存吗？如何使用？</strong></h3><p>​        Flink实现的分布式缓存和Hadoop有异曲同工之妙。目的是在本地读取文件，并把他放在 taskmanager 节点中，防止task重复拉取。</p><pre class="line-numbers language-text"><code class="language-text">val env = ExecutionEnvironment.getExecutionEnvironment// register a file from HDFSenv.registerCachedFile("hdfs:///path/to/your/file", "hdfsFile")// register a local executable file (script, executable, ...)env.registerCachedFile("file:///path/to/exec/file", "localExecFile", true)// define your program and execute...val input: DataSet[String] = ...val result: DataSet[Integer] = input.map(new MyMapper())...env.execute()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="15-说说Flink中的广播变量，使用时需要注意什么？"><a href="#15-说说Flink中的广播变量，使用时需要注意什么？" class="headerlink" title="15. 说说Flink中的广播变量，使用时需要注意什么？"></a><strong>15. 说说Flink中的广播变量，使用时需要注意什么？</strong></h3><p>​        我们知道Flink是并行的，计算过程可能不在一个 Slot 中进行，那么有一种情况即：当我们需要访问同一份数据。那么Flink中的广播变量就是为了解决这种情况。</p><p>​        我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。</p><h3 id="16-说说Flink中的窗口？"><a href="#16-说说Flink中的窗口？" class="headerlink" title="16. 说说Flink中的窗口？"></a><strong>16. 说说Flink中的窗口？</strong></h3><p>​      来一张官网经典的图：</p><p><img src="https://pic3.zhimg.com/80/v2-c4faa8da0a50b8dbf8f07ed978890106_1440w.jpg" alt="img"></p><p>​        Flink 支持两种划分窗口的方式，按照time和count。如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window。</p><p>​        flink支持窗口的两个重要属性（size和interval）</p><p>​        如果size=interval,那么就会形成tumbling-window(无重叠数据)如果size&gt;interval,那么就会形成sliding-window(有重叠数据)如果size&lt; interval, 那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。</p><p>​        通过组合可以得出四种基本窗口：</p><ul><li><p>time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5))</p></li><li><p>time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3))</p></li><li><p>count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5)</p></li><li><p>count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3)</p></li></ul><h3 id="17-说说Flink中的状态存储？"><a href="#17-说说Flink中的状态存储？" class="headerlink" title="17. 说说Flink中的状态存储？"></a><strong>17. 说说Flink中的状态存储？</strong></h3><p>​        Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和 checkpoint 交互。</p><p>​        Flink提供了三种状态存储方式：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。</p><h3 id="18-Flink-中的时间有哪几类"><a href="#18-Flink-中的时间有哪几类" class="headerlink" title="18. Flink 中的时间有哪几类"></a><strong>18. Flink 中的时间有哪几类</strong></h3><p>​        Flink 中的时间和其他流式计算系统的时间一样分为三类：事件时间，摄入时间，处理时间三种。</p><ul><li>如果以 EventTime 为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime。</li><li>如果以 IngesingtTime 为基准来定义时间窗口将形成 IngestingTimeWindow,以 source 的systemTime为准。</li><li>如果以 ProcessingTime 基准来定义时间窗口将形成 ProcessingTimeWindow，以 operator 的systemTime 为准。</li></ul><h3 id="19-Flink-中水印是什么概念，起到什么作用？"><a href="#19-Flink-中水印是什么概念，起到什么作用？" class="headerlink" title="19. Flink 中水印是什么概念，起到什么作用？"></a><strong>19. Flink 中水印是什么概念，起到什么作用？</strong></h3><p>​       Watermark 是 Apache Flink 为了处理 EventTime 窗口计算提出的一种机制, 本质上是一种时间戳。一般来讲Watermark经常和Window一起被用来处理乱序事件。</p><h3 id="20-Flink-Table-amp-SQL-熟悉吗？TableEnvironment这个类有什么作用"><a href="#20-Flink-Table-amp-SQL-熟悉吗？TableEnvironment这个类有什么作用" class="headerlink" title="20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用"></a><strong>20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用</strong></h3><p>​       TableEnvironment是Table API和SQL集成的核心概念。</p><p>这个类主要用来：</p><ul><li><p>在内部catalog中注册表</p></li><li><p>注册外部catalog</p></li><li><p>执行SQL查询</p></li><li><p>注册用户定义（标量，表或聚合）函数</p></li><li><p>将DataStream或DataSet转换为表</p></li><li><p>持有对ExecutionEnvironment或StreamExecutionEnvironment的引用</p></li></ul><h3 id="21-Flink-SQL的实现原理是什么？-是如何实现-SQL-解析的呢？"><a href="#21-Flink-SQL的实现原理是什么？-是如何实现-SQL-解析的呢？" class="headerlink" title="21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？"></a><strong>21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？</strong></h3><p>​      首先大家要知道 Flink 的SQL解析是基于Apache Calcite这个开源框架。</p><p><img src="https://pic2.zhimg.com/80/v2-c3877f035a976a213eab756465c2ebfd_1440w.jpg" alt="img"></p><p>基于此，一次完整的SQL解析过程如下：</p><blockquote><ul><li><p>用户使用对外提供Stream SQL的语法开发业务应用</p></li><li><p>用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划</p></li><li><p>采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划</p></li><li><p>对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行</p></li></ul></blockquote><h2 id="第二部分：Flink-面试进阶篇"><a href="#第二部分：Flink-面试进阶篇" class="headerlink" title="第二部分：Flink 面试进阶篇"></a><strong>第二部分：Flink 面试进阶篇</strong></h2><h3 id="1-Flink是如何支持批流一体的？"><a href="#1-Flink是如何支持批流一体的？" class="headerlink" title="1. Flink是如何支持批流一体的？"></a><strong>1. Flink是如何支持批流一体的？</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-36a937f492576709a89e8f25c1bed4a9_1440w.jpg" alt="img"></p><p>​       本道面试题考察的其实就是一句话：Flink的开发者认为批处理是流处理的一种特殊情况。批处理是有限的流处理。Flink 使用一个引擎支持了DataSet API 和 DataStream API。</p><h3 id="2-Flink是如何做到高效的数据交换的？"><a href="#2-Flink是如何做到高效的数据交换的？" class="headerlink" title="2. Flink是如何做到高效的数据交换的？"></a><strong>2. Flink是如何做到高效的数据交换的？</strong></h3><p>​         在一个Flink Job中，数据需要在不同的task中进行交换，整个数据交换是有 TaskManager 负责的，TaskManager 的网络组件首先从缓冲buffer中收集records，然后再发送。Records 并不是一个一个被发送的，二是积累一个批次再发送，batch 技术可以更加高效的利用网络资源。</p><h3 id="3-Flink是如何做容错的？"><a href="#3-Flink是如何做容错的？" class="headerlink" title="3. Flink是如何做容错的？"></a><strong>3. Flink是如何做容错的？</strong></h3><p>​        Flink 实现容错主要靠强大的CheckPoint机制和State机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态。</p><h3 id="4-Flink-分布式快照的原理是什么？"><a href="#4-Flink-分布式快照的原理是什么？" class="headerlink" title="4. Flink 分布式快照的原理是什么？"></a><strong>4. Flink 分布式快照的原理是什么？</strong></h3><p>​        Flink的分布式快照是根据Chandy-Lamport算法量身定做的。简单来说就是持续创建分布式数据流及其状态的一致快照。</p><p><img src="https://pic2.zhimg.com/80/v2-4207ea26cfbf1c4d6528137a50b7add9_1440w.jpg" alt="img"></p><p>​       核心思想是在 input source 端插入 barrier，控制 barrier 的同步来实现 snapshot 的备份和 exactly-once 语义。</p><h3 id="5-Flink-是如何保证Exactly-once语义的？"><a href="#5-Flink-是如何保证Exactly-once语义的？" class="headerlink" title="5. Flink 是如何保证Exactly-once语义的？"></a><strong>5. Flink 是如何保证Exactly-once语义的？</strong></h3><p>​      Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。分为以下几个步骤：</p><ul><li><p>开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面</p></li><li><p>预提交（preCommit）将内存中缓存的数据写入文件并关闭</p></li><li><p>正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟</p></li><li><p>丢弃（abort）丢弃临时文件</p></li></ul><p>​    若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。</p><h3 id="6-Flink-的-kafka-连接器有什么特别的地方？"><a href="#6-Flink-的-kafka-连接器有什么特别的地方？" class="headerlink" title="6. Flink 的 kafka 连接器有什么特别的地方？"></a><strong>6. Flink 的 kafka 连接器有什么特别的地方？</strong></h3><p>​     Flink源码中有一个独立的connector模块，所有的其他connector都依赖于此模块，Flink 在1.9版本发布的全新kafka连接器，摒弃了之前连接不同版本的kafka集群需要依赖不同版本的connector这种做法，只需要依赖一个connector即可。</p><h3 id="7-说说-Flink的内存管理是如何做的"><a href="#7-说说-Flink的内存管理是如何做的" class="headerlink" title="7. 说说 Flink的内存管理是如何做的?"></a><strong>7. 说说 Flink的内存管理是如何做的?</strong></h3><p>​       Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。</p><p>理论上Flink的内存管理分为三部分：</p><ul><li><p>Network Buffers：这个是在TaskManager启动的时候分配的，这是一组用于缓存网络数据的内存，每个块是32K，默认分配2048个，可以通过“taskmanager.network.numberOfBuffers”修改</p></li><li><p>Memory Manage pool：大量的Memory Segment块，用于运行时的算法（Sort/Join/Shuffle等），这部分启动的时候就会分配。下面这段代码，根据配置文件中的各种参数来计算内存的分配方法。（heap or off-heap，这个放到下节谈），内存的分配支持预分配和lazy load，默认懒加载的方式。</p></li><li><p>User Code，这部分是除了Memory Manager之外的内存用于User code和TaskManager本身的数据结构。</p></li></ul><h3 id="8-说说-Flink的序列化如何做的"><a href="#8-说说-Flink的序列化如何做的" class="headerlink" title="8. 说说 Flink的序列化如何做的?"></a><strong>8. 说说 Flink的序列化如何做的?</strong></h3><p>​       Java本身自带的序列化和反序列化的功能，但是辅助信息占用空间比较大，在序列化对象时记录了过多的类信息。</p><p>​      Apache Flink摒弃了Java原生的序列化方法，以独特的方式处理数据类型和序列化，包含自己的类型描述符，泛型类型提取和类型序列化框架。</p><p>​     TypeInformation 是所有类型描述符的基类。它揭示了该类型的一些基本属性，并且可以生成序列化器。TypeInformation 支持以下几种类型：</p><ul><li><p>BasicTypeInfo: 任意Java 基本类型或 String 类型</p></li><li><p>BasicArrayTypeInfo: 任意Java基本类型数组或 String 数组</p></li><li><p>WritableTypeInfo: 任意 Hadoop Writable 接口的实现类</p></li><li><p>TupleTypeInfo: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现</p></li><li><p>CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples)</p></li><li><p>PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法</p></li><li><p>GenericTypeInfo: 任意无法匹配之前几种类型的类</p></li></ul><p>​       针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。</p><h3 id="9-Flink中的Window出现了数据倾斜，你有什么解决办法？"><a href="#9-Flink中的Window出现了数据倾斜，你有什么解决办法？" class="headerlink" title="9.  Flink中的Window出现了数据倾斜，你有什么解决办法？"></a><strong>9.  Flink中的Window出现了数据倾斜，你有什么解决办法？</strong></h3><p>​       window产生数据倾斜指的是数据在不同的窗口内堆积的数据量相差过多。本质上产生这种情况的原因是数据源头发送的数据量速度不同导致的。出现这种情况一般通过两种方式来解决：</p><ul><li><p>在数据进入窗口前做预聚合</p></li><li><p>重新设计窗口聚合的key</p></li></ul><h3 id="10-Flink中在使用聚合函数-GroupBy、Distinct、KeyBy-等函数时出现数据热点该如何解决？"><a href="#10-Flink中在使用聚合函数-GroupBy、Distinct、KeyBy-等函数时出现数据热点该如何解决？" class="headerlink" title="10.  Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？"></a><strong>10.  Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？</strong></h3><p>​      数据倾斜和数据热点是所有大数据框架绕不过去的问题。处理这类问题主要从3个方面入手：</p><ul><li>在业务上规避这类问题</li></ul><p>​      例如一个假设订单场景，北京和上海两个城市订单量增长几十倍，其余城市的数据量不变。这时候我们在进行聚合的时候，北京和上海就会出现数据堆积，我们可以单独数据北京和上海的数据。</p><ul><li>Key的设计上</li></ul><p>​     把热key进行拆分，比如上个例子中的北京和上海，可以把北京和上海按照地区进行拆分聚合。</p><ul><li>参数设置</li></ul><p>​     Flink 1.9.0 SQL(Blink Planner) 性能优化中一项重要的改进就是升级了微批模型，即 MiniBatch。原理是缓存一定的数据后再触发处理，以减少对State的访问，从而提升吞吐和减少数据的输出量。</p><h3 id="11-Flink任务延迟高，想解决这个问题，你会如何入手？"><a href="#11-Flink任务延迟高，想解决这个问题，你会如何入手？" class="headerlink" title="11. Flink任务延迟高，想解决这个问题，你会如何入手？"></a><strong>11. Flink任务延迟高，想解决这个问题，你会如何入手？</strong></h3><p>​     在Flink的后台任务管理中，我们可以看到Flink的哪个算子和task出现了反压。最主要的手段是资源调优和算子调优。资源调优即是对作业中的Operator的并发数（parallelism）、CPU（core）、堆内存（heap_memory）等参数进行调优。作业参数调优包括：并行度的设置，State的设置，checkpoint的设置。</p><h3 id="12-Flink是如何处理反压的？"><a href="#12-Flink是如何处理反压的？" class="headerlink" title="12. Flink是如何处理反压的？"></a><strong>12. Flink是如何处理反压的？</strong></h3><p>​       Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。下游消费者消费变慢，上游就会受到阻塞。</p><h3 id="13-Flink的反压和Storm有哪些不同？"><a href="#13-Flink的反压和Storm有哪些不同？" class="headerlink" title="13. Flink的反压和Storm有哪些不同？"></a><strong>13. Flink的反压和Storm有哪些不同？</strong></h3><p>​      Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。</p><p>​      Flink中的反压使用了高效有界的分布式阻塞队列，下游消费变慢会导致发送端阻塞。</p><p>​      二者最大的区别是Flink是逐级反压，而Storm是直接从源头降速。</p><h3 id="14-Operator-Chains（算子链）这个概念你了解吗？"><a href="#14-Operator-Chains（算子链）这个概念你了解吗？" class="headerlink" title="14. Operator Chains（算子链）这个概念你了解吗？"></a><strong>14. Operator Chains（算子链）这个概念你了解吗？</strong></h3><p>​      为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。这就是我们所说的算子链。</p><h3 id="15-Flink什么情况下才会把Operator-chain在一起形成算子链？"><a href="#15-Flink什么情况下才会把Operator-chain在一起形成算子链？" class="headerlink" title="15. Flink什么情况下才会把Operator chain在一起形成算子链？"></a><strong>15. Flink什么情况下才会把Operator chain在一起形成算子链？</strong></h3><p>​    两个operator chain在一起的的条件：</p><ul><li><p>上下游的并行度一致</p></li><li><p>下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</p></li><li><p>上下游节点都在同一个 slot group 中（下面会解释 slot group）</p></li><li><p>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</p></li><li><p>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</p></li><li><p>两个节点间数据分区方式是 forward（参考理解数据流的分区）</p></li><li><p>用户没有禁用 chain</p></li></ul><h3 id="16-说说Flink1-9的新特性？"><a href="#16-说说Flink1-9的新特性？" class="headerlink" title="16. 说说Flink1.9的新特性？"></a><strong>16. 说说Flink1.9的新特性？</strong></h3><ul><li><p>支持hive读写，支持UDF</p></li><li><p>Flink SQL TopN和GroupBy等优化</p></li><li><p>Checkpoint跟savepoint针对实际业务场景做了优化</p></li><li><p>Flink state查询</p></li></ul><h3 id="17-消费kafka数据的时候，如何处理脏数据？"><a href="#17-消费kafka数据的时候，如何处理脏数据？" class="headerlink" title="17. 消费kafka数据的时候，如何处理脏数据？"></a><strong>17. 消费kafka数据的时候，如何处理脏数据？</strong></h3><p>   可以在处理前加一个fliter算子，将不符合规则的数据过滤出去。</p><h2 id="第三部分：Flink-面试源码篇"><a href="#第三部分：Flink-面试源码篇" class="headerlink" title="第三部分：Flink 面试源码篇"></a><strong>第三部分：Flink 面试源码篇</strong></h2><h3 id="1-Flink-Job的提交流程"><a href="#1-Flink-Job的提交流程" class="headerlink" title="1. Flink Job的提交流程"></a><strong>1. Flink Job的提交流程</strong></h3><p>​       用户提交的Flink Job会被转化成一个DAG任务运行，分别是：StreamGraph、JobGraph、ExecutionGraph，Flink中JobManager与TaskManager，JobManager与Client的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。</p><h3 id="2-Flink所谓”三层图”结构是哪几个”图”？"><a href="#2-Flink所谓”三层图”结构是哪几个”图”？" class="headerlink" title="2. Flink所谓”三层图”结构是哪几个”图”？"></a><strong>2. Flink所谓”三层图”结构是哪几个”图”？</strong></h3><blockquote><p>一个Flink任务的DAG生成计算图大致经历以下三个过程：</p><ul><li><p>StreamGraph<br>最接近代码所表达的逻辑层面的计算拓扑结构，按照用户代码的执行顺序向StreamExecutionEnvironment添加StreamTransformation构成流式图。</p></li><li><p>JobGraph<br>从StreamGraph生成，将可以串联合并的节点进行合并，设置节点之间的边，安排资源共享slot槽位和放置相关联的节点，上传任务所需的文件，设置检查点配置等。相当于经过部分初始化和优化处理的任务图。</p></li><li><p>ExecutionGraph<br>由JobGraph转换而来，包含了任务具体执行所需的内容，是最贴近底层实现的执行图。</p></li></ul></blockquote><h3 id="3-JobManger在集群中扮演了什么角色？"><a href="#3-JobManger在集群中扮演了什么角色？" class="headerlink" title="3. JobManger在集群中扮演了什么角色？"></a><strong>3. JobManger在集群中扮演了什么角色？</strong></h3><p>​      JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。</p><p>​     JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务管理和资源管理。</p><p>​     JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。</p><p>​    同时在任务执行的过程中，Flink JobManager 会触发 Checkpoint 操作，每个 TaskManager 节点 收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Fink JobManager 中完成。</p><p>​     当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。</p><h3 id="4-JobManger在集群启动过程中起到什么作用？"><a href="#4-JobManger在集群启动过程中起到什么作用？" class="headerlink" title="4. JobManger在集群启动过程中起到什么作用？"></a><strong>4. JobManger在集群启动过程中起到什么作用？</strong></h3><p>​     JobManager的职责主要是接收Flink作业，调度Task，收集作业状态和管理TaskManager。它包含一个Actor，并且做如下操作：</p><ul><li><p>RegisterTaskManager: 它由想要注册到JobManager的TaskManager发送。注册成功会通过AcknowledgeRegistration消息进行Ack。</p></li><li><p>SubmitJob: 由提交作业到系统的Client发送。提交的信息是JobGraph形式的作业描述信息。</p></li><li><p>CancelJob: 请求取消指定id的作业。成功会返回CancellationSuccess，否则返回CancellationFailure。</p></li><li><p>UpdateTaskExecutionState: 由TaskManager发送，用来更新执行节点(ExecutionVertex)的状态。成功则返回true，否则返回false。</p></li><li><p>RequestNextInputSplit: TaskManager上的Task请求下一个输入split，成功则返回NextInputSplit，否则返回null。</p></li><li><p>JobStatusChanged： 它意味着作业的状态(RUNNING, CANCELING, FINISHED,等)发生变化。这个消息由ExecutionGraph发送。</p></li></ul><h3 id="5-TaskManager在集群中扮演了什么角色？"><a href="#5-TaskManager在集群中扮演了什么角色？" class="headerlink" title="5. TaskManager在集群中扮演了什么角色？"></a><strong>5. TaskManager在集群中扮演了什么角色？</strong></h3><p>​     TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。</p><p>​    客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。</p><p>​     TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。</p><p>​      可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进行的方式有很大的区别，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。</p><h3 id="6-TaskManager在集群启动过程中起到什么作用？"><a href="#6-TaskManager在集群启动过程中起到什么作用？" class="headerlink" title="6. TaskManager在集群启动过程中起到什么作用？"></a><strong>6. TaskManager在集群启动过程中起到什么作用？</strong></h3><p>​     TaskManager的启动流程较为简单：启动类：org.apache.flink.runtime.taskmanager.TaskManager核心启动方法 ： selectNetworkInterfaceAndRunTaskManager启动后直接向JobManager注册自己，注册完成后，进行部分模块的初始化。</p><h3 id="7-Flink-计算资源的调度是如何实现的？"><a href="#7-Flink-计算资源的调度是如何实现的？" class="headerlink" title="7. Flink 计算资源的调度是如何实现的？"></a><strong>7. Flink 计算资源的调度是如何实现的？</strong></h3><p>​     TaskManager中最细粒度的资源是Task slot，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。</p><p>​     通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。</p><p>​      而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot：</p><p><img src="https://pic2.zhimg.com/80/v2-00e4b6d488ca209d7b02580ece13a905_1440w.jpg" alt="img"></p><h3 id="8-简述Flink的数据抽象及数据交换过程？"><a href="#8-简述Flink的数据抽象及数据交换过程？" class="headerlink" title="8. 简述Flink的数据抽象及数据交换过程？"></a><strong>8. 简述Flink的数据抽象及数据交换过程？</strong></h3><p>​        Flink 为了避免JVM的固有缺陷例如java对象存储密度低，FGC影响吞吐和响应等，实现了自主管理内存。MemorySegment就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。</p><p>​        在MemorySegment这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是Buffer。</p><p>​        对接从Java对象转为Buffer的中间对象是另一个抽象StreamRecord。</p><h3 id="9-Flink-中的分布式快照机制是如何实现的？"><a href="#9-Flink-中的分布式快照机制是如何实现的？" class="headerlink" title="9. Flink 中的分布式快照机制是如何实现的？"></a><strong>9. Flink 中的分布式快照机制是如何实现的？</strong></h3><p>​        Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。</p><p><img src="https://pic3.zhimg.com/80/v2-897c8116fc9c17b50068810949311d52_1440w.jpg" alt="img"></p><p>​       barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。</p><p>​       然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。 一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。在所有sink确认快照后，意味快照着已完成。</p><p>​        一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。</p><h3 id="10-简单说说FlinkSQL的是如何实现的？"><a href="#10-简单说说FlinkSQL的是如何实现的？" class="headerlink" title="10. 简单说说FlinkSQL的是如何实现的？"></a><strong>10. 简单说说FlinkSQL的是如何实现的？</strong></h3><p>​        Flink 将 SQL 校验、SQL 解析以及 SQL 优化交给了Apache Calcite。Calcite 在其他很多开源项目里也都应用到了，譬如 Apache Hive, Apache Drill, Apache Kylin, Cascading。Calcite 在新的架构中处于核心的地位，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-6f52103fd8b0bbd77e3c35f9cdbf4df9_1440w.jpg" alt="img"></p><p>构建抽象语法树的事情交给了 Calcite 去做。SQL query 会经过 Calcite 解析器转变成 SQL 节点树，通过验证后构建成 Calcite 的抽象语法树（也就是图中的 Logical Plan）。另一边，Table API 上的调用会构建成 Table API 的抽象语法树，并通过 Calcite 提供的 RelBuilder 转变成 Calcite 的抽象语法树。然后依次被转换成逻辑执行计划和物理执行计划。</p><p>在提交任务后会分发到各个 TaskManager 中运行，在运行时会使用 Janino 编译器编译代码后运行。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第一部分：Flink-面试基础篇&quot;&gt;&lt;a href=&quot;#第一部分：Flink-面试基础篇&quot; class=&quot;headerlink&quot; title=&quot;第一部分：Flink 面试基础篇&quot;&gt;&lt;/a&gt;&lt;strong&gt;第一部分：Flink 面试基础篇&lt;/strong&gt;&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="Flink" scheme="https://dataquaner.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://dataquaner.github.io/tags/Flink/"/>
    
      <category term="面试必备" scheme="https://dataquaner.github.io/tags/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系列之决策树算法（03）：决策树的剪枝</title>
    <link href="https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/"/>
    <id>https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/</id>
    <published>2020-04-11T11:32:09.079Z</published>
    <updated>2020-04-11T11:32:09.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现<strong>过拟合现象</strong>。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为<strong>剪枝(pruning)。</strong></p><p>具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。</p><blockquote><p>决策树的剪枝往往通过<strong>极小化</strong>决策树整体的<strong>损失函数(loss funtion)</strong>或<strong>代价函数(cost funtion)</strong>来实现。</p></blockquote><h1 id="2-剪枝算法"><a href="#2-剪枝算法" class="headerlink" title="2.剪枝算法"></a>2.剪枝算法</h1><h2 id="2-1-为什么要剪枝"><a href="#2-1-为什么要剪枝" class="headerlink" title="2.1 为什么要剪枝"></a>2.1 为什么要剪枝</h2><p><strong>现象</strong></p><p>接上一次讲的生成决策树，下面给出一张图。</p><img src="https://pic2.zhimg.com/80/v2-d6588457cc144c1bad2f87ec77081af1_hd.jpg" alt="决策树学习中的过渡拟合" style="zoom:50%;"><ul><li>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。</li><li>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</li></ul><p><strong>可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</strong></p><p><strong>原因</strong></p><p><img src="https://pic2.zhimg.com/80/v2-677a5e08d5b55b3b4cb14f7ad6f8eb31_hd.jpg" alt="原因"></p><ul><li>原因1：噪声、样本冲突，即错误的样本数据。</li><li>原因2：特征即属性不能完全作为分类标准。</li><li>原因3：巧合的规律性，数据量不够大。</li></ul><p>这个时候，就需要对生成树进行修剪，也就是<strong>剪枝</strong>。</p><h2 id="2-2-如何进行剪枝"><a href="#2-2-如何进行剪枝" class="headerlink" title="2.2 如何进行剪枝"></a>2.2 如何进行剪枝</h2><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><strong>预剪枝</strong></h3><p>预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法:<br>        (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。<br>        (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。<br>        (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。</p><p>(4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。</p><p><strong>优点&amp;缺点</strong></p><ul><li><p>由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【<strong>怎样精确地估计何时停止树的增长是相当困难的</strong>】。</p></li><li><p>预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。</p></li></ul><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a><strong>后剪枝</strong></h3><p>后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。</p><p>这里主要介绍四种：</p><ul><li>REP-错误率降低剪枝</li><li>PEP-悲观剪枝</li><li>CCP-代价复杂度剪枝</li><li>MEP-最小错误剪枝</li></ul><h4 id="REP-Reduced-Error-Pruning-方法"><a href="#REP-Reduced-Error-Pruning-方法" class="headerlink" title="REP(Reduced Error Pruning)方法"></a><strong>REP(Reduced Error Pruning)方法</strong></h4><blockquote><p>对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S</p></blockquote><p><img src="https://pic2.zhimg.com/80/v2-ff11945f2e5a8319d82ab53c363ef441_hd.jpg" alt="img"></p><p><strong>优点：</strong></p><ul><li>REP 是当前最简单的事后剪枝方法之一。</li><li>它的计算复杂性是线性的。</li><li>和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。</li></ul><p><strong>缺点：</strong></p><ul><li>但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意.</li></ul><h4 id="PEP-Pessimistic-Error-Pruning-方法"><a href="#PEP-Pessimistic-Error-Pruning-方法" class="headerlink" title="PEP(Pessimistic Error Pruning)方法"></a><strong>PEP(Pessimistic Error Pruning)方法</strong></h4><blockquote><p>为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, <strong>PEP 方法对误差估计增加了连续性校正(continuity correction)</strong>。关于PEP方法的数据解释待后续开专题梳理。</p></blockquote><p><strong>优点：</strong></p><ul><li>PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一</li><li>PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利</li><li>它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .</li></ul><p><strong>缺点：</strong></p><p>PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。</p><p><strong>TIPS：</strong></p><p>个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。</p><h4 id="MEP-Minimum-Error-Pruning-方法"><a href="#MEP-Minimum-Error-Pruning-方法" class="headerlink" title="MEP(Minimum Error Pruning)方法"></a><strong>MEP(Minimum Error Pruning)方法</strong></h4><blockquote><p>MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。</p></blockquote><p><strong>优点：</strong></p><ul><li>MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集.</li><li>它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .</li></ul><p><strong>缺点：</strong></p><p>类别平均分配的前提假设现实几率不大&amp;对K太过敏感</p><p><img src="https://pic3.zhimg.com/80/v2-5e7deee0ee978be2eec60328192affc6_hd.jpg" alt="img"></p><p>对此，也有改进算法，我没有深入研究。</p><p><img src="https://pic1.zhimg.com/80/v2-3ffc529242dbb52dcb4946e37fed92f0_hd.jpg" alt="img"></p><h4 id="CCP-Cost-Complexity-Pruning-方法"><a href="#CCP-Cost-Complexity-Pruning-方法" class="headerlink" title="CCP(Cost-Complexity Pruning)方法"></a><strong>CCP(Cost-Complexity Pruning)方法</strong></h4><blockquote><p>CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤:<br>                (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。</p><p>​        (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。</p></blockquote><p><strong>缺点：</strong></p><p>生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的.</p><p><strong>对比四种方法</strong></p><table><thead><tr><th><strong>剪枝名称</strong></th><th><strong>剪枝方式</strong></th><th><strong>计算复杂度</strong></th><th><strong>误差估计</strong></th></tr></thead><tbody><tr><td>REP</td><td>自底向上</td><td>0(n)</td><td>剪枝集上误差估计</td></tr><tr><td>PEP</td><td>自顶向下</td><td>o(n)</td><td>使用连续纠正</td></tr><tr><td>CCP</td><td>自底向上</td><td>o(n2)</td><td>标准误差</td></tr><tr><td>MEP</td><td>自底向上</td><td>o(n)</td><td>使用连续纠正</td></tr></tbody></table><p>① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。</p><p>② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。</p><p>③ 如果数据集多（REP&amp;CCP←复杂但树小）</p><p>④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定）</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h1><p>决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。</p><p>剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。</p><p><strong>预剪枝：</strong></p><p>预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。</p><p>简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。</p><p>这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。</p><p><strong>后剪枝：</strong></p><p>后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。</p><p>同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。</p><p>后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。</p><p><strong>剪枝方法的选择</strong></p><p>如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。</p><p>后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。</p><p>有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。</p><p>经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。</p><p>无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。</p><p>函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点</p><p>函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。</p><p>函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值</p><p>上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      决策树
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Decision Tree" scheme="https://dataquaner.github.io/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系列之决策树算法（02）：决策树的生成</title>
    <link href="https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/"/>
    <id>https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/</id>
    <published>2020-04-11T11:31:53.052Z</published>
    <updated>2020-04-11T11:31:53.052Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。</p><h2 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h2><p> <strong>示例数据表格</strong></p><p>    文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中</p><p><img src="https://pic3.zhimg.com/80/v2-393024075528471f43f52891d29320be_hd.jpg" alt="img"></p><h3 id="2-1-相关概念阐述"><a href="#2-1-相关概念阐述" class="headerlink" title="2.1 相关概念阐述"></a><strong>2.1 相关概念阐述</strong></h3><h4 id="2-1-1-决策树"><a href="#2-1-1-决策树" class="headerlink" title="2.1.1 决策树"></a><strong>2.1.1 决策树</strong></h4><p> 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。</p><h4 id="2-1-2-ID3、C4-5、CART"><a href="#2-1-2-ID3、C4-5、CART" class="headerlink" title="2.1.2 ID3、C4.5、CART"></a><strong>2.1.2 ID3、C4.5、CART</strong></h4><p>在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【<a href="https://dataquaner.github.io/2019/12/17/机器学习系列之决策树算法（01）：决策树特征选择/">机器学习系列之决策树算法（01）：决策树特征选择</a>】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。</p><h5 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a><strong>信息熵</strong></h5><p> 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。</p><p>​                                                            Entopy(t)=-Σp(i|t)log2p(i|t)</p><h5 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a><strong>信息增益</strong></h5><p>信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为：</p><p>​                                        Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di)</p><h5 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a><strong>信息增益率</strong></h5><p> 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7：</p><p>​                H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7))</p><h5 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a><strong>基尼系数</strong></h5><p> 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0">基尼系数</a></p><p> 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低.</p><p> CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分.</p><p> 基尼系数的计算公式如下:</p><p>​                            Gini = 1 – Σ (Pi)2 for i=1 to number of classes</p><h3 id="2-2-完整生成过程"><a href="#2-2-完整生成过程" class="headerlink" title="2.2 完整生成过程"></a><strong>2.2 完整生成过程</strong></h3><p> 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例</p><h4 id="2-2-1-根节点的选择"><a href="#2-2-1-根节点的选择" class="headerlink" title="2.2.1 根节点的选择"></a><strong>2.2.1 根节点的选择</strong></h4><p> 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数</p><p> 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为：</p><p>​                        Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985</p><h5 id="天气"><a href="#天气" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-4977ea2a875ea67cb75588eb04b6aee5_hd.jpg" alt="img"></p><h6 id="信息增益计算"><a href="#信息增益计算" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下：</p><p>D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981</p><p>D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为：</p><p>3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965</p><p>其信息增益为：</p><p>Gain(天气)=0.985 - 0.965 = 0.020</p><h6 id="信息增益率计算"><a href="#信息增益率计算" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p> 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为：</p><p>H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556</p><p> 则其信息增益率为：</p><p>Gain_ratio(天气)=0.020/1.556=0.012</p><h6 id="基尼系数计算"><a href="#基尼系数计算" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9</li><li>Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21</li></ul><h5 id="温度"><a href="#温度" class="headerlink" title="温度"></a><strong>温度</strong></h5><p>  其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-74ad946b56a27f3bc480ba07f31552de_hd.jpg" alt="img"></p><h6 id="信息增益计算-1"><a href="#信息增益计算-1" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857</p><p>    其信息增益为：</p><p>Gain(温度)=0.985 - 0.857 = 0.128</p><h6 id="信息增益率计算-1"><a href="#信息增益率计算-1" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378</p><p>    则其信息增益率为：</p><p>Gain_ratio(温度)=0.128/1.378=0.0928</p><h6 id="基尼系数计算-1"><a href="#基尼系数计算-1" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0</li><li>Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7</li></ul><h5 id="湿度"><a href="#湿度" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic1.zhimg.com/80/v2-423f80f054a8d86eed652afff6a6c914_hd.jpg" alt="img"></p><h6 id="信息增益计算-2"><a href="#信息增益计算-2" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>    作为子节点的归一化信息熵为：</p><p>4/7 * 1.0 + 3/7 * 0.918 = 0.964</p><p>    其信息增益为：</p><p>Gain(湿度)=0.985 - 0.964 = 0.021</p><h6 id="信息增益率计算-2"><a href="#信息增益率计算-2" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=0.021/0.985=0.021</p><h6 id="基尼系数计算-2"><a href="#基尼系数计算-2" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</li><li>Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><h6 id="刮风"><a href="#刮风" class="headerlink" title="刮风"></a><strong>刮风</strong></h6><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-edb75f5790519fcee2dd6317f4d5557d_hd.jpg" alt="img"></p><h6 id="信息增益计算-3"><a href="#信息增益计算-3" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>    作为子节点的归一化信息熵为：</p><p>3/7 * 1.0 + 4/7 * 0.918 = 0.964</p><p>    其信息增益为：</p><p>Gain(刮风)=0.985 - 0.964 = 0.021</p><h6 id="信息增益率计算-3"><a href="#信息增益率计算-3" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0.021/0.985=0.021</p><h6 id="基尼系数计算-3"><a href="#基尼系数计算-3" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</li><li>Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><h5 id="根节点的选择"><a href="#根节点的选择" class="headerlink" title="根节点的选择"></a><strong>根节点的选择</strong></h5><p>  如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度</p><p><strong>信息增益</strong></p><ul><li><p>Gain(天气)=0.985 - 0.965 = 0.020</p></li><li><p>Gain(温度)=0.985 - 0.857 = 0.128</p></li><li><p>Gain(湿度)=0.985 - 0.964 = 0.021</p></li><li><p>Gain(刮风)=0.985 - 0.964 = 0.021</p></li></ul><p><strong>信息增益率</strong></p><ul><li>Gain_ratio(天气)=0.020/1.556=0.012</li><li>Gain_ratio(温度)=0.128/1.378=0.0928</li><li>Gain_ratio(湿度)=0.021/0.985=0.021</li><li>Gain_ratio(刮风)=0.021/0.985=0.021</li></ul><p><strong>基尼系数</strong></p><ul><li>Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619</li><li>Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857</li><li>Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li><li>Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><p>确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择:</p><ul><li>根节点<ul><li>子节点温度高:(待进一步进行选择)</li><li>子节点温度中:(待进一步进行选择)</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul><h4 id="2-2-2-子节点温度高的选择"><a href="#2-2-2-子节点温度高的选择" class="headerlink" title="2.2.2 子节点温度高的选择"></a><strong>2.2.2 子节点温度高的选择</strong></h4><p>    其剩下的数据集如下,温度不再进行下面的节点选择参与:</p><p><img src="https://pic1.zhimg.com/80/v2-86c2e574fc2309e9dff98e8205c0dff4_hd.jpg" alt="img"></p><p>    根据信息熵的计算公式可以得到子节点温度高的信息熵为：</p><p>​                                                Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><h5 id="天气-1"><a href="#天气-1" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-72771013ffea869ed0ce76c7f8998f79_hd.jpg" alt="img"></p><h6 id="信息增益计算-4"><a href="#信息增益计算-4" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    相应的信息熵如下：</p><p>D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>    归一化信息熵为：</p><p>2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5</p><p>    其信息增益为：</p><p>Gain(天气)=1.0 - 0.5 = 0.5</p><h6 id="信息增益率计算-4"><a href="#信息增益率计算-4" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    对应的属性熵为：</p><p>H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5</p><p>    则其信息增益率为：</p><p>Gain_ratio(天气)=0.5/1.5=0.33333</p><h6 id="基尼系数计算-4"><a href="#基尼系数计算-4" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25</li></ul><h5 id="湿度-1"><a href="#湿度-1" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-ea98c22052cf1246618f266c52be998e_hd.jpg" alt="img"></p><h6 id="信息增益计算-5"><a href="#信息增益计算-5" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0</p><p>D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>2/4 * 0.0 + 2/4 * 0.0 = 0.0</p><p>    其信息增益为：</p><p>Gain(湿度)=1.0 - 0.0 = 1.0</p><h6 id="信息增益率计算-5"><a href="#信息增益率计算-5" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=1.0/1.0=1.0</p><h6 id="基尼系数计算-5"><a href="#基尼系数计算-5" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0</li><li>Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0</li><li>Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0</li></ul><h5 id="刮风-1"><a href="#刮风-1" class="headerlink" title="刮风"></a><strong>刮风</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-ba94b90f801904bae450796e8a5db0be_hd.jpg" alt="img"></p><h6 id="信息增益计算-6"><a href="#信息增益计算-6" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0</p><p>D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>    作为子节点的归一化信息熵为：</p><p>1/4 * 0.0 + 3/4 * 0.918 = 0.688</p><p>    其信息增益为：</p><p>Gain(刮风)=1.0 - 0.688 = 0.312</p><h6 id="信息增益率计算-6"><a href="#信息增益率计算-6" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0.312/0.918=0.349</p><h6 id="基尼系数计算-6"><a href="#基尼系数计算-6" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li><p>Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0</p></li><li><p>Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</p></li><li><p>Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333</p></li></ul><p><strong>子节点温度高的选择</strong></p><p>    如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度</p><ul><li>Gain(天气)=1.0 - 0.5 = 0.5</li><li>Gain(湿度)=1.0 - 0.0 = 1.0</li><li>Gain(刮风)=1.0 - 0.688 = 0.312</li><li>Gain_ratio(天气)=0.5/1.5=0.33333</li><li>Gain_ratio(湿度)=1.0/1.0=1.0</li><li>Gain_ratio(刮风)=0.312/0.918=0.349</li><li>Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25</li><li>Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0</li><li>Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333</li></ul><p>    确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果:</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中:(待进一步进行选择)<ul><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h4 id="2-2-3-子节点温度中的选择"><a href="#2-2-3-子节点温度中的选择" class="headerlink" title="2.2.3 子节点温度中的选择"></a><strong>2.2.3 子节点温度中的选择</strong></h4><p>    其剩下的数据集如下,温度不再进行下面的节点选择参与:</p><p><img src="https://pic1.zhimg.com/80/v2-3f1b0c5e060288599f90c428441aaabc_hd.jpg" alt="img"></p><p>    根据信息熵的计算公式可以得到子节点温度高的信息熵为：</p><p>Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><h5 id="天气-2"><a href="#天气-2" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic4.zhimg.com/80/v2-7ac9cd99fc8388c6c3f34fdb2e132b57_hd.jpg" alt="img"></p><h6 id="信息增益计算-7"><a href="#信息增益计算-7" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    相应的信息熵如下：</p><p>D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D</p><p>(阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>    归一化信息熵为：</p><p>1/2 * 0.0 + 1/2 * 0.0 = 0</p><p>    其信息增益为：</p><p>Gain(天气)=1.0 - 0 = 1.0</p><h6 id="信息增益率计算-7"><a href="#信息增益率计算-7" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    对应的属性熵为：</p><p>H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(天气)=1.0/1.0=1.0</p><h6 id="基尼系数计算-7"><a href="#基尼系数计算-7" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0</li><li>Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0</li></ul><h5 id="湿度-2"><a href="#湿度-2" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-a27cfcdbbde0c07ab39cbdac34b8e6da_hd.jpg" alt="img"></p><h6 id="信息增益计算-8"><a href="#信息增益计算-8" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>1/2 * 0.0 + 1/2 * 0.0 = 0</p><p>    其信息增益为：</p><p>Gain(湿度)=1.0 - 0.0 = 1.0</p><h6 id="信息增益率计算-8"><a href="#信息增益率计算-8" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=1.0/1.0=1.0</p><h6 id="基尼系数计算-8"><a href="#基尼系数计算-8" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0</li><li>Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0</li></ul><h5 id="刮风-2"><a href="#刮风-2" class="headerlink" title="刮风"></a><strong>刮风</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-887ebcf7eecd07ee13ca02d024a3b321_hd.jpg" alt="img"></p><h6 id="信息增益计算-9"><a href="#信息增益计算-9" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    作为子节点的归一化信息熵为：</p><p>1/1 * 1.0 = 1.0</p><p>    其信息增益为：</p><p>Gain(刮风)=1.0 - 1.0 = 0</p><h6 id="信息增益率计算-9"><a href="#信息增益率计算-9" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(2/2 * log2(2/2) = 0.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0/0 = 0</p><h6 id="基尼系数计算-9"><a href="#基尼系数计算-9" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5</li><li>Gini(刮风)=2/2 * 0.5 = 0.5</li></ul><p><strong>子节点温度中的选择</strong></p><p>如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧</p><ul><li>Gain(天气)=1.0 - 0 = 1.0</li><li>Gain(湿度)=1.0 - 0.0 = 1.0</li><li>Gain(刮风)=1.0 - 1.0 = 0</li><li>Gain_ratio(天气)=1.0/1.0=1.0</li><li>Gain_ratio(湿度)=1.0/1.0=1.0</li><li>Gain_ratio(刮风)=0/0 = 0</li><li>Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0</li><li>Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0</li><li>Gini(刮风)=2/2 * 0.5 = 0.5</li></ul><p>    确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果:</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中<ul><li>叶节点天气晴：打篮球</li><li>叶节点天气阴：不打篮球</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h4 id="2-2-4-最终的决策树"><a href="#2-2-4-最终的决策树" class="headerlink" title="2.2.4 最终的决策树"></a><strong>2.2.4 最终的决策树</strong></h4><p>    在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树：</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中<ul><li>叶节点天气晴：打篮球</li><li>叶节点天气阴：不打篮球</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a><strong>3. 思考</strong></h2><p> 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？</p><p> 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      决策树
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Decision Tree" scheme="https://dataquaner.github.io/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>数据存储之MySQL系列（01）：MySQL体系结构</title>
    <link href="https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/"/>
    <id>https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/</id>
    <published>2020-04-11T11:31:10.255Z</published>
    <updated>2020-04-11T11:31:10.255Z</updated>
    
    <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      MySQL
    
    </summary>
    
    
      <category term="DataBase" scheme="https://dataquaner.github.io/categories/DataBase/"/>
    
    
      <category term="MySQL" scheme="https://dataquaner.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>xgboost算法模型输出的解释</title>
    <link href="https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/"/>
    <id>https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/</id>
    <published>2020-04-11T11:30:52.923Z</published>
    <updated>2020-04-11T11:30:52.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p> 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。</p><h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2. 数据集"></a>2. 数据集</h2><p> 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasetsiris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>data <span class="token operator">=</span> iris<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span> data<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true">#(100L, 4L)</span><span class="token comment" spellcheck="true">#一共有100个样本数据, 维度为4维</span>label <span class="token operator">=</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span> label<span class="token comment" spellcheck="true">#正好选取label为0和1的数据</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-训练集与测试集"><a href="#3-训练集与测试集" class="headerlink" title="3. 训练集与测试集"></a>3. 训练集与测试集</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cross_validation <span class="token keyword">import</span> train_test_splittrain_x<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="4-Xgboost建模"><a href="#4-Xgboost建模" class="headerlink" title="4. Xgboost建模"></a>4. Xgboost建模</h2><h3 id="4-1-模型初始化设置"><a href="#4-1-模型初始化设置" class="headerlink" title="4.1 模型初始化设置"></a>4.1 模型初始化设置</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgbdtrain<span class="token operator">=</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>train_x<span class="token punctuation">,</span>label<span class="token operator">=</span>train_y<span class="token punctuation">)</span>dtest<span class="token operator">=</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>test_x<span class="token punctuation">)</span>params<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'booster'</span><span class="token punctuation">:</span><span class="token string">'gbtree'</span><span class="token punctuation">,</span>    <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'binary:logistic'</span><span class="token punctuation">,</span>    <span class="token string">'eval_metric'</span><span class="token punctuation">:</span> <span class="token string">'auc'</span><span class="token punctuation">,</span>    <span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token string">'lambda'</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">,</span>    <span class="token string">'subsample'</span><span class="token punctuation">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>    <span class="token string">'colsample_bytree'</span><span class="token punctuation">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>    <span class="token string">'min_child_weight'</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span>    <span class="token string">'eta'</span><span class="token punctuation">:</span> <span class="token number">0.025</span><span class="token punctuation">,</span>    <span class="token string">'seed'</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token string">'nthread'</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">,</span>     <span class="token string">'silent'</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">}</span>watchlist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>dtrain<span class="token punctuation">,</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-建模与预测"><a href="#4-2-建模与预测" class="headerlink" title="4.2 建模与预测"></a>4.2 建模与预测</h3><pre class="line-numbers language-python"><code class="language-python">bst<span class="token operator">=</span>xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>params<span class="token punctuation">,</span>dtrain<span class="token punctuation">,</span>num_boost_round<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>evals<span class="token operator">=</span>watchlist<span class="token punctuation">)</span>ypred<span class="token operator">=</span>bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 设置阈值, 输出一些评价指标</span>y_pred <span class="token operator">=</span> <span class="token punctuation">(</span>ypred <span class="token operator">>=</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1</span><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token keyword">print</span> <span class="token string">'AUC: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>roc_auc_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>ypred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'ACC: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'Recall: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>recall_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'F1-score: %.4f'</span> <span class="token operator">%</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'Precesion: %.4f'</span> <span class="token operator">%</span>metrics<span class="token punctuation">.</span>precision_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Out[23]:</p><pre class="line-numbers language-python"><code class="language-python">AUC<span class="token punctuation">:</span> <span class="token number">1.0000</span>ACC<span class="token punctuation">:</span> <span class="token number">1.0000</span>Recall<span class="token punctuation">:</span> <span class="token number">1.0000</span>F1<span class="token operator">-</span>score<span class="token punctuation">:</span> <span class="token number">1.0000</span>Precesion<span class="token punctuation">:</span> <span class="token number">1.0000</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>int64<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Yeah, 完美的模型, 完美的预测!</p><h3 id="4-3-可视化输出"><a href="#4-3-可视化输出" class="headerlink" title="4.3 可视化输出"></a>4.3 可视化输出</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#对于预测的输出有三种方式</span>?bst<span class="token punctuation">.</span>predictSignature<span class="token punctuation">:</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>data<span class="token punctuation">,</span> output_margin<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> ntree_limit<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> pred_leaf<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> pred_contribs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> approx_contribs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>pred_leaf <span class="token punctuation">:</span> bool    When this option <span class="token keyword">is</span> on<span class="token punctuation">,</span> the output will be a matrix of <span class="token punctuation">(</span>nsample<span class="token punctuation">,</span> ntrees<span class="token punctuation">)</span>    <span class="token keyword">with</span> each record indicating the predicted leaf index of each sample <span class="token keyword">in</span> each tree<span class="token punctuation">.</span>    Note that the leaf index of a tree <span class="token keyword">is</span> unique per tree<span class="token punctuation">,</span> so you may find leaf <span class="token number">1</span>    <span class="token keyword">in</span> both tree <span class="token number">1</span> <span class="token operator">and</span> tree <span class="token number">0</span><span class="token punctuation">.</span>pred_contribs <span class="token punctuation">:</span> bool    When this option <span class="token keyword">is</span> on<span class="token punctuation">,</span> the output will be a matrix of <span class="token punctuation">(</span>nsample<span class="token punctuation">,</span> nfeats<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> each record indicating the feature contributions <span class="token punctuation">(</span>SHAP values<span class="token punctuation">)</span> <span class="token keyword">for</span> that    prediction<span class="token punctuation">.</span> The sum of all feature contributions <span class="token keyword">is</span> equal to the prediction<span class="token punctuation">.</span>    Note that the bias <span class="token keyword">is</span> added <span class="token keyword">as</span> the final column<span class="token punctuation">,</span> on top of the regular features<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-3-1-得分"><a href="#4-3-1-得分" class="headerlink" title="4.3.1 得分"></a>4.3.1 得分</h4><p>默认的输出就是得分, 这没什么好说的, 直接上code.</p><pre class="line-numbers language-python"><code class="language-python">ypred <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span>ypred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[32]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>        <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>        <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>        <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>        <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出.</p><h4 id="4-3-2-所属的叶子节点"><a href="#4-3-2-所属的叶子节点" class="headerlink" title="4.3.2 所属的叶子节点"></a>4.3.2 所属的叶子节点</h4><p>当设置<code>pred_leaf=True</code>的时候, 这时就会输出每个样本在所有树中的叶子节点</p><pre class="line-numbers language-python"><code class="language-python">ypred_leaf <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">,</span> pred_leaf<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>ypred_leaf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[33]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以<code>ypred_leaf</code>的维度为<code>[100*100]</code>.</p><p>对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树).</p><p>那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型.</p><pre class="line-numbers language-python"><code class="language-python">xgb<span class="token punctuation">.</span>to_graphviz<span class="token punctuation">(</span>bst<span class="token punctuation">,</span> num_trees<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可视化第一棵树的生成情况</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://images2017.cnblogs.com/blog/957413/201710/957413-20171017204407818-1932629185.png" alt="img"></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#直接输出模型的迭代工程</span>bst<span class="token punctuation">.</span>dump_model<span class="token punctuation">(</span><span class="token string">"model.txt"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">booster<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f3<span class="token operator">&lt;</span><span class="token number">0.75</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.019697</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0214286</span>booster<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f2<span class="token operator">&lt;</span><span class="token number">2.35</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.0212184</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0212</span>booster<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f2<span class="token operator">&lt;</span><span class="token number">2.35</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.0197404</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0197235</span>booster<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">:</span> ……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值.</p><p>在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值.</p><p>同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值.</p><pre class="line-numbers language-python"><code class="language-python">leaf1   <span class="token operator">-</span><span class="token number">1.381214</span>leaf2    <span class="token number">1.410950</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了<code>'objective': 'binary:logistic'</code>, 因此使用函数将累加的值转换为实际的打分:</p><p>f(x)=1/(1+exp(−x))</p><pre class="line-numbers language-python"><code class="language-python"><span class="token number">1</span><span class="token operator">/</span>float<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1.38121416</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">0.20081407112186503</span><span class="token number">1</span><span class="token operator">/</span>float<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.410950</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">0.8039157403338895</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这就与<code>ypred = bst.predict(dtest)</code> 的分值相对应上了.</p><h4 id="4-3-2-特征重要性"><a href="#4-3-2-特征重要性" class="headerlink" title="4.3.2 特征重要性"></a>4.3.2 特征重要性</h4><p>接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性.</p><pre class="line-numbers language-python"><code class="language-python">ypred_contribs <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">,</span> pred_contribs<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>ypred_contribs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[37]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的<code>ypred_contribs</code>的维度为<code>[100,5]</code>, 通过阅读前面的文档注释就可以知道, 最后一列是<code>bias</code>, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的.</p><p>通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例.</p><pre class="line-numbers language-python"><code class="language-python">score_a <span class="token operator">=</span> sum<span class="token punctuation">(</span>ypred_contribs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> score_a<span class="token comment" spellcheck="true"># -1.38121373579</span>score_b <span class="token operator">=</span> sum<span class="token punctuation">(</span>ypred_contribs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> score_b<span class="token comment" spellcheck="true"># 1.41094945744</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相同的分值, 相同的处理情况.</p><p>到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      xgboost算法模型输出的解释
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://dataquaner.github.io/categories/Machine-Learning/"/>
    
    
      <category term="XGBoost" scheme="https://dataquaner.github.io/tags/XGBoost/"/>
    
  </entry>
  
</feed>
