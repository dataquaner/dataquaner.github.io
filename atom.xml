<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DataQuaner</title>
  
  <subtitle>DataQuaner</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://dataquaner.github.io/"/>
  <updated>2021-03-28T16:11:34.096Z</updated>
  <id>https://dataquaner.github.io/</id>
  
  <author>
    <name>Leon</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据计算离线实时概念区别梳理</title>
    <link href="https://dataquaner.github.io/2021/03/26/da-shu-ju-ji-suan-chi-xian-ji-suan-vs-shi-shi-ji-suan-vs-pi-ji-suan-vs-liu-ji-suan/"/>
    <id>https://dataquaner.github.io/2021/03/26/da-shu-ju-ji-suan-chi-xian-ji-suan-vs-shi-shi-ji-suan-vs-pi-ji-suan-vs-liu-ji-suan/</id>
    <published>2021-03-26T13:25:00.000Z</published>
    <updated>2021-03-28T16:11:34.096Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-离线计算VS实时计算"><a href="#1-离线计算VS实时计算" class="headerlink" title="1. 离线计算VS实时计算"></a>1. 离线计算VS实时计算</h1><h2 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a>离线计算</h2><p>​      离线计算，通常也称为“批处理”，表示那些离线批量、延时较高的<strong>静态数据</strong>处理过程。<br>离线计算适用于实时性要求不高的场景，比如离线报表、数据分析等，延时一般在分钟级或小时级，多数场景是定时周期性执行一个Job任务，任务周期可以小到分钟级，比如每五分钟做一次统计分析，大到月级别、年级别，比如每月执行一次任务。我们最熟悉的MapReduce就是一个离线计算框架，Spark SQL也通常用于离线计算任务。</p><h2 id="实时计算"><a href="#实时计算" class="headerlink" title="实时计算"></a>实时计算</h2><p>​      实时计算，通常也称为“实时流计算”、“流式计算”，表示那些实时或者低延时的<strong>动态流数据</strong>处理过程。<br>实时计算通常应用在实时性要求高的场景，比如实时ETL、实时监控等，延时一般都在毫秒级甚至更低。目前比较流行的实时框架有Spark Streaming与Flink。其中，Spark Streaming属于微批处理，是一种把流当作一种批的设计思想，具有非常高的吞吐量但延时也较高，这使得Streaming的场景也得到了一定的限制；Flink则是事件驱动的流处理引擎，是一种把批当作一种有限的流的设计思想，具有高吞吐，低延时，高性能的特点</p><ul><li><p>离线和实时应该指的是：数据处理的延迟；</p></li><li><p>批量和流式指的是：数据处理的方式。</p></li></ul><blockquote><p><strong>首先先说下批量计算和流式计算：</strong></p><p>图中显示了一个计算的基本流程，receiver处负责从数据源接收数据，并发送给下游的task，数据由task处理后由sink端输出。</p><p>以图为例，批量和流式处理数据粒度不一样，批量每次处理一定大小的数据块（输入一般采用文件系统），一个task处理完一个数据块之后，才将处理好的中间数据发送给下游。流式计算则是以record为单位，task在处理完一条记录之后，立马发送给下游。</p><p>假如我们是对一些固定大小的数据做统计，那么采用批量和流式效果基本相同，但是流式有一个好处就是可以实时得到计算中的结果，这对某些应用很有帮助，比如每1分钟统计一下请求server的request次数。</p><p><strong>那问题来了</strong>，既然流式系统也可以做批量系统的事情，而且还提供了更多的功能，那为什么还需要批量系统呢？因为早期的流式系统并不成熟，存在如下问题：</p><p>1.流式系统的吞吐不如批量系统</p><p>2.流式系统无法提供精准的计算</p><p><em>后面的介绍Storm、Spark streaming、Flink主要根据这两点来进行介绍。</em></p><p><strong>批量和流式的区别：</strong></p><p><strong>1.数据处理单位：</strong></p><p>批量计算按数据块来处理数据，每一个task接收一定大小的数据块，比如MR，map任务在处理完一个完整的数据块后（比如128M），然后将中间数据发送给reduce任务。</p><p>流式计算的上游算子处理完一条数据后，会立马发送给下游算子，所以一条数据从进入流式系统到输出结果的时间间隔较短（当然有的流式系统为了保证吞吐，也会对数据做buffer）。</p><p>这样的结果就是：批量计算往往得等任务全部跑完之后才能得到结果，而流式计算则可以实时获取最新的计算结果。</p><p><strong>2.数据源：</strong></p><p>批量计算通常处理的是有限数据（bound data），数据源一般采用文件系统，而流式计算通常处理无限数据（unbound data），一般采用消息队列作为数据源。</p><p><strong>3.任务类型：</strong></p><p>批量计算中的每个任务都是短任务，任务在处理完其负责的数据后关闭，而流式计算往往是长任务，每个work一直运行，持续接受数据源传过来的数据。</p><p><strong>离线=批量？实时=流式？</strong></p><p>习惯上我们认为<strong>离线和批量等价；实时和流式等价</strong>，但其实这种观点并不完全正确。</p><p>假设一种情况：当我们拥有一个非常强大的硬件系统，可以毫秒级的处理Gb级别的数据，那么批量计算也可以毫秒级得到统计结果（当然这种情况非常极端，目前不可能），那我们还能说它是离线计算吗？</p><blockquote><p><strong>*所以说离线和实时应该指的是：数据处理的延迟；批量和流式指的是：数据处理的方式。</strong>两者并没有必然的关系。事实上Spark streaming就是采用小批量（batch）的方式来实现实时计算。*</p></blockquote><p><em>可以参考下面链接：<a href="https://link.zhihu.com/?target=https%3A//www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101</a>。作者是Google实时计算的负责人，里面阐述了他对批量和实时的理解，并且作者认为批量计算只是流式计算的子集，一个设计良好的流式系统完全可以替代批量系统。本人也从中受到了很多启发。</em></p></blockquote><h1 id="2-实时查询-Vs-即席查询"><a href="#2-实时查询-Vs-即席查询" class="headerlink" title="2. 实时查询 Vs 即席查询"></a>2. 实时查询 Vs 即席查询</h1><h2 id="实时查询"><a href="#实时查询" class="headerlink" title="实时查询"></a>实时查询</h2><p>实时查询，通常也称为<strong>在线查询</strong>，是对不断变化的数据进行实时的查询，要求数据修改后能够快速被查询到。通常我们见到的实时查询多是API的方式，少数以SQL方式。在线查询场景中最常见的生态组件大概就是HBase了，HBase能够提供强一致性的低延时数据访问，非常适合一般的在线业务。</p><h2 id="即席查询"><a href="#即席查询" class="headerlink" title="即席查询"></a>即席查询</h2><p>即席查询，英文名称为Ad hoc query，起初是在数据仓库领域中用户根据特定需求定义的一种实时查询方式。通常情况下，即席查询的表现是借助于大数据SQL查询组件进行交互式查询，比如Hive、Impala、Presto等SQL查询组件。因此严格意义上说，即席查询和上述中的实时查询还是有一定区别的。</p><h1 id="3-OLTP-Vs-OLAP"><a href="#3-OLTP-Vs-OLAP" class="headerlink" title="3. OLTP Vs OLAP"></a>3. OLTP Vs OLAP</h1><h2 id="OLTP"><a href="#OLTP" class="headerlink" title="OLTP"></a>OLTP</h2><p>OLTP（On-Line Transaction Processing），可称为<strong>在线事务处理</strong>，一般应用于在线业务交易系统，比如银行交易、订单交易等。OLTP的主要特点是能够支持频繁的在线操作（增删改），以及快速的访问查询。因为要用于在线交易，所以一般要求支持事务特性。</p><h2 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a>OLAP</h2><p>OLAP（On-Line Analytical Processing），可称为<strong>在线分析处理</strong>，较多的应用在数据仓库领域，支持复杂查询的数据分析，侧重于为业务提供决策支持。目前常见是的实时OLAP场景，比如Druid（Apache Druid，不同于阿里Druid）、ClickHouse等存储组件能够较好的满足需求。</p><h1 id="4-行式存储-Vs-列式存储"><a href="#4-行式存储-Vs-列式存储" class="headerlink" title="4. 行式存储 Vs 列式存储"></a>4. 行式存储 Vs 列式存储</h1><h2 id="行式存储"><a href="#行式存储" class="headerlink" title="行式存储"></a>行式存储</h2><p>行式存储（Row-based），简称“行存”，我们常见的关系型数据库比如MySQL、Oracle、DB2、SQL Server等都是采用行存的方式。总的来说，<strong>行存有利于写，但缺不利于读，</strong>因为行存是把同一条数据存放在相同位置，这样增删改比较高效，但是查询时会增加io的消耗。从上面举例我们也能看出，<strong>行存一般应用于OLTP场景</strong>。</p><h2 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h2><p>列式存储（Column-based），简称“列存”，这里是相对于行式存储的一种数据存储方式，一般应用于分布式存储/数据库中。总的来说，<strong>列存有利于读，但不利于写</strong>，这就意味着写路径上的增删改有一定的性能损耗。常见的列存包括Parquet、Arrow等，其最大特点是能够减少不必要的io消耗，主要表现在列裁剪与列压缩方面。与行存相反，<strong>列存更适应于OLAP场景</strong>。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>【1】<a href="https://zhuanlan.zhihu.com/p/42783335" target="_blank" rel="noopener">实时计算And 离线计算，都是计算啊，区别搁哪呢？</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-离线计算VS实时计算&quot;&gt;&lt;a href=&quot;#1-离线计算VS实时计算&quot; class=&quot;headerlink&quot; title=&quot;1. 离线计算VS实时计算&quot;&gt;&lt;/a&gt;1. 离线计算VS实时计算&lt;/h1&gt;&lt;h2 id=&quot;离线计算&quot;&gt;&lt;a href=&quot;#离线计算&quot; c
      
    
    </summary>
    
    
      <category term="BigData" scheme="https://dataquaner.github.io/categories/BigData/"/>
    
    
      <category term="DataComputing" scheme="https://dataquaner.github.io/tags/DataComputing/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师面试资料汇总</title>
    <link href="https://dataquaner.github.io/2021/02/18/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20210217/"/>
    <id>https://dataquaner.github.io/2021/02/18/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20210217/</id>
    <published>2021-02-17T17:25:00.000Z</published>
    <updated>2021-02-21T15:21:14.485Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>[TOC]</p><h1 id="一-Hadoop篇"><a href="#一-Hadoop篇" class="headerlink" title="一. Hadoop篇"></a>一. Hadoop篇</h1><h2 id="1-并行计算模型MapReduce"><a href="#1-并行计算模型MapReduce" class="headerlink" title="1. 并行计算模型MapReduce"></a>1. 并行计算模型MapReduce</h2><h3 id="1-1-MapReduce-的工作原理"><a href="#1-1-MapReduce-的工作原理" class="headerlink" title="1.1 MapReduce 的工作原理"></a>1.1 MapReduce 的工作原理</h3><blockquote><p>​       MapReduce是一个基于集群的计算<strong>平台</strong>，是一个简化分布式编程的计算<strong>框架</strong>，是一个将分布式计算抽象为<strong>Map</strong>和<strong>Reduce</strong>两个阶段的编程<strong>模型</strong>。<em>（这句话记住了是可以用来装逼的）</em></p></blockquote><p><img src="http://dl.iteye.com/upload/attachment/0066/0130/e1090dee-ee98-30d1-ad55-2f88f774fa73.jpg" alt="img"></p><p><strong>执行步骤：</strong>切片&gt;分词&gt;映射&gt;分区&gt;排序&gt;聚合&gt;shuffle&gt;reduce</p><p>1）<strong>Map()阶段</strong></p><ul><li><p>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数</p></li><li><p>重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出</p></li><li><p>对输出的key、value进行分区</p></li><li><p>对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中</p></li></ul><p>2）<strong>Reduce阶段</strong></p><ul><li>多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上</li><li>对多个map的输出进行合并、排序。</li><li>重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出</li><li>把reduce的输出保存到文件中</li></ul><p><strong>特别说明：</strong></p><p><strong>切片</strong> 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理</p><p><strong>分词</strong>，<strong>映射</strong>，<strong>分区</strong>，<strong>排序</strong>，<strong>聚合</strong> 都属map阶段</p><p><strong>混洗</strong>  横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段</p><p><strong>规约</strong> 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义</p><p>分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容</p><p><strong>补充</strong></p><p><strong>切片：</strong>HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</p><p><strong>Map任务的数量</strong>：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。<strong>大多数情况下，理想的分片大小是一个HDFS块</strong></p><p><strong>Reduce任务的数量：</strong> <strong>最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</strong> 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</p><h3 id="1-2-MapReduce中shuffle工作流程及优化"><a href="#1-2-MapReduce中shuffle工作流程及优化" class="headerlink" title="1.2 MapReduce中shuffle工作流程及优化"></a>1.2 MapReduce中shuffle工作流程及优化</h3><blockquote><p>shuffle主要功能是把map task的输出结果有效地传送到reduce端。</p><p>简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p><p>前奏：</p><p><strong>1.</strong> 在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。</p><p><strong>2.</strong> 在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。前面我们知道这个job有3个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。</p></blockquote><p>主要工作流程map端分区，排序，溢写，拷贝，reduce端合并</p><h4 id="1）Map端shuffle"><a href="#1）Map端shuffle" class="headerlink" title="1）Map端shuffle"></a>1）Map端shuffle</h4><p><img src="https://img-blog.csdnimg.cn/20190705232803270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190705232832585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><ul><li><p><strong>分区Partition</strong></p><p>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。 </p></li><li><p><strong>写入内存缓冲区：</strong> 在我们的例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入<strong>内存缓冲区</strong>中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。 这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为<strong>Spill</strong>，中文可译为<strong>溢写</strong>，字面意思很直观。</p></li><li><p><strong>溢写Spill：</strong> 这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</p></li><li><p><strong>排序Sort：</strong> 当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。 </p></li><li><p><strong>合并Map端</strong>：在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节在于，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p></li><li><p><strong>CombineReduce端</strong>：在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。 </p></li></ul><h4 id="2）Reduce端shuffle"><a href="#2）Reduce端shuffle" class="headerlink" title="2）Reduce端shuffle"></a>2）Reduce端shuffle</h4><p><strong>1.</strong> <strong>Copy过程</strong>，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 </p><p><strong>2.</strong> <strong>Merge阶段</strong>。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。</p><p>增加combiner，压缩溢写的文件。</p><p><strong>3.</strong> <strong>Reducer的输入文件</strong>。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。至于怎样才能让这个文件出现在内存中，之后的<a href="http://langyu.iteye.com/blog/1341267" target="_blank" rel="noopener">性能优化篇</a>我再说。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。 </p><p><img src="https://img-blog.csdnimg.cn/20190705233112741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190705233139761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="3）shuffle优化"><a href="#3）shuffle优化" class="headerlink" title="3）shuffle优化"></a>3）shuffle优化</h4><ul><li><p><strong>压缩</strong>：对数据进行压缩，减少写读数据量；</p></li><li><p><strong>减少不必要的排序</strong>：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；</p></li><li><p><strong>内存化</strong>：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；</p><p>补充</p><blockquote><p><strong>1. Map端</strong></p><p><strong>1) io.sort.mb</strong></p><p>用于map输出排序的内存缓冲区大小</p><p>类型：Int</p><p>默认：100mb</p><p>备注：如果能估算map输出大小，就可以合理设置该值来尽可能<strong>减少溢出写的次数</strong>，这对调优很有帮助。</p><p><strong>2) io.sort.spill.percent</strong></p><p>map输出排序时的spill阀值（即使用比例达到该值时，将缓冲区中的内容spill 到磁盘）</p><p>类型：float</p><p>默认：0.80</p><p><strong>3) io.sort.factor</strong></p><p>归并因子（归并时的最多合并的流数），map、reduce阶段都要用到</p><p>类型：Int</p><p>默认：10</p><p>备注：将此值增加到100是很常见的。</p><p><strong>4) min.num.spills.for.combine</strong></p><p>运行combiner所需的最少溢出写文件数（如果已指定combiner）</p><p>类型：Int</p><p>默认：3</p><p><strong>5) mapred.compress.map.output</strong></p><p>map输出是否压缩</p><p>类型：Boolean</p><p>默认：false</p><p>备注：如果map输出的数据量非常大，那么在写入磁盘时压缩数据往往是个很好的主意，因为这样会让写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量。</p><p><strong>6) mapred.map.output.compression.codec</strong></p><p>用于map输出的压缩编解码器</p><p>类型：Classname</p><p>默认：org.apache.hadoop.io.compress.DefaultCodec</p><p>备注：推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的 TeraSort作业，运行时间减少60%，且明显快于Zlib压缩。</p><p><strong>7) tasktracker.http.threads</strong></p><p>每个tasktracker的工作线程数，用于将map输出到reducer。</p><p>（注：这是集群范围的设置，不能由单个作业设置）</p><p>类型：Int</p><p>默认：40</p><p>备注：tasktracker开http服务的线程数。用于reduce拉取map输出数据，大集群可以将其设为40~50。</p><p><strong>2. reduce端</strong></p><p><strong>1) mapred.reduce.slowstart.completed.maps</strong></p><p>调用reduce之前，map必须完成的最少比例</p><p>类型：float</p><p>默认：0.05</p><p><strong>2) mapred.reduce.parallel.copies</strong></p><p>reducer在copy阶段同时从mapper上拉取的文件数</p><p>类型：int</p><p>默认：5</p><p><strong>3) mapred.job.shuffle.input.buffer.percent</strong></p><p>在shuffle的复制阶段，分配给map输出的缓冲区占堆空间的百分比</p><p>类型：float</p><p>默认：0.70</p><p><strong>4) mapred.job.shuffle.merge.percent</strong></p><p>map输出缓冲区（由mapred.job.shuffle.input.buffer.percent定义）使用比例阀值，当达到此阀值，缓冲区中的数据将会被归并然后spill 到磁盘。</p><p>类型：float</p><p>默认：0.66</p><p><strong>5) mapred.inmem.merge.threshold</strong></p><p>map输出缓冲区中文件数</p><p>类型：int</p><p>默认：1000</p><p>备注：0或小于0的数意味着没有阀值限制，溢出写将有mapred.job.shuffle.merge.percent单独控制。</p><p><strong>6) mapred.job.reduce.input.buffer.percent</strong></p><p>在reduce过程中，在内存中保存map输出的空间占整个堆空间的比例。</p><p>类型：float</p><p>默认：0.0</p><p>备注：reduce阶段开始时，内存中的map输出大小不能大于该值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存。然而，如果reducer需要的内存较少，则可以增加此值来最小化访问磁盘的次数，以提高reduce性能。</p></blockquote></li></ul><h2 id="4-数据仓库工具Hive"><a href="#4-数据仓库工具Hive" class="headerlink" title="4. 数据仓库工具Hive"></a>4. 数据仓库工具Hive</h2><h4 id="4-1-Hive-和普通关系型数据库的区别"><a href="#4-1-Hive-和普通关系型数据库的区别" class="headerlink" title="4.1 Hive 和普通关系型数据库的区别"></a>4.1 Hive 和普通关系型数据库的区别</h4><ul><li>Hive和关系型数据库存储文件的系统不同, Hive使用的是HDFS(Hadoop的分布式文件系统),关系型数据则是服务器本地的文件系统。</li><li>Hive使用的计算模型是MapReduce,而关系型数据库则是自己设计的计算模型.</li><li>关系型数据库都是为实时查询业务设计的,而Hive则是为海量数据做挖掘而设计的,实时性差;实时性的区别导致Hive的应用场景和关系型数据库有很大区别。</li><li>Hive很容易扩展自己的存储能力和计算能力,这几是继承Hadoop的,而关系型数据库在这方面要比Hive差很多。</li></ul><h4 id="4-2-Hive内部表和外部表的区别"><a href="#4-2-Hive内部表和外部表的区别" class="headerlink" title="4.2 Hive内部表和外部表的区别"></a>4.2 Hive内部表和外部表的区别</h4><ul><li><strong>创建表时</strong>：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径， 不对数据的位置做任何改变。</li><li><strong>删除表时</strong>：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</li></ul><h4 id="4-3-Hive分区表和分桶表的区别"><a href="#4-3-Hive分区表和分桶表的区别" class="headerlink" title="4.3 Hive分区表和分桶表的区别"></a>4.3 Hive分区表和分桶表的区别</h4><p><strong>分区</strong>在HDFS上的表现形式是一个<strong>目录</strong>， <strong>分桶</strong>是一个单独的<strong>文件</strong></p><p><strong>分区</strong>: 细化数据管理，直接读对应目录，缩小mapreduce程序要扫描的数据量</p><p><strong>分桶</strong>： 1、提高join查询的效率（用分桶字段做连接字段）2、提高采样的效率</p><h4 id="4-4-Hive-支持哪些数据格式"><a href="#4-4-Hive-支持哪些数据格式" class="headerlink" title="4.4 Hive 支持哪些数据格式"></a>4.4 Hive 支持哪些数据格式</h4><p>可支持Text，SequenceFile，ParquetFile，ORC，RCFILE等</p><p><strong>补充</strong></p><blockquote><ul><li><p><strong>TextFile：</strong> TextFile文件不支持块压缩，默认格式，数据不做压缩，磁盘开销大，数据解析开销大。这边不做深入介绍。 </p></li><li><p><strong>RCFile：</strong> Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能，但是不支持模式演进。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</p></li><li><p><strong>ORCFile</strong>： 存储方式：数据按行分块 每块按照列存储 ，压缩快 快速列存取，效率比rcfile高,是rcfile的改良版本，相比RC能够更好的压缩，能够更快的查询，但还是不支持模式演进。</p></li><li><p><strong>Parquet：</strong> Parquet能够很好的压缩，有很好的查询性能，支持有限的模式演进。但是写速度通常比较慢。这中文件格式主要是用在Cloudera Impala上面的。</p></li></ul></blockquote><p><strong>性能对比</strong></p><ul><li><strong>读操作</strong></li></ul><img src="https://img-blog.csdn.net/20180206102745607?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl6enh5Y2o=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:50%;"><ul><li><p><strong>存储效率</strong></p><img src="https://img-blog.csdn.net/20180206102837722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl6enh5Y2o=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:50%;"></li></ul><h4 id="4-5-Hive元数据库作用及存储内容"><a href="#4-5-Hive元数据库作用及存储内容" class="headerlink" title="4.5 Hive元数据库作用及存储内容"></a>4.5 Hive元数据库作用及存储内容</h4><p>​     本质上只是用来存储hive中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。为数据库创建的目录一般在hive数据仓库目录下</p><h4 id="4-6-HiveSQL-支持的几种排序区别"><a href="#4-6-HiveSQL-支持的几种排序区别" class="headerlink" title="4.6 HiveSQL 支持的几种排序区别"></a>4.6 HiveSQL 支持的几种排序区别</h4><p><strong>1）Order By：全局排序，只有一个Reducer</strong></p><ul><li><p>使用 ORDER BY 子句排序</p><p>ASC（ascend）: 升序（默认）</p><p>DESC（descend）: 降序</p></li><li><p>ORDER BY 子句在SELECT语句的结尾</p></li><li><p>案例实操 </p><ul><li><p>查询员工信息按工资升序排列</p><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp <span class="token keyword">order</span> <span class="token keyword">by</span> sal<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>查询员工信息按工资降序排列</p><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp <span class="token keyword">order</span> <span class="token keyword">by</span> sal <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><p><strong>2）Sort By：每个MapReduce内部排序</strong><br>      Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ul><li>设置reduce个数</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>查看设置reduce个数</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>根据部门编号降序查看员工信息</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp sort <span class="token keyword">by</span> deptno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>将查询结果导入到文件中（按照部门编号降序排序）</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/opt/module/datas/sortby-result'</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp sort <span class="token keyword">by</span> deptno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>3）Distribute By：分区排序</strong><br>      Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>案例实操：</p><ul><li>先按照部门编号分区，再按照员工编号降序排序。</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/opt/module/datas/distribute-result'</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> deptno sort <span class="token keyword">by</span> empno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：</p><p>​    distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。<br>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br><strong>4）Cluster By</strong><br>​      当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><ul><li>以下两种写法等价</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp cluster <span class="token keyword">by</span> deptno<span class="token punctuation">;</span>hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> deptno sort <span class="token keyword">by</span> deptno<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</span><span class="token comment" spellcheck="true">--cluster by  ：sort by 和 distribute by的组合</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-7-Hive-的动态分区"><a href="#4-7-Hive-的动态分区" class="headerlink" title="4.7 Hive 的动态分区"></a>4.7 Hive 的动态分区</h4><blockquote><p>​        往hive分区表中插入数据时，如果需要创建的分区很多，比如以表中某个字段进行分区存储，则需要复制粘贴修改很多sql去执行，效率低。因为hive是批处理系统，<strong>所以hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。</strong></p></blockquote><ul><li><p><strong>使用动态分区表必须配置的参数</strong></p><ul><li><code>set hive.exec.dynamic.partition =true</code>（默认false）,表示开启动态分区功能；</li><li><code>set hive.exec.dynamic.partition.mode = nonstrict</code>(默认strict),表示允许所有分区都是动态的，否则必须有静态分区字段；</li></ul></li><li><p><strong>动态分区相关调优参数</strong></p><ul><li><code>set  hive.exec.max.dynamic.partitions.pernode=100</code> （默认100，一般可以设置大一点，比如1000）； 表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。</li><li><code>set hive.exec.max.dynamic.partitions =1000</code>(默认值) ；  表示一个动态分区语句可以创建的最大动态分区个数，超出报错；</li><li><code>set hive.exec.max.created.files =10000</code>(默认) 全局可以创建的最大文件个数，超出报错。</li></ul></li></ul><h4 id="4-8-Hive-MapJoin"><a href="#4-8-Hive-MapJoin" class="headerlink" title="4.8 Hive MapJoin"></a>4.8 Hive MapJoin</h4><blockquote><p>​     MapJoin是Hive的一种优化操作，其适用于小表JOIN大表的场景，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率</p></blockquote><p><strong>使用</strong></p><p><strong>方法一：</strong></p><p>在Hive0.11前，必须使用MAPJOIN来标记显示地启动该优化操作，由于其需要将小表加载进内存所以要注意小表的大小</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token comment" spellcheck="true">/*+ MAPJOIN(smalltable)*/</span>  <span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">,</span><span class="token keyword">value</span><span class="token keyword">FROM</span> smalltable <span class="token keyword">JOIN</span> bigtable <span class="token keyword">ON</span> smalltable<span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> bigtable<span class="token punctuation">.</span><span class="token keyword">key</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>方法二</strong>：</p><p>在Hive0.11后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机</p><pre class="line-numbers language-sql"><code class="language-sql">hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>默认值为true，自动开启MAPJOIN优化</p><pre class="line-numbers language-sql"><code class="language-sql">hive<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>smalltable<span class="token punctuation">.</span>filesize<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中 </p><p>注意：使用默认启动该优化的方式如果出现默名奇妙的BUG(比如MAPJOIN并不起作用),就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化</p><pre class="line-numbers language-shell"><code class="language-shell">hive.auto.convert.join=false(关闭自动MAPJOIN转换操作)hive.ignore.mapjoin.hint=false(不忽略MAPJOIN标记)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> 对于以下查询是不支持使用方法二(MAPJOIN标记)来启动该优化的</p><pre class="line-numbers language-shell"><code class="language-shell">select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                    ) firstjoin                                                              JOIN                                                                   smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是，如果使用的是方法一即没有MAPJOIN标记则以上查询语句将会被作为两个MJ执行，进一步的，如果预先知道表大小是能够被加载进内存的，则可以通过以下属性来将两个MJ合并成一个MJ</p><pre class="line-numbers language-shell"><code class="language-shell">hive.auto.convert.join.noconditionaltask：Hive在基于输入文件大小的前提下将普通JOIN转换成MapJoin，并是否将多个MJ合并成一个hive.auto.convert.join.noconditionaltask.size：多个MJ合并成一个MJ时，其表的总的大小须小于该值，同时hive.auto.convert.join.noconditionaltask必须为true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="4-9-HQL-和-SQL-有哪些常见的区别"><a href="#4-9-HQL-和-SQL-有哪些常见的区别" class="headerlink" title="4.9 HQL 和 SQL 有哪些常见的区别"></a>4.9 HQL 和 SQL 有哪些常见的区别</h4><ul><li><p><strong>总体一致</strong>：Hive-sql与SQL基本上一样，因为当初的设计目的，就是让会SQL不会编程MapReduce的也能使用Hadoop进行处理数据。</p></li><li><p><strong>区别：</strong>Hive没有delete和update。</p><ul><li><p><strong>Hive不支持等值连接</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--SQL中对两表内联可以写成：</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dual <span class="token number">a</span><span class="token punctuation">,</span>dual <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">--Hive中应为</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dual <span class="token number">a</span> <span class="token keyword">join</span> dual <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--而不是传统的格式：</span><span class="token keyword">SELECT</span> t1<span class="token number">.a1</span> <span class="token keyword">as</span> <span class="token number">c1</span><span class="token punctuation">,</span> t2<span class="token number">.b1</span> <span class="token keyword">as</span> c2FROM t1<span class="token punctuation">,</span> t2<span class="token keyword">WHERE</span> t1<span class="token number">.a2</span> <span class="token operator">=</span> t2<span class="token number">.b2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>分号字符</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如：</span><span class="token keyword">select</span> concat<span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span>concat<span class="token punctuation">(</span><span class="token string">';'</span><span class="token punctuation">,</span><span class="token keyword">key</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> dual<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--但HiveQL在解析语句时提示：</span>FAILED: Parse Error: line <span class="token number">0</span>:<span class="token operator">-</span><span class="token number">1</span> mismatched input <span class="token string">'&lt;EOF>'</span> expecting <span class="token punctuation">)</span> <span class="token operator">in</span> <span class="token keyword">function</span> specification<span class="token comment" spellcheck="true">--解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成：</span><span class="token keyword">select</span> concat<span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span>concat<span class="token punctuation">(</span><span class="token string">'\073'</span><span class="token punctuation">,</span><span class="token keyword">key</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> dual<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>IS [NOT] NULL</strong></p><pre class="line-numbers language-sql"><code class="language-sql"> <span class="token comment" spellcheck="true">--SQL中null代表空值, 值得警惕的是, </span> <span class="token comment" spellcheck="true">--在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>Hive不支持将数据插入现有的表或分区中</strong>        </p></li><li><p><strong>hive不支持INSERT INTO 表 Values（）, UPDATE, DELETE操作</strong></p></li><li><p><strong>hive支持嵌入mapreduce程序，来处理复杂的逻辑</strong><br>如：</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">FROM</span> <span class="token punctuation">(</span> <span class="token number">1</span><span class="token punctuation">.</span> MAP doctext <span class="token keyword">USING</span> <span class="token string">'python wc_mapper.py'</span> <span class="token keyword">AS</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> cnt<span class="token punctuation">)</span> <span class="token number">2</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> docs <span class="token number">3</span><span class="token punctuation">.</span> CLUSTER <span class="token keyword">BY</span> word <span class="token number">4</span><span class="token punctuation">.</span> <span class="token punctuation">)</span> <span class="token number">a</span> <span class="token number">5</span><span class="token punctuation">.</span> REDUCE word<span class="token punctuation">,</span> cnt <span class="token keyword">USING</span> <span class="token string">'python wc_reduce.py'</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--doctext: 是输入</span><span class="token comment" spellcheck="true">--word, cnt: 是map程序的输出</span><span class="token comment" spellcheck="true">--CLUSTER BY: 将wordhash后，又作为reduce程序的输入并且map程序、reduce程序可以单独使用，如：</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> <span class="token punctuation">(</span> <span class="token number">2</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> session_table <span class="token number">3</span><span class="token punctuation">.</span> <span class="token keyword">SELECT</span> sessionid<span class="token punctuation">,</span> tstamp<span class="token punctuation">,</span> <span class="token keyword">data</span> <span class="token number">4</span><span class="token punctuation">.</span> DISTRIBUTE <span class="token keyword">BY</span> sessionid SORT <span class="token keyword">BY</span> tstamp <span class="token number">5</span><span class="token punctuation">.</span> <span class="token punctuation">)</span> <span class="token number">a</span> <span class="token number">6</span><span class="token punctuation">.</span> REDUCE sessionid<span class="token punctuation">,</span> tstamp<span class="token punctuation">,</span> <span class="token keyword">data</span> <span class="token keyword">USING</span> <span class="token string">'session_reducer.sh'</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--DISTRIBUTE BY: 用于给reduce程序分配行数据</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h4 id="4-10-Hive开窗函数"><a href="#4-10-Hive开窗函数" class="headerlink" title="4.10 Hive开窗函数"></a>4.10 Hive开窗函数</h4><p>假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。</p><p><img src="https://pic3.zhimg.com/80/v2-008682fd90478af4fb84e88bccd480ee_1440w.jpg" alt="img"></p><p><strong>1. SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">/*前三次贷款的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token number">3</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token keyword">CURRENT</span> <span class="token keyword">ROW</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv1<span class="token punctuation">,</span><span class="token comment" spellcheck="true">/*历史所有贷款 累加到下一次贷款 的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token keyword">UNBOUNDED</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token number">1</span> <span class="token keyword">FOLLOWING</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv2<span class="token keyword">FROM</span> loan <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​      其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。</p><p>窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用：</p><ul><li>n PRECEDING : 前n行    n preceding</li><li>n FOLLOWING：后n行</li><li>CURRENT ROW ： 当前行</li></ul><p>如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为:</p><p>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.</p><p>窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。</p><p><strong>2. NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#将数据按name切分成10区，并返回属于第几个分区</span>NTILE<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>ROW_NUMBER<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f3</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>DENSE_RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f4</span><span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中第一个函数<a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">NTILE(10)</a>是将数据按name切分成10区，并返回属于第几个分区。</p><blockquote><p>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p><p>然后可以根据桶号，选取前或后 n分之几的数据。</p></blockquote><p>后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。</p><ul><li>ROW_NUMBER()返回的是一列连续的序号。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-0f2c6da71227f7840aea5257acf8d88b_1440w.png" alt="img"></p><ul><li>RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}.</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f0bd3985ddacd2f347151dacc24cce_1440w.png" alt="img"></p><ul><li>DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-ac60db4d8a9c658ddf17fb43f09f616d_1440w.png" alt="img"></p><p><strong>3. LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取上一笔贷款的日期,缺失默认填NULL</span>LAG<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> last_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取下一笔贷款的日期,缺失指定填'1970-1-1'</span>LEAD<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'1970-1-1'</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> next_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取最早一笔贷款的日期</span>FIRST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> first_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取新一笔贷款的日期</span>LAST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> latest_dt<span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/pelifymeng2/article/details/70313943" target="_blank" rel="noopener">LAG(n)</a>将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录</p><p><strong>4. GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdateGROUPING SETS<span class="token punctuation">(</span>substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> orderdate<span class="token punctuation">)</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。</p><p>比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5.</p><p>如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null.</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdate<span class="token keyword">with</span> CUBE<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdate<span class="token keyword">with ROLLUP</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。</p><h4 id="4-11-HiveUDF-UDAF-UDTF函数"><a href="#4-11-HiveUDF-UDAF-UDTF函数" class="headerlink" title="4.11 HiveUDF UDAF UDTF函数"></a>4.11 HiveUDF UDAF UDTF函数</h4><p><strong>UDF：一进一出</strong></p><p><strong>实现方法</strong>：</p><ol><li><p>继承UDF类</p></li><li><p>重写evaluate方法</p></li><li><p>将该java文件编译成jar</p></li><li><p>在终端输入如下命令：</p><pre class="line-numbers language-powershell"><code class="language-powershell">hive> add jar test<span class="token punctuation">.</span>jar<span class="token punctuation">;</span>hive> create temporary <span class="token keyword">function</span> function_name as <span class="token string">'com.hrj.hive.udf.UDFClass'</span><span class="token punctuation">;</span>hive> <span class="token function">select</span> function_name<span class="token punctuation">(</span>t<span class="token punctuation">.</span>col1<span class="token punctuation">)</span> <span class="token keyword">from</span> table t<span class="token punctuation">;</span>hive> drop temporary <span class="token keyword">function</span> function_name<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p><strong>UDAF：多进一出</strong></p><p><strong>实现方法</strong>: </p><ol><li><p>用户的UDAF必须继承了<code>org.apache.hadoop.hive.ql.exec.UDAF；</code></p></li><li><p>用户的UDAF必须包含至少一个实现了<code>org.apache.hadoop.hive.ql.exec</code>的静态类，诸如实现了 UDAFEvaluator</p></li><li><p>一个计算函数必须实现的5个方法的具体含义如下：</p><ul><li><strong>init()：</strong>主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。</li><li><strong>iterate()：</strong>每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。</li><li><strong>terminatePartial()：</strong>Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。</li><li><strong>merge()：</strong>Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。</li><li><strong>terminate()：</strong>Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。</li></ul></li><li><p>部分聚集结果的数据类型和最终结果的数据类型可以不同。</p></li></ol><p><strong>UDTF：一进多出</strong></p><p><strong>实现方法</strong>：</p><ol><li><p>继承<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</code></p></li><li><p><strong>initialize()</strong>：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）</p></li><li><p><strong>process：</strong>初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列      可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数</p></li><li><p>最后close()方法调用，对需要清理的方法进行清理</p></li></ol><h4 id="4-12-Hive数据倾斜问题"><a href="#4-12-Hive数据倾斜问题" class="headerlink" title="4.12 Hive数据倾斜问题"></a>4.12 Hive数据倾斜问题</h4><p><strong>0. 什么是数据倾斜</strong></p><blockquote><p>​        对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。</p><p>来源百度百科</p></blockquote><p>​        对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><p><strong>1. 数据倾斜的现象</strong></p><p>​       多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。</p><p><strong>2. 数据倾斜的影响</strong></p><p>1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：<strong>Out Of Memory</strong>。</p><p>2）<strong>运行速度慢</strong> ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。</p><p><img src="https://pic1.zhimg.com/80/v2-b26e15f4b1c3ce2f78fba64397b6fd60_1440w.jpg" alt="img"></p><p><strong><em>一个经验结论是：一般情况下，OOM的原因都是数据倾斜\</em></strong></p><p><strong>3. 如何定位数据倾斜</strong></p><p>​         数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p><p><strong>原因</strong>： 查看任务-》查看Stage-》查看代码</p><p>​        某个task执行特别慢的情况</p><p>​        某个task莫名其妙内存溢出的情况</p><p>​        查看导致数据倾斜的key的数据分布情况</p><p><img src="https://pic1.zhimg.com/80/v2-b1b26a9b5e6a1d68d9aea4d1f2bc551c_1440w.jpg" alt="img"></p><p>也可从以下几种情况考虑：</p><p>1、是不是有OOM情况出现，一般是少数内存溢出的问题</p><p>2、是不是应用运行时间差异很大，总体时间很长</p><p>3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题</p><p>4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断</p><p>5、看看代码里面是否有一些导致Shuffle的算子出现</p><p><strong>4. 数据倾斜的几种典型情况（重点）</strong></p><ul><li>数据源中的数据分布不均匀，Spark需要频繁交互</li><li>数据集中的不同Key由于分区方式，导致数据倾斜</li><li>JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</li><li>聚合操作中，数据集中的数据分布不均匀（主要）</li><li>JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</li><li>JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</li><li>数据集中少数几个key数据量很大，不重要，其他数据均匀</li></ul><p>注意：</p><ul><li><p>需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题</p></li><li><p>只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以</p></li></ul><p><strong>5. 数据倾斜的处理方法</strong></p><p>​         发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。</p><p><strong>5.1 检查数据，找出异常的key</strong></p><p>​          如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。</p><p>选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"key"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​        如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况:</p><ul><li><p>null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。</p></li><li><p>无效数据，大量重复的测试数据或是对结果影响不大的有效数据。</p></li><li><p>有效数据，业务导致的正常数据分布。</p></li></ul><p><strong>解决办法</strong><br>  第1，2种情况，直接对数据进行过滤即可。</p><p>  第3种情况则需要进行一些特殊操作，常见的有以下几种做法。</p><ul><li><p>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。</p></li><li><p>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</p></li><li><p>使用reduceByKey 代替 groupByKey</p></li><li><p>使用map join。</p><p><strong>举例</strong>：<br>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下：</p><p>将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>对数据进行 reduceByKey(func)<br>将 key + 随机值 转成 key<br>再对数据进行 reduceByKey(func)<br>tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。</p><p>tips2: 单独处理异常数据时，可以配合使用Map Join解决</p></li></ul><p><strong>5.1.1</strong> 数据源中的数据分布不均匀，Spark需要频繁交互</p><p><strong>解决方案</strong>1：避免数据源的数据倾斜</p><p><strong>实现原理</strong>：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive或者Kafka中还是会发生数据倾斜。</p><p><strong>适用情况</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>总结</strong>：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题</p><p><strong>5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜</strong></p><p><strong>解决方案1</strong>：调整并行度</p><p><strong>实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。</p><p><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。</p><p><img src="https://pic4.zhimg.com/80/v2-9a1722a9ceb6fe125f7b36715f6dcfff_1440w.jpg" alt="img"></p><p><strong>总结</strong>：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效</p><p><strong>解决方案2</strong>：</p><p><strong>缓解数据倾斜**</strong>（自定义Partitioner）**</p><p><strong>适用场景</strong>：大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p><p><strong>解决方案</strong>： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p><p><strong>优势</strong>： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong>： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><p><strong>5.2 检查Spark运行过程相关操作</strong></p><p><strong>5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</strong></p><p><strong>解决方案</strong>：Reduce side Join转变为Map side Join</p><p><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。</p><p><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。</p><p><strong>5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）</strong></p><p><strong>解决方案</strong>：两阶段聚合（局部聚合+全局聚合）</p><p><strong>适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案</p><p><strong>实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案</p><p>将相同key的数据分拆处理</p><p><img src="https://pic3.zhimg.com/80/v2-495a5fed7eb38db37d2f0bd13c45a30e_1440w.jpg" alt="img"></p><p><strong>5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</strong></p><p><strong>解决方案</strong>：为倾斜key增加随机前/后缀</p><p><strong>适用场景</strong>：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong>：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong>：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong>：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>注意</strong>：具有倾斜Key的RDD数据集中，key的数量比较少</p><p><img src="https://pic4.zhimg.com/80/v2-248b0cead5e9fb8a7b1cec840dd61b2f_1440w.jpg" alt="img"></p><p><strong>5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</strong></p><p><strong>解决方案</strong>：随机前缀和扩容RDD进行join</p><p><strong>适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。</p><p><strong>实现思路</strong>：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><p>注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀）</p><p><img src="https://pic4.zhimg.com/80/v2-fa2211e3a343d7b68e83bfe83d67f0cb_1440w.jpg" alt="img"></p><p><strong>5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀</strong></p><p><strong>解决方案</strong>：过滤少数倾斜Key</p><p><strong>适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h4 id="4-13-HiveSQL-的优化（系统参数调整、SQL-语句优化）"><a href="#4-13-HiveSQL-的优化（系统参数调整、SQL-语句优化）" class="headerlink" title="4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化）"></a>4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化）</h4><ul><li><p><strong>Hive优化目标</strong></p></li><li><ul><li>在有限的资源下，执行效率更高</li></ul></li><li><p>常见问题</p></li><li><ul><li>数据倾斜</li><li>map数设置</li><li>reduce数设置</li><li>其他</li></ul></li><li><p><strong>Hive执行</strong></p></li><li><ul><li><p>HQL –&gt; Job –&gt; Map/Reduce</p></li><li><p>执行计划</p></li><li><ul><li>explain [extended] hql</li><li>样例</li><li>select col,count(1) from test2 group by col;</li><li>explain select col,count(1) from test2 group by col;</li></ul></li></ul></li><li><p><strong>Hive表优化</strong></p></li><li><ul><li><p>分区</p></li><li><ul><li>set hive.exec.dynamic.partition=true;</li><li>set hive.exec.dynamic.partition.mode=nonstrict;</li><li>静态分区</li><li>动态分区</li></ul></li><li><p>分桶</p></li><li><ul><li>set hive.enforce.bucketing=true;</li><li>set hive.enforce.sorting=true;</li></ul></li><li><p>数据</p></li><li><ul><li>相同数据尽量聚集在一起</li></ul></li></ul></li><li><p><strong>Hive Job优化</strong></p></li><li><ul><li><p><strong>并行化执行</strong></p></li><li><ul><li>每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间</li><li>set hive.exec.parallel= true;</li><li>set hive.exec.parallel.thread.numbe=8;</li></ul></li><li><p><strong>本地化执行</strong></p></li><li><ul><li>job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB)</li><li>job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4)</li><li>job的reduce数必须为0或者1</li><li>set hive.exec.mode.local.auto=true;</li><li>当一个job满足如下条件才能真正使用本地模式:</li></ul></li><li><p><strong>job合并输入小文件</strong></p></li><li><ul><li>set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</li><li>合并文件数由mapred.max.split.size限制的大小决定</li></ul></li><li><p><strong>job合并输出小文件**</strong></p></li><li><ul><li>set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件</li><li>set hive.merge.size.per.task=64000000;合并之后的文件大小</li></ul></li><li><p><strong>JVM重利用</strong></p></li><li><ul><li>set mapred.job.reuse.jvm.num.tasks=20;</li><li>JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。</li></ul></li><li><p>压缩数据</p></li><li><ul><li>set hive.exec.compress.output=true;</li><li>set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec;</li><li>set mapred.output.compression.type=BLOCK;</li><li>set hive.exec.compress.intermediate=true;</li><li>set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</li><li>set hive.intermediate.compression.type=BLOCK;</li><li>中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式</li><li>hive查询最终的输出也可以压缩</li></ul></li></ul></li><li><p><strong>Hive Map优化</strong></p></li><li><ul><li><p>set mapred.map.tasks =10; 无效</p></li><li><p>(1)默认map个数</p></li><li><ul><li>default_num=total_size/block_size;</li></ul></li><li><p>(2)期望大小</p></li><li><ul><li>goal_num=mapred.map.tasks;</li></ul></li><li><p>(3)设置处理的文件大小</p></li><li><ul><li>split_size=max(mapred.min.split.size,block_size);</li><li>split_num=total_size/split_size;</li></ul></li><li><p>(4)计算的map个数</p></li><li><ul><li>compute_map_num=min(split_num,max(default_num,goal_num))</li></ul></li><li><p>经过以上的分析，在设置map个数的时候，可以简答的总结为以下几点：</p></li><li><ul><li>增大mapred.min.split.size的值</li><li>如果想增加map个数，则设置mapred.map.tasks为一个较大的值</li><li>如果想减小map个数，则设置mapred.min.split.size为一个较大的值</li><li>情况1：输入文件size巨大，但不是小文件</li><li>情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。</li></ul></li><li><p>map端聚合</p></li><li><ul><li>set hive.map.aggr=true;</li></ul></li><li><p>推测执行</p></li><li><ul><li>mapred.map.tasks.apeculative.execution</li></ul></li></ul></li><li><p><strong>Hive Shuffle优化</strong></p></li><li><ul><li><p><strong>Map端</strong></p></li><li><ul><li>io.sort.mb</li><li>io.sort.spill.percent</li><li>min.num.spill.for.combine</li><li>io.sort.factor</li><li>io.sort.record.percent</li></ul></li><li><p><strong>Reduce端</strong></p></li><li><ul><li>mapred.reduce.parallel.copies</li><li>mapred.reduce.copy.backoff</li><li>io.sort.factor</li><li>mapred.job.shuffle.input.buffer.percent</li><li>mapred.job.shuffle.input.buffer.percent</li><li>mapred.job.shuffle.input.buffer.percent</li></ul></li></ul></li><li><p><strong>Hive Reduce优化</strong></p></li><li><ul><li><p><strong>需要reduce操作的查询</strong></p></li><li><ul><li>group by,join,distribute by,cluster by…</li><li>order by比较特殊,只需要一个reduce</li><li>sum,count,distinct…</li><li>聚合函数</li><li>高级查询</li></ul></li><li><p><strong>推测执行</strong></p></li><li><ul><li>mapred.reduce.tasks.speculative.execution</li><li>hive.mapred.reduce.tasks.speculative.execution</li></ul></li><li><p><strong>Reduce优化</strong></p></li><li><ul><li>numRTasks = min[maxReducers,input.size/perReducer]</li><li>maxReducers=hive.exec.reducers.max</li><li>perReducer = hive.exec.reducers.bytes.per.reducer</li><li>hive.exec.reducers.max 默认 ：999</li><li>hive.exec.reducers.bytes.per.reducer 默认:1G</li><li>set mapred.reduce.tasks=10;直接设置</li><li>计算公式</li></ul></li></ul></li><li><p><strong>Hive查询操作优化</strong></p></li><li><p><strong>join优化</strong></p></li><li><ul><li>关联操作中有一张表非常小</li><li>不等值的链接操作</li><li>set hive.auto.current.join=true;</li><li>hive.mapjoin.smalltable.filesize默认值是25mb</li><li>select <code>/*+mapjoin(A)*/</code> f.a,f.b from A t join B f on (f.a=t.a)</li><li>hive.optimize.skewjoin=true;如果是Join过程出现倾斜，应该设置为true</li><li>set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化</li><li>mapjoin</li><li>简单总结下,mapjoin的使用场景:</li></ul></li><li><p><strong>Bucket join</strong></p></li><li><ul><li>两个表以相同方式划分桶</li><li>两个表的桶个数是倍数关系</li><li>crete table order(cid int,price float) clustered by(cid) into 32 buckets;</li><li>crete table customer(id int,first string) clustered by(id) into 32 buckets;</li><li>select price from order t join customer s on t.cid=s.id</li></ul></li><li><p><strong>join 优化前</strong></p></li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> m<span class="token punctuation">.</span>cid<span class="token punctuation">,</span>u<span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token keyword">order</span> m <span class="token keyword">join</span> customer u <span class="token keyword">on</span> m<span class="token punctuation">.</span>cid<span class="token operator">=</span>u<span class="token punctuation">.</span>id <span class="token keyword">where</span> m<span class="token punctuation">.</span>dt<span class="token operator">=</span><span class="token string">'2013-12-12'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><strong>join优化后</strong></li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> m<span class="token punctuation">.</span>cid<span class="token punctuation">,</span>u<span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> cid <span class="token keyword">from</span> <span class="token keyword">order</span> <span class="token keyword">where</span> dt<span class="token operator">=</span><span class="token string">'2013-12-12'</span><span class="token punctuation">)</span>m <span class="token keyword">join</span> customer u <span class="token keyword">on</span> m<span class="token punctuation">.</span>cid<span class="token operator">=</span>u<span class="token punctuation">.</span>id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p><strong>group by 优化</strong></p><p><code>hive.groupby.skewindata=true;</code>如果是group by 过程出现倾斜 应该设置为true</p><p><code>set hive.groupby.mapaggr.checkinterval=100000;-</code>-这个是group的键对应的记录条数超过这个值则会进行优化</p></li></ul><ul><li><p><strong>count distinct 优化</strong></p><ul><li><p>优化前</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> id<span class="token punctuation">)</span> <span class="token keyword">from</span> tablename<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>优化后</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token keyword">distinct</span> id <span class="token keyword">from</span> tablename<span class="token punctuation">)</span> tmp<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> tablename <span class="token keyword">group</span> <span class="token keyword">by</span> id<span class="token punctuation">)</span> tmp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>优化前</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> <span class="token number">c</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> <span class="token number">d</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>优化后</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">c</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">d</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">0</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">c</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">0</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token number">d</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">d</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span> test<span class="token punctuation">)</span>tmp1 <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><p>#二. Spark篇</p><h2 id="1-SparkCore"><a href="#1-SparkCore" class="headerlink" title="1. SparkCore"></a>1. SparkCore</h2><h3 id="1-1-Spark工作原理"><a href="#1-1-Spark工作原理" class="headerlink" title="1.1 Spark工作原理"></a>1.1 Spark工作原理</h3><h4 id="1-Spark是什么"><a href="#1-Spark是什么" class="headerlink" title="1. Spark是什么"></a>1. Spark是什么</h4><p>Spark是一种通用分布式并行计算框架。和Mapreduce最大不同就是spark是基于内存的迭代式计算。</p><blockquote><p>Spark的Job处理的中间输出结果可以保存在内存中，从而不再需要读写HDFS，除此之外，一个MapReduce 在计算过程中只有map 和reduce 两个阶段，处理之后就结束了，而在Spark的计算模型中，可以分为n阶段，因为它内存迭代式的，我们在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。 </p><p>因此<strong>Spark能更好地适用于数据挖掘与<a href="http://lib.csdn.net/base/machinelearning" target="_blank" rel="noopener">机器学习</a>等需要迭代的MapReduce的<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="noopener">算法</a></strong>。其<strong>不仅实现了MapReduce的算子map 函数和reduce函数及计算模型</strong>，还提供更为丰富的<strong>算子</strong>，如filter、join、groupByKey等。是一个用来实现快速而同用的集群计算的平台。 </p></blockquote><h4 id="2-Spark工作原理框图"><a href="#2-Spark工作原理框图" class="headerlink" title="2. Spark工作原理框图"></a>2. Spark工作原理框图</h4><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cimage-20200627101139297.png" alt="image-20200627101139297"></p><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cimage-20200627101150890.png" alt="image-20200627101150890"></p><h5 id="第一层级"><a href="#第一层级" class="headerlink" title="第一层级"></a>第一层级</h5><p>工作流程</p><p>a. 构建Spark Application的运行环境（启动SparkContext）</p><p>b. SparkContext在初始化过程中分别创建DAGScheduler作业调度和TaskScheduler任务调度两级调度模块</p><p>c. SparkContext向资源管理器（可以是Standalone、Mesos、Yarn）申请运行Executor资源；</p><p>d. 由资源管理器分配资源并启动StandaloneExecutorBackend，executor，之后向SparkContext申请Task；</p><p>e. DAGScheduler将job 划分为多个stage,并将Stage提交给TaskScheduler;</p><p>g. Task在Executor上运行，运行完毕释放所有资源。</p><h5 id="第二层级"><a href="#第二层级" class="headerlink" title="第二层级"></a>第二层级</h5><p><strong>DAGScheduler作业调度生成过程</strong></p><p><strong>DAGScheduler</strong>是一个面向stage 的作业调度器。</p><blockquote><p>作业调度模块是<strong>基于任务阶段</strong>的高层调度模块，它为每个Spark作业计算具有依赖关系的多个调度阶段（通常根据<strong>shuffle</strong>来划分），然后为每个阶段构建出一组具体的任务（通常会考虑数据的本地性等），然后以TaskSets（任务组）的形式提交给任务调度模块来具体执行。</p></blockquote><p><strong>主要三大功能</strong></p><ol><li><p><strong>接受用户提交的job</strong>。将job根据类型划分为不同的stage，记录哪些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</p></li><li><p><strong>决定每个task的最佳位置</strong>，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给<strong>TaskScheduler</strong>；</p></li><li><p><strong>重新提交shuffle输出丢失的stage给taskScheduler；</strong></p></li></ol><p><strong>DAG如何将Job划分为多个stage</strong></p><p><img src="https://img-blog.csdn.net/20180909161915933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="图解"></p><p><strong>划分依据：**</strong>宽窄依赖**。何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p><p><strong>切割规则：</strong>从后往前，遇到宽依赖就切割stage；</p><blockquote><p> Spark任务会根据<strong>RDD</strong>之间的依赖关系，形成一个<strong>DAG有向无环图</strong>，<strong>DAG</strong>会提交给<strong>DAGScheduler</strong>，<strong>DAGScheduler</strong>会把<strong>DAG</strong>划分相互依赖的多个<strong>stage</strong>，划分依据就是<strong>宽窄依赖</strong>，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以<strong>taskSet</strong>的形式提交给<strong>TaskScheduler</strong>运行，stage是由一组并行的task组成。  </p><p> <strong>一旦driver程序中出现action，就会生成一个job，比如count等</strong>，</p><p> ​     向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。</p><p> ​        每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是<strong>宽依赖</strong>，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p></blockquote><h5 id="第三层级"><a href="#第三层级" class="headerlink" title="第三层级"></a>第三层级</h5><p><strong>谈谈spark中的宽窄依赖</strong></p><p>RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="1-2-Spark的shuffle原理和过程"><a href="#1-2-Spark的shuffle原理和过程" class="headerlink" title="1.2 Spark的shuffle原理和过程"></a>1.2 Spark的shuffle原理和过程</h3><h5 id="Shuffle过程框图"><a href="#Shuffle过程框图" class="headerlink" title="Shuffle过程框图"></a>Shuffle过程框图</h5><p><img src="https://yqfile.alicdn.com/278d5baeff935b9f4339f94fc85ccab67241927b.png" alt="img"></p><p>主要逻辑如下：</p><blockquote><p>1）首先每一个MapTask会根据ReduceTask的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</p><p>2）其次MapTask产生的结果会根据设置的partition算法填充到每个bucket中。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中。</p><p>当ReduceTask启动时，它会根据自己task的id和所依赖的Mapper的id从远端或本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</p><p>这里的bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等。</p></blockquote><p>Spark shuffle可以分为两部分：Shuffle Write 和 Shuffle Fetch</p><h5 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h5><blockquote><p>由于不要求数据有序，shuffle write 的任务很简单：<strong>将数据 partition 好，并持久化</strong>。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</p></blockquote><p>shuffle write 的任务很简单，那么实现也很简单：将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。图示如下：</p><p><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/shuffle-write-no-consolidation.png" alt="shuffle write"></p><p>上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数），缓冲区被称为 bucket，其大小为<code>spark.shuffle.file.buffer.kb</code> ，默认是 32KB（Spark 1.1 版本以前是 100KB）。</p><blockquote><p>其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。</p></blockquote><p>ShuffleMapTask 的执行过程很简单：先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由<code>partitioner.partition(record.getKey()))</code>决定。每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 <strong>FileSegment</strong>。之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。</p><p>这样的实现很简单，但有几个问题：</p><ol><li><strong>产生的 FileSegment 过多。</strong>每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。</li><li><strong>缓冲区占用内存空间大。</strong>每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M <em>R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores</em> R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了<code>cores * R * 32 KB</code>。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。</li></ol><p>目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图：</p><p><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/shuffle-write-consolidation.png" alt="shuffle-write-consolidation"></p><p>可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。先执行完的 ShuffleMapTask 形成 ShuffleBlocki，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlocki’，每个 ShuffleBlock 被称为 <strong>FileSegment</strong>。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过<code>spark.shuffle.consolidateFiles=true</code>来开启。</p><h5 id="Shuffle-Fetch"><a href="#Shuffle-Fetch" class="headerlink" title="Shuffle Fetch"></a>Shuffle Fetch</h5><p>先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey：<br><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/reduceByKeyStage.png" alt="reduceByKey"></p><p>很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了：</p><ul><li>在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？</li><li>边 fetch 边处理还是一次性 fetch 完再处理？</li><li>fetch 来的数据存放到哪里？</li><li>怎么获得要 fetch 的数据的存放位置？</li></ul><blockquote><ul><li><p><strong>在什么时候 fetch？</strong>当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。理论上讲，一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 <code>spark.reducer.maxMbInFlight</code>，这里用 <strong>softBuffer</strong> 表示，默认大小为 48MB。一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。</p></li><li><p><strong>边 fetch 边处理还是一次性 fetch 完再处理？</strong>边 fetch 边处理。本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。<strong>那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？</strong>答案是使用可以 aggregate 的数据结构，比如 HashMap。每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 \ record，直接将其放进 HashMap 里面。如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 <code>func(hashMap.get(Key), Value)</code>，比如上面 WordCount 例子中的 func 就是 <code>hashMap.get(Key) ＋ Value</code>，并将 func 的结果重新 put(key) 到 HashMap 中去。这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。</p><pre class="line-numbers language-java"><code class="language-java">  <span class="token comment" spellcheck="true">// MapReduce</span>  <span class="token function">reduce</span><span class="token punctuation">(</span>K key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>V<span class="token operator">></span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>       result <span class="token operator">=</span> <span class="token function">process</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> values<span class="token punctuation">)</span>      <span class="token keyword">return</span> result      <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">// Spark</span>  <span class="token function">reduce</span><span class="token punctuation">(</span>K key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>V<span class="token operator">></span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>      result <span class="token operator">=</span> null       <span class="token keyword">for</span> <span class="token punctuation">(</span>V value <span class="token operator">:</span> values<span class="token punctuation">)</span>           result  <span class="token operator">=</span> <span class="token function">func</span><span class="token punctuation">(</span>result<span class="token punctuation">,</span> value<span class="token punctuation">)</span>      <span class="token keyword">return</span> result  <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。因此一些算法比如求平均数，在 process 里面很好实现，直接<code>sum(values)/values.length</code>，而在 Spark 中 func 可以实现<code>sum(values)</code>，但不好实现<code>/values.length</code>。更多的 func 将会在下面的章节细致分析。</p></li><li><p><strong>fetch 来的数据存放到哪里？</strong>刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。如果<code>spark.shuffle.spill = false</code>就只用内存。内存使用的是<code>AppendOnlyMap</code> ，类似 Java 的<code>HashMap</code>，内存＋磁盘使用的是<code>ExternalAppendOnlyMap</code>，如果内存空间不足时，<code>ExternalAppendOnlyMap</code>可以将 \ records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。<strong>使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？</strong>在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。</p></li><li><p><strong>怎么获得要 fetch 的数据的存放位置？</strong>在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 <code>MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)</code>，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。</p></li></ul></blockquote><h3 id="1-3-Spark的Stage划分及优化"><a href="#1-3-Spark的Stage划分及优化" class="headerlink" title="1.3 Spark的Stage划分及优化"></a>1.3 Spark的Stage划分及优化</h3><blockquote><p>窄依赖指父RDD的每一个分区最多被一个子RDD的分区所用，表现为</p><p> 一个父RDD的分区对应于一个子RDD的分区<br> 两个父RDD的分区对应于一个子RDD 的分区。<br> 宽依赖指子RDD的每个分区都要依赖于父RDD的所有分区，这是shuffle类操作</p></blockquote><p><strong>Stage:</strong></p><blockquote><p>一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。Stage的划分，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。</p><p>比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；</p><p>如果是rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage.</p><p>会根据RDD之间的依赖关系将DAG图划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中</p></blockquote><p><strong>Stage划分思路</strong></p><p>因此spark划分stage的整体思路是：<strong>从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。</strong></p><p>在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！</p><p>而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。</p><p><strong>总结</strong></p><p>map,filter为窄依赖，<br> groupbykey为宽依赖<br> 遇到一个宽依赖就分一个stage</p><h3 id="1-4-Spark和MapReduce的区别"><a href="#1-4-Spark和MapReduce的区别" class="headerlink" title="1.4 Spark和MapReduce的区别"></a>1.4 Spark和MapReduce的区别</h3><p><strong>整体对比概念</strong></p><p>Spark Shuffle 与MapReduce Shuffle的设计思想相同，但是实现细节优化方式不同。</p><blockquote><p><strong>1. 从逻辑角度来讲</strong>，Shuffle 过程就是一个 GroupByKey 的过程，两者没有本质区别。<br>只是 MapReduce 为了方便 GroupBy 存在于不同 partition 中的 key/value records，就<u>提前对 key 进行排序</u>。Spark 认为很多应用不需要对 key 排序，就默认没有在 GroupBy 的过程中对 key 排序。</p><p><strong>2. 从数据流角度讲，两者有差别。</strong><br>  MapReduce 只能从一个 Map Stage shuffle 数据，Spark 可以从多个 Map Stages shuffle 数据</p><p><strong>3 .Shuffle write/read 实现上有一些区别。</strong><br>   以前对 shuffle write/read 的分类是 <strong>sort-based</strong> 和 <strong>hash-based</strong>。MapReduce 可以说是 <strong>sort-based</strong>，shuffle write 和 shuffle read 过程都是基于key sorting 的 (buffering records + in-memory sort + on-disk external sorting)。早期的 Spark 是 hash-based，shuffle write 和 shuffle read 都使用 HashMap-like 的数据结构进行 aggregate (without key sorting)。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based (only sort partition id, without key sorting)，shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经“你中有我，我中有你”，界限已经不那么清晰。</p><p><strong>4. 从数据 fetch 与数据计算的重叠粒度来讲，两者有细微区别。</strong><br>   MapReduce 是<strong>粗粒度</strong>，reducer fetch 到的 records 先被放到 <code>shuffle buffer</code> 中休息，当 shuffle buffer 快满时，才对它们进行 combine()。而 Spark 是<strong>细粒度</strong>，可以即时将 fetch 到的 record 与 HashMap 中相同 key 的 record 进行 aggregate。</p></blockquote><blockquote><p><img src="https://pic4.zhimg.com/b5a8d3294a7c99f065896fee00f910e4_r.jpg" alt="img"></p><p>解说：<br>1、MapReduce在Map阶段完成之后数据会被写入到内存中的一个环形缓冲区（后续的分区/分组/排序在这里完成）；Spark的Map阶段完成之后直接输出到磁盘。<br>2、受第一步的影响，MapReduce输出的数据是有序的（针对单个Map数据来说）；Spark的数据是无序的（可以使用RDD算子达到排序的效果）。<br>3、MapReduce缓冲区的数据处理完之后会spill到磁盘形成一个文件，文件数量达到阈值之后将会进行merge操作，将多个小文件合并为一个大文件；Spark没有merge过程，一个Map中如果有对应多个Reduce的数据，则直接写多个磁盘文件。<br>4、MapReduce全部通过网络来获得数据；对于本地数据Spark可以直接读取</p></blockquote><h3 id="1-5-宽依赖与窄依赖区别"><a href="#1-5-宽依赖与窄依赖区别" class="headerlink" title="1.5 宽依赖与窄依赖区别"></a>1.5 宽依赖与窄依赖区别</h3><p>RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="1-6-Spark-RDD-原理"><a href="#1-6-Spark-RDD-原理" class="headerlink" title="1.6 Spark RDD 原理"></a>1.6 Spark RDD 原理</h3><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1. RDD是什么"></a>1. RDD是什么</h4><blockquote><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合</p><ul><li><strong>Dataset</strong>：就是一个集合，用于存放数据的</li><li><strong>Destributed</strong>：分布式，可以并行在集群计算</li><li><strong>Resilient</strong>：表示弹性的，弹性表示<ul><li>RDD中的数据可以存储在内存或者磁盘中；</li><li>RDD中的分区是可以改变的；</li></ul></li></ul></blockquote><ol><li><strong>A list of partitions</strong>：一个分区列表，RDD中的数据都存储在一个分区列表中</li><li><strong>A function for computing each split</strong>：作用在每一个分区中的函数</li><li><strong>A list of dependencies on other RDDs</strong>：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的</li><li><strong>Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)</strong>：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向</li><li>可选项，数据本地性，数据位置最优</li></ol><h4 id="2-RDD操作"><a href="#2-RDD操作" class="headerlink" title="2. RDD操作"></a>2. RDD操作</h4><p>​        RDD创建后就可以在RDD上进行数据处理。RDD支持两种操作：<strong>转换</strong>（transformation），即从现有的数据集创建一个新的数据集；<strong>动作</strong>（action），即在数据集上进行计算后，返回一个值给Driver程序。</p><ul><li><strong>转换</strong>（transformation）</li></ul><blockquote><p>​        RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。</p><p>​        RDD中所有的Transformation都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的Action时，这些Transformation才会真正运行。</p></blockquote><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#### map(func)**</span>返回一个新的分布式数据集，该数据集由每一个输入元素经过func函数转换后组成<span class="token comment" spellcheck="true">#### **fitler(func)**</span>返回一个新的数据集，该数据集由经过func函数计算后返回值为true的输入元素组成<span class="token comment" spellcheck="true">#### **flatMap(func)**</span>类似于map，但是每一个输入元素可以被映射为<span class="token number">0</span>或多个输出元素（因此func返回一个序列，而不是单一元素）<span class="token comment" spellcheck="true">#### **mapPartitions(func)**</span>类似于map，但独立地在RDD上每一个分片上运行，因此在类型为T的RDD上运行时，func函数类型必须是Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token operator">=</span><span class="token operator">></span>Iterator<span class="token punctuation">[</span>U<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#### **mapPartitionsWithSplit(func)**</span>类似于mapPartitons，但func带有一个整数参数表示分片的索引值。因此在类型为T的RDD上运行时，func函数类型必须是<span class="token punctuation">(</span>Int<span class="token punctuation">,</span>Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">></span>Iterator<span class="token punctuation">[</span>U<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#### **sample(withReplacement,fraction,seed)**</span>根据fraction指定的比例对数据进行采样，可以选择是否用随机数进行替换，seed用于随机数生成器种子<span class="token comment" spellcheck="true">#### **union(otherDataSet)**</span>返回一个新数据集，新数据集是由原数据集和参数数据集联合而成<span class="token comment" spellcheck="true">#### **distinct([numTasks])**</span>返回一个包含原数据集中所有不重复元素的新数据集<span class="token comment" spellcheck="true">#### **groupByKey([numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">)</span>对的数据集。注意默认情况下，只有<span class="token number">8</span>个并行任务来操作，但是可以传入一个可选的numTasks参数来改变它<span class="token comment" spellcheck="true">#### **reduceByKey(func,[numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。与groupByKey类似，reduceByKey任务的个数是可以通过第二个可选参数来设置的<span class="token comment" spellcheck="true">#### **sortByKey([[ascending],numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对数据集。升序或降序由ascending布尔参数决定<span class="token comment" spellcheck="true">#### **join(otherDataset0,[numTasks])**</span>在类型为<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>和<span class="token punctuation">(</span>K<span class="token punctuation">,</span>W<span class="token punctuation">)</span>数据集上调用，返回一个相同的key对应的所有元素在一起的<span class="token punctuation">(</span>K<span class="token punctuation">,</span><span class="token punctuation">(</span>V<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">)</span>数据集<span class="token comment" spellcheck="true">#### **cogroup(otherDataset,[numTasks])**</span>在类型为<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>和<span class="token punctuation">(</span>K<span class="token punctuation">,</span>W<span class="token punctuation">)</span>数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>W<span class="token punctuation">]</span><span class="token punctuation">)</span>元祖的数据集。这个操作也可以称为groupwith<span class="token comment" spellcheck="true">#### **cartesain(ohterDataset)**</span>笛卡尔积，在类型为T和U类型的数据集上调用，返回一个<span class="token punctuation">(</span>T<span class="token punctuation">,</span>U<span class="token punctuation">)</span>对数据集<span class="token punctuation">(</span>两两的元素对<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>动作</strong>（action）</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#### **reduce(func)**</span>通过函数func<span class="token punctuation">(</span>接收两个参数，返回一个参数<span class="token punctuation">)</span>聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的并行运行<span class="token comment" spellcheck="true">#### **collect()**</span>在驱动程序中，以数组形式返回数据集中的所有元素。通常在使用filter或者其他操作返回一个足够小的数据子集后再使用会比较有用<span class="token comment" spellcheck="true">#### **count()**</span>返回数据集元素个数<span class="token comment" spellcheck="true">#### **first()**</span>返回数据集第一个元素<span class="token punctuation">(</span>类似于take<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#### **take(n)**</span>返回一个由数据集前n个元素组成的数组注意 这个操作目前并非并行执行，而是由驱动程序计算所有的元素<span class="token comment" spellcheck="true">#### **takeSample(withReplacement,num,seed)**</span>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否由随机数替换不足的部分，seed用户指定随机数生成器种子<span class="token comment" spellcheck="true">#### **saveAsTextFile(path)**</span>将数据集的元素以textfile的形式保存到本地文件系统—HDFS或者任何其他Hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行<span class="token comment" spellcheck="true">#### **saveAsSequenceFile(path)**</span>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是本地系统、HDFS或者任何其他的Hadoop支持的文件系统。这个只限于由key<span class="token operator">-</span>value对组成，并实现了Hadoop的Writable接口，或者可以隐式的转换为Writable的RDD<span class="token punctuation">(</span>Spark包括了基本类型转换，例如Int、Double、String等<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#### **countByKey()**</span>对<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>类型的RDD有效，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Int<span class="token punctuation">)</span>对的map，表示每一个key对应的元素个数<span class="token comment" spellcheck="true">#### **foreach(func)**</span>在数据集的每一个元素上，运行函数func进行更新。通常用于边缘效果，例如更新一个叠加器，或者和外部存储系统进行交互，如HBase<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-RDD共享变量"><a href="#3-RDD共享变量" class="headerlink" title="3. RDD共享变量"></a>3. RDD共享变量</h4><p>在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：<strong>广播变量和累加器</strong>。</p><p>(1). <strong>广播变量（Broadcast Variables）</strong></p><p>– 广播变量缓存到各个节点的内存中，而不是每个 Task</p><p>– 广播变量被创建后，能在集群中运行的任何函数调用</p><p>– 广播变量是只读的，不能在被广播后修改</p><p>– 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本</p><p>val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量<br>(2). <strong>累加器</strong></p><p>​    累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值。</p><h4 id="4-RDD缓存"><a href="#4-RDD缓存" class="headerlink" title="4. RDD缓存"></a>4. RDD缓存</h4><p>Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 <strong>transformation</strong>自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。</p><p>​         Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。</p><p>​          Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li><p>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</p></li><li><p>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p></li><li><p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p></li><li><p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p></li><li><p>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</p></li><li><p>在不会使用cached RDD的时候，及时使用unpersist方法来释放它。</p></li></ul><h3 id="1-7-RDD有哪几种创建方式"><a href="#1-7-RDD有哪几种创建方式" class="headerlink" title="1.7 RDD有哪几种创建方式"></a>1.7 RDD有哪几种创建方式</h3><p>1) 使用程序中的集合创建rdd<br>2) 使用本地文件系统创建rdd<br>3) 使用hdfs创建rdd，<br>4) 基于数据库db创建rdd<br>5) 基于Nosql创建rdd，如hbase<br>6) 基于s3创建rdd，<br>7) 基于数据流，如socket创建rdd</p><h3 id="1-8-Spark的RDD-DataFrame和DataSet的区别"><a href="#1-8-Spark的RDD-DataFrame和DataSet的区别" class="headerlink" title="1.8 Spark的RDD DataFrame和DataSet的区别"></a>1.8 Spark的RDD DataFrame和DataSet的区别</h3><p><strong>RDD的优点：</strong></p><ol><li>相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</li><li>面向对象编程，直接存储的java对象，类型转化也安全</li></ol><p><strong>RDD的缺点：</strong></p><ol><li>由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦</li><li>默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁</li></ol><p><strong>DataFrame的优点：</strong></p><ol><li><p>结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表</p></li><li><p>有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。</p></li><li><p>hive兼容，支持hql、udf等</p></li></ol><p><strong>DataFrame的缺点：</strong></p><ol><li>编译时不能类型转化安全检查，运行时才能确定是否有问题</li><li>对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象</li></ol><p><strong>DateSet的优点：</strong></p><ol><li><p>DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据</p></li><li><p>和RDD一样，支持自定义对象存储</p></li><li><p>和DataFrame一样，支持结构化数据的sql查询</p></li><li><p>采用堆外内存存储，gc友好</p></li><li><p>类型转化安全，代码友好</p><p>如此回答有3个坑（容易引起面试官追问）：</p><p>1）Spark shuffle 与 MapReduce shuffle（或者Spark 与 MR 的区别）</p><p>2）Spark内存模型 </p><p>3）对gc（垃圾回收）的了解</p></li></ol><h3 id="1-10-Spark-的通信机制"><a href="#1-10-Spark-的通信机制" class="headerlink" title="1.10 Spark 的通信机制"></a>1.10 Spark 的通信机制</h3><h4 id="分布式的通信方式"><a href="#分布式的通信方式" class="headerlink" title="分布式的通信方式"></a>分布式的通信方式</h4><ul><li>RPC</li><li>RMI</li><li>JMS</li><li>EJB</li><li>Web Serivice</li></ul><h4 id="通信框架Akka"><a href="#通信框架Akka" class="headerlink" title="通信框架Akka"></a>通信框架Akka</h4><p>​       Hadoop MR中的计算框架，jobTracker和TaskTracker间是由于通过<strong>heartbeat</strong>的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色的高效的<strong>Akka</strong>和<strong>netty</strong>通信系统</p><h3 id="1-11-Spark的数据容错机制"><a href="#1-11-Spark的数据容错机制" class="headerlink" title="1.11 Spark的数据容错机制"></a>1.11 Spark的数据容错机制</h3><p>一般而言，对于分布式系统，数据集的容错性通常有两种方式：</p><p>1） <strong>数据检查点（在Spark中对应Checkpoint机制）</strong>。</p><p>2） <strong>记录数据的更新（在Spark中对应Lineage血统机制</strong>）。</p><p>对于大数据分析而言，数据检查点操作成本较高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低，同时会消耗大量存储资源。</p><p>Spark选择记录更新的方式。但更新粒度过细时，记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列记录下来，以便恢复丢失的分区。</p><h4 id="Lineage（血统）机制"><a href="#Lineage（血统）机制" class="headerlink" title="Lineage（血统）机制"></a>Lineage（血统）机制</h4><p>​      每个RDD除了包含分区信息外，还包含它从父辈RDD变换过来的步骤，以及如何重建某一块数据的信息，因此RDD的这种容错机制又称“血统”（Lineage）容错。Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做以便恢复数据。</p><p>​       相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新计算和恢复丢失的数据分区。但这种数据模型粒度较粗，因此限制了Spark的应用场景。所以可以说Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能方面的提升。</p><p>​      RDD在Lineage容错方面采用如下两种依赖来保证容错方面的性能：</p><p><strong>窄依赖（Narrow Dependeny）</strong>：窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。其中，1个父RDD分区对应1个子RDD分区，可以分为如下两种情况：</p><p> 子RDD分区与父RDD分区一一对应（如map、filter等算子）。一个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。</p><p><strong>宽依赖（Wide Dependency，源码中称为Shuffle Dependency）：</strong></p><p>宽依赖是指一个父RDD分区对应多个子RDD分区，可以分为如下两种情况：</p><p>一个父RDD对应所有子RDD分区（未经协同划分的Join）。</p><p>一个父RDD对应多个RDD分区（非全部分区）（如groupByKey）。</p><p>窄依赖与宽依赖关系如图3-10所示。</p><p>从图3-10可以看出对依赖类型的划分：根据父RDD分区是对应一个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，对于需要重新计算的子分区而言，只需要父分区的一部分数据，因此其余数据的重算就导致了冗余计算。</p><p><img src="https://yqfile.alicdn.com/f319c81b2aaeb9f7b29dfeff3ef1cd19ec64ca9b.png" alt="f319c81b2aaeb9f7b29dfeff3ef1cd19ec64ca9b"></p><p>图3-10　两种依赖关系</p><p>对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，在通过重新计算恢复数据的情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。</p><p>窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点（前面调度机制中已讲过）。</p><p>依赖关系在lineage容错中的应用总结如下：</p><p>1）窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成，并且父RDD的计算结果进行hash并传到对应节点上之后，才能计算子RDD。</p><p>2）数据丢失时，对于窄依赖，只需要重新计算丢失的那一块数据来恢复；对于宽依赖，则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖时，需要在适当的时机设置数据检查点（checkpoint机制在下节讲述）。可见Spark在容错性方面要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。</p><p>在Spark容错机制中，如果一个节点宕机了，而且运算属于窄依赖，则只要重算丢失的父RDD分区即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。更深入地来说：在窄依赖关系中，当子RDD的分区丢失，重算其父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，因此不存在冗余计算。而在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区使用，其中有一部分数据对应的是其他不需要重新计算的子RDD分区中的数据，因此在宽依赖关系下，这样计算就会产生冗余开销，这也是宽依赖开销更大的原因。为了减少这种冗余开销，通常在Lineage血统链比较长，并且含有宽依赖关系的容错中使用Checkpoint机制设置检查点。</p><h4 id="Checkpoint（检查点）机制"><a href="#Checkpoint（检查点）机制" class="headerlink" title="Checkpoint（检查点）机制"></a>Checkpoint（检查点）机制</h4><p>通过上述分析可以看出Checkpoint的本质是将RDD写入Disk来作为检查点。这种做法是为了通过lineage血统做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p><h3 id="1-13-Spark性能调优"><a href="#1-13-Spark性能调优" class="headerlink" title="1.13 Spark性能调优"></a>1.13 Spark性能调优</h3><h4 id="1-常用参数说明"><a href="#1-常用参数说明" class="headerlink" title="1) 常用参数说明"></a>1) 常用参数说明</h4><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可</span><span class="token comment" spellcheck="true">--executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时处理大批量数据时容易内存不足，再多申请一点，如6G</span><span class="token comment" spellcheck="true">--num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等</span><span class="token comment" spellcheck="true">--executor-cores 2  : 每个executor内的核数，即每个executor中的任务task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce任务的并行度是executor数目*executor中的任务数</span>yarn集群中一般有资源申请上限，如，executor<span class="token operator">-</span>memory<span class="token operator">*</span>num<span class="token operator">-</span>executors <span class="token operator">&lt;</span> 400G 等，所以调试参数时要注意这一点—<span class="token operator">-</span>spark<span class="token punctuation">.</span><span class="token keyword">default</span><span class="token punctuation">.</span>parallelism <span class="token number">200</span> ： Spark作业的默认为<span class="token number">500</span><span class="token operator">~</span><span class="token number">1000</span>个比较合适<span class="token punctuation">,</span>如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num<span class="token operator">-</span>executors <span class="token operator">*</span> executor<span class="token operator">-</span>cores的<span class="token number">2</span><span class="token operator">~</span><span class="token number">3</span>倍比较合适。<span class="token comment" spellcheck="true">-- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor内存中能占的最大比例。默认值是0.6</span>—<span class="token operator">-</span>spark<span class="token punctuation">.</span>shuffle<span class="token punctuation">.</span>memoryFraction <span class="token number">0.2</span> ： 设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是<span class="token number">0.2</span>，如果shuffle聚合时使用的内存超出了这个<span class="token number">20</span><span class="token operator">%</span>的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能—<span class="token operator">-</span>spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor<span class="token operator">-</span>memory，所以会为executor额外预留一部分内存，spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead即代表这部分内存<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-Spark常用编程建议"><a href="#2-Spark常用编程建议" class="headerlink" title="2) Spark常用编程建议"></a>2) Spark常用编程建议</h4><ol><li><p>避免创建重复的RDD，尽量复用同一份数据。</p></li><li><p>尽量避免使用shuffle类算子，因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子</p></li><li><p>用aggregateByKey和reduceByKey替代groupByKey,因为前两个是预聚合操作，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。</p></li><li><p>repartition适用于RDD[V], partitionBy适用于RDD[K, V]</p></li><li><p>mapPartitions操作替代普通map，foreachPartitions替代foreach</p></li><li><p>filter操作之后进行coalesce操作，可以减少RDD的partition数量</p></li><li><p>如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER</p></li><li><p>尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作</p></li><li><p>多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow</p></li><li><p>spark中的Group/join/XXXByKey等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数</p></li><li><p>尽量保证每轮Stage里每个task处理的数据量&gt;128M</p></li><li><p>如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲</p></li><li><p>2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD)</p></li><li><p>若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores=1， executor-memory=m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores=size/m(向上取整)</p></li><li><p>如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以hash join的方式实现，具体原理参考下一节的shuffle过程</p></li><li><p>如果需要在repartition重分区之后还要进行排序，可直接使用repartitionAndSortWithinPartitions，比分解操作效率高，因为它可以一边shuffle一边排序</p></li></ol><h4 id="3-shuffle性能优化"><a href="#3-shuffle性能优化" class="headerlink" title="3) shuffle性能优化"></a>3) shuffle性能优化</h4><p><strong>3.1 什么是shuffle操作</strong></p><p>spark中的shuffle操作功能：将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join操作，类似洗牌的操作。这些分布在各个存储节点上的数据重新打乱然后汇聚到不同节点的过程就是shuffle过程。</p><p><strong>3.2 哪些操作中包含shuffle操作</strong></p><p>RDD的特性是不可变的带分区的记录集合，Spark提供了Transformation和Action两种操作RDD的方式。Transformation是生成新的RDD，包括map, flatMap, filter, union, sample, join, groupByKey, cogroup, ReduceByKey, cros, sortByKey, mapValues等；Action只是返回一个结果，包括collect，reduce，count，save，lookupKey等</p><p>Spark所有的算子操作中是否使用shuffle过程要看计算后对应多少分区：</p><ul><li>若一个操作执行过程中，结果RDD的每个分区只依赖上一个RDD的同一个分区，即属于窄依赖，如map、filter、union等操作，这种情况是不需要进行shuffle的，同时还可以按照pipeline的方式，把一个分区上的多个操作放在同一个Task中进行</li><li>若结果RDD的每个分区需要依赖上一个RDD的全部分区，即属于宽依赖，如repartition相关操作（repartition，coalesce）、*ByKey操作（groupByKey，ReduceByKey，combineByKey、aggregateByKey等）、join相关操作（cogroup，join）、distinct操作，这种依赖是需要进行shuffle操作的</li></ul><p><strong>3.3 shuffle操作过程</strong></p><p>shuffle过程分为shuffle write和shuffle read两部分</p><ul><li>shuffle write： 分区数由上一阶段的RDD分区数控制，shuffle write过程主要是将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上（当前stage结束之后，每个task处理的数据按key进行分类，数据先写入内存缓冲区，缓冲区满，溢写spill到磁盘文件，最终相同key被写入同一个磁盘文件）创建的磁盘文件数量=当前stage中task数量*下一个stage的task数量</li><li>shuffle read：从上游stage的所有task节点上拉取属于自己的磁盘文件，每个read task会有自己的buffer缓冲，每次只能拉取与buffer缓冲相同大小的数据，然后聚合，聚合完一批后拉取下一批，边拉取边聚合。分区数由Spark提供的一些参数控制，如果这个参数值设置的很小，同时shuffle read的数据量很大，会导致一个task需要处理的数据非常大，容易发生JVM crash，从而导致shuffle数据失败，同时executor也丢失了，就会看到Failed to connect to host 的错误(即executor lost)。</li></ul><p>shuffle过程中，各个节点会通过shuffle write过程将相同key都会先写入本地磁盘文件中，然后其他节点的shuffle read过程通过网络传输拉取各个节点上的磁盘文件中的相同key。这其中大量数据交换涉及到的网络传输和文件读写操作是shuffle操作十分耗时的根本原因</p><p><strong>3.4 spark的shuffle类型</strong></p><p>参数spark.shuffle.manager用于设置ShuffleManager的类型。Spark1.5以后，该参数有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2以前的默认值，Spark1.2之后的默认值都是SortShuffleManager。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</p><p>由于SortShuffleManager默认会对数据进行排序，因此如果业务需求中需要排序的话，使用默认的SortShuffleManager就可以；但如果不需要排序，可以通过bypass机制或设置HashShuffleManager避免排序，同时也能提供较好的磁盘读写性能。</p><p>HashShuffleManager流程：</p><p><img src="https://pic3.zhimg.com/80/v2-ac4da726d0fd470fa4a058b750d79ba6_1440w.jpg" alt="img"></p><p>SortShuffleManager流程：</p><p><img src="https://pic2.zhimg.com/80/v2-6e277c1ca522102acf693b2fe6e36775_1440w.jpg" alt="img"></p><p><strong>3.5 如何开启bypass机制</strong></p><p>bypass机制通过参数spark.shuffle.sort.bypassMergeThreshold设置，默认值是200，表示当ShuffleManager是SortShuffleManager时，若shuffle read task的数量小于这个阈值（默认200）时，则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式写数据，但最后会将每个task产生的所有临时磁盘文件合并成一个文件，并创建索引文件。</p><p>这里给出的调优建议是，当使用SortShuffleManager时，如果的确不需要排序，可以将这个参数值调大一些，大于shuffle read task的数量。那么此时就会自动开启bypass机制，map-side就不会进行排序了，减少排序的性能开销，提升shuffle操作效率。但这种方式并没有减少shuffle write过程产生的磁盘文件数量，所以写的性能没有改变。</p><p><strong>3.6 HashShuffleManager优化建议</strong></p><p>如果使用HashShuffleManager，可以设置spark.shuffle.consolidateFiles参数。该参数默认为false，只有当使用HashShuffleManager且该参数设置为True时，才会开启consolidate机制，大幅度合并shuffle write过程产生的输出文件，对于shuffle read task 数量特别多的情况下，可以极大地减少磁盘IO开销，提升shuffle性能。参考社区同学给出的数据，consolidate性能比开启bypass机制的SortShuffleManager高出10% ~ 30%。</p><p><strong>3.7 shuffle调优建议</strong></p><p>除了上述的几个参数调优，shuffle过程还有一些参数可以提高性能：</p><pre class="line-numbers language-text"><code class="language-text">- spark.shuffle.file.buffer : 默认32M，shuffle Write阶段写文件时的buffer大小，若内存资源比较充足，可适当将其值调大一些（如64M），减少executor的IO读写次数，提高shuffle性能- spark.shuffle.io.maxRetries ： 默认3次，Shuffle Read阶段取数据的重试次数，若shuffle处理的数据量很大，可适当将该参数调大。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3.8 shuffle操作过程中的常见错误</p><p>SparkSQL中的shuffle错误：</p><pre class="line-numbers language-text"><code class="language-text">org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0org.apache.spark.shuffle.FetchFailedException:Failed to connect to hostname/192.168.xx.xxx:50268<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>RDD中的shuffle错误：</p><pre class="line-numbers language-text"><code class="language-text">WARN TaskSetManager: Lost task 17.1 in stage 4.1 (TID 1386, spark050013): java.io.FileNotFoundException: /data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/2f/temp_shuffle_e22e013a-5392-4edb-9874-a196a1dad97c (没有那个文件或目录)FetchFailed(BlockManagerId(6083b277-119a-49e8-8a49-3539690a2a3f-S155, spark050013, 8533), shuffleId=1, mapId=143, reduceId=3, message=org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/0e/shuffle_1_143_0.data, offset=997061, length=112503}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>处理shuffle类操作的注意事项：</p><ul><li>减少shuffle数据量：在shuffle前过滤掉不必要的数据，只选取需要的字段处理</li><li>针对SparkSQL和DataFrame的join、group by等操作：可以通过 spark.sql.shuffle.partitions控制分区数，默认设置为200，可根据shuffle的量以及计算的复杂度提高这个值，如2000等</li><li>RDD的join、group by、reduceByKey等操作：通过spark.default.parallelism控制shuffle read与reduce处理的分区数，默认为运行任务的core总数，官方建议为设置成运行任务的core的2~3倍</li><li>提高executor的内存：即spark.executor.memory的值</li><li>分析数据验证是否存在数据倾斜的问题：如空值如何处理，异常数据（某个key对应的数据量特别大）时是否可以单独处理，可以考虑自定义数据分区规则，如何自定义可以参考下面的join优化环节</li></ul><h4 id="4-join性能优化"><a href="#4-join性能优化" class="headerlink" title="4) join性能优化"></a>4) join性能优化</h4><p>Spark所有的操作中，join操作是最复杂、代价最大的操作，也是大部分业务场景的性能瓶颈所在。所以针对join操作的优化是使用spark必须要学会的技能。</p><p>spark的join操作也分为Spark SQL的join和Spark RDD的join。</p><p><strong>4.1 Spark SQL 的join操作</strong></p><p>4.1.1 Hash Join</p><p>Hash Join的执行方式是先将小表映射成Hash Table的方式，再将大表使用相同方式映射到Hash Table，在同一个hash分区内做join匹配。</p><p>hash join又分为broadcast hash join和shuffle hash join两种。其中Broadcast hash join，顾名思义，就是把小表广播到每一个节点上的内存中，大表按Key保存到各个分区中，小表和每个分区的大表做join匹配。这种情况适合一个小表和一个大表做join且小表能够在内存中保存的情况。如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-e738483e795e49c5c58a943c0d914e9f_1440w.jpg" alt="img"></p><p>当Hash Join不能适用的场景就需要Shuffle Hash Join了，Shuffle Hash Join的原理是按照join Key分区，key相同的数据必然分配到同一分区中，将大表join分而治之，变成小表的join，可以提高并行度。执行过程也分为两个阶段：</p><ul><li>shuffle阶段：分别将两个表按照join key进行分区，将相同的join key数据重分区到同一节点</li><li>hash join阶段：每个分区节点上的数据单独执行单机hash join算法</li></ul><p>Shuffle Hash Join的过程如下图所示：</p><p><img src="https://pic2.zhimg.com/80/v2-f1b93fcb9091521f24aa2c26ff31c771_1440w.jpg" alt="img"></p><p>4.1.2 Sort-Merge Join</p><p>SparkSQL针对两张大表join的情况提供了全新的算法——Sort-merge join，整个过程分为三个步骤：</p><ul><li>Shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式进行处理</li><li>sort阶段：对单个分区节点的两表数据，分别进行排序</li><li>merge阶段：对排好序的两张分区表数据执行join操作。分别遍历两个有序序列，遇到相同的join key就merge输出，否则继续取更小一边的key，即合并两个有序列表的方式。</li></ul><p>sort-merge join流程如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-63765c1a2359fce326413ac546cefd3e_1440w.jpg" alt="img"></p><p><strong>4.2 Spark RDD的join操作</strong></p><p>Spark的RDD join没有上面这么多的分类，但是面临的业务需求是一样的。如果是大表join小表的情况，则可以将小表声明为broadcast变量，使用map操作快速实现join功能，但又不必执行Spark core中的join操作。</p><p>如果是两个大表join，则必须依赖Spark Core中的join操作了。Spark RDD Join的过程可以自行阅读源码了解，这里只做一个大概的讲解。</p><p>spark的join过程中最核心的函数是cogroup方法，这个方法中会判断join的两个RDD所使用的partitioner是否一样，如果分区相同，即存在OneToOneDependency依赖，不用进行hash分区，可直接join；如果要关联的RDD和当前RDD的分区不一致时，就要对RDD进行重新hash分区，分到正确的分区中，即存在ShuffleDependency，需要先进行shuffle操作再join。因此提升join效率的一个思路就是使得两个RDD具有相同的partitioners。</p><p>所以针对Spark RDD的join操作的优化建议是：</p><ul><li>如果需要join的其中一个RDD比较小，可以直接将其存入内存，使用broadcast hash join</li><li>在对两个RDD进行join操作之前，使其使用同一个partitioners，避免join操作的shuffle过程</li><li>如果两个RDD其一存在重复的key也会导致join操作性能变低，因此最好先进行key值的去重处理</li></ul><p><strong>4.3 数据倾斜优化</strong></p><p>均匀数据分布的情况下，前面所说的优化建议就足够了。但存在数据倾斜时，仍然会有性能问题。主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。</p><p>shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。</p><p>4.3.1 分析数据分布</p><p>如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。</p><p>4.3.2 数据倾斜的解决方案</p><p>这里参考美团技术博客中给出的几个方案。</p><p>1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。</p><p>2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key</p><p>3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量</p><p>4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。</p><p>5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。</p><p>6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作</p><h4 id="5-其他错误总结"><a href="#5-其他错误总结" class="headerlink" title="5) 其他错误总结"></a>5) 其他错误总结</h4><p>(1) 报错信息</p><pre class="line-numbers language-text"><code class="language-text">java.lang.OutOfMemory, unable to create new native thread Caused by: java.lang.OutOfMemoryError: unable to create new native thread         at java.lang.Thread.start0(Native Method)         at java.lang.Thread.start(Thread.java:640) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>解决方案：</p><p>上面这段错误提示的本质是Linux操作系统无法创建更多进程，导致出错，并不是系统的内存不足。因此要解决这个问题需要修改Linux允许创建更多的进程，就需要修改Linux最大进程数</p><p>（2）报错信息</p><p>由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实就是将/tmp目录挂载到内存当中, 那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误<br>No Space Left on the device（Shuffle临时文件过多）</p><p>解决方案：</p><p>修改配置文件spark-env.sh,把临时文件引入到一个自定义的目录中去, 即:</p><p>export SPARK_LOCAL_DIRS=/home/utoken/datadir/spark/tmp</p><p>（3）报错信息</p><p>Worker节点中的work目录占用许多磁盘空间, 这些是Driver上传到worker的文件, 会占用许多磁盘空间</p><p>解决方案：</p><p>需要定时做手工清理work目录</p><p>（4）spark-shell提交Spark Application如何解决依赖库</p><p>解决方案：</p><p>利用–driver-class-path选项来指定所依赖的jar文件，注意的是–driver-class-path后如果需要跟着多个jar文件的话，jar文件之间使用冒号:来分割。</p><p>（5）内存不足或数据倾斜导致Executor Lost，shuffle fetch失败，Task重试失败等（spark-submit提交）</p><pre class="line-numbers language-text"><code class="language-text">TaskSetManager: Lost task 1.0 in stage 6.0 (TID 100, 192.168.10.37): java.lang.OutOfMemoryError: Java heap spaceINFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.37:57139 (size: 42.0 KB, free: 24.2 MB)INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.38:53816 (size: 42.0 KB, free: 24.2 MB)INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 102, 192.168.10.37, ANY, 2152 bytes)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>解决方案：</p><p>增加worker内存，或者相同资源下增加partition数目，这样每个task要处理的数据变少，占用内存变少</p><p>如果存在shuffle过程，设置shuffle read阶段的并行数</p><h2 id="2-SparkSQL"><a href="#2-SparkSQL" class="headerlink" title="2. SparkSQL"></a>2. SparkSQL</h2><h3 id="2-1-Spark-SQL-的原理和运行机制"><a href="#2-1-Spark-SQL-的原理和运行机制" class="headerlink" title="2.1 Spark SQL 的原理和运行机制"></a>2.1 Spark SQL 的原理和运行机制</h3><img src="https://pic2.zhimg.com/80/v2-9338f9e4a1d5ce568f47097f5b56f285_1440w.jpg" alt="img" style="zoom:50%;"><p>从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作</p><ul><li>Parser 解析 SQL，生成 Unresolved Logical Plan</li><li>由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan</li><li>Optimizer根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan</li><li>Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan</li><li>CBO 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan</li><li>Spark 以 DAG 的方法执行上述 Physical Plan</li><li>在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率</li></ul><p><strong>Parser</strong></p><p>Spark SQL 使用 Antlr 进行记法和语法解析，并生成 UnresolvedPlan。</p><p>当用户使用 SparkSession.sql(sqlText : String) 提交 SQL 时，SparkSession 最终会调用 SparkSqlParser 的 parsePlan 方法。该方法分两步</p><ul><li>使用 Antlr 生成的 SqlBaseLexer 对 SQL 进行词法分析，生成 CommonTokenStream</li><li>使用 Antlr 生成的 SqlBaseParser 进行语法分析，得到 LogicalPlan</li></ul><p><strong>Analyzer</strong></p><p>从 Analyzer 的构造方法可见</p><ul><li><p>Analyzer 持有一个 SessionCatalog 对象的引用</p></li><li><p>Analyzer 继承自 RuleExecutor[LogicalPlan]，因此可对 LogicalPlan 进行转换</p><p><strong>Optimizer</strong></p></li></ul><p>Spark SQL 目前的优化主要是基于规则的优化，即 RBO （Rule-based optimization）</p><ul><li>每个优化以 Rule 的形式存在，每条 Rule 都是对 Analyzed Plan 的等价转换</li><li>RBO 设计良好，易于扩展，新的规则可以非常方便地嵌入进 Optimizer</li><li>RBO 目前已经足够好，但仍然需要更多规则来 cover 更多的场景</li><li>优化思路主要是减少参与计算的数据量以及计算本身的代价</li></ul><p><strong>PushdownPredicate</strong><br>PushdownPredicate 是最常见的用于减少参与计算的数据量的方法。</p><p><strong>SparkPlanner</strong></p><p>得到优化后的 LogicalPlan 后，SparkPlanner 将其转化为 SparkPlan 即物理计划。</p><p>本例中由于 score 表数据量较小，Spark 使用了 BroadcastJoin。因此 score 表经过 Filter 后直接使用 BroadcastExchangeExec 将数据广播出去，然后结合广播数据对 people 表使用 BroadcastHashJoinExec 进行 Join。再经过 Project 后使用 HashAggregateExec 进行分组聚合。</p><p>至此，一条 SQL 从提交到<strong>解析</strong>、<strong>分析</strong>、<strong>优化</strong>以及执行的完整过程就介绍完毕。</p><h3 id="2-3-Spark-SQL-的优化策略"><a href="#2-3-Spark-SQL-的优化策略" class="headerlink" title="2.3 Spark SQL 的优化策略"></a>2.3 Spark SQL 的优化策略</h3><p><strong>1）内存列式存储与内存缓存表</strong><br> Spark SQL可以通过cacheTable将数据存储转换为列式存储，同时将数据加载到内存缓存。cacheTable相当于在分布式集群的内存物化视图，将数据缓存，这样迭代的或者交互式的查询不用再从HDFS读数据，直接从内存读取数据大大减少了I/O开销。列式存储的优势在于Spark SQL只需要读出用户需要的列，而不需要像行存储那样每次都将所有列读出，从而大大减少内存缓存数据量，更高效地利用内存数据缓存，同时减少网络传输和I/O开销。数据按照列式存储，由于是数据类型相同的数据连续存储，所以能够利用序列化和压缩减少内存空间的占用。</p><p> <strong>2）列存储压缩</strong><br> 为了减少内存和硬盘空间占用，Spark SQL采用了一些压缩策略对内存列存储数据进行压缩。Spark SQL的压缩方式要比Shark丰富很多，如它支持PassThrough、RunLengthEncoding、DictionaryEncoding、BooleanBitSet、IntDelta、LongDelta等多种压缩方式，这样能够大幅度减少内存空间占用、网络传输和I/O开销。</p><p> <strong>3）逻辑查询优化</strong><br> SparkSQL在逻辑查询优化（见图8-4）上支持列剪枝、谓词下压、属性合并等逻辑查询优化方法。列剪枝为了减少读取不必要的属性列、减少数据传输和计算开销，在查询优化器进行转换的过程中会优化列剪枝。<br> 下面介绍一个逻辑优化的例子。<br> SELECT Class FROM （SELECT ID，Name，Class  FROM STUDENT ） S WHERE S.ID=1</p><p>Catalyst将原有查询通过谓词下压，将选择操作ID=1优先执行，这样过滤大部分数据，通过属性合并将最后的投影只做一次，最终保留Class属性列。<br> <strong>4）Join优化</strong><br> Spark SQL深度借鉴传统数据库的查询优化技术的精髓，同时在分布式环境下调整和创新特定的优化策略。现在Spark SQL对Join进行了优化，支持多种连接算法，现在的连接算法已经比Shark丰富，而且很多原来Shark的元素也逐步迁移过来，如BroadcastHashJoin、BroadcastNestedLoopJoin、HashJoin、LeftSemiJoin，等等。<br> 下面介绍其中的一个Join算法。<br> BroadcastHashJoin将小表转化为广播变量进行广播，这样避免Shuffle开销，最后在分区内做Hash连接。这里使用的就是Hive中Map Side Join的思想，同时使用DBMS中的Hash连接算法做连接。 随着Spark SQL的发展，未来会有更多的查询优化策略加入进来，同时后续Spark SQL会支持像Shark Server一样的服务端和JDBC接口，兼容更多的持久化层，如NoSQL、传统的DBMS等。一个强有力的结构化大数据查询引擎正在崛起。</p><h2 id="3-SparkStreaming"><a href="#3-SparkStreaming" class="headerlink" title="3. SparkStreaming"></a>3. SparkStreaming</h2><p>3.1 原理剖析（源码级别）和运行机制</p><p>3.2 Spark Dstream 及其 API 操作</p><p>3.3 Spark Streaming 消费 Kafka 的两种方式</p><p>3.4 Spark 消费 Kafka 消息的 Offset 处理</p><p>3.5 窗口操作</p><h2 id="4-SparkMlib"><a href="#4-SparkMlib" class="headerlink" title="4. SparkMlib"></a>4. SparkMlib</h2><p>可实现聚类、分类、推荐等算法</p><h1 id="三-Flink"><a href="#三-Flink" class="headerlink" title="三. Flink"></a>三. Flink</h1><ul><li>Flink 集群的搭建</li><li>Flink 的架构原理</li><li>Flink 的编程模型</li><li>Flink 集群的 HA 配置</li><li>Flink DataSet 和 DataSteam API</li><li>序列化</li><li>Flink 累加器</li><li>状态 State 的管理和恢复</li><li>窗口和时间</li><li>并行度</li><li>Flink 和消息中间件 Kafka 的结合</li><li>Flink Table 和 SQL 的原理和用法</li></ul><h1 id="四-Kafka"><a href="#四-Kafka" class="headerlink" title="四. Kafka"></a>四. Kafka</h1><h2 id="1-Kafka-的设计"><a href="#1-Kafka-的设计" class="headerlink" title="1. Kafka 的设计"></a>1. Kafka 的设计</h2><p>Kafka 将消息以 topic 为单位进行归纳</p><p>将向 Kafka topic 发布消息的程序成为 producers.</p><p>将预订 topics 并消费消息的程序成为 consumer.</p><p>Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker.</p><p>producers 通过网络将消息发送到 Kafka 集群，集群向消费者提供消息</p><h2 id="2-数据传输的三种事务定义"><a href="#2-数据传输的三种事务定义" class="headerlink" title="2. 数据传输的三种事务定义"></a>2. 数据传输的三种事务定义</h2><p>数据传输的事务定义通常有以下三种级别：</p><p>（1）最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输</p><p>（2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.</p><p>（3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而</p><p>且仅仅被传输一次，这是大家所期望的</p><h2 id="3-Kafka-判断一个节点是否活着两大条件"><a href="#3-Kafka-判断一个节点是否活着两大条件" class="headerlink" title="3. Kafka 判断一个节点是否活着两大条件"></a>3. Kafka 判断一个节点是否活着两大条件</h2><p>（1）节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连</p><p>接</p><p>（2）如果节点是个 follower,他必须能及时的同步 leader 的写操作，延时不能太久</p><h2 id="4-Kafa-consumer-是否可以消费指定分区消息？"><a href="#4-Kafa-consumer-是否可以消费指定分区消息？" class="headerlink" title="4. Kafa consumer 是否可以消费指定分区消息？"></a>4. Kafa consumer 是否可以消费指定分区消息？</h2><p>​      Kafa consumer 消费消息时，向 broker 发出”fetch”请求去消费特定分区的消息，consumer</p><p>指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer 拥有</p><p>了 offset 的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的</p><h2 id="5-Kafka-消息是采用-Pull-模式or-Push-模式？"><a href="#5-Kafka-消息是采用-Pull-模式or-Push-模式？" class="headerlink" title="5. Kafka 消息是采用 Pull 模式or Push 模式？"></a>5. Kafka 消息是采用 Pull 模式or Push 模式？</h2><p>​          Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到</p><p>consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统</p><p>的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息</p><p>一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的</p><p>consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的</p><p>consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消</p><p>息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，</p><p>consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式</p><p>​          Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push</p><p>模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还</p><p>是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一</p><p>次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决</p><p>定这些策略</p><p>​          Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，</p><p>直到新消息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达</p><p>(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发</p><h2 id="6-Kafka-存储在硬盘上的消息格式是什么？"><a href="#6-Kafka-存储在硬盘上的消息格式是什么？" class="headerlink" title="6. Kafka 存储在硬盘上的消息格式是什么？"></a>6. Kafka 存储在硬盘上的消息格式是什么？</h2><p>消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和 CRC32</p><p>校验码。</p><ul><li>消息长度: 4 bytes (value: 1+4+n)</li><li>版本号: 1 byte</li><li>CRC 校验码: 4 bytes</li><li>具体的消息: n bytes</li></ul><h2 id="7-Kafka-高效文件存储设计特点"><a href="#7-Kafka-高效文件存储设计特点" class="headerlink" title="7. Kafka 高效文件存储设计特点"></a>7. Kafka 高效文件存储设计特点</h2><p>(1).Kafka 把 topic 中一个 parition 大文件分成多个小文件段，通过多个小文件段，就容易定</p><p>期清除或删除已经消费完文件，减少磁盘占用。</p><p>(2).通过索引信息可以快速定位 message 和确定 response 的最大大小。</p><p>(3).通过 index 元数据全部映射到 memory，可以避免 segment file 的 IO 磁盘操作。</p><p>(4).通过索引文件稀疏存储，可以大幅降低 index 文件元数据占用空间大小。</p><h2 id="8-Kafka-与传统消息系统之间有三个关键区别"><a href="#8-Kafka-与传统消息系统之间有三个关键区别" class="headerlink" title="8. Kafka 与传统消息系统之间有三个关键区别"></a>8. Kafka 与传统消息系统之间有三个关键区别</h2><p>(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留</p><p>(2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据</p><p>提升容错能力和高可用性</p><p>(3).Kafka 支持实时的流式处理</p><h2 id="9-Kafka-创建-Topic-时如何将分区放置到不同的-Broker-中"><a href="#9-Kafka-创建-Topic-时如何将分区放置到不同的-Broker-中" class="headerlink" title="9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中"></a>9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中</h2><ul><li>副本因子不能大于 Broker 的个数；</li><li>第一个分区（编号为 0）的第一个副本放置位置是随机从 brokerList 选择的；</li><li>其他分区的第一个副本放置位置相对于第 0 个分区依次往后移。也就是如果我们有 5 个Broker，5 个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个Broker 上，依次类推；</li><li>剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的</li></ul><h2 id="10-Kafka-新建的分区会在哪个目录下创建"><a href="#10-Kafka-新建的分区会在哪个目录下创建" class="headerlink" title="10. Kafka 新建的分区会在哪个目录下创建"></a>10. Kafka 新建的分区会在哪个目录下创建</h2><p>在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，</p><p>这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘</p><p>上用于提高读写性能。</p><p>当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。</p><p>如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个</p><p>目录下创建文件夹用于存放数据。</p><p>但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？</p><p>答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic</p><p>名+分区 ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就</p><p>是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁</p><p>盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。</p><h2 id="11-partition-的数据如何保存到硬盘"><a href="#11-partition-的数据如何保存到硬盘" class="headerlink" title="11. partition 的数据如何保存到硬盘"></a>11. partition 的数据如何保存到硬盘</h2><p>topic 中的多个 partition 以文件夹的形式保存到 broker，每个分区序号从 0 递增，</p><p>且消息有序</p><p>Partition 文件下有多个 segment（xxx.index，xxx.log）</p><p>segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为 1g</p><p>如果大小大于 1g 时，会滚动一个新的 segment 并且以上一个 segment 最后一条消息的偏移</p><p>量命名</p><h2 id="12-kafka-的-ack-机制"><a href="#12-kafka-的-ack-机制" class="headerlink" title="12. kafka 的 ack 机制"></a>12. kafka 的 ack 机制</h2><p>request.required.acks 有三个值 0 1 -1</p><p>0:生产者不会等待 broker 的 ack，这个延迟最低但是存储的保证最弱当 server 挂掉的时候</p><p>就会丢数据</p><p>1：服务端会等待 ack 值 leader 副本确认接收到消息后发送 ack 但是如果 leader 挂掉后他</p><p>不确保是否复制完成新 leader 也会导致数据丢失</p><p>-1：同样在 1 的基础上 服务端会等所有的 follower 的副本受到数据后才会受到 leader 发出</p><p>的 ack，这样数据不会丢失</p><h2 id="13-Kafka-的消费者如何消费数据"><a href="#13-Kafka-的消费者如何消费数据" class="headerlink" title="13. Kafka 的消费者如何消费数据"></a>13. Kafka 的消费者如何消费数据</h2><p>​       消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置</p><p>等到下次消费时，他会接着上次位置继续消费</p><h2 id="14-消费者负载均衡策略"><a href="#14-消费者负载均衡策略" class="headerlink" title="14. 消费者负载均衡策略"></a>14. 消费者负载均衡策略</h2><p>​       一个消费者组中的一个分片对应一个消费者成员，他能保证每个消费者成员都能访问，如</p><p>果组中成员太多会有空闲的成员</p><h2 id="15-数据有序"><a href="#15-数据有序" class="headerlink" title="15. 数据有序"></a>15. 数据有序</h2><p>​       一个消费者组里它的内部是有序的</p><p>​       消费者组与消费者组之间是无序的</p><h2 id="16-kafaka-生产数据时数据的分组策略"><a href="#16-kafaka-生产数据时数据的分组策略" class="headerlink" title="16. kafaka 生产数据时数据的分组策略"></a>16. kafaka 生产数据时数据的分组策略</h2><p>​        生产者决定数据产生到集群的哪个 partition 中</p><p>​        每一条消息都是以（key，value）格式</p><p>​        Key 是由生产者发送数据传入</p><p>​        所以生产者（key）决定了数据产生到集群的哪个 partition</p><h1 id="五-数据仓库"><a href="#五-数据仓库" class="headerlink" title="五. 数据仓库"></a>五. 数据仓库</h1><h2 id="5-1-数仓概念相关"><a href="#5-1-数仓概念相关" class="headerlink" title="5.1 数仓概念相关"></a>5.1 数仓概念相关</h2><h3 id="1-数据仓库、数据集市、数据库之间的区别"><a href="#1-数据仓库、数据集市、数据库之间的区别" class="headerlink" title="1. 数据仓库、数据集市、数据库之间的区别"></a>1. 数据仓库、数据集市、数据库之间的区别</h3><ul><li><p><strong>数据仓库 ：</strong>数据仓库是一个面向主题的、集成的、随时间变化的、但信息本身相对稳定的数据集合，用于对管理决策过程的支持。是企业级的，能为整个企业各个部门的运行提供决策支持手段；</p></li><li><p><strong>数据集市：</strong>则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。</p></li><li><p><strong>数据库：</strong>是一种软件，用来实现数据库逻辑过程，属于物理层；</p><blockquote><p>数据仓库是数据库概念的升级，从数据量来说，数据仓库要比数据库更庞大德多，主要用于数据挖掘和数据分析，辅助领导做决策</p><p>只是数据库内的数据时限要远远的长于操作型环境中的数据时限。在操作型环境中一般只保存有60<del>90天的数据，而在数据仓库中则要需要保存较长时限的数据（例如：5</del>10年），以适应DSS进行趋势分析的要求。</p></blockquote></li></ul><h3 id="2-OLAP、OLTP概念及用途"><a href="#2-OLAP、OLTP概念及用途" class="headerlink" title="2. OLAP、OLTP概念及用途"></a>2. OLAP、OLTP概念及用途</h3><ul><li><p><strong>OLAP:</strong>  即<code>On-Line Analysis Processing</code>在线分析处理。</p><blockquote><p><strong>OLAP的特点</strong>：联机分析处理的主要特点，是直接仿照用户的多角度思考模式，预先为用户组建<strong>多维</strong>的数据模型，维指的是用户的分析角度。</p></blockquote></li><li><p><strong>OLTP:</strong>  即<code>On-Line Transaction Processing</code>联机事务处理过程(OLTP)</p><blockquote><p>OLTP的特点：结构复杂、实时性要求高。</p></blockquote><p><strong>OLAP和OLTP区别</strong></p><blockquote><p>1、<strong>基本含义不同</strong>：OLTP是传统的关系型数据库的bai主要应用，主要是基本的、日常的事务处理，记du录即时的增、删、改、查，比如在银行存取一笔款，就是一个事务交易。OLAP即联机分析处理，是数据仓库的核心部心，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。典型的应用就是复杂的动态报表系统。</p><p>2、<strong>实时性要求不同</strong>：OLTP实时性要求高，OLTP 数据库旨在使事务应用程序仅写入所需的数据，以便尽快处理单个事务。OLAP的实时性要求不是很高，很多应用顶多是每天更新一下数据。</p><p>3、<strong>数据量不同</strong>：OLTP数据量不是很大，一般只读/写数十条记录，处理简单的事务。OLAP数据量大，因为OLAP支持的是动态查询，所以用户也许要通过将很多数据的统计后才能得到想要知道的信息，例如时间序列分析等等，所以处理的数据量很大。</p><p>4、<strong>用户和系统的面向性不同</strong>：OLTP是面向顾客的,用于事务和查询处理。OLAP是面向市场的,用于数据分析。</p><p>5、<strong>数据库设计不同</strong>：OLTP采用实体-联系ER模型和面向应用的数据库设计。OLAP采用星型或雪花模型和面向主题的数据库设计。</p></blockquote></li></ul><p><img src="https://iknow-pic.cdn.bcebos.com/80cb39dbb6fd526618590e1fa618972bd407364e?x-bce-process=image/resize,m_lfit,w_600,h_800,limit_1" alt="OLAP和OLTP区别"></p><h3 id="3-事实表、维度表、拉链表概念及区别"><a href="#3-事实表、维度表、拉链表概念及区别" class="headerlink" title="3. 事实表、维度表、拉链表概念及区别"></a>3. 事实表、维度表、拉链表概念及区别</h3><ul><li><strong>事实表：</strong>事实表其实质就是通过各种维度和一些指标值得组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。每行数据代表一个<strong>业务事件</strong>，（下单、支付、退款、评价等） 。“事实”这个术语表示的是<strong>业务事件的度量值</strong>（可统计次数、个数、金额等） <ul><li>事务型事实表</li><li>周期快照型事实表</li><li>累积快照型事实表</li></ul></li><li><h2 id="维度表：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。"><a href="#维度表：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。" class="headerlink" title="维度表：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。"></a><strong>维度表：</strong>维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。</h2></li><li><strong>拉链表：</strong>拉链表，它是一种维护<strong>历史状态</strong>，以及<strong>最新状态数据</strong>的一种表。拉链表也是分区表，有些不变的数据或者是已经达到状态终点的数据就会把它放在分区里面，分区字段一般为开始时间：start_date和结束时间：end_date。一般在该天有效的数据，它的end_date是大于等于该天的日期的。获取某一天全量的数据，可以通过表中的start_date和end_date来做筛选，选出固定某一天的数据。例如我想取截止到20190813的全量数据，其where过滤条件就是where start_date&lt;=’20190813’ and end_date&gt;=20190813。</li></ul><p><strong>拉链表使用场景和实现方式？</strong><br>  <strong>【yy总结】</strong><br>  拉链表使用场景：需要查看历史某一时间节点的状态，同时考虑到存储空间。<br>  <strong>实现方式：</strong><br>  首先是拉链表dw_order_his的设置，有start_date和end_date两个字段；<br>  其次在ods层创建一个ods_order_update表，储存当变化数据（包括insert和update的数据）<br>  源表（order）  ods_order_update表和dw_order_his表的交集进行封链操作，end_date=current_date<br>  ods_oder_update数据插入到his表中，对于记录的end_date=9999-12-31,start_date=current_date</p><p>  <strong>【使用场景】</strong><br>  在数据仓库的数据模型设计过程中，经常会遇到下面这种表的设计：<br>   有一些表的数据量很大，比如一张用户表，大约10亿条记录，50个字段，这种表，即使使用ORC压缩，单张表的存储也会超过100G，在HDFS使用双备份或者三备份的话就更大一些。<br>   表中的部分字段会被update更新操作，如用户联系方式，产品的描述信息，订单的状态等等。<br>   需要查看某一个时间点或者时间段的历史快照信息，比如，查看某一个订单在历史某一个时间点的状态。<br>   表中的记录变化的比例和频率不是很大，比如，总共有10亿的用户，每天新增和发生变化的有200万左右，变化的比例占的很小。</p><p>  <strong>【实现方式】</strong><br>  全量主要数据表加载的策略为每次加载时需要根据主键将目标表的数据与源表数据进行比对，删除目标表中在源数据表中的相关记录，然后将源表数据全部插入目标表。表现在脚本上为先delete相关记录，然后insert所有记录。主表加载策略主要用于大部分主表的加载，比如客户信息等主要数据表。<br>  增量拉链是指每次加载时，将源表数据视为增量抽取后的结果，加载到目标表时需要考虑数据历史情况。一般数据发生变化时关闭旧数据链，然后开新数据链。增量拉链针对的是历史表情况，由于数据仓库中记录了大部分数据历史表变化情况，因此增量拉链加载策略在数据仓库中是使用比较广泛的一种加载策略。通常这种历史表都含有start_date和end_date字段，首先全字段对比源数据和目标表得出真正的增量数据，这里的全字段不包含start_date和end_date字段，然后根据主键对目标表进行关旧链操作，然后对新增数据开新链，这种拉链策略同样可以处理全量数据。</p><p>【<strong>拉链表性能优化</strong>】</p><p>  拉链表当然也会遇到查询性能的问题，比如说我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了，个人认为两个思路来解决：</p><ol><li>在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。</li><li>保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。</li></ol><h3 id="4-全量表、增量表、快照表概念及区别"><a href="#4-全量表、增量表、快照表概念及区别" class="headerlink" title="4. 全量表、增量表、快照表概念及区别"></a>4. 全量表、增量表、快照表概念及区别</h3><ul><li><strong>全量表：</strong>全量表没有分区，表中的数据是前一天的所有数据，比如说今天是24号，那么全量表里面拥有的数据是23号的所有数据，每次往全量表里面写数据都会覆盖之前的数据，所以<strong>全量表不能记录历史的数据情况，只有截止到当前最新的、全量的数据</strong>。</li><li><strong>增量表：</strong>增量表，就是<strong>记录每天新增数据的表</strong>，比如说，从24号到25号新增了那些数据，改变了哪些数据，这些都会存储在增量表的25号分区里面。上面说的快照表的25号分区和24号分区（都是t+1，实际时间分别对应26号和25号），它两的数据相减就是实际时间25号到26号有变化的、增加的数据，也就相当于增量表里面25号分区的数据。</li><li><strong>快照表：</strong>那么要能查到历史数据情况又该怎么办呢？这个时候快照表就派上用途了，快照表是有时间分区的，每个分区里面的数据都是分区时间对应的前一天的所有全量数据，比如说当前数据表有3个分区，24号，25号，26号。其中，24号分区里面的数据就是从历史到23号的所有数据，25号分区里面的数据就是从历史到24号的所有数据，以此类推。</li></ul><h3 id="5-什么叫维度和度量值"><a href="#5-什么叫维度和度量值" class="headerlink" title="5. 什么叫维度和度量值"></a>5. 什么叫维度和度量值</h3><ul><li><p><strong>维度</strong>：说明数据，维度是指可指定不同值的对象的描述性<strong>属性或特征</strong>。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。 </p></li><li><p><strong>度量</strong>：事实表和维度交叉汇聚的点，度量和维度构成OLAP的主要概念，这里面对于在事实表或者一个多维立方体里面存放的数值型的、连续的字段，就是度量。这符合上面的意思，有标准，一个度量字段肯定是统一单位，例如元、户数。如果一个度量字段，其中的度量值可能是欧元又有可能是美元，那这个度量可没法汇总。在统一计量单位下，对不同维度的描述。</p></li></ul><h3 id="6-什么叫缓慢变化维（Slowly-Changing-Dimensions，SCD"><a href="#6-什么叫缓慢变化维（Slowly-Changing-Dimensions，SCD" class="headerlink" title="6. 什么叫缓慢变化维（Slowly Changing Dimensions，SCD)"></a>6. 什么叫缓慢变化维（Slowly Changing Dimensions，SCD)</h3><p>​       维度建模的数据仓库中，有一个概念叫Slowly Changing Dimensions，中文一般翻译成缓慢变化维，经常被简写为SCD。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随时间发生变化的维度我们一般称之为缓慢变化维，并且把处理维度表的历史变化信息的问题称为处理缓慢变化维的问题，有时也简称为处理SCD的问题。</p><p><strong>处理缓慢变化维的方法通常分为三种方式：</strong></p><ul><li><p><strong>第一种方式是直接覆盖原值</strong>。这样处理，最容易实现，但是没有保留历史数据，无法分析历史变化信息。第一种方式通常简称为“TYPE 1”。</p></li><li><p><strong>第二种方式是添加维度行</strong>。这样处理，需要<strong>代理键</strong>的支持。实现方式是当有维度属性发生变化时，生成一条新的维度记录，主键是新分配的代理键，通过自然键可以和原维度记录保持关联。第二种方式通常简称为“TYPE 2”。例如将当前行的状态设置为off，并设置一个endtime时间戳，将当前时间标记上。同时新增1行，将其状态标记为on，设置begintime时间戳为上一个记录的endtime+1。</p></li><li><p><strong>第三种方式是添加属性列</strong>。这种处理的实现方式是对于需要分析历史信息的属性添加一列，来记录该属性变化前的值，而本属性字段使用TYPE 1来直接覆盖。这种方式的优点是可以同时分析当前及前一次变化的属性值，缺点是只保留了最后一次变化信息。第三种方式通常简称为“TYPE 3”。</p></li><li><h4 id="6-3-维度表"><a href="#6-3-维度表" class="headerlink" title="6.3 维度表"></a>6.3 维度表</h4><p>Ø 维度表可以看作是用户来分析数据的窗口，<br>Ø 维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息，<br>Ø 维度表包含帮助汇总数据的特性的层次结构。</p><h4 id="6-4-维度分类"><a href="#6-4-维度分类" class="headerlink" title="6.4 维度分类"></a>6.4 维度分类</h4><p><strong>维度的类型:</strong></p><ul><li>缓慢变化维(Slowly Changing Dimension)</li><li>快速变化维(Rapidly Changing Dimension)</li><li>大维(Huge Dimension)和迷你维(Mini-Dimension)</li><li>退化维(Degenerate Dimension)</li></ul><h5 id="缓慢变化维-SCD"><a href="#缓慢变化维-SCD" class="headerlink" title="**缓慢变化维(SCD):"></a><em>**缓慢变化维(SCD):</em></h5><p>大多数的维度的内容都会有不同程度的改变。比如：<br>雇员的升职<br>客户更改了他的名称或地址<br>我们如何去处理这些维度中的变化呢？<br>下面提供了三个处理缓慢变化维的方式<br>直接更新到原先记录中<br>标记记录有效时间的开始日期和结束日期，加入版本控制<br>在记录中添加一个字段来记录历史</p><h5 id="快速变化维-FCD"><a href="#快速变化维-FCD" class="headerlink" title="*快速变化维(FCD):"></a><em>*快速变化维(FCD):</em></h5><p>当某个维度的变化是非常快的时候，我们认定他为快速变化维(具体要看实际的变化频率)，比如：<br>产品的价格，地产的价格等<br>例如在一个分析用户如何使用搜索引擎的DW项目中，将用户搜索的关键字作为一个维度。由于用户使用的关键字会快速变化，因此关键字维度中的数据量会迅速增加。<br>另外一个例子就是精度为秒的时间维度，每秒就会增加一个维度值</p><p><strong>通常RCD的处理可以分为3类：</strong></p><p>Rapidly Changing Small Dimensions – 即维度表字段并不多，表的数据量也不大的情况。这种情形应用SCD中的Type2就可以了。(即：新增一行，旧行置过期)<br>Rapidly Changing Large Dimensions – 即维度表字段较多，表的数据量较大的情况。这种情形Type2会增加过多的行并导致效率降低，因此通常采用Type3.（新增列：仅保存上一个值previous_value,current_value）<br>Rapidly Changing Monster Dimensions – 最糟糕的情况，即维度表字段较多，表的数据量很大，且维度表中的一部分字段频繁变化的情况。此时应将相对稳定的字段和频繁变化的字段分割开，频繁变化的字段独立出来形成新的维度表与事实表相连或形成新的雪花关系。（维表分离）</p><p><strong>大维度(HugeDimension):</strong></p><p>数据仓库中最有意思的维度是一些非常大的维度，比如客户，产品等等。一个大的企业客户维度往往有上百万记录，每条记录又有上百个字段。而大的个人客户维度则会超过千万条记录，这些个人客户维度有时也会有十多个字段，但大多数时候比较少见的维度也只有不多的几个属性。<br>大维度需要特殊的处理。由于数据量大，很多涉及大维度数据仓库功能可能会很慢，效率很低。你需要采用高效率的设计方法、选择正确的索引、或者采用其它优化技术来处理以下问题，包括：<br>向大维度表填充数据<br>非限制维度的浏览性能，尤其是那些属性较少的维度<br>多限制的维度属性值的浏览时间<br>涉及大维度表的对事实表查询的低效率问题<br>为处理第二类修改所需要增加的额外的记录</p><p><strong>迷你维(MiniDimension):</strong></p><p>将常用的大维度中的少数字段提取出来，形成一个字段少的维度，在查询的时候便可以使用迷你维中的字段<br>这样的设计明显提高查询效率</p><p><strong>普通维:</strong></p><p>普通维是基于一个维表的维度，由维表中的不同列来表示维度中的不同级别。</p><p><strong>雪花维：</strong></p><p>雪花维是基于多个维表的维度，各个维表间以外键关联，分别存储在同一维度中不同级别的成员列值。</p><p><strong>父子维：</strong></p><p>父子维是基于两个维表列的维度，由维表中的两列来共同定义各个成员的隶属关系，一列称为成员键列，标识每个成员，另一列称为父键列，标识每个成员的父代。<br>父子维度通俗的话来讲，这个表是自反 的，即外键本身就是引用的主键；类似这样的关系，如公司组织结构，分公司是总公司的一部分，部门是分公司的一部分，当然如果定义得好的话员工是部门的一部 分；通常公司的组织架构并非处在等层次上的，例如总公司下面的部门看起来就和分公司是一样的层次。因此父子维的层次通常不固定的。<br>因为父子维的复杂的自引用关系，如果按照缓慢维度的全历史记录方式来处理，必然导致逻辑关系混乱，处理起来比较棘手；任何一个组织的变动 (修改名称，更改引用，新增等等操作 )将会引起其下属节点相应的变动；任何一个意外都会导致整个结构的变化，同时发生意外后所带来的逻辑关系很难理顺。而 SQLServer2000中 Analysis Service对于这种急剧的变化处理并不稳定。<br>因此建议按照缓慢变化维的覆盖方式解决，即只根据主键这个唯一标志进行判断是否是新增还是修改。<br><strong>索引：</strong><br>与在其他关系数据库中一样，索引对数据仓库的性能具有重要作用。每个维度表都必须在主键上建立索引。在其他列如标识层次结构级别的列上，索引对于某些专用查询的性能也很游泳。事实数据表必须在由维度表外键构成的组件主键上建立索引。<br><strong>粒度(Grain) 层次(Hierarchy)：</strong><br>Ø 粒度是指数据仓库的数据单位中保存数据的细化或综合程度的级别。细化程度越高，粒度级就越小；相反，细化程度越低，粒度级就越大。设计粒度是设计数据仓库中的一个重要的前提<br>Ø 层次指描述明细数据的层次<br>一些影响维度建模的因素:<br>Ø 数据或展现的安全性<br>Ø 复杂的查询和分析</p></li></ul><h2 id="5-2-数仓分层设计"><a href="#5-2-数仓分层设计" class="headerlink" title="5.2 数仓分层设计"></a>5.2 数仓分层设计</h2><h3 id="1-数据仓库分为5层："><a href="#1-数据仓库分为5层：" class="headerlink" title="1. 数据仓库分为5层："></a>1. 数据仓库分为5层：</h3><ul><li>ODS层 （原始数据层）  BDM</li><li>DWD层 （明细数据层） FDM </li><li>DWS层 （服务数据层） GDM</li><li>DWT层（数据主题层）  ADM</li><li>ADS层 （数据应用层）  APP</li></ul><h3 id="2-各层主要负责职责"><a href="#2-各层主要负责职责" class="headerlink" title="2. 各层主要负责职责"></a>2. 各层主要负责职责</h3><ul><li><strong>ODS层（原始数据层）</strong>：存放原始数据，直接加载原始日志、数据，数据保存原貌不做处理。</li><li><strong>DWD层（明细数据层）</strong>：结构与粒度原始表保持一致，对ODS层数据进行清洗（去除空值、脏数据、超过极限范围的数据）</li><li><strong>DWS层 （服务数据层）</strong>：以DWD为基础，进行轻度汇总</li><li><strong>DWT层（数据主题层）</strong>：以DWS为基础，进行累积汇总</li><li><strong>ADS层 （数据应用层）</strong>：为各种统计报表提供数据</li></ul><h3 id="3-为什么要分层？"><a href="#3-为什么要分层？" class="headerlink" title="3. **为什么要分层？**"></a><strong>3. **为什么要分层</strong>？**</h3><ul><li><p><strong>减少重复开发：</strong>规范数据分层， 通过的中间层数据， 能够减少极大的重复计算， 增加一次计算结果的复用性。</p></li><li><p><strong>把复杂问题简单化：一</strong>个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较<strong>简单和容易理解</strong>。而且<strong>便于维护</strong>数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复</p></li><li><p><strong>隔离原始数据：</strong>不论是数据的异常还是数据的敏感性， 使真实数据与统计数据解耦开</p></li></ul><h3 id="4-数仓中每层表的建模？怎么建模？"><a href="#4-数仓中每层表的建模？怎么建模？" class="headerlink" title="4. 数仓中每层表的建模？怎么建模？"></a>4. 数仓中每层表的建模？怎么建模？</h3><p><strong>（1）ODS：</strong> 特点是保持原始数据的原貌，不作修改！</p><p>原始数据怎么建模，ODS就怎么建模！举例： 用户行为数据特征是一条记录就是一行！</p><p>ODS层表(line string) 业务数据，参考Sqoop导入的数据类型进行建模！</p><p>（2）<strong>DWD层</strong>：特点从ODS层，将数据进行ETL（清洗），轻度聚合，再展开明细！</p><ul><li>在展开明细时，对部分维度表进行降维操作</li></ul><blockquote><p>例如：将商品一二三级分类表，sku商品表，spu商品表，商品品牌表合并汇总为一张维度表！</p></blockquote><ul><li>对事实表，参考星型模型的建模策略，按照<strong>选择业务过程→声明粒度→确认维度→确认事实</strong>思路进行建模</li></ul><blockquote><p><strong>选择业务过程</strong>： 选择感兴趣的事实表<br><strong>声明粒度</strong>： 选择最细的粒度！可以由最细的粒度通过聚合的方式得到粗粒度！<br><strong>确认维度</strong>： 根据3w原则确认维度，挑选自己感兴趣的维度<br><strong>确认事实</strong>： 挑选感兴趣的度量字段，一般是从事实表中选取！</p></blockquote><ul><li><strong>DWS层：</strong> 根据业务需求进行分主题建模！一般是建宽表！</li><li><strong>DWT层：</strong> 根据业务需求进行分主题建模！一般是建宽表！</li><li><strong>ADS层：</strong> 根据业务需求进行建模！</li></ul><h2 id="5-3-数仓建模"><a href="#5-3-数仓建模" class="headerlink" title="5.3 数仓建模"></a>5.3 数仓建模</h2><h3 id="1-维度建模概念、类型、过程"><a href="#1-维度建模概念、类型、过程" class="headerlink" title="1. 维度建模概念、类型、过程"></a>1. 维度建模概念、类型、过程</h3><p><strong>维度建模：</strong>维度建模是一种将数据结构化的逻辑设计方法，它将客观世界划分为<strong>度量</strong>和<strong>上下文</strong>。度量是常常是以数值形式出现，事实周围有上下文包围着，这种上下文被直观地分成独立的逻辑块，称之为<strong>维度</strong>。它与实体-关系建模有很大的区别，实体-关系建模是面向应用，遵循第三范式，以消除数据冗余为目标的设计技术。维度建模是面向分析，为了提高查询性能可以增加数据冗余，反规范化的设计技术。</p><p><strong>维度建模过程：</strong>确定业务流程-&gt;确定粒度-&gt;确定纬度-&gt;确定事实</p><blockquote><p>建模四步走：</p><p><strong>1.选取要建模的业务处理流程</strong></p><p>　　　　关注业务处理流程，而不是业务部门！</p><p><strong>2.定义业务处理的粒度</strong></p><p>　　　　“如何描述事实表的单个行？”</p><p><strong>3.选定用于每个事实表行的维度</strong></p><p>　　　　常见维度包括日期、产品等</p><p><strong>4.确定用于形成每个事实表行的数字型事实</strong></p><p>　　　　典型的事实包括订货量、支出额这样的可加性数据</p></blockquote><h3 id="2-星型模型和雪花模型概念、区别"><a href="#2-星型模型和雪花模型概念、区别" class="headerlink" title="2. 星型模型和雪花模型概念、区别"></a>2. 星型模型和雪花模型概念、区别</h3><p>​      在维度建模的基础上又分为三种模型： 星型模型、 雪花模型、 星座模型。</p><ul><li><p><strong>星型模型：</strong>雪花模型与星型模型的区别主要在于<strong>维度的层级</strong>，标准的星型模型维度只有一层，而雪花模型可能会涉及多级。</p></li><li><p><strong>雪花模型：</strong> 比较靠近3NF， 但是无法完全遵守， 因为遵循3NF的性能成本太高。</p></li><li><p><strong>星座模型：</strong>星座模型与前两种情况的区别是事实表的数量， 星座模型是基于多个事实表。基本上是很多数据仓库的常态， 因为很多数据仓库都是多个事实表的。 所以星座不星座只反映是否有多个事实表， 他们之间是否共享一些维度表。所以星座模型并不和前两个模型冲突。</p></li><li><p><strong>如何选择</strong></p></li></ul><p>​         模型的选择首先就是星座不星座这个只跟数据和需求有关系， 跟设计没关系， 不用选择。星型还是雪花， <strong>取决于性能优先， 还是灵活更优先。</strong><br>​        目前实际企业开发中， 不会绝对选择一种， 根据情况灵活组合， 甚至并存（一层维度和多层维度都保存） 。 但是整体来看， 更倾向于维度更少的星型模型。 尤其是Hadoop体系， 减少Join就是减少Shuffle， 性能差距很大。（关系型数据可以依靠强大的主键索引）</p><p>　　<strong>总结</strong></p><p>　　通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。</p><h2 id="5-4-数据治理"><a href="#5-4-数据治理" class="headerlink" title="5. 4 数据治理"></a>5. 4 数据治理</h2><p>1）数据压缩</p><p>2）小文件合并</p><p>3）冷数据处理</p><h2 id="5-5-总线架构"><a href="#5-5-总线架构" class="headerlink" title="5.5 总线架构"></a>5.5 总线架构</h2><ul><li><p>总线架构</p></li><li><p>事实一致性</p></li><li><p>维度一致性</p></li></ul><h2 id="5-6-数仓总结"><a href="#5-6-数仓总结" class="headerlink" title="5.6 数仓总结"></a>5.6 数仓总结</h2><p>![image-20210220171930469](/Users/liyu/Library/Application Support/typora-user-images/image-20210220171930469.png)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;一-Hadoop篇&quot;&gt;&lt;a href=&quot;#一-Hadoop篇&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="Job" scheme="https://dataquaner.github.io/categories/Job/"/>
    
    
      <category term="Job" scheme="https://dataquaner.github.io/tags/Job/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化分析平台开源方案集锦</title>
    <link href="https://dataquaner.github.io/2021/02/11/2021-02-11-shu-ju-ke-shi-hua-fen-xi-ping-tai-kai-yuan-fang-an-ji-jin/"/>
    <id>https://dataquaner.github.io/2021/02/11/2021-02-11-shu-ju-ke-shi-hua-fen-xi-ping-tai-kai-yuan-fang-an-ji-jin/</id>
    <published>2021-02-11T01:25:00.000Z</published>
    <updated>2021-02-15T03:21:39.690Z</updated>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>B/S 架构的数据可视化分析平台开源方案不完全集锦，供各位参考。 排名不分先后。欢迎补充。</p><h3 id="kibana"><a href="#kibana" class="headerlink" title="kibana"></a><a href="https://github.com/elastic/kibana" target="_blank" rel="noopener">kibana</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684a55be82fe5c5?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>Elasticsearch 专用的数据分析检索仪表盘。ELK Stack 中的 K。</p><p>日志系统常见的可视化开源解决方案。</p><p>使用 Nodejs+AnglarJs+React 开发,元数据存储在 ES 的一个索引中。</p><p>Elastic公司维护开源，社区非常活跃，持续迭代中。</p><h3 id="grafana"><a href="#grafana" class="headerlink" title="grafana"></a><a href="https://github.com/grafana/grafana" target="_blank" rel="noopener">grafana</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684a57abdddaac8?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>可视化仪表盘和图形编辑器，是一款常用的指标分析和监控工具。支持Graphite、Elasticsearch、OpenTSDB、Prometheus 和 InfluxDB 作为数据源。</p><p>使用 Golang+TypeScript+AngularJS 开发，元数据支持 mysql 和 postgres。</p><p>Grafana Labs 公司维护，社区非常活跃，持续迭代中。</p><h3 id="Superset"><a href="#Superset" class="headerlink" title="Superset"></a><a href="https://github.com/apache/incubator-superset" target="_blank" rel="noopener">Superset</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684a6b7e42c4ab9?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>孵化中的准企业级 BI 应用。</p><p>很多大公司都在内部使用。</p><p>支持的数据源有 MySQL、Postgres、Vertica、Oracle、Microsoft SQL Server、SQLite、Greenplum、Firebird、MariaDB、Sybase、IBM DB2、Exasol、MonetDB、Snowflake、Redshift、Clickhouse、Apache Kylin 等！</p><p>使用 Python+Flask+react+jQuery开发，默认使用 sqlite 存储元数据。 由Airbnb开源，现已归属于 Apache 孵化项目，社区非常活跃，持续迭代中。</p><h3 id="Zeppelin"><a href="#Zeppelin" class="headerlink" title="Zeppelin"></a><a href="https://github.com/apache/zeppelin" target="_blank" rel="noopener">Zeppelin</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684a864ce7f4ee0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>支持交互式数据分析的多用途 notebook 编辑器工具，可以接入不同的数据处理引擎和解释器，包括 Apache Spark，Python，JDBC，Markdown和Shell 等。内置Apache Spark集成。</p><p>Java+Angular 开发，元数据 notebook 默认使用本地文件系统存储在git仓库中。 由 Apache 开源，持续迭代中，目前版本 0.8。</p><h3 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a><a href="https://github.com/cloudera/hue" target="_blank" rel="noopener">Hue</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684a970964dc810?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>开发和访问SQL、数据应用的工作台，支持智能的SQL和任务编辑器、Dashboard 、任务工作流调度、数据浏览器。 Hadoop生态系统可视化利器。</p><p>SQL支持： Hive、Impala、MySQL、Oracle、KSQL / Kafka SQL、Solr SQL、Presto、PostgreSQL、Redshift、BigQuery、AWS Athena、Spark SQL、Phoenix、Kylin等。</p><p>任务支持：MapReduce、Java、Pig、Sqoop、Shell、DistCp、Spark等。</p><p>使用Python+Django+jquery 开发，元数据默认使用 SQLite存储。</p><p>Hue 由 Cloudera Desktop 演化而来，最后 Cloudera 公司将其贡献给Apache基金会的Hadoop社区。</p><h3 id="CBoard"><a href="#CBoard" class="headerlink" title="CBoard"></a><a href="https://github.com/TuiQiao/CBoard" target="_blank" rel="noopener">CBoard</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684ab8e306ce74d?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>国产BI 报表和dashboard平台。</p><p>支持JDBC数据源，Saiku2.x数据源，Kylin1.6，Elasticsearch 1.x, 2.x, 5.x。</p><p>使用 Java Spring+MyBatis+AngularJS+Bootstrap 开发。元数据使用MySQL5+/SQLServer。</p><p>上海楚国公司开源，最近发现官方出了收费的企业版，这个社区版显得low了很多。</p><h3 id="Mining"><a href="#Mining" class="headerlink" title="Mining"></a><a href="https://github.com/mining/mining" target="_blank" rel="noopener">Mining</a></h3><p>![img](data:image/svg+xml;utf8,<!--?xml version="1.0"?--><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="1280" height="1279"></svg>)</p><p>Python写的BI应用（Pandas web 界面）</p><p>OpenMining 支持基于 ORM SQLAlchemy 的所有数据库。</p><p>使用 Python+Lua+AngularJs+jQuery开发，元数据存储在MongoDB。</p><p>由Avelino 和 UP! Essência开发，master分支的最新 commit 已经是2016年了</p><h3 id="Saiku"><a href="#Saiku" class="headerlink" title="Saiku"></a><a href="https://github.com/OSBI/saiku" target="_blank" rel="noopener">Saiku</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684b02d9aecc1b0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>经典的OLAP开源方案，Saiku是一个模块化分析套件，提供轻量级OLAP，易于嵌入，可扩展和可配置。 支持 Mondrian, XMLA 或者 Mongo数据源链接类型。</p><p>其提供一个Schema设计器、交互式的报表引擎、展示板和nosql连接技术。使用REST API连接OLAP系统。</p><p>使用Java+backbone+jQuery开发，使用JackRabbit管理树状元数据。</p><p>最初叫做Pentaho分析工具，起初是基于OLAP4J库用GWT（google web toolkit）包装的一个前端分析工具。后改名Saiku，Analytical Labs 提供支持。</p><h3 id="Metabase"><a href="#Metabase" class="headerlink" title="Metabase"></a><a href="https://github.com/metabase/metabase" target="_blank" rel="noopener">Metabase</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684b0fc70167d7b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>简单快速的方式使用BI和分析。支持Postgres、MySQL、Druid、SQL Server、Redshift、MongoDB、Google BigQuery、SQLite、H2、Oracle、Vertica、Presto、Snowflake。支持不写SQL 的方式做可视化分析。支持docker、jar包方式安装。</p><p>使用clojure和node开发，前端使用react框架。元数据默认存储在H2数据库中。</p><p>社区较为活跃，项目也在持续更新中。</p><h3 id="redash"><a href="#redash" class="headerlink" title="redash"></a><a href="https://github.com/getredash/redash" target="_blank" rel="noopener">redash</a></h3><p>![img](data:image/svg+xml;utf8,<!--?xml version="1.0"?--><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="714" height="491"></svg>)</p><p>SQL editor+可视化，支持35种数据源：Amazon Athena、Amazon DynamoDB、Amazon Redshift、Axibase Time Series Database、Cassandra、ClickHouse、CockroachDB、CSV、Databricks、DB2 by IBM、Druid、Elasticsearch、Google Analytics、Google BigQuery、Google Spreadsheets、Graphite、Greenplum、Hive、Impala、InfluxDB、JIRA、JSON、Apache Kylin、MapD、MemSQL、Microsoft SQL Server、MongoDB、MySQL、Oracle、PostgreSQL、Presto、Prometheus、Python、Qubole、Rockset、Salesforce、ScyllaDB、Shell Scripts、Snowflake、SQLite、TreasureData、Vertica、Yandex AppMetrrica、Yandex Metrica。</p><p>后端使用Python 前端使用Angular、React，元数据环境使用PostgreSQL &amp; Redis。</p><p>该项目目前也比较活跃，持续迭代中。</p><h3 id="SqlPad"><a href="#SqlPad" class="headerlink" title="SqlPad"></a><a href="https://github.com/rickbergfalk/sqlpad" target="_blank" rel="noopener">SqlPad</a></h3><p><img src="https://user-gold-cdn.xitu.io/2019/1/14/1684b0878632378f?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>不知放在这里是否合适，SqlPad一款基于web 的 SQL 编辑器，支持MySQL, Postgres, SQL Server, Vertica, Crate, Presto, SAP HANA, 和 Cassandra，支持数据可视化。但不支持仪表板等功能。</p><p>使用Nodejs+React开发，元数据存储在 Nedb中。</p><p>由Rick Bergfalk开发，持续维护中。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>这些是我收集或调研过的一些数据可视化开源方案，它们或许在成熟稳定的企业级方案面前显得支离杂乱，也或许在牛人遍地的大厂内部显得不够专业。但它们开发者给提供了优秀的参考案例和二次开发的母版，给小企业带来了几乎免费的数据分析和可视化的能力。由衷的感谢这些令人兴奋的项目，感谢为开源奉献的人们。</p><p>由于本人没有全部体验和深入调研上述项目，上述简介仅供参考，以官方为准。</p><p>作者：磊仔🙈🙈🙈🙈🙈🙈<br>链接：<a href="https://juejin.cn/post/6844903760867622920" target="_blank" rel="noopener">https://juejin.cn/post/6844903760867622920</a><br>来源：掘金<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;B/S 架构的数据可视化分析平台开源方案不完全集锦，供各位参考。 排名不分先后。欢迎补充。&lt;/p&gt;
&lt;h3 id=&quot;kibana&quot;&gt;&lt;a href=&quot;#ki
      
    
    </summary>
    
    
      <category term="Data Analysis" scheme="https://dataquaner.github.io/categories/Data-Analysis/"/>
    
    
      <category term="Data Analysis" scheme="https://dataquaner.github.io/tags/Data-Analysis/"/>
    
      <category term="BI" scheme="https://dataquaner.github.io/tags/BI/"/>
    
      <category term="Superset" scheme="https://dataquaner.github.io/tags/Superset/"/>
    
  </entry>
  
  <entry>
    <title>如何构建用户标签体系（转载）</title>
    <link href="https://dataquaner.github.io/2020/12/27/ru-he-gou-jian-yong-hu-biao-qian-ti-xi/"/>
    <id>https://dataquaner.github.io/2020/12/27/ru-he-gou-jian-yong-hu-biao-qian-ti-xi/</id>
    <published>2020-12-27T06:35:00.000Z</published>
    <updated>2020-12-28T07:38:32.252Z</updated>
    
    <content type="html"><![CDATA[<p>亚马逊的CEO Jeff Bezos曾说过他的梦想，「如果我有一百万的用户，我就会做一百万个不同的网站！」，做这个基础是先对用户打标签。</p><p><img src="https://pic1.zhimg.com/80/v2-579db58bbdd920ad4da6bea7875e8840_720w.jpg" alt="img"></p><p>而目前基于标签的智能推荐系统，已经有了成熟商业应用，比如：淘宝的千人千面，美团外卖的智能推荐，腾讯的社交广告。</p><h2 id="一、思考的背景"><a href="#一、思考的背景" class="headerlink" title="一、思考的背景"></a>一、思考的背景</h2><p>从16年开始，互联网用户增长趋缓，同比仅增长。一方面，不论是线上还是线下，新用户的获取成本都很高。另一方面，用户时间增长也在趋缓。在用户花费时间趋向饱和情况下，不同的产品之间同样存在竞争关系。</p><p>在这个背景下，随着用户量增长，运营人员面临新的挑战，有以下核心诉求：</p><ol><li>一般运营活动中，怎么对不同用户群体分层，提高流量的分发效率？</li><li>对于个体用户，怎么深入到日常使用场景，提高流量的转化效率？</li></ol><p>落到产品设计层面，需要解决以下问题：</p><ol><li>怎么设计一个完善的用户标签体系？怎么打标签？打哪些标签？谁来打？</li><li>怎么使用用户标签，创造商业价值？</li></ol><h2 id="二、怎样建立用户的标签体系？"><a href="#二、怎样建立用户的标签体系？" class="headerlink" title="二、怎样建立用户的标签体系？"></a>二、怎样建立用户的标签体系？</h2><p>讲到用户运营，我觉得有两项基本工作是可以拿出来讲讲的：</p><p>一个是<strong>用户触达体系</strong>，一个是<strong>用户成长激励体系</strong>。</p><p>用户触达简单来说就是给用户推送提醒、活动、召回等各类消息，加上标签化，就可以更有针对性及个性化的为用户推送；</p><p>而用户成长激励就是我们在各类APP上常见的新手任务、日常任务、营销活动、会员体系等等；两者亦有相辅相成的作用。</p><p>其实，这两项内容的基础建设做起来并不难，但由于网上缺少这类实操案例的讲解，让很多初次上手去做的运营缺少认知和了解（比如：我早期需要独立完成这些工作的时候，就搜不到很清楚的内容能够帮助我），导致接到任务在构思时，缺少参考内容。</p><p>所以，这次就跟大家聊聊这两块的经验，希望对你所有启发。</p><h3 id="2-1-做标签体系前先理清“用户类型”"><a href="#2-1-做标签体系前先理清“用户类型”" class="headerlink" title="2.1 做标签体系前先理清“用户类型”"></a>2.1 做标签体系前先理清“用户类型”</h3><p>首先，运营推送分两种，一种我们叫全量推送，比如节日活动、产品重大更新等，我们需要尽可能多的覆盖用户；所以这种直接通过后台push、短信、公众号等就可以操作。</p><p>而另一种是精准推送，比如新手引导、沉默预警、流失召回等行为，我们需要更有针对性的进行推送，但在推送之前，我们肯定要事先知道“这些用户”是谁，对吧。</p><p>因此，用户标签体系工作的开展逻辑就分为三部分，我们运营先理清楚这个“人”，然后通过“条件”标签化定义这个人，最后是策略“触达”这个人。</p><p>给谁推送（运营需求）&gt;如何定义这个人（技术实现）- 何时触发推送（ 运营制定）</p><p>题外话：我们运营在上手执行任何工作之前，一定要自我事先理清楚任务的目的、逻辑和顺序，这样才不会忙一两天的作业交上去，又被领导打回来改来改去，好像领导不理解你的想法或者你做完成却发现不理解领导到底想要什么；原因就是虽然你的执行效率上去了，但却没留时间让自己思考这到底是为什么。</p><p>现在流行的精细化运营，需要我们对用户进行多维度多方面的划分；我就不详细列举了，这里概以我们常用到的用户分层为例讲解。</p><p>那不同的产品根据业务类型不同，对用户层级的划分会有不同的界定，以我们家的工具产品为例。大致可分为四层：新增用户-活跃用户-会员用户-核心用户</p><h3 id="2-2-做标签体系前理清“推送需求”"><a href="#2-2-做标签体系前理清“推送需求”" class="headerlink" title="2.2 做标签体系前理清“推送需求”"></a>2.2 做标签体系前理清“推送需求”</h3><p>通过用户分层，我们明确了想要推送的四类用户，那下一步要思考的是，我要针对不同层级的用户哪些行为进行推送呢？例如：</p><ul><li><strong>新增用户</strong>：作为运营，我想在用户注册后推送一条欢迎&amp;引导上手文案，并且希望在当天内触成用户完成核心功能的体验，三天内督促完善新手任务；这期间我就需要根据用户行为和完成状态的不同，推送不同的文案；</li><li><strong>活跃用户</strong>：作为运营，我想能够判断出当前用户的活跃次数&amp;天数，以此发放不同的优惠政策，看是否通过不同的营销切入点，最大化转化非会员用户；</li><li><strong>再如分群</strong>：运营希望给当前的月卡且在本月将到期的会员用户，单独推送到期提醒及续费优惠活动等。</li></ul><p>以此类推，那可能有人要问了，为什么做标签体系之前，我自己要梳理的这么清楚？直接按照人口属性、行为属性定义后，提需求给技术不就行了，需要说明两点：</p><ol><li>标签体系化工作是一个长期过程，需要阶段性的进行，因此你不可能一上来全面覆盖全面标签化，它是随着产品&amp;业务的发展深度，而进行动态演化的；所以你也不要担心想的不全面；回归需求本身，清楚现产品阶段，你最关心什么，要给谁推送，按照节奏进行。</li><li>标签是分级的，等下看图会更详细，那为什么标签要这样划分级别要清楚，假设第一级标签，你在后台筛选的是活跃用户，那么按照用户的活跃状态，我还可以再分一级，筛选出低频活跃用户或者高频活跃用户。如果你不自己不清楚怎么算低频活跃怎么算高频活跃，那怎么对标签下定义呢？</li></ol><h3 id="2-3-标签定义的四个维度"><a href="#2-3-标签定义的四个维度" class="headerlink" title="2.3 标签定义的四个维度"></a>2.3 标签定义的四个维度</h3><p>当你理清楚了人并且也清楚当前最想要对他的什么行为&amp;状态进行推送，那标签体系化的第一期工作，就可以进入到落地执行环节了。</p><p>我们一般做标签体系，大致先满足四个维度：<strong>人口属性</strong>、<strong>行为属性</strong>、<strong>商业属性</strong>、<strong>消费属性</strong>，四个方面来进行标签填充。</p><p><strong>人口属性</strong>：指的是用户运营画像。比如性别、年龄、地域、设备型号等。这一维度告诉我们他是谁。</p><p>题外话：用户画像一般分两种，一种是用户产品画像，产品经理通过海量用户抽象出产品使用者画像，类似于高度概括的那种，以便判断这个功能是否能够满足这一群体用户的需要；另一种是用户运营画像，因为运营更多是精准触达、活动等，所需的是具体的属性。</p><p><strong>行为属性</strong>：指用户使用产品的日常行为和关键行为，比如注册、签到、活跃状态、功能使用等，这一维度告诉我们他当前做了什么。</p><p>商业属性：指用户在产品上的当前付费状态。比如免费用户、会员用户、免费用户也可根据时间划分为 3 天/ 7 天/ 30天的免费试用用户，按付费类型可为年卡/季卡/月卡用户。这一维度告诉了我们他在产品上的状态。</p><p><strong>消费属性</strong>：这里用到的是 RFM 模型。RFM 模型大家都知道， Rencency（最近一次消费），Frequency（消费频率）、Monetary（消费金额），我们一般分为8个维度。</p><p>除了上述四个基本层面的标签建设外，还有一个可根据运营需要添加或不添加。</p><p><strong>偏好属性</strong>：记录用户在产品上的行为偏好。比如每日都会签到的用户、参与过会员促销活动的用户、参与过拉新活动的用户等等。这一部分用户有的属于利益驱动型用户，可以作为运营上线活动，第一批种子用户或者测试用户。</p><p><img src="https://pic3.zhimg.com/80/v2-ce674b2404d0e9c76f61716f2e420c6e_720w.jpg" alt="img"></p><h3 id="2-4-标签化的推送制定"><a href="#2-4-标签化的推送制定" class="headerlink" title="2.4 标签化的推送制定"></a>2.4 标签化的推送制定</h3><p>在完成了标签定义之后，就是推送制定，针对不同的用户层级，我们的运营动作也所有不同，在上面略有提及，如果是固定的文案&amp;营销推送，可以做成自动化，比如会员到期提醒、沉默预警提醒、流失召回等。</p><p>如果有活动或者AB测试的需求，则可以通过后台按照以上标签筛选进行针对性推送。</p><p>至于推送的文案&amp;时间&amp;活动类型，各家的产品&amp;使用环境都不同，我就不要误导你什么21点推送&amp;新手推引导，活跃推营销等等啦，方法只有一个站在你的角度，多推多看多琢磨。</p><h2 id="三、标签系统的结构"><a href="#三、标签系统的结构" class="headerlink" title="三、标签系统的结构"></a>三、标签系统的结构</h2><p>标签系统可以分为三个部分：<strong>数据加工层</strong>，<strong>数据服务层</strong>，<strong>数据应用层</strong>。每个层面面向用户对象不一样，处理事务有所不同。层级越往下，与业务的耦合度就越小。层级越往上，业务关联性就越强。</p><p><img src="https://pic2.zhimg.com/80/v2-b35ef1e2172c5596a0a168603c6cc001_720w.jpg" alt="img"></p><p>以M电商公司为例，来说明该系统的构成。</p><p><strong>数据加工层。</strong>数据加工层收集，清洗和提取来处理数据。M公司有多个产品线：电商交易，电子书阅读，金融支付，智能硬件等等。每个产品线的业务数据又是分属在不同位置。为了搭建完善的用户标签体系，需要尽可能汇总最大范围内的数据。同时每个产品线的也要集合所有端的数据，比如：App，web，微信，其它第三方合作渠道。</p><p>收集了所有数据之后，需要经过清洗：去重，去刷单数据，去无效数据，去异常数据等等。然后再是提取特征数据，这部分就要根据产品和运营人员提的业务数据要求来做就好。</p><p><strong>数据业务层。</strong>数据加工层为业务层提供最基础数据能力，提供数据原材料。业务层属于公共资源层，并不归属某个产品或业务线。它主要用来维护整个标签体系，集中在一个地方来进行管理。</p><p>在这一层，运营人员和产品能够参与进来，提出业务要求：将原材料进行切割。主要完成以下核心任务：</p><ol><li>定义业务方需要的标签。</li><li>创建标签实例。</li><li>执行业务标签实例，提供相应数据。</li></ol><p><strong>数据应用层。</strong>应用层的任务是赋予产品和运营人员标签的工具能力，聚合业务数据，转化为用户的枪火弹药，提供数据应用服务。</p><p>业务方能够根据自己的需求来使用，共享业务标签，但彼此业务又互不影响。实践中可应用到以下几块：</p><ol><li>精准化营销。</li><li>个性化推送。</li></ol><h2 id="四、标签体系的设计"><a href="#四、标签体系的设计" class="headerlink" title="四、标签体系的设计"></a>四、标签体系的设计</h2><h3 id="3-1-业务梳理"><a href="#3-1-业务梳理" class="headerlink" title="3.1. 业务梳理"></a><strong>3.1. 业务梳理</strong></h3><p>搭建用户标签体系容易陷入用户画像陷阱，照葫芦画瓢，不利于标签体系的维护和后期的扩展。可以按下面的思路来梳理标签体系：</p><ol><li>有哪些产品线？产品线有哪些来源渠道？一一列出。</li><li>每个产品线有哪些业务对象？比如用户，商品。</li><li>最后再根据对象聚合业务，每个对象涉及哪些业务？每个业务下哪些业务数据和用户行为？</li></ol><p>结果类似如下：</p><p><img src="https://pic4.zhimg.com/80/v2-2ca31a7a84d54827d55796069aa71ce3_720w.jpg" alt="img"></p><h3 id="3-2-标签的分类"><a href="#3-2-标签的分类" class="headerlink" title="3.2. 标签的分类"></a><strong>3.2. 标签的分类</strong></h3><p>按业务对象整理了业务数据后，可以继续按照对象的属性来进行分类，主要目的：</p><ol><li>方便管理标签，便于维护和扩展。</li><li>结构清晰，展示标签之间的关联关系。</li><li>为标签建模提供子集。方便独立计算某个标签下的属性偏好或者权重。</li></ol><p>梳理标签分类时，尽可能按照MECE原则，相互独立，完全穷尽。每一个子集的组合都能覆盖到父集所有数据。标签深度控制在四级比较合适，方便管理，到了第四级就是具体的标签实例。</p><p><img src="https://pic4.zhimg.com/80/v2-38f923cfb23bcb2552f25f79dd8eb8eb_720w.jpg" alt="img"></p><h3 id="3-3-标签的模型"><a href="#3-3-标签的模型" class="headerlink" title="3.3 标签的模型"></a><strong>3.3 标签的模型</strong></h3><p>按数据的实效性来看，标签可分为</p><ul><li><strong>静态属性标签</strong>。长期甚至永远都不会发生改变。比如性别，出生日期，这些数据都是既定的事实，几乎不会改变。</li><li><strong>动态属性标签</strong>。存在有效期，需要定期地更新，保证标签的有效性。比如用户的购买力，用户的活跃情况。</li></ul><p>从数据提取维度来看，标签数据又可以分为3种类型。</p><ul><li><strong>事实标签</strong>。既定事实，从原始数据中提取。比如通过用户设置获取性别，通过实名认证获取生日，星座等信息。</li><li><strong>模型标签</strong>。没有对应数据，需要定义规则，建立模型来计算得出标签实例。比如支付偏好度。</li><li><strong>预测标签</strong>。参考已有事实数据，来预测用户的行为或偏好。比如用户a的历史购物行为与群体A相似，使用协同过滤算法，预测用户a也会喜欢某件物品。</li></ul><p><img src="https://pic3.zhimg.com/80/v2-8405c26e59bffa04db2dcb804b119abe_720w.jpg" alt="img"></p><h3 id="3-4-标签的处理"><a href="#3-4-标签的处理" class="headerlink" title="3.4. 标签的处理"></a><strong>3.4. 标签的处理</strong></h3><p>为什么要从两个维度来对标签区分？这是为了方便用户标签的进一步处理。</p><p><strong>静态动态的划分是面向业务维度</strong>，便于运营人员理解业务。这一点能帮助他们：</p><ul><li>理解标签体系的设计。</li><li>表达自己的需求。</li></ul><p><strong>事实标签，模型标签，预测标签是面向数据处理维度</strong>，便于技术人员理解标签模块功能分类，帮助他们：</p><ul><li>设计合理数据处理单元，相互独立，协同处理。</li><li>标签的及时更新及数据响应的效率。</li></ul><p>以上面的标签图表为例，面临以下问题：</p><ol><li>属性信息缺失怎么办？比如，现实中总有用户未设置用户性别，那怎么才能知道用户的性别呢？</li><li>行为属性，消费属性的标签能不能灵活设置？比如，活跃运营中需要做A/B test，不能将品牌偏好规则写死，怎么办？</li><li>既有的属性创建不了我想要的标签？比如，用户消费能力需要综合结合多项业务的数据才合理，如何解决？</li></ol><p>模型标签的定义解决的就是从无到有的问题。建立模型，计算用户相应属性匹配度。现实中，事实标签也存在数据缺失情况。</p><blockquote><p>比如用户性别未知，但是可以根据用户浏览商品，购买商品的历史行为来计算性别偏好度。当用户购买的女性化妆品和内衣较多，偏好值趋近于性别女，即可以推断用户性别为女。</p></blockquote><p>模型计算规则的开放解决的是标签灵活配置的问题。运营人员能够根据自己的需求，灵活更改标签实例的定义规则。比如图表中支付频度实例的规则定义，可以做到：</p><ol><li>时间的开放。支持时间任意选择：昨天，前天，近x天，自定义某段时间等等。</li><li>支付笔数的开放。大于，等于，小于某个值，或者在某两个值区间。</li></ol><p>标签的组合解决就是标签扩展的问题。除了原有属性的规则定义，还可以使用对多个标签进行组合，创建新的复合型标签。比如定义用户的消费能力等级。</p><p>标签最终呈现的形态要满足两个需求：</p><ol><li><strong>标签的最小颗粒度要触达到具体业务事实数据</strong>，<strong>同时支持对应标签实例的规则自定义。</strong></li><li><strong>不同的标签可以相互自由组合为新的标签，同时支持标签间的关系，权重自定义</strong>。</li></ol><h2 id="五、实践分享"><a href="#五、实践分享" class="headerlink" title="五、实践分享"></a>五、实践分享</h2><p>数据应用层即为标签的使用场景，最典型的应用场景是：精准推送。</p><p><strong>精准推送。</strong>该场景对标签的实效性要求并不高，可以只考虑离线的历史数据，不需要结合实时数据，是标签首选的实践场景。运营人员使用标签筛选出目标用户，定向推送活动。推送渠道根据活动的需要来进行多渠道投放，能够支持微信，App，短信。</p><p>运营主要工作基本就是不停地生产活动，向用户投食，监测活动的效果，不断优化投放策略：找到不同用户对应的最佳匹配活动。这块主要关注活动以下环节：</p><ul><li>活动前：目标用户，活动内容，投放渠道。</li><li>活动中：效果监控和跟踪。</li><li>活动后：效果复盘和优化。</li></ul><p>除精准推送外，用户标签还有其它的应用场景。在技术层面上，对算法建模及响应性能也有更高的要求：</p><ul><li>推荐栏位</li><li>消费周期评估</li><li>广告投放</li><li>促销排期</li></ul><p>另外，用户的数据信息不仅局限于应用内本身。仅通过用户昵称或手机号已经足以爬取到用户在全网内留下的所有信息，从而构建丰富的用户画像。你多大？在哪里工作？家庭人员情况？在技术面前，都是一张透明的白纸。只不过目前这样做要花费很多人力，成本太高。</p><p>前天，产品群里有人问为啥有的产品引导用户关联第三方账号？同样是为了获取用户数据，用户一般并不知晓，以为只是增加新的登录方式。</p><p><img src="https://pic4.zhimg.com/80/v2-b61296cde7387bac62cd6fff0e9e5127_720w.jpg" alt="img"></p><p><img src="https://pic4.zhimg.com/80/v2-7bf74eb6b2368191c5a3e4f7e6f63d03_720w.jpg" alt="img"></p><h2 id="建议及想法"><a href="#建议及想法" class="headerlink" title="建议及想法"></a>建议及想法</h2><p>如果你的产品微信粉丝数量接近千万级，不防试一试，用标签做精准营销。微信聚合了大量的粉丝，向商家端开放了粉丝的基本信息，提供了开放接口能力及多种消息触达方式，是最好的试验场。</p><p>微信聚合了最大和最优质的流量。从这个角度出发，基于微信提供的能力，做一款针对C端营销的CRM营销产品，存在着很大的商业机会。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;亚马逊的CEO Jeff Bezos曾说过他的梦想，「如果我有一百万的用户，我就会做一百万个不同的网站！」，做这个基础是先对用户打标签。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-579db58bbdd920ad4da6be
      
    
    </summary>
    
    
      <category term="用户标签" scheme="https://dataquaner.github.io/categories/%E7%94%A8%E6%88%B7%E6%A0%87%E7%AD%BE/"/>
    
    
      <category term="用户" scheme="https://dataquaner.github.io/tags/%E7%94%A8%E6%88%B7/"/>
    
      <category term="标签" scheme="https://dataquaner.github.io/tags/%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>Java程序的层级结构（Controller、Service、Dao、Entity层）</title>
    <link href="https://dataquaner.github.io/2020/11/12/java-cheng-xu-de-ceng-ji-jie-gou-controller-service-dao-entity-ceng/"/>
    <id>https://dataquaner.github.io/2020/11/12/java-cheng-xu-de-ceng-ji-jie-gou-controller-service-dao-entity-ceng/</id>
    <published>2020-11-12T12:47:00.000Z</published>
    <updated>2020-11-12T12:49:52.226Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Entity层"><a href="#Entity层" class="headerlink" title="Entity层"></a>Entity层</h3><p>Entity层：实体层，用于存放实体类，与数据库中的属性值基本保持一致，包含有该实体类的属性和对应属性的get、set方法。</p><h3 id="DAO层"><a href="#DAO层" class="headerlink" title="DAO层"></a>DAO层</h3><p>DAO层：持久层，与数据库进行交互。<br>DAO层首先会创建DAO接口，然后在配置文件中定义该接口的具体实现类，接着就可以在模块中调用DAO的接口并进行相应数据业务的处理，不需要去关注该接口的具体实现类是什么。<br>DAO层的数据源和数据库连接的参数都是在配置文件中进行配置的，主要对数据进行持久化操作，对外提供对数据库的增删改查操作。</p><h3 id="Service层"><a href="#Service层" class="headerlink" title="Service层"></a>Service层</h3><p>service层：业务层，用来控制业务。<br>Service层主要负责业务模块的逻辑应用设计，先创建接口，再创建相应的实现类，然后在配置文件里进行配置实现其关联，接着就可以调用service层的接口进行业务逻辑应用的处理。<br>对Service层的业务逻辑进行封装有利于业务逻辑的独立性和重复利用性。</p><h3 id="Controller层"><a href="#Controller层" class="headerlink" title="Controller层"></a>Controller层</h3><p>Controller层：控制层，控制业务逻辑流程。<br>Controller层负责具体的业务模块流程的控制，主要调用Service层里面的接口去控制具体的业务流程，控制的配置也需要在配置文件中进行配置。<br>与Service层不同，Controller层负责具体的业务模块流程的控制，Service层负责业务模块的逻辑应用设置。<br>Controller层一般会与前台的js文件进行数据的交互。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>具体项目中，主要流程为Controller层调用Service层，Service层调用Dao层，调用的参数在Entity层进行定义。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Entity层&quot;&gt;&lt;a href=&quot;#Entity层&quot; class=&quot;headerlink&quot; title=&quot;Entity层&quot;&gt;&lt;/a&gt;Entity层&lt;/h3&gt;&lt;p&gt;Entity层：实体层，用于存放实体类，与数据库中的属性值基本保持一致，包含有该实体类的属性和对应属
      
    
    </summary>
    
    
      <category term="Java" scheme="https://dataquaner.github.io/categories/Java/"/>
    
    
      <category term="Java" scheme="https://dataquaner.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Flink三天光速入门</title>
    <link href="https://dataquaner.github.io/2020/11/11/flink-san-tian-ru-men/"/>
    <id>https://dataquaner.github.io/2020/11/11/flink-san-tian-ru-men/</id>
    <published>2020-11-11T06:35:00.000Z</published>
    <updated>2020-11-11T08:52:10.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-初识-Flink"><a href="#1-初识-Flink" class="headerlink" title="1. 初识 Flink"></a>1. 初识 Flink</h1><p>在当前数据量激增的时代，各种业务场景都有大量的业务数据产生，对于这些不断产的数据应该如何进行有效的处理，成为当下大多数公司所面临的问题。目前比较流行的大数据处理引擎 <code>Apache Spark</code>，基本上已经取代了 MapReduce 成为当前大数据处理的标准。但 对实时数据处理来说，Apache Spark 的 Spark-Streaming 还有性能改进的空间。对于 Spark-Streaming 的<code>流计算本质上还是批（微批）计算</code>，Apache <a href="https://flink.apache.org/" target="_blank" rel="noopener">Flink</a> 就是近年来在开源社区不断发展的技术中的能够同时支持<code>高吞吐</code>、<code>低延迟</code>、<code>高性能</code>的纯实时的分布式处理框架(主要贡献者是阿里(官网支持汉化阅读)，QPS可达30W+)。</p><h3 id="Flink-是什么"><a href="#Flink-是什么" class="headerlink" title="Flink 是什么"></a>Flink 是什么</h3><h5 id="1-Flink-的发展历史"><a href="#1-Flink-的发展历史" class="headerlink" title="1. Flink 的发展历史"></a>1. Flink 的发展历史</h5><p>在 2010 年至 2014 年间，由柏林工业大学、柏林洪堡大学和哈索普拉特纳研究所联合发 起名为<code>Stratosphere:Information Management on the Cloud</code>研究项目，该项目在当时的社区逐渐具有了一定的社区知名度。<strong>2014</strong> 年 4 月，Stratosphere 代码被贡献给 Apache 软件基金会，成为 Apache 基金会孵化器项目。初期参与该项目的核心成员均是 Stratosphere 曾经的核心成员，之后团队的大部分创始成员离开学校，共同创办了一家名叫 Data Artisans 的公司，其主要业务便是将 Stratosphere，也就是之后的 Flink 实现商业化。在项目孵化 期间，项目 Stratosphere 改名为 Flink。Flink 在德语中是<strong>快速</strong>和<strong>灵敏</strong>的意思，用来体现流 式数据处理器速度快和灵活性强等特点，同时使用棕<strong>红色松鼠</strong>图案作为 Flink 项目的 Logo， 也是为了突出松鼠灵活快速的特点，由此，Flink 正式进入社区开发者的视线。 2014 年 12 月，该项目成为 Apache 软件基金会顶级项目，从 <code>2015 年 9 月</code>发布第一个稳 定版本 0.9，到目前为止已经发布到 1.9 的版本，更多的社区开发成员逐步加入，现在 Flink 在全球范围内拥有 350 多位开发人员，不断有新的特性发布。同时在全球范围内，越来越多 的公司开始使用 Flink，在国内比较出名的互联网公司如阿里巴巴、美团、滴滴等，都在大 规模使用 Flink 作为企业的分布式大数据处理引擎。</p><h5 id="2-Flink-的定义"><a href="#2-Flink-的定义" class="headerlink" title="2. Flink 的定义"></a>2. Flink 的定义</h5><p>Apache <a href="https://flink.apache.org/zh/flink-architecture.html" target="_blank" rel="noopener">Flink</a> 是一个框架和分布式处理引擎，用于在<code>无边界</code>和<code>有边界</code>数据流上进行<code>有状态</code>的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。</p><blockquote><p>Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale</p></blockquote><h5 id="3-有界流和无界流"><a href="#3-有界流和无界流" class="headerlink" title="3. 有界流和无界流"></a>3. 有界流和无界流</h5><p>任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或 移动应用程序上的用户交互记录，所有这些数据都形成一种流。</p><p>无界流： 有定义流的开始，但<strong>没有定义流的结束</strong>。它们会无休止地产生数据。无界流 的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理， 因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事 件，例如事件发生的顺序，以便能够推断结果的完整性。</p><p>有界流： 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行 计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。跟Spark-Stream类似。<br><img src="https://img-blog.csdnimg.cn/20200713092013397.png#pic_center" alt="在这里插入图片描述"><br>Apache Flink 擅长处理无界和有界数据集精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。</p><h5 id="4-有状态的计算架构"><a href="#4-有状态的计算架构" class="headerlink" title="4. 有状态的计算架构"></a>4. 有状态的计算架构</h5><p>数据产生的<strong>本质</strong>，其实是一条条真实存在的事件按照时间顺序源源不断的产生，我们很难在数据产生的过程中进行计算并直接产生统计结果，因为这不仅对系统有非常高的要求， 还必须要满足高性能、高吞吐、低延时等众多目标。而有状态流计算架构（如图所示）的提 出，从一定程度上满足了企业的这种需求，企业基于实时的流式数据，维护所有计算过程的 状态，所谓状态就是<code>计算过程中产生的中间计算结果</code>，<code>每次计算新的数据进入到流式系统中 都是基于中间状态结果的基础上进行运算</code>，最终产生正确的统计结果。基于有状态计算的方式最大的优势是<code>不需要将原始数据重新从外部存储中拿出来</code>，从而进行全量计算，因为这种计算方式的代价可能是非常高的。从另一个角度讲，<strong>用户无须通过调度和协调各种批量计算 工具，从数据仓库中获取数据统计结果，然后再落地存储，这些操作全部都可以基于流式计 算完成，可以极大地减轻系统对其他框架的依赖，减少数据计算过程中的时间损耗以及硬件存储</strong>。<br><img src="https://img-blog.csdnimg.cn/20200721101922544.png#pic_center" alt="在这里插入图片描述"></p><h3 id="2-为什么要使用-Flink"><a href="#2-为什么要使用-Flink" class="headerlink" title="2. 为什么要使用 Flink"></a>2. 为什么要使用 Flink</h3><p>可以看出<code>有状态流计算将会逐步成为企业作为构建数据平台的架构模式</code>，而目前从社区 来看，能够满足的只有 Apache Flink。Flink 通过实现 <code>Google Dataflow</code> 流式计算模型实现 了高吞吐、低延迟、高性能兼具实时流式计算框架。同时 Flink 支持高度容错的状态管理， 防止状态在计算过程中因为系统异常而出现丢失，Flink 周期性地通过分布式快照技术 <code>Checkpoints</code>实现状态的<strong>持久化维护</strong>，使得即使在系统停机或者异常的情况下都能计算出正 确的结果。</p><p><a href="https://flink.apache.org/zh/poweredby.html" target="_blank" rel="noopener">Flink用户</a> 众多，自2019年1月起，阿里巴巴逐步将内部维护的Blink回馈给Flink开源社区，目前贡献代码已超过100万行，国内包括腾讯、百度、字节跳动等公司，国外包括Uber、Lyft、Netflix等公司都是Flink的使用者。<br><img src="https://img-blog.csdnimg.cn/20200713092742123.png#pic_center" alt="在这里插入图片描述"></p><h3 id="3-Flink-的应用场景"><a href="#3-Flink-的应用场景" class="headerlink" title="3. Flink 的应用场景"></a>3. Flink 的应用场景</h3><p>在实际生产的过程中，大量数据在不断地产生，例如金融交易数据、互联网订单数据、 GPS 定位数据、传感器信号、移动终端产生的数据、通信信号数据等，以及我们熟悉的网络 流量监控、服务器产生的日志数据，这些数据最大的共同点就是<strong>实时从不同的数据源中产生</strong>， 然后<strong>再传输到下游的分析系统</strong>。针对这些数据类型主要包括实时<strong>智能推荐</strong>、<strong>复杂事件处理</strong>、 <strong>实时欺诈检测</strong>、<strong>实时数仓</strong>与 <strong>ETL 类型</strong>、<strong>流数据分析类型</strong>、<strong>实时报表类型</strong>等实时业务场景，而 Flink 对于这些类型的场景都有着非常好的支持</p><h5 id="1-实时智能推荐"><a href="#1-实时智能推荐" class="headerlink" title="1. 实时智能推荐"></a>1. 实时智能推荐</h5><p>智能推荐会根据用户历史的购买行为，通过推荐算法训练模型，预测用户未来可能会购 买的物品。对个人来说，推荐系统起着信息过滤的作用，对 Web/App 服务端来说，推荐系统 起着满足用户个性化需求，提升用户满意度的作用。推荐系统本身也在飞速发展，除了算法 <strong>越来越完善</strong>，对<strong>时延</strong>的要求也越来越苛刻和实时化。利用 Flink 流计算帮助用户构建更加实 时的智能推荐系统，对用户行为指标进行实时计算，对模型进行实时更新，对用户指标进行 实时预测，并将预测的信息推送给 Wep/App 端，帮助用户获取想要的商品信息，另一方面也 帮助企业提升销售额，创造更大的商业价值。</p><h5 id="2-复杂事件处理"><a href="#2-复杂事件处理" class="headerlink" title="2. 复杂事件处理"></a>2. 复杂事件处理</h5><p>对于复杂事件处理，比较常见的案例主要集中于工业领域，例如对车载传感器、机械设备等实时故障检测，这些业务类型通常数据量都非常大，且对数据处理的时效性要求非常高。 通过利用 Flink 提供的 <code>CEP</code>（复杂事件处理）进行事件模式的抽取，同时应用 Flink 的 Sql 进行事件数据的转换，在流式系统中构建实时规则引擎，一旦事件触发报警规则，便立即将 告警结果传输至下游通知系统，从而实现对设备故障快速预警监测，车辆状态监控等目的。</p><h5 id="3-实时欺诈检测"><a href="#3-实时欺诈检测" class="headerlink" title="3. 实时欺诈检测"></a>3. 实时欺诈检测</h5><p>在金融领域的业务中，常常出现各种类型的欺诈行为，例如信用卡欺诈、信贷申请欺诈等，而如何保证用户和公司的资金安全，是近年来许多金融公司及银行共同面对的挑战。 随着不法分子欺诈手段的不断升级，传统的反欺诈手段已经不足以解决目前所面临的问题。 以往可能需要几个小时才能通过交易数据计算出用户的<code>行为指标</code>，然后通过规则判别出具有 欺诈行为嫌疑的用户，再进行案件调查处理，在这种情况下资金可能早已被不法分子转移， 从而给企业和用户造成大量的经济损失。而运用 Flink 流式计算技术能够在<strong>毫秒内就完成对 欺诈判断行为指标的计算</strong>，然后实时对交易流水进行规则判断或者模型预测，这样一旦检测 出交易中存在欺诈嫌疑，则直接对交易进行实时拦截，避免因为处理不及时而导致的经济损 失。</p><h5 id="4-实时数仓与-ETL-结合离线数仓"><a href="#4-实时数仓与-ETL-结合离线数仓" class="headerlink" title="4. 实时数仓与 ETL 结合离线数仓"></a>4. 实时数仓与 ETL 结合离线数仓</h5><p>通过利用流计算诸多优势和 SQL 灵活的加工能力，对流式数据进行实时<code>清洗</code>、<code>归并</code>、<code>结构化处理</code>，为离线数仓进行补充和优化。另一方面结合实时数据 ETL 处理能 力，利用有状态流式计算技术，可以尽可能降低企业由于在离线数据计算过程中调度逻辑的复杂度，高效快速地处理企业需要的统计结果，帮助企业更好地应用实时数据所分析出来的结果。</p><h5 id="5-流数据分析"><a href="#5-流数据分析" class="headerlink" title="5. 流数据分析"></a>5. 流数据分析</h5><p>实时计算各类<code>数据指标</code>，并利用<code>实时结果</code>及时调整在线系统<strong>相关策略</strong>，在各类内容投放、 无线智能推送领域有大量的应用。流式计算技术将数据分析场景实时化，帮助企业做到实时化分析 Web 应用或者 App 应用的各项指标，包括 App 版本分布情况、Crash 检测和分布等， 同时提供多维度用户行为分析，支持日志自主分析，助力开发者实现基于大数据技术的精细 化运营、提升产品质量和体验、增强用户黏性。</p><h5 id="6-实时报表分析"><a href="#6-实时报表分析" class="headerlink" title="6. 实时报表分析"></a>6. 实时报表分析</h5><p><code>实时报表分析</code>是近年来很多公司采用的报表统计方案之一，其中最主要的应用便是实时大屏展示。利用流式计算实时得出的结果直接被推送到前端应用，实时显示出重要指标的变 换情况。最典型的案例便是淘宝的<code>双十一活动</code>，每年双十一购物节，除疯狂购物外，最引人 注目的就是天猫双十一大屏不停跳跃的成交总额。在整个计算链路中包括从天猫交易下单购买到<code>数据采集</code>、<code>数据计算</code>、<code>数据校验</code>，最终落到双十一大屏上展现的全链路时间压缩在 5 秒以内，顶峰计算性能高达数<code>三十万笔订单/秒</code>，通过多条链路流计算备份确保<code>万无一失</code>。 而在其他行业，企业也在构建自己的实时报表系统，让企业能够依托于自身的业务数据，快 速提取出更多的数据价值，从而更好地服务于企业运行过程中。<br><img src="https://img-blog.csdnimg.cn/20200721102303696.png#pic_center" alt="在这里插入图片描述"></p><h3 id="4-Flink-的特点和优势"><a href="#4-Flink-的特点和优势" class="headerlink" title="4. Flink 的特点和优势"></a>4. Flink 的特点和优势</h3><p>Flink 的具体优势和特点有以下几点</p><h5 id="1-同时支持高吞吐、低延迟、高性能"><a href="#1-同时支持高吞吐、低延迟、高性能" class="headerlink" title="1. 同时支持高吞吐、低延迟、高性能"></a>1. 同时支持高吞吐、低延迟、高性能</h5><p>Flink 是目前开源社区中唯 一 一套集<code>高吞吐</code>、<code>低延迟</code>、<code>高性能</code>三者于一身的<code>分布式流式数据处理框架</code>。像 Apache Spark 也只能兼顾高吞吐和高性能特性，主要因为在 Spark Streaming 流式计算中无法做到低延迟保障；而流式计算框架 <code>Apache Storm</code> 只能支持低延迟和高性能特性，但是无法满足<strong>高吞吐</strong>的要求。而满足高吞吐、低延迟、高 性能这三个目标对分布式流式计算框架来说是非常重要的。</p><h5 id="2-支持事件时间（Event-Time）概念"><a href="#2-支持事件时间（Event-Time）概念" class="headerlink" title="2. 支持事件时间（Event Time）概念"></a>2. 支持事件时间（Event Time）概念</h5><p>在流式计算领域中，窗口计算的地位举足轻重，但目前大多数框架窗口计算采用的都是<code>系统时间</code>(Process Time)，也是事件传输到计算框架处理时，<strong>系统主机的当前时间</strong>。Flink 能够支持基于<code>事件时间</code>(Event Time)语义进行窗口计算，也就是使用事件产生的时间，这种基于事件驱动的机制使得事件即使乱序到达，流系统也能够计算出 确的结果，<strong>保持了事件原本产生时的时序性</strong>，尽可能避免网络传输或硬件系统的影响。</p><p><strong>Event Time/Processing Time/Ingestion Time，也就是事件时间、处理时间、提取时间，那么这三个时间有什么区别和联系</strong></p><p>下图是一个信号站，分别列出了事件时间、处理时间、提取时间的先后顺序。当然上面图示需要你对Flink有一个基本的了解。我们先白话解释，然后在官方解释。<br><img src="https://img-blog.csdnimg.cn/20200721103304536.png#pic_center" alt="在这里插入图片描述"></p><ul><li>Event Time：也就是事件发生的时间，事件的发生时间。我们有些同学可能会模糊，这里举个例子，我们<code>产生日志的时间</code>，这个应该清楚的，日志的时间戳就是<code>发生时间</code>。</li><li>Processing Time：也就是处理时间，我们看到了这个已经进入Flink程序，也就是我们读取数据源时间，也就是日志到达Flink的时间，但是这个时间是<code>本地机器的时间</code>。</li><li>Ingestion Time：也就是提取时间，我们看到它比处理时间还晚一些，这个时候数据已经发送给窗口，也就是<code>发送给窗口</code>的时间，也就是程序处理计算的时间。</li></ul><h5 id="3-支持有状态计算"><a href="#3-支持有状态计算" class="headerlink" title="3. 支持有状态计算"></a>3. 支持有状态计算</h5><p>Flink 在 1.4 版本中实现了<code>状态管理</code>，所谓状态就是<strong>在流式计算过程中将算子的中间结果数据保存在内存或者文件系统中，等下一个事件进入算子后可以从之前的状态中 获取中间结果中计算当前的结果</strong>，从而无须每次都基于全部的原始数据来统计结果，这 种方式极大地提升了<code>系统的性能</code>，并降低了数据计算过程的资源消耗。对于数据量大且运算逻辑非常复杂的流式计算场景，有状态计算发挥了非常重要的作用。</p><h5 id="4-支持高度灵活的窗口（Window）操作"><a href="#4-支持高度灵活的窗口（Window）操作" class="headerlink" title="4.支持高度灵活的窗口（Window）操作"></a>4.支持高度灵活的窗口（Window）操作</h5><p>在流处理应用中，数据是连续不断的，需要通过<code>窗口</code>的方式对流数据进行一定范围 的聚合计算，例如统计在过去的 1 分钟内有多少用户点击某一网页，在这种情况下，我 们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行再计 算。Flink 将窗口划分为基于 Time、Count、Session，以及 Data-driven 等类型的窗口 操作，<code>窗口</code>可以用灵活的触发条件定制化来达到对复杂的流传输模式的支持，用户可以 定义不同的窗口触发机制来满足不同的需求。</p><h5 id="5-基于轻量级分布式快照（CheckPoint）实现的容错"><a href="#5-基于轻量级分布式快照（CheckPoint）实现的容错" class="headerlink" title="5.基于轻量级分布式快照（CheckPoint）实现的容错"></a>5.基于轻量级分布式快照（CheckPoint）实现的容错</h5><p>Flink 能够分布式运行在<code>上千个节点</code>上，<code>将一个大型计算任务的流程拆解成小的计算过程</code>，然后将 tesk 分布到并行节点上进行处理。在任务执行过程中，能够自动发现事件处理过程中的错误而导致数据不一致的问题，比如：节点宕机、网路传输问题，或 是由于用户因为升级或修复问题而导致计算服务重启等。在这些情况下，通过基于分布 式快照技术的<code>Checkpoints</code>，将执行过程中的状态信息进行<code>持久化存储</code>，一旦任务出现异常停止，<code>Flink 就能够从 Checkpoints 中进行任务的自动恢复</code>，以确保数据在处理过 程中的精准一致性（<code>Exactly-Once</code>）。快照是默认自动开启实现的。</p><h5 id="6-基于-JVM-实现独立的内存管理"><a href="#6-基于-JVM-实现独立的内存管理" class="headerlink" title="6.基于 JVM 实现独立的内存管理"></a>6.基于 JVM 实现独立的内存管理</h5><p>内存管理是所有计算框架需要重点考虑的部分，尤其对于计算量比较大的计算场 景，数据在内存中该如何进行管理显得至关重要。针对内存管理，Flink 实现了<code>自身管理内存的机制</code>，尽可能减少 JVM GC 对系统的影响。另外，Flink 通过序列化/反序列化 方法将所有的数据对象转换成二进制在内存中存储，<code>降低数据存储的大小</code>的同时，能够<code>更加有效地对内存空间进行利用</code>，降低 GC 带来的性能下降或任务异常的风险，因此 Flink 较其他分布式处理的框架会显得更加稳定，不会因为 JVM GC 等问题而影响整个 应用的运行。</p><h5 id="7-Save-Points（保存点）"><a href="#7-Save-Points（保存点）" class="headerlink" title="7. Save Points（保存点）"></a>7. Save Points（保存点）</h5><p>对于 7*24 小时运行的流式应用，数据源源不断地接入，在一段时间内应用的终止有可能导致数据的丢失或者计算结果的不准确，例如进行集群版本的升级、停机运维操 作等操作。值得一提的是，Flink 通过 <code>SavePoints</code>技术将任务执行的快照保存在存储介质上，当任务重启的时候可以直接从事先保存的 Save Points 恢复原有的计算状态， 使得任务继续按照停机之前的状态运行，Save Points 技术可以让用户更好地管理和运 维实时流式应用。不过需要手动启动跟恢复数据。</p><h3 id="5-常见实时计算框架对比"><a href="#5-常见实时计算框架对比" class="headerlink" title="5. 常见实时计算框架对比"></a>5. 常见实时计算框架对比</h3><table><thead><tr><th>产品</th><th>模型</th><th>API</th><th>保证次数</th><th>容错机制</th><th>状态管理</th><th>延时</th><th>吞吐量</th></tr></thead><tbody><tr><td>Storm</td><td>Native(数据实时进入处理)</td><td>组合式(基础API)</td><td>At-least-once(至少一次)</td><td>Record ACK(ACK机制)</td><td>无</td><td>低</td><td>低</td></tr><tr><td>Trident</td><td>Micro-Batching(划分为小批次处理)</td><td>组合式</td><td>Exactly-once(精准一致性)</td><td>Record ACK</td><td>基于操作(每个操作都有一个状态)</td><td>中等</td><td>中等</td></tr><tr><td>Spark-Streaming</td><td>Micro-Batching</td><td>声明式(提供封装后的高阶函数，比如Count)</td><td>Exactly-once</td><td>RDD CheckPoint(基于RDD做CheckPoint)</td><td>基于DStream</td><td>中等</td><td>高</td></tr><tr><td>Flink</td><td>Native</td><td>声明式</td><td>Exactly-once</td><td>CheckPoint(Flink的一种快照)</td><td>基于操作</td><td>低</td><td>高</td></tr></tbody></table><ol><li><code>模型</code>：<br>Storm 和 Flink 是真正的一条一条处理数据；而 Trident（Storm 的封装框架） 和 Spark Streaming 其实都是<strong>小批处理</strong>，一次处理一批数据（小批量）。</li><li><code>API</code>：<br>Storm 和 Trident 都使用基础 API 进行开发，比如实现一个简单的 sum 求和操作； 而 Spark Streaming 和 Flink 中都<strong>提供封装后的高阶函数</strong>，可以直接拿来使用，这样就 比较方便了。</li><li><code>保证次数</code>：<br>在数据处理方面，Storm 可以实现至少处理一次，但不能保证仅处理一次， 这样就会导致数据<strong>重复处理</strong>问题，所以针对计数类的需求，可能会产生一些误差； Trident 通过<strong>事务</strong>可以保证对数据实现仅一次的处理，Spark Streaming 和 Flink 也是 如此。</li><li><code>容错机制</code>：<br>Storm和Trident可以通过<code>ACK</code>机制实现数据的容错机制，而Spark Streaming 和 Flink 可以通过 <code>CheckPoint</code> 机制实现容错机制。</li><li><code>状态管理</code>：<br>Storm 中没有实现状态管理，Spark Streaming 实现了基于 DStream 的状态 管理，而 Trident 和 Flink 实现了<strong>基于操作</strong>的状态管理。</li><li><code>延时</code>：<br>表示数据处理的延时情况，因此 Storm 和 Flink 接收到一条数据就处理一条数据， 其数据处理的延时性是很低的；而 Trident 和 Spark Streaming 都是小型批处理，它们 数据处理的延时性相对会偏高。</li><li><code>吞吐量</code>：<br>Storm 的吞吐量其实也不低，只是相对于其他几个框架而言较低；Trident 属于中等；而 Spark Streaming 和 Flink 的吞吐量是比较高的。<br><img src="https://img-blog.csdnimg.cn/20200713100821813.png#pic_center" alt="在这里插入图片描述"></li></ol><h1 id="2-Flink编程入门"><a href="#2-Flink编程入门" class="headerlink" title="2.Flink编程入门"></a>2.Flink编程入门</h1><h3 id="1-Flink-的开发环境"><a href="#1-Flink-的开发环境" class="headerlink" title="1. Flink 的开发环境"></a>1. Flink 的开发环境</h3><p>Flink 课程选择的是 Apache Flink 1.9.1 版本，是目前较的稳定版本，并且 兼容性比较好。<br>下载地址： <a href="https://flink.apache.org/zh/downloads.html" target="_blank" rel="noopener">https://flink.apache.org/zh/downloads.html</a></p><h5 id="1-开发工具"><a href="#1-开发工具" class="headerlink" title="1. 开发工具"></a>1. 开发工具</h5><p>先说明一下开发工具的问题。官方建议使用<code>IntelliJ IDEA</code>，因为它默认集成了 <code>Scala</code>和<code>Maven</code>环境，使用更加方便，当然使用 Eclipse 也是可以的。本文使用 IDEA。开发Flink 程序时，可以使用<code>Java</code>、<code>Python</code>或者<code>Scala</code>语言，本教程使用 Scala，因为 使用 Scala 实现函数式编程会比较简洁。</p><h5 id="2-配置依赖"><a href="#2-配置依赖" class="headerlink" title="2. 配置依赖"></a>2. 配置依赖</h5><p>开发 Flink 应用程序需要最低限度的 API 依赖。最低的依赖库包括：<code>flink-scala</code>和 <code>flink-streaming-scala</code>。大多数应用需要依赖特定的连接器或其他类库，例如 Kafka 的连 接器、TableAPI、CEP 库等。这些不是 Flink 核心依赖的一部分，因此必须作为依赖项手 动添加到应用程序中。</p><p>与其他运行用户自定义应用的大多数系统一样，Flink 中有两大类依赖类库</p><ul><li>Flink 核心依赖：<br>Flink 本身包含运行所需的一组类和依赖，比如协调、网络通讯、checkpoint、容错处理、API、算子(如窗口操作)、 资源管理等，这些类和依赖形成了 Flink 运行时的核心。当 Flink 应用启动时，这些依赖必须可用。<br>这些核心类和依赖被打包在 flink-dist jar 里。它们是 Flink lib 文件夹下的一部分，也是 Flink 基本容器镜像的一部分。 这些依赖类似 Java String 和 List 的核心类库(rt.jar, charsets.jar等)。<br>Flink 核心依赖不包含连接器和类库（如 CEP、SQL、ML 等），这样做的目的是默认情况下避免在类路径中具有过多的依赖项和类。 实际上，我们希望尽可能保持核心依赖足够精简，以保证一个较小的默认类路径，并且避免依赖冲突。</li><li>用户应用依赖：<br>是指特定的应用程序需要的类库，如连接器，formats等。用户应用代码和所需的连接器以及其他类库依赖通常被打包到 application jar 中。用户应用程序依赖项不需包括 Flink DataSet / DataStream API 以及运行时依赖项，因为它们已经是 Flink 核心依赖项的一部分。</li></ul><p>Flink官方依赖文档说明：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/projectsetup/dependencies.html" target="_blank" rel="noopener">官方依赖入手</a></p><h3 id="2-WordCount演示"><a href="#2-WordCount演示" class="headerlink" title="2.WordCount演示"></a>2.WordCount演示</h3><p>添加pom依赖</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>project</span> <span class="token attr-name">xmlns</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/POM/4.0.0<span class="token punctuation">"</span></span>         <span class="token attr-name"><span class="token namespace">xmlns:</span>xsi</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.w3.org/2001/XMLSchema-instance<span class="token punctuation">"</span></span>         <span class="token attr-name"><span class="token namespace">xsi:</span>schemaLocation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>modelVersion</span><span class="token punctuation">></span></span>4.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>modelVersion</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.sowhat<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>Flink-Test<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.0-SNAPSHOT<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-scala_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-streaming-scala_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!--    上述两个是核心依赖--></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hadoop-common<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-kafka_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.kafka<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>kafka-clients<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>0.11.0.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-filesystem_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.bahir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-redis_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>mysql<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>mysql-connector-java<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>5.1.44<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-table-planner_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-table-api-scala-bridge_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-cep-scala_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>            <span class="token comment" spellcheck="true">&lt;!-- 该插件用于将Scala代码编译成class文件 --></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>net.alchim31.maven<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>scala-maven-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.4.6<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>                        <span class="token comment" spellcheck="true">&lt;!-- 声明绑定到maven的compile阶段 --></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>testCompile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>compile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>            <span class="token comment" spellcheck="true">&lt;!-- Java Compiler   https://blog.csdn.net/liupeifeng3514/article/details/80236077  --></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-compiler-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!-- 开始代码时指定的JDK版本--></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>target</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>target</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!-- 编译成.class 文件所需版本--></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                             <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>!--</span> <span class="token attr-name">表示会出现一个无任何依赖的jar，还有一个包含所有依赖的jar</span> <span class="token attr-name">--</span> <span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>project</span><span class="token punctuation">></span></span>123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意事项</strong>: 所有这些 <a href="https://sowhat.blog.csdn.net/article/details/75349274" target="_blank" rel="noopener">依赖项</a> 的作用域都应该设置为 <code>provided</code> 。 这意味着需要这些依赖进行编译，但<code>不应</code>将它们打包到项目生成的应用程序jar文件中，因为这些依赖项是 Flink 的核心依赖，服务器运行环境在应用启动前已经是可用的状态了。</p><p>我们<strong>强烈建议保持这些依赖的作用域</strong>为 <code>provided</code>。 如果它们的作用域未设置为 provided ，则典型的情况是因为包含了 Flink 的核心依赖而导致生成的jar包变得过大。 最糟糕的情况是添加到应用程序的 Flink 核心依赖项与你自己的一些依赖项版本冲突（通常通过反向类加载来避免）。</p><p><code>IntelliJ 上的一些注意事项</code>: 为了可以让 Flink 应用在 IntelliJ IDEA 中运行，这些 Flink 核心依赖的作用域需要设置为 <code>compile</code> 而不是<code>provided</code> 。 <strong>否则 IntelliJ 不会添加这些依赖到 classpath</strong>，会导致应用运行时抛出 NoClassDefFountError 异常。为了避免声明这些依赖的作用域为 compile (因为我们不推荐这样做)， 上文给出的 Java 和 Scala 项目模板使用了一个小技巧：添加了一个 profile，仅当应用程序在 IntelliJ 中运行时该 profile 才会被激活， 然后将依赖作用域设置为 compile ，从而不影响应用 jar 包。</p><h5 id="1-流式接受数据"><a href="#1-流式接受数据" class="headerlink" title="1. 流式接受数据"></a>1. 流式接受数据</h5><p><code>案例需求</code>：采用 Netcat 数据源发送数据，使用 Flink 统计每个单词的数量。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token comment" spellcheck="true">/**  * flink的流计算的WordCount  */</span>object FlinkStreamWordCount <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//2、导入隐式转换</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//3、读取数据,读取sock流中的数据</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"IP"</span><span class="token punctuation">,</span> <span class="token number">8899</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//DataStream ==> spark 中Dstream</span>    <span class="token comment" spellcheck="true">//4、转换和处理数据</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//分组算子  : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//聚会累加算子</span>    <span class="token comment" spellcheck="true">//5、打印结果</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"结果"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//6、启动流计算程序</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"wordcount"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>找一个服务可以接受到的接口发送若干信息：</p><pre class="line-numbers language-bash"><code class="language-bash">$ nc -lk 8899hadoop spark hive flinkflink sowhat liu sparkflink sowhat---结果<span class="token operator">></span> <span class="token punctuation">(</span>hive,1<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>spark,1<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>hadoop,1<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>flink,1<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>sowhat,1<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>spark,2<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>liu,1<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>flink,2<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>sowhat,2<span class="token punctuation">)</span>结果<span class="token operator">></span> <span class="token punctuation">(</span>flink,3<span class="token punctuation">)</span>123456789101112131415<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>PS：如果将代码中所有关于并行度的全部屏蔽掉，系统会自动的将全部CPU利用起来，然后利用Hash算法来将数据归类给不同的CPU核心来处理，结果可能如下：</p><pre class="line-numbers language-bash"><code class="language-bash">结果:1<span class="token operator">></span> <span class="token punctuation">(</span>hive,1<span class="token punctuation">)</span> // 表示第几个核给出的结果结果:4<span class="token operator">></span> <span class="token punctuation">(</span>flink,1<span class="token punctuation">)</span>结果:1<span class="token operator">></span> <span class="token punctuation">(</span>spark,1<span class="token punctuation">)</span>结果:4<span class="token operator">></span> <span class="token punctuation">(</span>sohat,1<span class="token punctuation">)</span>结果:1<span class="token operator">></span> <span class="token punctuation">(</span>hive,2<span class="token punctuation">)</span>结果:2<span class="token operator">></span> <span class="token punctuation">(</span>node,1<span class="token punctuation">)</span>结果:4<span class="token operator">></span> <span class="token punctuation">(</span>zookeeper,1<span class="token punctuation">)</span>结果:3<span class="token operator">></span> <span class="token punctuation">(</span>manager,1<span class="token punctuation">)</span>12345678<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-直接统计指定文件WordCount"><a href="#2-直接统计指定文件WordCount" class="headerlink" title="2. 直接统计指定文件WordCount"></a>2. 直接统计指定文件WordCount</h5><p><code>需求</code>：读取本地数据文件，统计文件中每个单词出现的次数。 根据需求，很明显是有界流（批计算），所以采用另外一个上下文环境：ExecutionEnvironment<br>在IDEA的<code>resources</code>目录下创建个<code>wc.txt</code> 文件内容如下：</p><pre><code>hello flink sparkhello sparkspark core flink streamhello fink1234</code></pre><p>批量统计代码如下：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token keyword">import</span> java<span class="token punctuation">.</span>net<span class="token punctuation">.</span><span class="token punctuation">{</span>URL<span class="token punctuation">,</span> URLDecoder<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span><span class="token punctuation">{</span>DataSet<span class="token punctuation">,</span> ExecutionEnvironment<span class="token punctuation">,</span> _<span class="token punctuation">}</span><span class="token comment" spellcheck="true">/**  * Flink的批计算案例  */</span>object BatchWordCount <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//初始化Flink批处理环境</span>    val env<span class="token operator">:</span> ExecutionEnvironment <span class="token operator">=</span> ExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    val dataPath<span class="token operator">:</span> URL <span class="token operator">=</span> getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/wc.txt"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//使用相对路径来得到完整的文件路径</span>    var packagePath<span class="token operator">:</span> String <span class="token operator">=</span> dataPath<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">replaceAll</span><span class="token punctuation">(</span><span class="token string">"%20"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//解决路径中含有空格的情况</span>    val str<span class="token operator">:</span>String <span class="token operator">=</span> URLDecoder<span class="token punctuation">.</span><span class="token function">decode</span><span class="token punctuation">(</span>packagePath<span class="token punctuation">,</span> <span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//解决路径包含中文的情况</span>    <span class="token function">println</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//读数据</span>    val data<span class="token operator">:</span> DataSet<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//DataSet ==> spark RDD</span>    <span class="token comment" spellcheck="true">//计算并且打印结果</span>    data<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">groupBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//分组算子  : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-Flink-的安装和部署"><a href="#3-Flink-的安装和部署" class="headerlink" title="3. Flink 的安装和部署"></a>3. Flink 的安装和部署</h1><p>Flink 的安装和部署主要分为本地（<code>单机</code>）模式和<code>集群</code>模式，其中本地模式只需直接解压就可以使用，不用修改任何参数，一般在做一些简单测试的时候使用。本地模式不再赘述。集群模式包含：</p><ol><li>Standalone</li><li>Flink on Yarn(重点)</li><li>Mesos</li><li>Docker</li><li>Kubernetes</li><li>AWS</li><li>Goole Compute Engine</li></ol><p>目前在企业中使用最多的是 <strong>Flink on Yarn</strong> 模式。本文主讲<code>Standalone</code> 和<code>Flink on Yarn</code>这两种模式。</p><h3 id="1-集群基本架构"><a href="#1-集群基本架构" class="headerlink" title="1. 集群基本架构"></a>1. 集群基本架构</h3><p>Flink 整个系统主要由两个组件组成，分别为 <code>JobManager</code> 和 <code>TaskManager</code>，Flink 架构也遵循<code>Master-Slave</code> 架构设计原则，JobManager 为 Master 节点，TaskManager 为 Worker （Slave）节点。所有组件之间的通信都是借助于 <code>Akka Framework</code>，包括任务的状态以及 Checkpoint 触发等信息。<br><img src="https://img-blog.csdnimg.cn/20200713141655879.png#pic_center" alt="在这里插入图片描述"></p><h5 id="1-Client-客户端"><a href="#1-Client-客户端" class="headerlink" title="1. Client 客户端"></a>1. Client 客户端</h5><p>客户端负责将任务提交到集群，与 <code>JobManager</code> 构建 <code>Akka</code>连接，然后将任务提交到 <code>JobManager</code>，通过和 <code>JobManager</code>之间进行交互获取任务执行状态。客户端提交任务可以采 用 <code>CLI 方式</code>或者通过使用 <code>Flink WebUI</code>提交，也可以在应用程序中指定<code>JobManager</code>的<code>RPC</code>网络端口构建 ExecutionEnvironment 提交 Flink 应用。</p><h5 id="2-JobManager"><a href="#2-JobManager" class="headerlink" title="2.JobManager"></a>2.JobManager</h5><p>JobManager <code>负责整个 Flink 集群任务的调度以及资源的管理</code>，从客户端中获取提交的 应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlots 资源并命令 TaskManger 启动从客户端中获取的应用。JobManager 相当于整个集 群的 Master 节点，且整个集群中有且仅有一个活跃的 JobManager，<code>负责整个集群的任务管 理和资源管理</code>。JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执 行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。同时在任务执行过程 中，<code>Flink JobManager</code>会触发 <code>Checkpoints</code> 操作，每个<code>TaskManager</code> 节点收到 Checkpoint 触发指令后，完成 <code>Checkpoint</code>操作，所有的<code>Checkpoint</code>协调过程都是在 <code>Flink JobManager</code>中完成。当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 <code>TaskManager</code> 中的资源以供下一次提交任务使用。</p><h5 id="3-TaskManager"><a href="#3-TaskManager" class="headerlink" title="3. TaskManager"></a>3. TaskManager</h5><p>TaskManager 相当于整个集群的 Slave 节点，<strong>负责具体的任务执行和对应任务在每个节 点上的资源申请与管理</strong>。客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager， 然后 JobManager 会根据已经注册在 JobManager 中 TaskManager 的资源情况，将任务分配给 有资源的 TaskManager 节点，然后启动并运行任务。TaskManager 从 JobManager 接收需要 部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数 据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。 可以看出，Flink 的任务运行<code>其实是采用多线程的方式</code>，这和 MapReduce 多 JVM 进程的 方式有很大的区别<code>Fink 能够极大提高 CPU 使用效率</code>，在多个任务和 Task 之间通过 <code>TaskSlot</code>方式共享系统资源，<strong>每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有 效管理</strong>。</p><p><code>PS</code>：可以认为JobManager类似Hadoop中ApplicationMaster，然后一个机器就是一个TaskManager，一个TaskManager可以分解成若干个Flink基本工作单元<code>TaskSlot</code>。</p><h3 id="2-Standalone-集群安装和部署"><a href="#2-Standalone-集群安装和部署" class="headerlink" title="2. Standalone 集群安装和部署"></a>2. Standalone 集群安装和部署</h3><p>Standalone 是 Flink 的独立部署模式，它不依赖其他平台。在使用这种模式搭建 Flink 集群之前，需要先规划集群机器信息。在这里为了搭建一个标准的 Flink 集群，需要准备 3 台 Linux。<br><img src="https://img-blog.csdnimg.cn/20200713142222760.png#pic_center" alt="在这里插入图片描述"></p><ol><li>下载并解压文件到指定目录</li><li>修改配置文件<br>进入到 conf 目录下，编辑 flink-conf.yaml 配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash">jobmanager.rpc.address: hadoop101<span class="token comment" spellcheck="true"># The RPC port where the JobManager is reachable.</span>jobmanager.rpc.port: 6123<span class="token comment" spellcheck="true"># The heap size for the JobManager JVM</span>jobmanager.heap.size: 1024m <span class="token comment" spellcheck="true"># JobManager 内存大小</span><span class="token comment" spellcheck="true"># The total process memory size for the TaskManager.</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Note this accounts for all memory usage within the TaskManager process, including JVM metaspace and other overhead.</span>taskmanager.memory.process.size: 1024m <span class="token comment" spellcheck="true"># TaskManager初始化内存大小 </span><span class="token comment" spellcheck="true"># To exclude JVM metaspace and overhead, please, use total Flink memory size instead of 'taskmanager.memory.process.size'.</span><span class="token comment" spellcheck="true"># It is not recommended to set both 'taskmanager.memory.process.size' and Flink memory.</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># taskmanager.memory.flink.size: 1280m</span><span class="token comment" spellcheck="true"># The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.</span>taskmanager.numberOfTaskSlots: 3 <span class="token comment" spellcheck="true"># 每一个TaskManager 有几个TaskSlots</span><span class="token comment" spellcheck="true"># The parallelism used for programs that did not specify and other parallelism.</span>parallelism.default: 1 <span class="token comment" spellcheck="true"># 默认并行度</span>1234567891011121314151617181920212223242526272829<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>编辑slaves文件</li></ol><pre><code>vi slaveshadoop101hadoop102hadoop1031234</code></pre><ol><li>信息分发</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop101 home<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># scp -r flink-1.9.1 root@hadoop102:`pwd`</span><span class="token punctuation">[</span>root@hadoop101 home<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># scp -r flink-1.9.1 root@hadoop103:`pwd`</span>12<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol><li>主节点启动集群<br><img src="https://img-blog.csdnimg.cn/20200713143822269.png#pic_center" alt="在这里插入图片描述"></li><li>WebUI 访问<br><code>flink-conf.yaml</code>的配置文件中<code>rest.port</code>是WebUI的对外端口，服务器输入<code>hadoop101:8081</code>即可访问(我这里随便找个别人搭建看的集群看下WebUI)。<br><img src="https://img-blog.csdnimg.cn/20200713144415670.png#pic_center" alt="在这里插入图片描述"><br>左侧栏多点点看看即可，相对来说比较简单。</li><li>将IDEA代码中的两个Flink核心依赖设置为<code>provided</code>然后打包(<strong>打包的时候经常性出现问题需检查</strong>)通过WebUI上传。<br><img src="https://img-blog.csdnimg.cn/20200713151105102.png#pic_center" alt="在这里插入图片描述"><br>测试结果如下：</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop101 home<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#  nc -lk 8899</span>12 21 2112<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/20200713162651693.png#pic_center" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20200713162743635.png#pic_center" alt="在这里插入图片描述"><br><code>PS</code>：<strong>敲重点 IDEA中用到的Flink-scala核心依赖要跟服务器集群的核心依赖版本</strong><code>一致</code>，否则会 <a href="https://www.jianshu.com/p/91bb14306100" target="_blank" rel="noopener">报错</a>！</p><blockquote><p>java.lang.NoSuchMethodError: scala.Predef$.refArrayOps</p></blockquote><ol><li>命令行提交<br><a href="https://blog.csdn.net/Dax1n/article/details/72885035" target="_blank" rel="noopener">命令行提交</a> flink同样支持两种提交方式，默认不指定就是客户端方式。如果需要使用集群方式提交的话。可以在提交作业的命令行中指定-d或者–detached 进行进群模式提交。</li></ol><blockquote><p>-d,–detached If present, runs the job in detached mode（分离模式）</p></blockquote><pre class="line-numbers language-bash"><code class="language-bash">客户端提交方式：<span class="token variable">$FLINK_HOME</span>/bin/flink run   -c com.daxin.batch.App flinkwordcount.jar 客户端会多出来一个CliFrontend进程，就是驱动进程。集群模式提交：<span class="token variable">$FLINK_HOME</span>/bin/flink run -d  -c com.daxin.batch.App flinkwordcount.jar 程序提交完毕退出客户端，不再打印作业进度等信息！1234<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>重要参数说明：下面针对 flink-conf.yaml 文件中的几个重要参数进行分析：</li></ol><ul><li>jobmanager.heap.size：JobManager 节点可用的内存大小。</li><li>taskmanager.heap.size：TaskManager 节点可用的内存大小。</li><li>taskmanager.numberOfTaskSlots：每台机器可用的 Slot 数量。</li><li>parallelism.default：默认情况下 Flink 任务的并行度。</li></ul><p>上面参数中所说的 <code>Slot</code>和 <code>parallelism</code>的区别：</p><ul><li>Slot 是静态的概念，是指 TaskManager 具有的并发执行能力。</li><li>parallelism 是动态的概念，是指程序运行时实际使用的并发能力。</li><li>设置合适的 parallelism 能提高运算效率。</li><li>比如我又4个跑道(Slot )，本次任务我占用2个(parallelism)。 一般情况下Slot <code>&gt;=</code> parallelism</li></ul><h3 id="3-Flink-提交到-Yarn"><a href="#3-Flink-提交到-Yarn" class="headerlink" title="3. Flink 提交到 Yarn"></a>3. Flink 提交到 Yarn</h3><p>Flink on Yarn 模式的原理是<code>依靠 YARN 来调度 Flink 任务</code>，目前在企业中使用<strong>较多</strong>。 这种模式的好处是可以充分利用集群资源，提高集群机器的利用率，并且只需要 1 套 Hadoop 集群，就可以执行 MapReduce 和 Spark 任务，还可以执行 Flink 任务等，操作非常方便，不 需要维护多套集群，运维方面也很轻松。Flink on Yarn 模式需要依赖 Hadoop 集群，并且 Hadoop 的版本需要是 2.2 及以上。本文选择的 Hadoop 版本是 2.7.2。</p><p>Flink On Yarn 的内部实现原理(Snagit Editor绘制)：<br><img src="https://img-blog.csdnimg.cn/20200712212850155.png#pic_center" alt="在这里插入图片描述"></p><ul><li>当启动一个新的 Flink YARN Client 会话时，客户端首先会检查所请求的资源（容器和内存）是否可用。之后，它会上传 Flink 配置和 JAR 文件到 HDFS。</li><li>客 户 端 的 下 一 步 是 请 求 一 个 YARN 容 器 启 动 ApplicationMaster 。 JobManager 和 ApplicationMaster(AM)运行在同一个容器中，一旦它们成功地启动了，AM 就能够知道 JobManager 的地址，它会为 TaskManager 生成一个新的 Flink 配置文件（这样它才能连 上 JobManager），该文件也同样会被上传到 HDFS。另外，AM 容器还提供了 Flink 的 Web 界面服务。Flink 用来提供服务的端口是由用户和应用程序 ID 作为偏移配置的，这 使得用户能够并行执行多个 YARN 会话。</li><li>之后，AM 开始为 Flink 的 TaskManager 分配容器（Container），从 HDFS 下载 JAR 文件 和修改过的配置文件。一旦这些步骤完成了，Flink 就安装完成并准备接受任务了</li></ul><p>Flink on Yarn 模式在使用的时候又可以分为<code>两种</code>：</p><h5 id="第-1-种模式-Session-Cluster-："><a href="#第-1-种模式-Session-Cluster-：" class="headerlink" title="第 1 种模式(Session-Cluster)："></a>第 1 种模式(Session-Cluster)：</h5><p>是在 YARN 中<code>提前</code>初始化一个 Flink 集群(称为 Flink yarn-session)，开辟指定的资源，以后的 Flink 任务都提交到这里。这个 Flink 集群会<code>常驻</code>在 YARN 集群中，除非手工停止。这种方式创建的 Flink 集群会<code>独占资源</code>，不管有没有 Flink 任务在执行，YARN 上面的其他任务都无法使用这些资源。一般此种方式用的较少。<br><img src="https://img-blog.csdnimg.cn/20200713165033200.png#pic_center" alt="在这里插入图片描述"></p><h5 id="第-2-种模式-Per-Job-Cluster-："><a href="#第-2-种模式-Per-Job-Cluster-：" class="headerlink" title="第 2 种模式(Per-Job-Cluster)："></a>第 2 种模式(Per-Job-Cluster)：</h5><p><strong>每次提交 Flink 任务都会创建一个新的 Flink 集群</strong>， 每个 Flink 任务之间相互独立、互不影响，管理方便。任务执行完成之后创建的 Flink 集群也会消失，不会额外占用资源，按需使用，这使资源利用率达到最大，<strong>在工作中推荐使用这种模式</strong>。<br><img src="https://img-blog.csdnimg.cn/20200713165343219.png#pic_center" alt="在这里插入图片描述"><br><code>注意</code>：Flink on Yarn 还需要两个先决条件：</p><ol><li>配置 Hadoop 的环境变量</li><li><a href="https://flink.apache.org/zh/downloads.html" target="_blank" rel="noopener">下载</a> Flink 提交到 Hadoop 的连接器(jar 包 大约40M)，并把 jar 拷贝到 Flink 的 lib 目录下<img src="https://img-blog.csdnimg.cn/20200713165601842.png#pic_center" alt="在这里插入图片描述"></li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop101 flink-1.9.1<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># cp  /home/flink-shaded-hadoop-2-uber-2.7.5-7.0.jar   lib/</span>1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="启动第一种-Session-Cluster-模式（yarn-session）"><a href="#启动第一种-Session-Cluster-模式（yarn-session）" class="headerlink" title="启动第一种 Session-Cluster 模式（yarn-session）"></a>启动第一种 Session-Cluster 模式（yarn-session）</h3><p>1 先启动 Hadoop 集群，然后通过命令启动一个 Flink 的 yarn-session 集群：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop101 flink-1.9.1<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># bin/yarn-session.sh -n 3 -s 3 -nm sowhat -d </span>1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其中 yarn-session.sh 后面支持多个参数。下面针对一些常见的参数进行讲解：</p><ol><li>-n、–container 表示分配容器的数量（也就是 TaskManager 的数量）。</li><li>-D 动态属性。</li><li>-d、–detached 在后台独立运行。</li><li>-jm、–jobManagerMemory ：设置 JobManager 的内存，单位是 MB。</li><li>-nm、–name：在 YARN 上为一个自定义的应用设置一个名字。</li><li>-q、–query：显示 YARN 中可用的资源（内存、cpu 核数）。</li><li>-qu、–queue ：指定 YARN 队列。</li><li>-s、–slots ：每个 TaskManager 使用的 Slot 数量。</li><li>-tm、–taskManagerMemory ：每个 TaskManager 的内存，单位是 MB。</li><li>-z、–zookeeperNamespace ：针对 HA 模式在 ZooKeeper 上创建 NameSpace。</li><li>-id、–applicationId ：指定 YARN 集群上的任务 ID，附着到一个后台独 立运行的 yarn session 中。<br><img src="https://img-blog.csdnimg.cn/20200713170654200.png#pic_center" alt="在这里插入图片描述"><br>查看 WebUI: 由于还没有提交 Flink job，所以都是 0。<br><img src="https://img-blog.csdnimg.cn/20200713170554981.png#pic_center" alt="在这里插入图片描述"><br><strong>这个时候注意查看本地文件系统中有一个临时文件</strong>。有了这个文件可以提交 job 到 Yarn<br><img src="https://img-blog.csdnimg.cn/20200713170722129.png#pic_center" alt="在这里插入图片描述"><br>提交 Job : 由于有了之前的配置，所以自动会提交到 Yarn 中。</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/flink run -c com.bjsxt.flink.StreamWordCount /home/Flink-Demo-1.0-SNAPSHOT.jar1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/20200713170816557.png#pic_center" alt="在这里插入图片描述"><br>至此第一种模式全部完成。</p><h3 id="启动第二种模式"><a href="#启动第二种模式" class="headerlink" title="启动第二种模式"></a>启动第二种模式</h3><p>这种模式下<strong>不需要</strong>先启动 yarn-session。所以我们可以把前面启动的 yarn-session 集 群先停止，停止的命令是:</p><pre class="line-numbers language-bash"><code class="language-bash">yarn application -kill application_1576832892572_0002 //其中 application_1576832892572_0002 是ID1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>确保 Hadoop 集群是健康的情况下直接提交 Job 命令：</p><pre class="line-numbers language-bash"><code class="language-bash">bin/flink run -m yarn-cluster -yn 3 -ys 3 -ynm sowhat02 \-c com.sowhat.flink.StreamWordCount /home/Flink-Demo-1.0-SNAPSHOT.jar12<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可以看到一个全新的 yarn-session<br><img src="https://img-blog.csdnimg.cn/2020071317130741.png#pic_center" alt="在这里插入图片描述"><br>任务提交参数讲解：相对于 Yarn-Session 参数而言，只是前面加了 y。</p><ol><li>-yn、–container 表示分配容器的数量，也就是 TaskManager 的数量。</li><li>-d、–detached：设置在后台运行。</li><li>-yjm、–jobManagerMemory:设置 JobManager 的内存，单位是 MB。</li><li>-ytm、–taskManagerMemory:设置每个 TaskManager 的内存，单位是 MB。</li><li>-ynm、–name:给当前 Flink application 在 Yarn 上指定名称。</li><li>-yq、–query：显示 yarn 中可用的资源（内存、cpu 核数）</li><li>-yqu、–queue :指定 yarn 资源队列</li><li>-ys、–slots :每个 TaskManager 使用的 Slot 数量。</li><li>-yz、–zookeeperNamespace:针对 HA 模式在 Zookeeper 上创建 NameSpace</li><li>-yid、–applicationID : 指定 Yarn 集群上的任务 ID,附着到一个后台独 立运行的 Yarn Session 中。</li></ol><h3 id="4-Flink-的HA"><a href="#4-Flink-的HA" class="headerlink" title="4. Flink 的HA"></a>4. Flink 的HA</h3><p>默认情况下，每个 Flink 集群<strong>只有一个 JobManager</strong>，这将导致单点故障（<code>SPOF</code>），如 果这个 JobManager 挂了，则不能提交新的任务，并且运行中的程序也会失败。使用 JobManager HA，集群可以从 JobManager 故障中恢复，从而避免单点故障。用户可以在 <code>Standalone</code>或<code>Flink on Yarn</code> 集群模式下配置 Flink 集群 HA（高可用性）。</p><h5 id="Standalone-HA"><a href="#Standalone-HA" class="headerlink" title="Standalone HA"></a>Standalone HA</h5><p>Standalone 模式下，JobManager 的高可用性的基本思想是，任何时候都有一个 Alive JobManager 和多个 Standby JobManager。Standby JobManager 可以在 Alive JobManager 挂掉的情况下接管集群成为 Alive JobManager，这样避免了单点故障，一旦某一个 Standby JobManager 接管集群，程序就可以继续运行。Standby JobManagers 和 Alive JobManager 实例之间<strong>没有明确区别</strong>，每个 JobManager 都可以成为 Alive 或 Standby<img src="https://img-blog.csdnimg.cn/20200713172223651.png#pic_center" alt="在这里插入图片描述"><br>Flink Standalone 集群的 HA 安装和配置<br>实现 HA 还需要依赖 ZooKeeper 和 HDFS，因此要有一个 ZooKeeper 集群和 Hadoop 集群， 首先启动 Zookeeper 集群和 HDFS 集群。本文中分配 3 台 JobManager，如下表：</p><table><thead><tr><th>hadoop101</th><th>hadoop102</th><th>hadoop103</th></tr></thead><tbody><tr><td>JobManager</td><td>JobManager</td><td>JobManager</td></tr><tr><td>TaskManager</td><td>TaskManager</td><td>TaskManager</td></tr></tbody></table><ol><li>修改配置文件 conf/masters</li></ol><pre class="line-numbers language-bash"><code class="language-bash">hadoop101:8081hadoop102:8081hadoop103:8081123<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol><li>修改配置文件 conf/flink-conf.yaml</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#要启用高可用，设置修改为zookeeper</span> high-availability: zookeeper <span class="token comment" spellcheck="true">#Zookeeper的主机名和端口信息，多个参数之间用逗号隔开</span>high-availability.zookeeper.quorum: hadoop103:2181,hadoop101:2181,hadoop102:2181 <span class="token comment" spellcheck="true"># 建议指定HDFS的全路径。如果某个Flink节点没有配置HDFS的话，不指定HDFS的全路径 则无法识到，</span><span class="token comment" spellcheck="true"># storageDir存储了恢复一个JobManager所需的所有元数据。</span>high-availability.storageDir: hdfs://hadoop101:9000/flink/h1234567<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>把修改的配置文件拷贝其他服务器中</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop101 conf<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># scp masters flink-conf.yaml root@hadoop102:`pwd` </span><span class="token punctuation">[</span>root@hadoop101 conf<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># scp masters flink-conf.yaml root@hadoop103:`pwd`</span>12<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol><li>启动集群<br><img src="https://img-blog.csdnimg.cn/20200713172811146.png#pic_center" alt="在这里插入图片描述"><br>版本问题：目前使用 Flink1.7.1 版本测试没有问题，使用 Flink1.9 版本存在 HA 界面不能自动跳转到对应的 Alive jobManager。</li></ol><h5 id="Flink-On-Yarn-HA"><a href="#Flink-On-Yarn-HA" class="headerlink" title="Flink On Yarn HA"></a>Flink On Yarn HA</h5><p>正常基于 Yarn 提交 Flink 程序，无论是使用 <code>yarn-session</code> 模式还是 <code>yarn-cluster</code>模 式 ， 基 于 yarn 运 行 后 的 application 只 要 kill 掉 对 应 的 Flink 集 群 进 程<code>YarnSessionClusterEntrypoint</code>后，基于 Yarn 的 Flink 任务就失败了，<code>不会</code>自动进行重试，所以基于 Yarn 运行 Flink 任务，也有必要搭建 HA，这里同样还是需要借助 zookeeper 来完成，步骤如下：</p><ol><li>修改所有 Hadoop 节点的 yarn-site.xml 将所有 Hadoop 节点的 yarn-site.xml 中的提交应用程序最大尝试次数调大</li></ol><pre class="line-numbers language-xml"><code class="language-xml">#在每台hadoop节点yarn-site.xml中设置提交应用程序的最大尝试次数，建议不低于4，# 这里重试指的ApplicationMaster <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.am.max-attempts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>123456<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>启动 zookeeper，启动 Hadoop 集群</li><li>修改 Flink 对应 flink-conf.yaml 配置，配置内容如下：</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#配置依赖zookeeper模式进行HA搭建 </span>high-availability: zookeeper <span class="token comment" spellcheck="true">#配置JobManager原数据存储路径 high-availability.storageDir: hdfs://hadoop101:9000/flink/yarnha/ </span><span class="token comment" spellcheck="true">#配置zookeeper集群节点 </span>high-availability.zookeeper.quorum: hadoop101:2181,hadoop102:2181,hadoop103:2181 <span class="token comment" spellcheck="true">#yarn停止一个application重试的次数 </span>yarn.application-attempts: 101234567<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>启动 yarn-session.sh 测试 HA： yarn-session.sh -n 2 ，也可以直接提交 Job 启动之后，可以登录 yarn 中对应的 flink WebUI，如下图示：<img src="https://img-blog.csdnimg.cn/20200713174155511.png#pic_center" alt="在这里插入图片描述"></li><li>点击对应的 Tracking UI，进入 Flink 集群 UI<br><img src="https://img-blog.csdnimg.cn/20200713174255954.png#pic_center" alt="在这里插入图片描述"><br>查看对应的 JobManager 在哪台节点上启动：<img src="https://img-blog.csdnimg.cn/20200713174342414.png#pic_center" alt="在这里插入图片描述"><br>进入对应的节点，kill 掉对应的<code>YarnSessionClusterEntrypoint</code>进程。然后进入到 Yarn 中观察<code>applicationxxxx_0001</code>job 信息：<img src="https://img-blog.csdnimg.cn/20200713174452942.png#pic_center" alt="在这里插入图片描述"><br>点击 job ID,发现会有对应的重试信息：<img src="https://img-blog.csdnimg.cn/20200713174527546.png#pic_center" alt="在这里插入图片描述"><br>点击对应的<code>Tracking U</code>进入到 Flink 集群 UI，查看新的 JobManager 节点由原来的 hadoop103 变成了 hadoop101，说明 HA 起作用。<img src="https://img-blog.csdnimg.cn/20200713174555148.png#pic_center" alt="在这里插入图片描述"></li></ol><h1 id="4-Flink-并行度和-Slot"><a href="#4-Flink-并行度和-Slot" class="headerlink" title="4. Flink 并行度和 Slot"></a>4. Flink 并行度和 Slot</h1><p>Flink中每一个worker(TaskManager)都是一个JVM进程，它可能会在独立的线程（Solt） 上执行一个或多个 subtask。Flink 的每个 TaskManager 为集群提供 Solt。<strong>Solt 的数量通常 与每个 TaskManager 节点的可用 CPU 内核数成比例</strong>，<code>一般情况下 Slot 的数量就是每个节点 的 CPU 的核数</code>。 Slot 的 数 量 由 集 群 中 flink-conf.yaml 配 置 文 件 中 设 置 taskmanager.numberOfTaskSlots，这个值的大小<code>建议</code>和节点 CPU 的数量保持一致。比如我设置=3。<br><code>并行度</code>=2的情况下：<br><img src="https://img-blog.csdnimg.cn/20200713175851646.png#pic_center" alt="在这里插入图片描述"><br>注意一点：一个TaskSlot可能执行多个job。</p><p>一个任务的并行度设置可以从 4 个层面指定:</p><ol><li>Operator Level（算子层面）。</li><li>Execution Environment Level（执行环境层面）。</li><li>Client Level（客户端层面）。</li><li>System Level（系统层面）。</li></ol><p>这 些 并 行 度 的 优 先 级 为 ：<br>Operator Level <code>&gt;</code>Execution Environment Level <code>&gt;</code> Client Level <code>&gt;</code> System Level。</p><h5 id="1-并行度设置之-Operator-Level"><a href="#1-并行度设置之-Operator-Level" class="headerlink" title="1. 并行度设置之 Operator Level"></a>1. 并行度设置之 Operator Level</h5><p>Operator、Source 和 Sink 目的地的并行度可以通过调用 <code>setParallelism()</code>方法来指定</p><pre class="line-numbers language-java"><code class="language-java">    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>     <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//分组算子  : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数</span>     <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//聚会累加算子</span>      <span class="token comment" spellcheck="true">//5、打印结果</span>   result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"结果"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token number">123456</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-并行度设置之-Execution-Environment-Level"><a href="#2-并行度设置之-Execution-Environment-Level" class="headerlink" title="2. 并行度设置之 Execution Environment Level"></a>2. 并行度设置之 Execution Environment Level</h5><p>任务的默认并行度可以通过调用 <code>setParallelism()</code>方法指定。为了以并行度 3 来执行 <code>所有</code>的 Operator、Source 和 Sink，可以通过如下方式设置执行环境的并行度</p><pre class="line-numbers language-java"><code class="language-java">    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为3</span><span class="token number">1234</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="3-并行度设置之-Client-Level"><a href="#3-并行度设置之-Client-Level" class="headerlink" title="3. 并行度设置之 Client Level"></a>3. 并行度设置之 Client Level</h5><p>并行度还可以在客户端提交 Job 到 Flink 时设定。对于 CLI 客户端，可以通过-p 参数指定并行度。</p><pre class="line-numbers language-bash"><code class="language-bash">bin/flink run -p 10  WordCount.jar1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h5 id="4-并行度设置之-System-Level"><a href="#4-并行度设置之-System-Level" class="headerlink" title="4. 并行度设置之 System Level"></a>4. 并行度设置之 System Level</h5><p>在系统级可以通过设置flink-conf.yaml文件中的parallelism.default属性来指定所 有执行环境的默认并行度。</p><pre class="line-numbers language-xml"><code class="language-xml"># The parallelism used for programs that did not specify and other parallelism.parallelism.default: 112<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h5 id="5-并行度案例分析"><a href="#5-并行度案例分析" class="headerlink" title="5. 并行度案例分析"></a>5. 并行度案例分析</h5><p>Flink 集群中有 3 个 TaskManager 节点，每个 TaskManager 的 Slot 数量为 3<br><img src="https://img-blog.csdnimg.cn/2020071318385934.png#pic_center" alt="在这里插入图片描述"><br>全部默认情况下：<br><img src="https://img-blog.csdnimg.cn/2020071318404774.png#pic_center" alt="在这里插入图片描述"><br>全局并行度=2<br><img src="https://img-blog.csdnimg.cn/20200713184201354.png#pic_center" alt="在这里插入图片描述"><br><code>End</code>：牢记并行度设置的优先级，根据集群配置合理设置参数。</p><h1 id="4-Flink-常用API详解"><a href="#4-Flink-常用API详解" class="headerlink" title="4. Flink 常用API详解"></a>4. Flink 常用API详解</h1><h3 id="1-函数阶层"><a href="#1-函数阶层" class="headerlink" title="1. 函数阶层"></a>1. 函数阶层</h3><p>Flink 根据抽象程度分层，提供了三种不同的 API 和库。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。<img src="https://img-blog.csdnimg.cn/20200713190046742.png#pic_center" alt="在这里插入图片描述"></p><ol><li><code>ProcessFunction</code><br>ProcessFunction 是 Flink 所提供<code>最底层接口</code>。ProcessFunction 可以处理一或两条 输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的 某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件 驱动应用所需要的基于单个事件的复杂业务逻辑。</li><li><code>DataStream API</code><br>DataStream API 为许多通用的流处理操作提供了<strong>处理原语</strong>。这些操作包括窗口、逐条 记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如<code>map()</code>、<code>reduce()</code>、<code>aggregate()</code> 等函数。你可以通过扩 展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。</li><li><code>SQL &amp; Table API</code>：<br>Flink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是<code>批处理</code>和<code>流处理</code>统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API 和 SQL 借助了 Apache Calcite 来进行查询的解析，校验以及优化。它们可以与 DataStream 和 DataSet API 无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。</li></ol><p>另外 Flink 具有数个适用于常见数据处理应用场景的<strong>扩展库</strong>。</p><ol><li><code>复杂事件处理(CEP)</code>：<br>模式检测是事件流处理中的一个非常常见的用例。Flink 的 CEP 库提供了 API，使用户能够以例如正则表达式或状态机的方式指定事件模式。CEP 库与 Flink 的 DataStream API 集成，以便在 DataStream 上评估模式。CEP 库的应用包括 网络入侵检测，业务流程监控和欺诈检测。</li><li><code>DataSet API</code>：<br>DataSet API 是 Flink 用于<code>批处理</code>应用程序的核心 API。DataSet API 所提供的基础算子包括 map、reduce、(outer) join、co-group、iterate 等。所有算子都有相应的算法和数据结构支持，对内存中的序列化数据进行操作。如果数据大小超过预留内存，则过量数据将存储到磁盘。Flink 的 DataSet API 的数据处理算法借鉴了传统数据库算法的实现，例如混合散列连接（hybrid hash-join）和外部归并排序 （external merge-sort）。</li><li><code>Gelly</code>:<br>Gelly 是一个可扩展的图形处理和分析库。Gelly 是在 DataSet API 之上实现 的，并与 DataSet API 集成。因此，它能够受益于其可扩展且健壮的操作符。Gelly 提 供了内置算法，如 label propagation、triangle enumeration 和 PageRank 算法， 也提供了一个简化自定义图算法实现的 Graph API。</li></ol><h3 id="2-DataStream-的编程模型"><a href="#2-DataStream-的编程模型" class="headerlink" title="2. DataStream 的编程模型"></a>2. DataStream 的编程模型</h3><p>DataStream 的编程模型包括<code>四</code>个部分：<code>Environment</code>，<code>DataSource</code>，<code>Transformation</code>，<code>Sink</code>。<code>此乃重点</code>，接下来主要按照这四部分讲解。<br><img src="https://img-blog.csdnimg.cn/20200713190722337.png#pic_center" alt="在这里插入图片描述"></p><h3 id="3-Flink-的-DataSource-数据源"><a href="#3-Flink-的-DataSource-数据源" class="headerlink" title="3. Flink 的 DataSource 数据源"></a>3. Flink 的 DataSource 数据源</h3><p>基于文件、基于集合、基于Kafka、自定义的DataSource</p><h5 id="1-基于文件的Source"><a href="#1-基于文件的Source" class="headerlink" title="1. 基于文件的Source"></a>1. 基于文件的Source</h5><p>读取本地文件系统的数据，前面的案例已经讲过了。本课程主要讲基于<code>HDFS</code>文件系统的 Source。首先需要配置 Hadoop 的依赖</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hadoop-common<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>12345678910<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject HDFSFileSource <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//2、导入隐式转换</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取HDFS文件系统上的文件</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop101:9000/wc.txt"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//单词统计的计算</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//定义sink</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"wordcount"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">123456789101112131415161718192021222324252627282930</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-基于集合的Source"><a href="#2-基于集合的Source" class="headerlink" title="2. 基于集合的Source"></a>2. 基于集合的Source</h5><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token comment" spellcheck="true">/**  * 基站日志  * @param sid      基站的id  * @param callOut  主叫号码  * @param callInt  被叫号码  * @param callType 呼叫类型  * @param callTime 呼叫时间 (毫秒)  * @param duration 通话时长 （秒）  */</span><span class="token keyword">case</span> <span class="token keyword">class</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span>sid<span class="token operator">:</span> String<span class="token punctuation">,</span> var callOut<span class="token operator">:</span> String<span class="token punctuation">,</span> var callInt<span class="token operator">:</span> String<span class="token punctuation">,</span> callType<span class="token operator">:</span> String<span class="token punctuation">,</span> callTime<span class="token operator">:</span> Long<span class="token punctuation">,</span> duration<span class="token operator">:</span> Long<span class="token punctuation">)</span>object CollectionSource <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//2、导入隐式转换</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromCollection</span><span class="token punctuation">(</span><span class="token function">Array</span><span class="token punctuation">(</span>      <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token string">"001"</span><span class="token punctuation">,</span> <span class="token string">"1866"</span><span class="token punctuation">,</span> <span class="token string">"189"</span><span class="token punctuation">,</span> <span class="token string">"busy"</span><span class="token punctuation">,</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token string">"002"</span><span class="token punctuation">,</span> <span class="token string">"1866"</span><span class="token punctuation">,</span> <span class="token string">"188"</span><span class="token punctuation">,</span> <span class="token string">"busy"</span><span class="token punctuation">,</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token string">"004"</span><span class="token punctuation">,</span> <span class="token string">"1876"</span><span class="token punctuation">,</span> <span class="token string">"183"</span><span class="token punctuation">,</span> <span class="token string">"busy"</span><span class="token punctuation">,</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token string">"005"</span><span class="token punctuation">,</span> <span class="token string">"1856"</span><span class="token punctuation">,</span> <span class="token string">"186"</span><span class="token punctuation">,</span> <span class="token string">"success"</span><span class="token punctuation">,</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="3-基于Kafka的Source"><a href="#3-基于Kafka的Source" class="headerlink" title="3. 基于Kafka的Source"></a>3. 基于Kafka的Source</h5><p>首 先 需 要 配 置 Kafka 连 接 器 的 依 赖 ， 另 外 更 多 的 连 接 器 可 以 查 看 官 网 ：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/connectors/" target="_blank" rel="noopener">连接器</a></p><pre class="line-numbers language-xml"><code class="language-xml">   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-kafka_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.kafka<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>kafka-clients<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>0.11.0.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>12345678910<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>关于Kafka的demo参考 <a href="https://blog.csdn.net/qq_31821675/category_10138824.html" target="_blank" rel="noopener">文章</a></p><h6 id="1-消费普通String"><a href="#1-消费普通String" class="headerlink" title="1. 消费普通String"></a>1. 消费普通String</h6><p>Kafka生产者：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token punctuation">[</span>atguigu<span class="token annotation punctuation">@hadoop102</span> kafka<span class="token punctuation">]</span>$ bin<span class="token operator">/</span>kafka<span class="token operator">-</span>console<span class="token operator">-</span>producer<span class="token punctuation">.</span>sh <span class="token operator">--</span>broker<span class="token operator">-</span>list hadoop102<span class="token operator">:</span><span class="token number">9092</span> <span class="token operator">--</span>topic sowhat<span class="token operator">></span>hello world<span class="token operator">></span>sowhat <span class="token number">1234</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>消费者</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Properties<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>SimpleStringSchema<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>FlinkKafkaConsumer<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>StringDeserializerobject KafkaSource1 <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//2、导入隐式转换</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//连接Kafka，并且Kafka中的数据是普通字符串（String）</span>    val props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 链接的Kafka 集群</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 指定组名</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"group.id"</span><span class="token punctuation">,</span> <span class="token string">"fink01"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 指定KV序列化类</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"key.deserializer"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>StringDeserializer<span class="token punctuation">]</span><span class="token punctuation">.</span>getName<span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"value.deserializer"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>StringDeserializer<span class="token punctuation">]</span><span class="token punctuation">.</span>getName<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 从最新数据开始读</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"auto.offset.reset"</span><span class="token punctuation">,</span> <span class="token string">"latest"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 订阅主题</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FlinkKafkaConsumer</span><span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"sowhat"</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">SimpleStringSchema</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> props<span class="token punctuation">)</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="2-消费KV形式"><a href="#2-消费KV形式" class="headerlink" title="2. 消费KV形式"></a>2. 消费KV形式</h6><p>Kafka模式就是输入的KV只是平常只用V而已，如果用消费者KV则我们需要代码编写生产者跟消费者。<br><strong>生产者：</strong></p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Properties<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span><span class="token punctuation">{</span>KafkaProducer<span class="token punctuation">,</span> ProducerRecord<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>StringSerializer<span class="token keyword">import</span> scala<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Randomobject MyKafkaProducer <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//连接Kafka的属性</span>    val props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 链接的集群</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 序列化KV类</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"key.serializer"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>StringSerializer<span class="token punctuation">]</span><span class="token punctuation">.</span>getName<span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"value.serializer"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>StringSerializer<span class="token punctuation">]</span><span class="token punctuation">.</span>getName<span class="token punctuation">)</span>    var producer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaProducer</span><span class="token punctuation">[</span>String<span class="token punctuation">,</span> String<span class="token punctuation">]</span><span class="token punctuation">(</span>props<span class="token punctuation">)</span>    var r <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Random</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//死循环生成键值对的数据</span>      val data <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">[</span>String<span class="token punctuation">,</span> String<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"sowhat"</span><span class="token punctuation">,</span> <span class="token string">"key"</span> <span class="token operator">+</span> r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"value"</span> <span class="token operator">+</span> r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>      Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    producer<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">123456789101112131415161718192021222324252627282930</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>消费者：</strong></p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Properties<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>typeinfo<span class="token punctuation">.</span>TypeInformation<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span><span class="token punctuation">{</span>FlinkKafkaConsumer<span class="token punctuation">,</span> KafkaDeserializationSchema<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span>ConsumerRecord<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>StringDeserializer<span class="token comment" spellcheck="true">//2、导入隐式转换</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_object KafkaSourceByKeyValue <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//连接Kafka的属性</span>    val props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span><span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"group.id"</span><span class="token punctuation">,</span> <span class="token string">"flink002"</span><span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"key.deserializer"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>StringDeserializer<span class="token punctuation">]</span><span class="token punctuation">.</span>getName<span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"value.deserializer"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>StringDeserializer<span class="token punctuation">]</span><span class="token punctuation">.</span>getName<span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"auto.offset.reset"</span><span class="token punctuation">,</span> <span class="token string">"latest"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//设置Kafka数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FlinkKafkaConsumer</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"sowhat"</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">MyKafkaReader</span><span class="token punctuation">,</span> props<span class="token punctuation">)</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//自定义一个类，从Kafka中读取键值对的数据</span>  <span class="token keyword">class</span> <span class="token class-name">MyKafkaReader</span> <span class="token keyword">extends</span> <span class="token class-name">KafkaDeserializationSchema</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//是否流结束</span>    override def <span class="token function">isEndOfStream</span><span class="token punctuation">(</span>nextElement<span class="token operator">:</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">:</span> Boolean <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token boolean">false</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 反序列化</span>    override def <span class="token function">deserialize</span><span class="token punctuation">(</span>record<span class="token operator">:</span> ConsumerRecord<span class="token punctuation">[</span>Array<span class="token punctuation">[</span>Byte<span class="token punctuation">]</span><span class="token punctuation">,</span> Array<span class="token punctuation">[</span>Byte<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>record <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        var key <span class="token operator">=</span> <span class="token string">"null"</span>        var value <span class="token operator">=</span> <span class="token string">"null"</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>record<span class="token punctuation">.</span><span class="token function">key</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>          key <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>record<span class="token punctuation">.</span><span class="token function">key</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"UTF-8"</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>record<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//从Kafka记录中得到Value</span>          value <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>record<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"UTF-8"</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>        <span class="token punctuation">(</span>key<span class="token punctuation">,</span> value<span class="token punctuation">)</span>      <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//数据为空</span>        <span class="token punctuation">(</span><span class="token string">"null"</span><span class="token punctuation">,</span> <span class="token string">"null"</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//指定类型</span>    override def getProducedType<span class="token operator">:</span> TypeInformation<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token function">createTuple2TypeInformation</span><span class="token punctuation">(</span>createTypeInformation<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">,</span> createTypeInformation<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="4-自定义Source"><a href="#4-自定义Source" class="headerlink" title="4. 自定义Source"></a>4. 自定义Source</h5><p>当然也可以自定义数据源，有<code>两种</code>方式实现：</p><ol><li>通过实现 <code>SourceFunction</code>接口来自定义无并行度（也就是并行度只能为 1）的 Source。</li><li>通过实现 <code>ParallelSourceFunction</code> 接口或者继承 <code>RichParallelSourceFunction</code> 来自 定义有并行度的数据源。</li></ol><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>source<span class="token punctuation">.</span>SourceFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span><span class="token punctuation">{</span>StreamExecutionEnvironment<span class="token punctuation">,</span> _<span class="token punctuation">}</span><span class="token keyword">import</span> scala<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Random<span class="token keyword">case</span> <span class="token keyword">class</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span>sid<span class="token operator">:</span> String<span class="token punctuation">,</span> var callOut<span class="token operator">:</span> String<span class="token punctuation">,</span> var callInt<span class="token operator">:</span> String<span class="token punctuation">,</span> callType<span class="token operator">:</span> String<span class="token punctuation">,</span> callTime<span class="token operator">:</span> Long<span class="token punctuation">,</span> duration<span class="token operator">:</span> Long<span class="token punctuation">)</span><span class="token comment" spellcheck="true">/**  * 自定义的Source,需求：每隔两秒钟，生成10条随机基站通话日志数据  */</span><span class="token keyword">class</span> <span class="token class-name">MyCustomerSource</span> <span class="token keyword">extends</span> <span class="token class-name">SourceFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//是否终止数据流的标记</span>  var flag <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">/**    * 主要的方法，启动一个Source，并且从Source中返回数据    * 如果run方法停止，则数据流终止    */</span>  override def <span class="token function">run</span><span class="token punctuation">(</span>ctx<span class="token operator">:</span> SourceFunction<span class="token punctuation">.</span>SourceContext<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val r <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Random</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    var types <span class="token operator">=</span> <span class="token function">Array</span><span class="token punctuation">(</span><span class="token string">"fail"</span><span class="token punctuation">,</span> <span class="token string">"basy"</span><span class="token punctuation">,</span> <span class="token string">"barring"</span><span class="token punctuation">,</span> <span class="token string">"success"</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>flag<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token number">1</span><span class="token punctuation">.</span><span class="token function">to</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>_ <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        var callOut <span class="token operator">=</span> <span class="token string">"1860000%04d"</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//主叫号码</span>        var callIn <span class="token operator">=</span> <span class="token string">"1890000%04d"</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//被叫号码</span>        <span class="token comment" spellcheck="true">//生成一条数据</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token string">"station_"</span> <span class="token operator">+</span> r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> callOut<span class="token punctuation">,</span> callIn<span class="token punctuation">,</span> <span class="token function">types</span><span class="token punctuation">(</span>r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> System<span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> r<span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">foreach</span><span class="token punctuation">(</span>ctx<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span>_<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//发送数据到流</span>      Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//每隔2秒发送一次数据</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//终止数据流</span>  override def <span class="token function">cancel</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    flag <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>object CustomerSource <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyCustomerSource</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"SelfSource"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-Flink-的-Sink-数据目标"><a href="#4-Flink-的-Sink-数据目标" class="headerlink" title="4. Flink 的 Sink 数据目标"></a>4. Flink 的 Sink 数据目标</h3><p>Flink 针对 DataStream 提供了大量的已经实现的<code>数据目标</code>（Sink），包括<code>文件</code>、<code>Kafka</code>、<code>Redis</code>、<code>HDFS</code>、<code>Elasticsearch</code> 等等。</p><h5 id="1-基于-HDFS-的-Sink"><a href="#1-基于-HDFS-的-Sink" class="headerlink" title="1. 基于 HDFS 的 Sink"></a>1. 基于 HDFS 的 Sink</h5><p>首先配置支持 Hadoop FileSystem 的连接器依赖。</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-filesystem_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>12345<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Streaming File Sink 能把数据写入 HDFS 中，还可以<code>支持分桶写入</code>，每一个 <a href="https://sowhat.blog.csdn.net/article/details/106488897" target="_blank" rel="noopener">分桶</a> 就对 应 HDFS 中的一个目录。默认按照<strong>小时来分桶</strong>，在一个桶内部，会进一步将输出基于滚动策 略切分成更小的文件。这有助于防止桶文件变得过大。滚动策略也是可以配置的，默认策略会根据文件大小和超时时间来滚动文件，超时时间是指没有新数据写入部分文件（part file）的时间。</p><p><code>需求</code>：把自定义的Source作为数据源，把基站日志数据 <a href="https://blog.csdn.net/bingque6535/article/details/107269077/" target="_blank" rel="noopener">写入HDFS</a> 并且每隔10秒钟生成一个文件</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>sink<span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span><span class="token punctuation">{</span>MyCustomerSource<span class="token punctuation">,</span> StationLog<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>SimpleStringEncoder<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>core<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span>filesystem<span class="token punctuation">.</span>StreamingFileSink<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span>filesystem<span class="token punctuation">.</span>rollingpolicies<span class="token punctuation">.</span>DefaultRollingPolicy<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span><span class="token punctuation">{</span>StreamExecutionEnvironment<span class="token punctuation">,</span> _<span class="token punctuation">}</span>object HDFSSink <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//需求：把自定义的Source作为数据源，把基站日志数据写入HDFS并且每隔10钟生成一个文件</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyCustomerSource</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//默认一个小时一个目录(分桶)</span>    <span class="token comment" spellcheck="true">//设置一个滚动策略</span>    val rolling<span class="token operator">:</span> DefaultRollingPolicy<span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> String<span class="token punctuation">]</span> <span class="token operator">=</span> DefaultRollingPolicy<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">withInactivityInterval</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//不活动的分桶时间</span>      <span class="token punctuation">.</span><span class="token function">withRolloverInterval</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//每隔10 生成一个文件</span>      <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//创建</span>    <span class="token comment" spellcheck="true">//创建HDFS的Sink</span>    val hdfsSink<span class="token operator">:</span> StreamingFileSink<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> StreamingFileSink<span class="token punctuation">.</span>forRowFormat<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">(</span>      <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop101:9000/MySink001/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      <span class="token keyword">new</span> <span class="token class-name">SimpleStringEncoder</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"UTF-8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">withRollingPolicy</span><span class="token punctuation">(</span>rolling<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">withBucketCheckInterval</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//检查间隔时间</span>      <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span>hdfsSink<span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-基于-Redis的-Sink"><a href="#2-基于-Redis的-Sink" class="headerlink" title="2. 基于 Redis的 Sink"></a>2. 基于 Redis的 Sink</h5><p>Flink 除了内置的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/connectors/" target="_blank" rel="noopener">连接器</a> 外，还有一些额外的连接器通过 Apache Bahir 发布，包括：</p><ul><li>Apache ActiveMQ (source/sink)</li><li>Apache Flume (sink)</li><li>Redis (sink)</li><li>Akka (sink)</li><li>Netty (source)</li></ul><p>这里我用 Redis 来举例，首先需要配置 Redis 连接器的依赖：<br><code>需求</code>：把netcat作为数据源，并且统计每个单词的次数，统计的结果写入Redis数据库中。<br>导入依赖：</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.bahir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-connector-redis_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>12345<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码如下：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>sink<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>redis<span class="token punctuation">.</span>RedisSink<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>redis<span class="token punctuation">.</span>common<span class="token punctuation">.</span>config<span class="token punctuation">.</span>FlinkJedisPoolConfig<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>redis<span class="token punctuation">.</span>common<span class="token punctuation">.</span>mapper<span class="token punctuation">.</span><span class="token punctuation">{</span>RedisCommand<span class="token punctuation">,</span> RedisCommandDescription<span class="token punctuation">,</span> RedisMapper<span class="token punctuation">}</span>object RedisSink <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//需求：把netcat作为数据源，并且统计每个单词的次数，统计的结果写入Redis数据库中。</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//计算</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">// 等价于groupbyKey</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//把结果写入Redis中 设置连接Redis的配置</span>    val config<span class="token operator">:</span> FlinkJedisPoolConfig <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FlinkJedisPoolConfig<span class="token punctuation">.</span>Builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setDatabase</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setHost</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setPort</span><span class="token punctuation">(</span><span class="token number">6379</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//设置Redis的Sink</span>    result<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">RedisSink</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">RedisMapper</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">//设置redis的命令</span>      override def getCommandDescription <span class="token operator">=</span> <span class="token punctuation">{</span>        <span class="token keyword">new</span> <span class="token class-name">RedisCommandDescription</span><span class="token punctuation">(</span>RedisCommand<span class="token punctuation">.</span>HSET<span class="token punctuation">,</span> <span class="token string">"sowhat"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">// https://bahir.apache.org/docs/flink/current/flink-streaming-redis/</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">//从数据中获取Key</span>      override def <span class="token function">getKeyFromData</span><span class="token punctuation">(</span>data<span class="token operator">:</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{</span>        data<span class="token punctuation">.</span>_1      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">//从数据中获取Value</span>      override def <span class="token function">getValueFromData</span><span class="token punctuation">(</span>data<span class="token operator">:</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{</span>        data<span class="token punctuation">.</span>_2 <span class="token operator">+</span> <span class="token string">""</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"redisSink"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940414243</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="3-基于-Kafka的-Sink"><a href="#3-基于-Kafka的-Sink" class="headerlink" title="3. 基于 Kafka的 Sink"></a>3. 基于 Kafka的 Sink</h5><p>由于前面有的课程已经讲过 Flink 的 Kafka 连接器，所以还是一样需要配置 Kafka 连接 器的依赖配置，接下我们还是把 WordCout 的结果写入 Kafka：</p><h6 id="1-Kafka作为Sink的第一种（String）"><a href="#1-Kafka作为Sink的第一种（String）" class="headerlink" title="1. Kafka作为Sink的第一种（String）"></a>1. Kafka作为Sink的第一种（String）</h6><p><code>需求</code>：把netcat数据源中每个单词写入Kafka</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>sink<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>SimpleStringSchema<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>FlinkKafkaProducerobject KafkaSinkByString <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//Kafka作为Sink的第一种（String）</span>  <span class="token comment" spellcheck="true">//需求：把netcat数据源中每个单词写入Kafka</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span><span class="token number">8888</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//计算</span>    val words<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//把单词写入Kafka</span>    words<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FlinkKafkaProducer</span><span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span><span class="token punctuation">,</span><span class="token string">"sowhat"</span><span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">SimpleStringSchema</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"kafkaSink"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>写入到Kafka后可以在终端开一个消费者。</p><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic sowhat1</code></pre><h6 id="2-Kafka作为Sink的第二种-KV"><a href="#2-Kafka作为Sink的第二种-KV" class="headerlink" title="2. Kafka作为Sink的第二种(KV)"></a>2. Kafka作为Sink的第二种(KV)</h6><p><code>需求</code>：把<code>netcat</code>作为数据源，统计每个单词的数量，并且把统计的结果写入Kafka</p><pre class="line-numbers language-java"><code class="language-java"> <span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>sink<span class="token keyword">import</span> java<span class="token punctuation">.</span>lang<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Properties<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span><span class="token punctuation">{</span>FlinkKafkaProducer<span class="token punctuation">,</span> KafkaSerializationSchema<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span>ProducerRecordobject KafkaSinkByKeyValue <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//Kafka作为Sink的第二种（KV）</span>  <span class="token comment" spellcheck="true">//把netcat作为数据源，统计每个单词的数量，并且把统计的结果写入Kafka</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_<span class="token punctuation">;</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//计算</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//创建连接Kafka的属性</span>    var props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    props<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//创建一个Kafka的sink</span>    var kafkaSink <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FlinkKafkaProducer</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>      <span class="token string">"sowhat"</span><span class="token punctuation">,</span>      <span class="token keyword">new</span> <span class="token class-name">KafkaSerializationSchema</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//自定义的匿名内部类</span>        override def <span class="token function">serialize</span><span class="token punctuation">(</span>element<span class="token operator">:</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> timestamp<span class="token operator">:</span> lang<span class="token punctuation">.</span>Long<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">{</span>          <span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token punctuation">(</span><span class="token string">"sowhat"</span><span class="token punctuation">,</span> element<span class="token punctuation">.</span>_1<span class="token punctuation">.</span>getBytes<span class="token punctuation">,</span> <span class="token punctuation">(</span>element<span class="token punctuation">.</span>_2 <span class="token operator">+</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getBytes<span class="token punctuation">)</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span><span class="token punctuation">,</span>      props<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//连接Kafka的数学</span>      FlinkKafkaProducer<span class="token punctuation">.</span>Semantic<span class="token punctuation">.</span>EXACTLY_ONCE <span class="token comment" spellcheck="true">//精确一次</span>    <span class="token punctuation">)</span>    result<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span>kafkaSink<span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"kafka的sink的第二种"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//--property print.key=true Kafka的命令加一个参数</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>生成写入KV后可以定义消费者：</p><pre class="line-numbers language-java"><code class="language-java">bin<span class="token operator">/</span>kafka<span class="token operator">-</span>console<span class="token operator">-</span>consumer<span class="token punctuation">.</span>sh <span class="token operator">--</span>bootstrap<span class="token operator">-</span>server hadoop102<span class="token operator">:</span><span class="token number">9092</span> <span class="token operator">--</span>from<span class="token operator">-</span>beginning \ <span class="token operator">--</span>topic sowhat <span class="token operator">--</span>property print<span class="token punctuation">.</span>key<span class="token operator">=</span><span class="token boolean">true</span>  Kafka的命令加一个参数<span class="token number">1234</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="4-基于HBase的Sink"><a href="#4-基于HBase的Sink" class="headerlink" title="4. 基于HBase的Sink"></a>4. 基于HBase的Sink</h5><p>引入依赖：</p><pre class="line-numbers language-xml"><code class="language-xml">        <span class="token comment" spellcheck="true">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-hbase --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-hbase_2.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.10.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>123456<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码：</p><pre class="line-numbers language-java"><code class="language-java">packge com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>demo<span class="token keyword">import</span> java<span class="token punctuation">.</span>text<span class="token punctuation">.</span>SimpleDateFormat<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Date<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>configuration<span class="token punctuation">.</span>Configuration<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span><span class="token punctuation">{</span>RichSinkFunction<span class="token punctuation">,</span> SinkFunction<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span><span class="token punctuation">{</span>HBaseConfiguration<span class="token punctuation">,</span> HConstants<span class="token punctuation">,</span> TableName<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>client<span class="token punctuation">.</span>_<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Bytes<span class="token keyword">class</span> <span class="token class-name">HBaseWriter</span> <span class="token keyword">extends</span> <span class="token class-name">RichSinkFunction</span><span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token punctuation">{</span>  var conn<span class="token operator">:</span> Connection <span class="token operator">=</span> null  val scan<span class="token operator">:</span> Scan <span class="token operator">=</span> null  var mutator<span class="token operator">:</span> BufferedMutator <span class="token operator">=</span> null  var count<span class="token operator">:</span>Int <span class="token operator">=</span> <span class="token number">0</span>  override def <span class="token function">open</span><span class="token punctuation">(</span>parameters<span class="token operator">:</span> Configuration<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val config<span class="token operator">:</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration <span class="token operator">=</span> HBaseConfiguration<span class="token punctuation">.</span>create    config<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>HConstants<span class="token punctuation">.</span>ZOOKEEPER_QUORUM<span class="token punctuation">,</span> <span class="token string">"IP1,IP2,IP3"</span><span class="token punctuation">)</span>    config<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>HConstants<span class="token punctuation">.</span>ZOOKEEPER_CLIENT_PORT<span class="token punctuation">,</span> <span class="token string">"2181"</span><span class="token punctuation">)</span>    config<span class="token punctuation">.</span><span class="token function">setInt</span><span class="token punctuation">(</span>HConstants<span class="token punctuation">.</span>HBASE_CLIENT_OPERATION_TIMEOUT<span class="token punctuation">,</span> <span class="token number">30000</span><span class="token punctuation">)</span>    config<span class="token punctuation">.</span><span class="token function">setInt</span><span class="token punctuation">(</span>HConstants<span class="token punctuation">.</span>HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD<span class="token punctuation">,</span> <span class="token number">30000</span><span class="token punctuation">)</span>    conn <span class="token operator">=</span> ConnectionFactory<span class="token punctuation">.</span><span class="token function">createConnection</span><span class="token punctuation">(</span>config<span class="token punctuation">)</span>    val tableName<span class="token operator">:</span> TableName <span class="token operator">=</span> TableName<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span><span class="token string">"sowhat"</span><span class="token punctuation">)</span>    val params<span class="token operator">:</span> BufferedMutatorParams <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">BufferedMutatorParams</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//设置缓存1m，当达到1m时数据会自动刷到hbase</span>    params<span class="token punctuation">.</span><span class="token function">writeBufferSize</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>    mutator <span class="token operator">=</span> conn<span class="token punctuation">.</span><span class="token function">getBufferedMutator</span><span class="token punctuation">(</span>params<span class="token punctuation">)</span>    count <span class="token operator">=</span> <span class="token number">0</span>  <span class="token punctuation">}</span>  override def <span class="token function">invoke</span><span class="token punctuation">(</span>value<span class="token operator">:</span> String<span class="token punctuation">,</span> context<span class="token operator">:</span> SinkFunction<span class="token punctuation">.</span>Context<span class="token punctuation">[</span>_<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val cf1 <span class="token operator">=</span> <span class="token string">"m"</span>    val value1 <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>    val put<span class="token operator">:</span> Put <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Put</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span><span class="token string">"rk"</span> <span class="token operator">+</span> value1<span class="token punctuation">)</span><span class="token punctuation">)</span>    put<span class="token punctuation">.</span><span class="token function">addColumn</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>cf1<span class="token punctuation">)</span><span class="token punctuation">,</span> Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span><span class="token string">"time"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span><span class="token string">"v"</span> <span class="token operator">+</span> value1<span class="token punctuation">)</span><span class="token punctuation">)</span>    mutator<span class="token punctuation">.</span><span class="token function">mutate</span><span class="token punctuation">(</span>put<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//每满2000条刷新一下数据</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>count <span class="token operator">>=</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      mutator<span class="token punctuation">.</span><span class="token function">flush</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      count <span class="token operator">=</span> <span class="token number">0</span>    <span class="token punctuation">}</span>    count <span class="token operator">=</span> count <span class="token operator">+</span> <span class="token number">1</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">/**    * 关闭    */</span>  override def <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>conn <span class="token operator">!=</span> null<span class="token punctuation">)</span> conn<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">--</span><span class="token operator">-</span><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>demo<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Properties<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>serialization<span class="token punctuation">.</span>SimpleStringSchema<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>connectors<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>FlinkKafkaConsumer011object HbaseRw <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"10.100.34.111:9092,10.100.34.133:9092"</span><span class="token punctuation">)</span>    properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"group.id"</span><span class="token punctuation">,</span> <span class="token string">"timer.hbase"</span><span class="token punctuation">)</span>    val env<span class="token operator">:</span>StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FlinkKafkaConsumer011</span><span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"sowhat"</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">SimpleStringSchema</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> properties<span class="token punctuation">)</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">HBaseWriter</span><span class="token punctuation">)</span>    env<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"hbase write"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="5-自定义-的-Sink"><a href="#5-自定义-的-Sink" class="headerlink" title="5. 自定义 的 Sink"></a>5. 自定义 的 Sink</h5><p>当然你可以自己定义 Sink，有两种实现方式：<br>1、实现 <code>SinkFunction</code>接口。<br>2、实现 <code>RichSinkFunction</code>类。后者增加了生命周期的管理功能。比如需要在 Sink 初始化的时候创 建连接对象，则最好使用第二种。<br><code>需求</code>：随机生成StationLog对象，写入MySQL数据库的表<code>t_station_log</code>中<br>引入依赖：</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>mysql<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>mysql-connector-java<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>5.1.44<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>12345<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码如下：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>sink<span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span><span class="token punctuation">{</span>Connection<span class="token punctuation">,</span> DriverManager<span class="token punctuation">,</span> PreparedStatement<span class="token punctuation">}</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span><span class="token punctuation">{</span>MyCustomerSource<span class="token punctuation">,</span> StationLog<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>configuration<span class="token punctuation">.</span>Configuration<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span><span class="token punctuation">{</span>RichSinkFunction<span class="token punctuation">,</span> SinkFunction<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">case</span> <span class="token keyword">class</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span>sid<span class="token operator">:</span> String<span class="token punctuation">,</span> var callOut<span class="token operator">:</span> String<span class="token punctuation">,</span> var callInt<span class="token operator">:</span> String<span class="token punctuation">,</span> callType<span class="token operator">:</span> String<span class="token punctuation">,</span> callTime<span class="token operator">:</span> Long<span class="token punctuation">,</span> duration<span class="token operator">:</span> Long<span class="token punctuation">)</span>object CustomerJdbcSink <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//需求：随机生成StationLog对象，写入Mysql数据库的表（t_station_log）中</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyCustomerSource</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//数据写入Mysql，所有需要创建一个自定义的sink</span>    stream<span class="token punctuation">.</span><span class="token function">addSink</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyCustomerJdbcSink</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"jdbcSink"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">/**   * 自定义的Sink类   */</span>  <span class="token keyword">class</span> <span class="token class-name">MyCustomerJdbcSink</span> <span class="token keyword">extends</span> <span class="token class-name">RichSinkFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">{</span>    var conn <span class="token operator">:</span>Connection<span class="token operator">=</span>_    var pst <span class="token operator">:</span>PreparedStatement<span class="token operator">=</span>_    <span class="token comment" spellcheck="true">//把StationLog对象写入Mysql表中，每写入一条执行一次</span>    override def <span class="token function">invoke</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> context<span class="token operator">:</span> SinkFunction<span class="token punctuation">.</span>Context<span class="token punctuation">[</span>_<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      pst<span class="token punctuation">.</span><span class="token function">setString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>value<span class="token punctuation">.</span>sid<span class="token punctuation">)</span>      pst<span class="token punctuation">.</span><span class="token function">setString</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>value<span class="token punctuation">.</span>callOut<span class="token punctuation">)</span>      pst<span class="token punctuation">.</span><span class="token function">setString</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>value<span class="token punctuation">.</span>callInt<span class="token punctuation">)</span>      pst<span class="token punctuation">.</span><span class="token function">setString</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span>value<span class="token punctuation">.</span>callType<span class="token punctuation">)</span>      pst<span class="token punctuation">.</span><span class="token function">setLong</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span>value<span class="token punctuation">.</span>callTime<span class="token punctuation">)</span>      pst<span class="token punctuation">.</span><span class="token function">setLong</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span>value<span class="token punctuation">.</span>duration<span class="token punctuation">)</span>      pst<span class="token punctuation">.</span><span class="token function">executeUpdate</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//Sink初始化的时候调用一次，一个并行度初始化一次</span>    <span class="token comment" spellcheck="true">//创建连接对象，和Statement对象</span>    override def <span class="token function">open</span><span class="token punctuation">(</span>parameters<span class="token operator">:</span> Configuration<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      conn <span class="token operator">=</span>DriverManager<span class="token punctuation">.</span><span class="token function">getConnection</span><span class="token punctuation">(</span><span class="token string">"jdbc:mysql://localhost/test"</span><span class="token punctuation">,</span><span class="token string">"root"</span><span class="token punctuation">,</span><span class="token string">"123123"</span><span class="token punctuation">)</span>      pst <span class="token operator">=</span>conn<span class="token punctuation">.</span><span class="token function">prepareStatement</span><span class="token punctuation">(</span><span class="token string">"insert into t_station_log (sid,call_out,call_in,call_type,call_time,duration) values (?,?,?,?,?,?)"</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    override def <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      pst<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      conn<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="5-DataStream转换算子"><a href="#5-DataStream转换算子" class="headerlink" title="5. DataStream转换算子"></a>5. DataStream转换算子</h1><p>此时再将中间的转换算子<code>Transformation</code>，即<strong>通过从一个或多个 DataStream 生成新的 DataStream 的过程被称为 Transformation 操作</strong>。在转换过程中，每种操作类型被定义为不同的 <strong>Operator</strong>，Flink 程序能够将多个 <strong>Transformation</strong> 组成一个 <strong>DataFlow</strong> 的拓扑。</p><h3 id="1-Map-DataStream-gt-DataStream"><a href="#1-Map-DataStream-gt-DataStream" class="headerlink" title="1. Map [DataStream->DataStream]"></a>1. Map [DataStream-&gt;DataStream]</h3><p>调 用 用 户 定 义 的 MapFunction 对 DataStream[T] 数 据 进 行 处 理 ， 形 成 新 的 DataStream[T]，其中数据格式可能会发生变化，常用作对数据集内数据的<code>清洗</code>和<code>转换</code>。例如将输入数据集中的每个数值全部加 1 处理，并且将数据输出到下游数据集。<br><img src="https://img-blog.csdnimg.cn/20200714144116504.png#pic_center" alt="在这里插入图片描述"></p><h3 id="2-FlatMap-DataStream-gt-DataStream"><a href="#2-FlatMap-DataStream-gt-DataStream" class="headerlink" title="2. FlatMap [DataStream->DataStream]"></a>2. FlatMap [DataStream-&gt;DataStream]</h3><p>该算子主要应用处理输入一个元素产生一个或者多个元素的计算场景，比较常见的是在 经典例子 WordCount 中，将每一行的文本数据切割，生成单词序列如在图所示，对于输入 DataStream[String]通过 FlatMap 函数进行处理，字符串数字按逗号切割，然后形成新的整 数数据集。</p><pre class="line-numbers language-java"><code class="language-java"> val resultStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> dataStream<span class="token punctuation">.</span>flatMap <span class="token punctuation">{</span> str <span class="token operator">=</span><span class="token operator">></span> str<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/2020071415253321.png#pic_center" alt="在这里插入图片描述"></p><h3 id="3-Filter-DataStream-gt-DataStream"><a href="#3-Filter-DataStream-gt-DataStream" class="headerlink" title="3. Filter [DataStream->DataStream]"></a>3. Filter [DataStream-&gt;DataStream]</h3><p>该算子将按照条件对输入数据集进行筛选操作，将符合条件(过滤表达式=true)的数据集输出，将不符合条件的数据过滤掉。如下图所示将输入数据集中偶数过滤出来，奇数从数据集中去除。</p><pre class="line-numbers language-java"><code class="language-java">val filter<span class="token operator">:</span>DataStream<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> dataStream<span class="token punctuation">.</span>filter <span class="token punctuation">{</span> _ <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token punctuation">}</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/20200714153037535.png#pic_center" alt="在这里插入图片描述"></p><h3 id="4-KeyBy-DataStream-gt-KeyedStream"><a href="#4-KeyBy-DataStream-gt-KeyedStream" class="headerlink" title="4. KeyBy [DataStream->KeyedStream]"></a>4. KeyBy [DataStream-&gt;KeyedStream]</h3><p>该算子根据指定的 Key 将输入的 DataStream[T]数据格式转换为 KeyedStream[T]，也就是在数据集中执行 Partition 操作，将相同的 Key 值的数据放置在相同的分区中。<br>默认是根据注定数据的<code>hashcode</code>来分的。</p><pre class="line-numbers language-java"><code class="language-java">    val test<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"2"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"2"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    val value<span class="token operator">:</span> KeyedStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> String<span class="token punctuation">]</span> <span class="token operator">=</span> test<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true">/**      * （String,Int)   => 是进行keyBy的数据类型      *   String        =>  是分流的key的数据类型      */</span><span class="token operator">--</span><span class="token operator">-</span>    val test<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"2"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"2"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    val value<span class="token operator">:</span> KeyedStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> Tuple<span class="token punctuation">]</span> <span class="token operator">=</span> test<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">/**      * （String,Int)   => 是进行keyBy的数据类型      *  Tuple        =>  是分流的key的数据类型      */</span><span class="token number">12345678910111213</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-Reduce-KeyedStream-gt-DataStream"><a href="#5-Reduce-KeyedStream-gt-DataStream" class="headerlink" title="5. Reduce [KeyedStream->DataStream]"></a>5. Reduce [KeyedStream-&gt;DataStream]</h3><p>该算子和 MapReduce 中 Reduce 原理基本一致，主要目的是将输入的<code>KeyedStream</code>通过 传 入 的 用 户 自 定 义 的 <code>ReduceFunction</code>滚 动 地 进 行 数 据 聚 合 处 理 ， 其 中 定 义 的 ReduceFunciton 必须满足运算<code>结合律</code>和<code>交换律</code>。如下代码对传入 keyedStream 数据集中相同的 key 值的数据独立进行求和运算，得到每个 key 所对应的求和值。</p><pre class="line-numbers language-java"><code class="language-java">    val test<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    val value<span class="token operator">:</span> KeyedStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> Tuple<span class="token punctuation">]</span> <span class="token operator">=</span> test<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 滚动对第二个字段进行reduce相加求和</span>    val reduceStream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> value<span class="token punctuation">.</span>reduce <span class="token punctuation">{</span> <span class="token punctuation">(</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span>t1<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>_2 <span class="token operator">+</span> t2<span class="token punctuation">.</span>_2<span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token number">1234</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre><code>2&gt; (c,2)3&gt; (a,3)3&gt; (d,4)2&gt; (c,7)3&gt; (a,8)12345</code></pre><p><code>PS</code>：<strong>对于该结果需要说明下为什么key相同的出现了多次，这主要是Flink流式处理思想的体现，迭代式的输出结果</strong>。</p><h3 id="6-Aggregations-KeyedStream-gt-DataStream"><a href="#6-Aggregations-KeyedStream-gt-DataStream" class="headerlink" title="6. Aggregations[KeyedStream->DataStream]"></a>6. Aggregations[KeyedStream-&gt;DataStream]</h3><p>Aggregations 是 KeyedDataStream 接口提供的聚合算子，根据指定的字段进行聚合操 作，滚动地产生一系列数据聚合结果。<code>其实是将 Reduce 算子中的函数进行了封装</code>，封装的 聚合操作有 sum、min、minBy、max、maxBy等，这样就不需要用户自己定义 Reduce 函数。 如下代码所示，指定数据集中第一个字段作为 key，用第二个字段作为累加字段，然后<code>滚动</code>地对第二个字段的数值进行累加并输出</p><pre class="line-numbers language-java"><code class="language-java">    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    val test<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    val value<span class="token operator">:</span> KeyedStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> Tuple<span class="token punctuation">]</span> <span class="token operator">=</span> test<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 滚动对第二个字段进行reduce相加求和</span>    val reduceStream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> value<span class="token punctuation">.</span>reduce <span class="token punctuation">{</span> <span class="token punctuation">(</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span>t1<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>_2 <span class="token operator">+</span> t2<span class="token punctuation">.</span>_2<span class="token punctuation">)</span> <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 相当于reduce更简化版的 聚合</span>    val sumStream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token number">1234567</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre><code>(a,3)(d,4)(c,2)(c,7)(a,8)12345</code></pre><h3 id="7-Union-DataStream-gt-DataStream"><a href="#7-Union-DataStream-gt-DataStream" class="headerlink" title="7. Union[DataStream ->DataStream]"></a>7. Union[DataStream -&gt;DataStream]</h3><p>Union 算子主要是将两个或者多个输入的数据集合并成一个数据集，需要保证两个数据 集的<code>格式一致</code>，输出的数据集的格式和输入的数据集格式保持一致。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject TestUnion <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    var stream1 <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    var stream2 <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    var stream3 <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"e"</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"f"</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream1<span class="token punctuation">.</span><span class="token function">union</span><span class="token punctuation">(</span>stream2<span class="token punctuation">,</span> stream3<span class="token punctuation">)</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre><code>(a,1)(b,2)(e,7)(f,8)(b,5)(d,6)123456</code></pre><h3 id="8-Connect、CoMap、CoFlatMap-DataStream-gt-ConnectedStream-gt-DataStream"><a href="#8-Connect、CoMap、CoFlatMap-DataStream-gt-ConnectedStream-gt-DataStream" class="headerlink" title="8. Connect、CoMap、CoFlatMap[DataStream ->ConnectedStream->DataStream]"></a>8. Connect、CoMap、CoFlatMap[DataStream -&gt;ConnectedStream-&gt;DataStream]</h3><p>Connect 算子主要是为了<code>合并</code>两种或者<code>多种不同数据类型</code>的数据集，<strong>合并后会保留原来 数据集的数据类型</strong>。<br>例如：dataStream1 数据集为(String, Int)元祖类型，dataStream2 数据集为 Int 类型，通过 connect 连接算子将两个不同数据类型的流结合在一起，形成格式 为 ConnectedStreams 的数据集，其内部数据为<code>[(String, Int), Int]</code>的混合数据类型，保留了两个原始数据集的数据类型。</p><p>需要注意的是，对于 ConnectedStreams 类型的数据集<code>不能</code>直接进行类似 Print()的操 作，需要再转换成 DataStream 类型数据集，在 Flink 中 ConnectedStreams 提供的 <code>map()</code>方 法和<code>flatMap()</code></p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject TestConnect <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    val stream1<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    val stream2<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">fromElements</span><span class="token punctuation">(</span><span class="token string">"e"</span><span class="token punctuation">,</span> <span class="token string">"f"</span><span class="token punctuation">,</span> <span class="token string">"g"</span><span class="token punctuation">)</span>    val stream3<span class="token operator">:</span> ConnectedStreams<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> String<span class="token punctuation">]</span> <span class="token operator">=</span> stream1<span class="token punctuation">.</span><span class="token function">connect</span><span class="token punctuation">(</span>stream2<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//注意得到ConnectedStreams，实际上里面的数据没有真正合并</span>    <span class="token comment" spellcheck="true">//使用CoMap,或者CoFlatmap</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream3<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>      <span class="token comment" spellcheck="true">//第一个处理的函数</span>      t <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        <span class="token punctuation">(</span>t<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t<span class="token punctuation">.</span>_2<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">,</span>      <span class="token comment" spellcheck="true">//第二个处理的函数</span>      t <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        <span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">)</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">123456789101112131415161718192021222324252627</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre><code>(e,0)(f,0)(g,0)(a,1)(b,2)(c,3)123456</code></pre><p><code>注意</code>：</p><ul><li>Union 之前两个流的类型<code>必须是一样</code>，Connect <code>可以不一样</code>，在之后的 coMap 中再去调 整成为一样的。</li><li>Connect <code>只能</code>操作两个流，Union <code>可以</code>操作多个。</li></ul><h3 id="9-Split-和-select-DataStream-gt-SplitStream-gt-DataStream"><a href="#9-Split-和-select-DataStream-gt-SplitStream-gt-DataStream" class="headerlink" title="9. Split 和 select [DataStream->SplitStream->DataStream]"></a>9. Split 和 select [DataStream-&gt;SplitStream-&gt;DataStream]</h3><p>Split 算子是将一个 DataStream 数据集<code>按照条件进行拆分</code>，形成两个数据集的过程， 也是 union 算子的逆向实现。每个接入的数据都会被<code>路由</code>到一个或者多个输出数据集中。<a href="https://blog.csdn.net/duxu24/article/details/105910476" target="_blank" rel="noopener">Side Output</a></p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span><span class="token punctuation">{</span>MyCustomerSource<span class="token punctuation">,</span> StationLog<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>ProcessFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Collectorobject TestSplitAndSelect <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//需求：从自定义的数据源中读取基站通话日志，把通话成功的和通话失败的分离出来</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">addSource</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyCustomerSource</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// this needs to be an anonymous inner class, so that we can analyze the type</span>    val successTag <span class="token operator">=</span> OutputTag<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span>    val nosuccessTag <span class="token operator">=</span> OutputTag<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"nosuccess"</span><span class="token punctuation">)</span>    val sideoutputStream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProcessFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> StationLog<span class="token punctuation">]</span> <span class="token punctuation">{</span>      override def <span class="token function">processElement</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> ctx<span class="token operator">:</span> ProcessFunction<span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> StationLog<span class="token punctuation">]</span>#Context<span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>value<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>          ctx<span class="token punctuation">.</span><span class="token function">output</span><span class="token punctuation">(</span>successTag<span class="token punctuation">,</span> value<span class="token punctuation">)</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span> <span class="token punctuation">{</span>          ctx<span class="token punctuation">.</span><span class="token function">output</span><span class="token punctuation">(</span>nosuccessTag<span class="token punctuation">,</span> value<span class="token punctuation">)</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span>    sideoutputStream<span class="token punctuation">.</span><span class="token function">getSideOutput</span><span class="token punctuation">(</span>successTag<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"成功数据"</span><span class="token punctuation">)</span>    sideoutputStream<span class="token punctuation">.</span><span class="token function">getSideOutput</span><span class="token punctuation">(</span>nosuccessTag<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"未成功数据"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//切割</span>    val splitStream<span class="token operator">:</span> SplitStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span> <span class="token comment" spellcheck="true">//流并没有真正切割</span>      log <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>log<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token function">Seq</span><span class="token punctuation">(</span><span class="token string">"Success"</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>          <span class="token function">Seq</span><span class="token punctuation">(</span><span class="token string">"NOSuccess"</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//选择不同的流  根据标签得到不同流</span>    val stream1<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> splitStream<span class="token punctuation">.</span><span class="token function">select</span><span class="token punctuation">(</span><span class="token string">"Success"</span><span class="token punctuation">)</span>    val stream2<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> splitStream<span class="token punctuation">.</span><span class="token function">select</span><span class="token punctuation">(</span><span class="token string">"NOSuccess"</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"原始数据"</span><span class="token punctuation">)</span>    stream1<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"通话成功"</span><span class="token punctuation">)</span>    stream2<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"通话不成功"</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="函数类和富函数类"><a href="#函数类和富函数类" class="headerlink" title="函数类和富函数类"></a>函数类和富函数类</h2><p><strong>前面学过的所有算子几乎都可以自定义一个函数类、富函数类作为参数</strong>。因为 Flink 暴露者两种函数类的接口，常见的函数接口有：</p><ul><li>MapFunction</li><li>FlatMapFunction</li><li>ReduceFunction</li><li>。。。。。</li></ul><p><code>富函数接口</code>它其他常规函数接口的不同在于：<code>可以获取运行环境的上下文，在上下文环境中可以管理状态</code>，并拥有一些生命周期方法，所以可以实现更复杂的功能。富函数的接口有：</p><ul><li>RichMapFunction</li><li>RichFlatMapFunction</li><li>RichFilterFunction</li><li>RichSinkFunction</li></ul><h5 id="1-普通函数类型"><a href="#1-普通函数类型" class="headerlink" title="1. 普通函数类型"></a>1. 普通函数类型</h5><p>普通函数类举例：按照指定的时间格式输出每个通话的拨号时间和结束时间。resources目录下station.log文件内容如下：</p><pre><code>station_0,18600003612,18900004575,barring,1577080453123,0station_9,18600003186,18900002113,success,1577080453123,32station_3,18600003794,18900009608,success,1577080453123,4station_1,18600000005,18900007729,fail,1577080453123,0station_1,18600000005,18900007729,success,1577080603123,349station_8,18600007461,18900006987,barring,1577080453123,0station_5,18600009356,18900006066,busy,1577080455129,0station_4,18600001941,18900003949,busy,1577080455129,012345678</code></pre><p>代码如下：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>transformation<span class="token keyword">import</span> java<span class="token punctuation">.</span>net<span class="token punctuation">.</span>URLDecoder<span class="token keyword">import</span> java<span class="token punctuation">.</span>text<span class="token punctuation">.</span>SimpleDateFormat<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Date<span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span>StationLog<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>MapFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject TestFunctionClass <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//计算出每个通话成功的日志中呼叫起始和结束时间,并且按照指定的时间格式</span>  <span class="token comment" spellcheck="true">//数据源来自本地文件</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    var filePath <span class="token operator">=</span> getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/station.log"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getPath    filePath <span class="token operator">=</span> URLDecoder<span class="token punctuation">.</span><span class="token function">decode</span><span class="token punctuation">(</span>filePath<span class="token punctuation">,</span> <span class="token string">"utf-8"</span><span class="token punctuation">)</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>filePath<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        var arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//定义一个时间格式</span>    val format <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SimpleDateFormat</span><span class="token punctuation">(</span><span class="token string">"yyyy-MM-dd HH:mm:ss"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//计算通话成功的起始和结束时间</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyMapFunction</span><span class="token punctuation">(</span>format<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//result.print()</span>    val result1<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map <span class="token punctuation">{</span>      x <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        val startTime <span class="token operator">=</span> x<span class="token punctuation">.</span>callTime        val endTime <span class="token operator">=</span> startTime <span class="token operator">+</span> x<span class="token punctuation">.</span>duration <span class="token operator">*</span> <span class="token number">1000</span>        <span class="token string">"主叫号码："</span> <span class="token operator">+</span> x<span class="token punctuation">.</span>callOut <span class="token operator">+</span> <span class="token string">",被叫号码:"</span> <span class="token operator">+</span> x<span class="token punctuation">.</span>callInt <span class="token operator">+</span> <span class="token string">",呼叫起始时间:"</span> <span class="token operator">+</span> format<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span>startTime<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">",呼叫结束时间:"</span> <span class="token operator">+</span> format<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span>endTime<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    result1<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//自定义一个函数类  指定输入 跟输出类型</span>  <span class="token keyword">class</span> <span class="token class-name">MyMapFunction</span><span class="token punctuation">(</span>format<span class="token operator">:</span> SimpleDateFormat<span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">MapFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> String<span class="token punctuation">]</span> <span class="token punctuation">{</span>    override def <span class="token function">map</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">)</span><span class="token operator">:</span> String <span class="token operator">=</span> <span class="token punctuation">{</span>      val startTime <span class="token operator">=</span> value<span class="token punctuation">.</span>callTime      val endTime <span class="token operator">=</span> startTime <span class="token operator">+</span> value<span class="token punctuation">.</span>duration <span class="token operator">*</span> <span class="token number">1000</span>      <span class="token string">"主叫号码："</span> <span class="token operator">+</span> value<span class="token punctuation">.</span>callOut <span class="token operator">+</span> <span class="token string">",被叫号码:"</span> <span class="token operator">+</span> value<span class="token punctuation">.</span>callInt <span class="token operator">+</span> <span class="token string">",呼叫起始时间:"</span> <span class="token operator">+</span> format<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span>startTime<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">",呼叫结束时间:"</span> <span class="token operator">+</span> format<span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span>endTime<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-富函数类型"><a href="#2-富函数类型" class="headerlink" title="2. 富函数类型"></a>2. 富函数类型</h5><p><code>富函数类举例</code>：把呼叫成功的通话信息转化成真实的用户姓名，通话用户对应的用户表 （在 Mysql 数据中）<br>由于需要从数据库中查询数据，就需要创建连接，创建连接的代码必须写在生命周期的 open 方法中。所以需要使用富函数类。<code>Rich Function</code> 有一个生命周期的概念。典型的生命周期方法有：</p><ul><li>open()方法是 rich function 的<code>初始化</code>方法，当一个算子例如 map 或者 filter 被调用 之前 open()会被调用。</li><li>close()方法是生命周期中的最后一个调用的方法，做一些<code>清理工作</code>。</li><li>getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函数执行的 并行度，任务的名字，以及 state 状态</li></ul><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>transformation<span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span><span class="token punctuation">{</span>Connection<span class="token punctuation">,</span> DriverManager<span class="token punctuation">,</span> PreparedStatement<span class="token punctuation">,</span> ResultSet<span class="token punctuation">}</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span>StationLog<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>RichMapFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>configuration<span class="token punctuation">.</span>Configuration<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject TestRichFunctionClass <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">/**    * 把通话成功的电话号码转换成真是用户姓名，用户姓名保存在Mysql表中    * @param args    */</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    var filePath <span class="token operator">=</span> getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/station.log"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getPath    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>filePath<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        var arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//计算：把电话号码变成用户姓名</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyRichMapFunction</span><span class="token punctuation">)</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//自定义一个富函数类</span>  <span class="token keyword">class</span> <span class="token class-name">MyRichMapFunction</span> <span class="token keyword">extends</span> <span class="token class-name">RichMapFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> StationLog<span class="token punctuation">]</span> <span class="token punctuation">{</span>    var conn<span class="token operator">:</span> Connection <span class="token operator">=</span> _    var pst<span class="token operator">:</span> PreparedStatement <span class="token operator">=</span> _    override def <span class="token function">open</span><span class="token punctuation">(</span>parameters<span class="token operator">:</span> Configuration<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      conn <span class="token operator">=</span> DriverManager<span class="token punctuation">.</span><span class="token function">getConnection</span><span class="token punctuation">(</span><span class="token string">"jdbc:mysql://localhost/test"</span><span class="token punctuation">,</span> <span class="token string">"root"</span><span class="token punctuation">,</span> <span class="token string">"123123"</span><span class="token punctuation">)</span>      pst <span class="token operator">=</span> conn<span class="token punctuation">.</span><span class="token function">prepareStatement</span><span class="token punctuation">(</span><span class="token string">"select name from t_phone where phone_number=?"</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    override def <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      pst<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      conn<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    override def <span class="token function">map</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">)</span><span class="token operator">:</span> StationLog <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">// 获取上下文信息 比如获取子线程</span>      <span class="token function">println</span><span class="token punctuation">(</span>getRuntimeContext<span class="token punctuation">.</span>getTaskNameWithSubtasks<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//查询主叫号码对应的姓名</span>      pst<span class="token punctuation">.</span><span class="token function">setString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> value<span class="token punctuation">.</span>callOut<span class="token punctuation">)</span>      val result<span class="token operator">:</span> ResultSet <span class="token operator">=</span> pst<span class="token punctuation">.</span><span class="token function">executeQuery</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>result<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        value<span class="token punctuation">.</span>callOut <span class="token operator">=</span> result<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">//查询被叫号码对应的姓名</span>      pst<span class="token punctuation">.</span><span class="token function">setString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> value<span class="token punctuation">.</span>callInt<span class="token punctuation">)</span>      val result2<span class="token operator">:</span> ResultSet <span class="token operator">=</span> pst<span class="token punctuation">.</span><span class="token function">executeQuery</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>result2<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        value<span class="token punctuation">.</span>callInt <span class="token operator">=</span> result2<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>      value    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="3-底层-ProcessFunctionAPI"><a href="#3-底层-ProcessFunctionAPI" class="headerlink" title="3. 底层 ProcessFunctionAPI"></a>3. 底层 ProcessFunctionAPI</h5><p>ProcessFunction 是一个低层次的流处理操作，允许返回所有 Stream 的基础构建模块，可以说是Flink的<code>杀手锏</code>了。</p><ul><li>访问 Event 本身数据（比如：Event 的时间，Event 的当前 Key 等）</li><li>管理状态 State（仅在 Keyed Stream 中）</li><li>管理定时器 Timer（包括：注册定时器，删除定时器等） 总而言之，ProcessFunction 是 Flink 最底层的 API，也是功能最强大的。</li></ul><p><code>需求</code>：监控每一个手机，如果在 5 秒内呼叫它的通话都是失败的，发出警告信息。<br><code>注意</code>： 本demo中会用到状态编程，只要知道状态的意思，不需要掌握。<a href="https://sowhat.blog.csdn.net/article/details/107379410" target="_blank" rel="noopener">后面的文章</a>中会详细讲解 State 编程。</p><pre class="line-numbers language-java"><code class="language-java">  <span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>transformation<span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span>StationLog<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>state<span class="token punctuation">.</span><span class="token punctuation">{</span>ValueState<span class="token punctuation">,</span> ValueStateDescriptor<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>KeyedProcessFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Collector<span class="token comment" spellcheck="true">/**  * 监控每一个手机号码，如果这个号码在5秒内，所有呼叫它的日志都是失败的，则发出告警信息  * 如果在5秒内只要有一个呼叫不是fail则不用告警  */</span><span class="token comment" spellcheck="true">/**  * 基站日志  * @param sid      基站的id  * @param callOut  主叫号码  * @param callInt  被叫号码  * @param callType 呼叫类型  * @param callTime 呼叫时间 (毫秒)  * @param duration 通话时长 （秒）  */</span><span class="token keyword">case</span> <span class="token keyword">class</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span>sid<span class="token operator">:</span> String<span class="token punctuation">,</span> var callOut<span class="token operator">:</span> String<span class="token punctuation">,</span> var callInt<span class="token operator">:</span> String<span class="token punctuation">,</span> callType<span class="token operator">:</span> String<span class="token punctuation">,</span> callTime<span class="token operator">:</span> Long<span class="token punctuation">,</span> duration<span class="token operator">:</span> Long<span class="token punctuation">)</span>object TestProcessFunction <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源 通过 netcat 发送 数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"IP1"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        val arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 按照呼入电话分组</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>callInt<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MonitorCallFail</span><span class="token punctuation">)</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//自定义一个底层的类 第一个是key类型，第二个是处理对象类型，第三个是返回类型</span>  <span class="token keyword">class</span> <span class="token class-name">MonitorCallFail</span> <span class="token keyword">extends</span> <span class="token class-name">KeyedProcessFunction</span><span class="token punctuation">[</span>String<span class="token punctuation">,</span> StationLog<span class="token punctuation">,</span> String<span class="token punctuation">]</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//使用一个状态对象记录时间</span>    lazy val timeState<span class="token operator">:</span> ValueState<span class="token punctuation">[</span>Long<span class="token punctuation">]</span> <span class="token operator">=</span> getRuntimeContext<span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ValueStateDescriptor</span><span class="token punctuation">[</span>Long<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"time"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>Long<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    override def <span class="token function">processElement</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> ctx<span class="token operator">:</span> KeyedProcessFunction<span class="token punctuation">[</span>String<span class="token punctuation">,</span> StationLog<span class="token punctuation">,</span> String<span class="token punctuation">]</span>#Context<span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">//从状态中取得时间</span>      val time<span class="token operator">:</span>Long <span class="token operator">=</span> timeState<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>time <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> value<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"fail"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//表示第一次发现呼叫失败，记录当前的时间</span>        <span class="token comment" spellcheck="true">//获取当前系统时间，并注册定时器</span>        val nowTime<span class="token operator">:</span>Long  <span class="token operator">=</span> ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">currentProcessingTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">//定时器在5秒后触发</span>        val onTime<span class="token operator">:</span>Long  <span class="token operator">=</span> nowTime <span class="token operator">+</span> <span class="token number">5</span> <span class="token operator">*</span> 1000L        ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">registerProcessingTimeTimer</span><span class="token punctuation">(</span>onTime<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">//把触发时间保存到状态中</span>        timeState<span class="token punctuation">.</span><span class="token function">update</span><span class="token punctuation">(</span>onTime<span class="token punctuation">)</span>      <span class="token punctuation">}</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>time <span class="token operator">!=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span>value<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"fail"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//表示有一次成功的呼叫,必须要删除定时器</span>        ctx<span class="token punctuation">.</span><span class="token function">timerService</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">deleteProcessingTimeTimer</span><span class="token punctuation">(</span>time<span class="token punctuation">)</span>        timeState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//清空状态中的时间</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//时间到了，定时器执行,</span>    override def <span class="token function">onTimer</span><span class="token punctuation">(</span>timestamp<span class="token operator">:</span> Long<span class="token punctuation">,</span> ctx<span class="token operator">:</span> KeyedProcessFunction<span class="token punctuation">[</span>String<span class="token punctuation">,</span> StationLog<span class="token punctuation">,</span> String<span class="token punctuation">]</span>#OnTimerContext<span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      val warnStr<span class="token operator">:</span>String  <span class="token operator">=</span> <span class="token string">"触发的时间："</span> <span class="token operator">+</span> timestamp <span class="token operator">+</span> <span class="token string">" 手机号 ："</span> <span class="token operator">+</span> ctx<span class="token punctuation">.</span>getCurrentKey      out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span>warnStr<span class="token punctuation">)</span>      timeState<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="4-侧输出流-Side-Output"><a href="#4-侧输出流-Side-Output" class="headerlink" title="4. 侧输出流 Side Output"></a>4. 侧输出流 Side Output</h5><p>在 Flink 处理数据流时，我们经常会遇到这样的情况：在处理一个数据源时，往往需要<strong>将该源中的不同类型的数据做分割处理</strong>，如果使用 filter 算子对数据源进行筛选分割的话，势必会造成数据流的<code>多次复制</code>，造成不必要的性能浪费；flink 中的<code>侧输出</code>就是将数据 流进行分割，而不对流进行复制的一种分流机制。flink 的侧输出的另一个作用就是<code>对延时迟到</code>的数据进行处理，这样就可以不必丢弃迟到的数据。在后面的文章中会讲到！<br><code>案例</code>：根据基站的日志，请把呼叫成功的 Stream（主流）和不成功的 Stream（侧流） 分别输出。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>transformation<span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span>StationLog<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>ProcessFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Collectorobject TestSideOutputStream <span class="token punctuation">{</span>  <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_  var notSuccessTag<span class="token operator">:</span> OutputTag<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">OutputTag</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"not_success"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//不成功的侧流标签</span>  <span class="token comment" spellcheck="true">//把呼叫成功的日志输出到主流，不成功的到侧流</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//读取数据源</span>    var filePath<span class="token operator">:</span> String <span class="token operator">=</span> getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/station.log"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getPath    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>filePath<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        var arr<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">CreateSideOuputStream</span><span class="token punctuation">(</span>notSuccessTag<span class="token punctuation">)</span><span class="token punctuation">)</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"主流"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//一定要根据主流得到侧流</span>    val sideStream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> result<span class="token punctuation">.</span><span class="token function">getSideOutput</span><span class="token punctuation">(</span>notSuccessTag<span class="token punctuation">)</span>    sideStream<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"侧流"</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token keyword">class</span> <span class="token class-name">CreateSideOuputStream</span><span class="token punctuation">(</span>tag<span class="token operator">:</span> OutputTag<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">extends</span> <span class="token class-name">ProcessFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> StationLog<span class="token punctuation">]</span> <span class="token punctuation">{</span>    override def <span class="token function">processElement</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> ctx<span class="token operator">:</span> ProcessFunction<span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> StationLog<span class="token punctuation">]</span>#Context<span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>value<span class="token punctuation">.</span>callType<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"success"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//输出主流</span>        out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span>      <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">//输出侧流</span>        ctx<span class="token punctuation">.</span><span class="token function">output</span><span class="token punctuation">(</span>tag<span class="token punctuation">,</span> value<span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="5-Flink-State管理跟恢复"><a href="#5-Flink-State管理跟恢复" class="headerlink" title="5. Flink State管理跟恢复"></a>5. Flink State管理跟恢复</h1><p>Flink 是一个<strong>默认就有状态的分析引擎</strong>，前面的 <a href="https://sowhat.blog.csdn.net/article/details/107296787" target="_blank" rel="noopener">WordCount</a> 案例可以做到单词的数量的累加，其实是因为在内存中保证了每个单词的出现的次数，这些数据其实就是状态数据。但是如果一个 Task 在处理过程中挂掉了，那么它在内存中的状态都会丢失，所有的数据都需要重新计算。从容错和消息处理的语义（At -least-once 和 Exactly-once）上来说，Flink 引入了 <code>State</code> 和<code>CheckPoint</code>。</p><ul><li><code>State</code> 一般指一个具体的 Task/Operator 的状态(Task Slot/ 转换算子)，State 数据默认保存在 Java 的<code>堆</code>内存中。</li><li>CheckPoint（可以理解为<code>CheckPoint是把State数据持久化存储了</code>）则表示了一个 Flink Job 在一个特定时刻的一份全局<strong>状态快照</strong>，即包含了所有<code>Task/Operator</code> 的状态。<br><img src="https://img-blog.csdnimg.cn/20200722191330798.png#pic_center" alt="在这里插入图片描述"></li></ul><h3 id="1-常用-State"><a href="#1-常用-State" class="headerlink" title="1. 常用 State"></a>1. 常用 State</h3><p>Flink 有两种常见的 State 类型，分别是:</p><ul><li><code>Keyed State</code>(键控状态)</li><li><code>Operator State</code>(算子状态)</li></ul><h5 id="1-Keyed-State（键控状态）"><a href="#1-Keyed-State（键控状态）" class="headerlink" title="1. Keyed State（键控状态）"></a>1. Keyed State（键控状态）</h5><p>Keyed State：顾名思义就是基于 <code>KeyedStream</code>上的状态，这个状态是跟特定的 Key 绑定的。KeyedStream 流上的每一个 Key，都对应一个 State。Flink 针对 Keyed State 提供了 以下可以保存 State 的数据结构：</p><ol><li><code>ValueState&lt;T&gt;</code>:<br>保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 update(T) 进行更新，通过 T value() 进行检索。</li><li><code>ListState&lt;T&gt;</code>:<br>保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上 进行检索。可以通过 add(T) 或者 addAll(List) 进行添加元素，通过 Iterable <code>get()</code>获得整个列表。还可以通过 <code>update</code>(List) 覆盖当前的列表。</li><li><code>ReducingState&lt;T&gt;</code>:<br>保存一个单值，表示添加到状态的所有值的聚合。接口与 ListState 类似，但使用 add(T) 增加元素，会使用提供的 ReduceFunction 进行聚合。</li><li><code>AggregatingState&lt;IN, OUT&gt;</code>:<br>保留一个单值，表示添加到状态的所有值的聚合。和 ReducingState 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 ListState 类似，但使用 add(IN) 添加的元素会用指定的 AggregateFunction 进行聚 合。</li><li><code>FoldingState&lt;T, ACC&gt;</code>:<br>保留一个单值，表示添加到状态的所有值的聚合。 与 ReducingState 相反，聚合类型可能与添加到状态的元素类型不同。接口与 ListState 类似，但使用 add（T）添加的元素会用指定的 FoldFunction 折叠成聚合值。</li><li><code>MapState&lt;UK, UV&gt;</code>:<br>维护了一个映射列表。 你可以添加键值对到状态中，也可以获得 反映当前所有映射的迭代器。使用 put(UK，UV) 或者 putAll(Map&lt;UK，UV&gt;) 添加映射。 使用 get(UK) 检索特定 key。 使用 entries()，keys() 和 values() 分别检索映射、 键和值的可迭代视图。</li></ol><h5 id="2-Operator-State（算子状态）"><a href="#2-Operator-State（算子状态）" class="headerlink" title="2. Operator State（算子状态）"></a>2. Operator State（算子状态）</h5><p>Operator State 与 Key 无关，而是与<code>Operator</code>绑定，整个 Operator 只对应一个 State。 比如：Flink 中的 Kafka Connector 就使用了 Operator State，它会在每个 Connector 实例 中，保存该实例消费 Topic 的所有(partition, offset)映射。<img src="https://img-blog.csdnimg.cn/20200716120444192.png#pic_center" alt="在这里插入图片描述"></p><h5 id="3-Keyed-State-案例"><a href="#3-Keyed-State-案例" class="headerlink" title="3. Keyed State 案例"></a>3. Keyed State 案例</h5><p><a href="https://sowhat.blog.csdn.net/article/details/107323074" target="_blank" rel="noopener">demo1</a>：监控每一个手机号码，如果这个号码在5秒内，所有呼叫它的日志都是失败的，<br><code>demo2 需求</code>：计算每个手机的呼叫间隔时间，单位是毫秒。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>state<span class="token keyword">import</span> java<span class="token punctuation">.</span>net<span class="token punctuation">.</span><span class="token punctuation">{</span>URL<span class="token punctuation">,</span> URLDecoder<span class="token punctuation">}</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>BatchWordCount<span class="token punctuation">.</span>getClass<span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span>StationLog<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>RichFlatMapFunction<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>common<span class="token punctuation">.</span>state<span class="token punctuation">.</span><span class="token punctuation">{</span>ValueState<span class="token punctuation">,</span> ValueStateDescriptor<span class="token punctuation">}</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>configuration<span class="token punctuation">.</span>Configuration<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Collector<span class="token comment" spellcheck="true">/**  * 基站日志  * @param sid      基站的id  * @param callOut  主叫号码  * @param callInt  被叫号码  * @param callType 呼叫类型  * @param callTime 呼叫时间 (毫秒)  * @param duration 通话时长 （秒）  */</span><span class="token keyword">case</span> <span class="token keyword">class</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span>sid<span class="token operator">:</span> String<span class="token punctuation">,</span> var callOut<span class="token operator">:</span> String<span class="token punctuation">,</span> var callInt<span class="token operator">:</span> String<span class="token punctuation">,</span> callType<span class="token operator">:</span> String<span class="token punctuation">,</span> callTime<span class="token operator">:</span> Long<span class="token punctuation">,</span> duration<span class="token operator">:</span> Long<span class="token punctuation">)</span><span class="token comment" spellcheck="true">/**  * 第一种方法的实现  * 统计每个手机的呼叫时间间隔，单位是毫秒  */</span>object TestKeyedState1 <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    val filePath<span class="token operator">:</span> URL <span class="token operator">=</span> getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/station.log"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//使用相对路径来得到完整的文件路径</span>    val packagePath<span class="token operator">:</span> String <span class="token operator">=</span> filePath<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">replaceAll</span><span class="token punctuation">(</span><span class="token string">"%20"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//解决路径中含有空格的情况</span>    val str<span class="token operator">:</span>String <span class="token operator">=</span> URLDecoder<span class="token punctuation">.</span><span class="token function">decode</span><span class="token punctuation">(</span>packagePath<span class="token punctuation">,</span> <span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//解决路径包含中文的情况</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        val arr<span class="token operator">:</span>Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>callOut<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//分组</span>      <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">CallIntervalFunction</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//输出的是一个二元组（手机号码，时间间隔）</span>  <span class="token keyword">class</span> <span class="token class-name">CallIntervalFunction</span> <span class="token keyword">extends</span> <span class="token class-name">RichFlatMapFunction</span><span class="token punctuation">[</span>StationLog<span class="token punctuation">,</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//定义一个状态，用于保存前一次呼叫的时间</span>    <span class="token keyword">private</span> var preCallTimeState<span class="token operator">:</span> ValueState<span class="token punctuation">[</span>Long<span class="token punctuation">]</span> <span class="token operator">=</span> _    override def <span class="token function">open</span><span class="token punctuation">(</span>parameters<span class="token operator">:</span> Configuration<span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      preCallTimeState <span class="token operator">=</span> getRuntimeContext<span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ValueStateDescriptor</span><span class="token punctuation">[</span>Long<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"pre"</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>Long<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    override def <span class="token function">flatMap</span><span class="token punctuation">(</span>value<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">//从状态中取得前一次呼叫的时间</span>      val preCallTime<span class="token operator">:</span>Long <span class="token operator">=</span> preCallTimeState<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>preCallTime <span class="token operator">==</span> null <span class="token operator">||</span> preCallTime <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//状态中没有，肯定是第一次呼叫</span>        preCallTimeState<span class="token punctuation">.</span><span class="token function">update</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>callTime<span class="token punctuation">)</span>      <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//状态中有数据,则要计算时间间隔</span>        val interval<span class="token operator">:</span>Long <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">abs</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>callTime <span class="token operator">-</span> preCallTime<span class="token punctuation">)</span>        out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span>callOut<span class="token punctuation">,</span> interval<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果：</p><pre><code>4&gt; (18600003532,7000)2&gt; (18600003713,0)1&gt; (18600003502,9000)1&gt; (18600003502,0)1&gt; (18600003502,9000)1&gt; (18600007699,0)1&gt; (18600000005,150000)1234567</code></pre><p>stationlog.txt文件信息如下：</p><pre><code>station_1,18600000005,18900007729,fail,1577080453123,0station_1,18600000005,18900007729,success,1577080603123,349station_8,18600007461,18900006987,barring,1577080453123,0station_5,18600009356,18900006066,busy,1577080455129,0station_4,18600001941,18900003949,busy,1577080455129,0...自己造数据即可123456</code></pre><p>还有第二种简单的方法：调用<code>flatMapWithState</code> 算子</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>state<span class="token keyword">import</span> java<span class="token punctuation">.</span>net<span class="token punctuation">.</span><span class="token punctuation">{</span>URL<span class="token punctuation">,</span> URLDecoder<span class="token punctuation">}</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>source<span class="token punctuation">.</span>StationLog<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironment<span class="token comment" spellcheck="true">/**  * 第二种方法的实现  * 统计每个手机的呼叫时间间隔，单位是毫秒  */</span>object TestKeyedState2 <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    val filePath<span class="token operator">:</span> URL <span class="token operator">=</span> getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/station.log"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//使用相对路径来得到完整的文件路径</span>    val packagePath<span class="token operator">:</span> String <span class="token operator">=</span> filePath<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">replaceAll</span><span class="token punctuation">(</span><span class="token string">"%20"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//解决路径中含有空格的情况</span>    val str<span class="token operator">:</span> String <span class="token operator">=</span> URLDecoder<span class="token punctuation">.</span><span class="token function">decode</span><span class="token punctuation">(</span>packagePath<span class="token punctuation">,</span> <span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//解决路径包含中文的情况</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        var arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    stream<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>callOut<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//分组</span>      <span class="token comment" spellcheck="true">//有两种情况1、状态中有上一次的通话时间，2、没有。采用scala中的模式匹配</span>      <span class="token punctuation">.</span>mapWithState<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">,</span> StationLog<span class="token punctuation">]</span> <span class="token punctuation">{</span>      <span class="token keyword">case</span> <span class="token punctuation">(</span>in<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> None<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">(</span>in<span class="token punctuation">.</span>callOut<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">Some</span><span class="token punctuation">(</span>in<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//状态中没有值 是第一次呼叫</span>      <span class="token keyword">case</span> <span class="token punctuation">(</span>in<span class="token operator">:</span> StationLog<span class="token punctuation">,</span> pre<span class="token operator">:</span> Some<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">//状态中有值，是第二次呼叫</span>        var interval<span class="token operator">:</span>Long <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">abs</span><span class="token punctuation">(</span>in<span class="token punctuation">.</span>callTime <span class="token operator">-</span> pre<span class="token punctuation">.</span>get<span class="token punctuation">.</span>callTime<span class="token punctuation">)</span>        <span class="token punctuation">(</span><span class="token punctuation">(</span>in<span class="token punctuation">.</span>callOut<span class="token punctuation">,</span> interval<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">Some</span><span class="token punctuation">(</span>in<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2 <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-CheckPoint"><a href="#2-CheckPoint" class="headerlink" title="2. CheckPoint"></a>2. CheckPoint</h3><p>当程序出现问题需要恢复<code>State</code> 数据的时候，只有程序提供支持才可以实现<code>State</code> 的容错。<code>State</code> 的容错需要依靠 <code>CheckPoint</code>机制，这样才可以保证 <code>Exactly-once</code> 这种语义，但是注意，<strong>它只能保证 Flink 系统内的 Exactly-once</strong>，比如 Flink 内置支持的算子。针对 Source 和 Sink 组件，如果想要保证 Exactly-once 的话，则<strong>这些组件本身应支持这种语义</strong>。</p><h5 id="1-CheckPoint-原理"><a href="#1-CheckPoint-原理" class="headerlink" title="1. CheckPoint 原理"></a>1. CheckPoint 原理</h5><p>Flink 中基于<code>异步</code>轻量级的分布式快照技术提供了 <code>Checkpoints</code>容错机制，分布式快照可以将同一时间点 <code>Task/Operator</code> 的状态数据全局统一快照处理，包括前面提到的 <code>Keyed State</code>和 <code>Operator State</code>。Flink 会在输入的数据集上间隔性地生成 <code>checkpoint barrier</code>， 通过<code>栅栏</code>（barrier）将间隔时间段内的数据划分到相应的 checkpoint 中。如下图:<br><img src="https://img-blog.csdnimg.cn/20200716181419653.png#pic_center" alt="在这里插入图片描述"><br>比如序列偶数求和跟奇数求和：<br><img src="https://img-blog.csdnimg.cn/2020071618421655.png#pic_center" alt="在这里插入图片描述"></p><h5 id="2-CheckPoint-参数和设置"><a href="#2-CheckPoint-参数和设置" class="headerlink" title="2. CheckPoint 参数和设置"></a>2. CheckPoint 参数和设置</h5><p>默认情况下 Flink <code>不开启</code>检查点的，用户需要在程序中通过调用方法配置和开启检查点，另外还可以调整其他相关参数：</p><ol><li>Checkpoint 开启和时间间隔指定： 开启检查点并且指定检查点时间间隔为 1000ms，根据实际情况自行选择，如果状态比较大，则建议适当增加该值。<code>streamEnv.enableCheckpointing(1000)</code></li><li><code>exactly-ance</code> 和 <code>at-least-once</code> 语义选择：<br>选择 exactly-once 语义保证<strong>整个应用内端到端的数据一致性</strong>，这种情况比较适合于数据要求比较高，不允许出现丢数据或者数据重复，与此同时，Flink 的性能也<strong>相对较弱</strong>，而 at-least-once 语义更适合于时廷和吞吐量要求非常高但对数据的一致性要求不高的场景。 如下通过<code>setCheckpointingMode()</code>方法来设 定语义模式， <strong>默认情况 使用的是 exactly-once 模式</strong>。</li></ol><pre class="line-numbers language-java"><code class="language-java"> streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setCheckpointingMode</span><span class="token punctuation">(</span>CheckpointingMode<span class="token punctuation">.</span>EXACT LY_ONCE<span class="token punctuation">)</span>； <span class="token comment" spellcheck="true">//或者 </span> streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setCheckpointingMode</span><span class="token punctuation">(</span>CheckpointingMode<span class="token punctuation">.</span>AT_LE AST_ONCE<span class="token punctuation">)</span><span class="token number">123</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol><li>Checkpoint 超时时间：<br>超时时间指定了每次 Checkpoint 执行过程中的上限时间范围，一旦 Checkpoint 执行时 间超过该阈值，Flink 将会中断 Checkpoint 过程，并按照超时处理。该指标可以通过 <code>setCheckpointTimeout</code> 方法设定，默认为 <code>10</code>分钟。<code>streamEnv.getCheckpointConfig.setCheckpointTimeout(50000)</code></li><li>检查点之间最小时间间隔：<br>该参数主要目的是<strong>设定两个 Checkpoint 之间的最小时间间隔</strong>，防止出现例如状态数据过大而导致 Checkpoint 执行时间过长，从而导致 Checkpoint 积压过多，最终 Flink 应用密集地触发 Checkpoint 操作，会占用了大量计算资源而影响到整个应用的性能。</li></ol><pre class="line-numbers language-java"><code class="language-java">streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setMinPauseBetweenCheckpoints</span><span class="token punctuation">(</span><span class="token number">600</span><span class="token punctuation">)</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>最大并行执行的检查点数量：<br>通过 <code>setMaxConcurrentCheckpoints()</code>方法设定能够最大同时执行的 Checkpoint 数量。 在默认情况下只有一个检查点可以运行，根据用户指定的数量可以同时触发多个 Checkpoint，进而提升 Checkpoint 整体的效率。</li></ol><pre class="line-numbers language-java"><code class="language-java">streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setMaxConcurrentCheckpoints</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>是否删除 Checkpoint 中保存的数据：<br>设置为 <code>RETAIN_ON_CANCELLATION</code>：表示一旦 Flink 处理程序被 cancel 后，会保留 CheckPoint 数据，以便根据实际需要恢复到指定的 CheckPoint。 设置为 <code>DELETE_ON_CANCELLATION</code>：表示一旦 Flink 处理程序被 cancel 后，会删除 CheckPoint 数据，只有 Job 执行失败的时候才会保存 CheckPoint。</li></ol><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//删除 </span>streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">enableExternalizedCheckpoints</span><span class="token punctuation">(</span>ExternalizedCheckp ointCleanup<span class="token punctuation">.</span>DELETE_ON_CANCELLATION<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//保留</span>streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">enableExternalizedCheckpoints</span><span class="token punctuation">(</span>ExternalizedCheckp ointCleanup<span class="token punctuation">.</span>RETAIN_ON_CANCELLATION<span class="token punctuation">)</span><span class="token number">1234</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>TolerableCheckpointFailureNumber：<br>设置可以容忍的检查的失败数，超过这个数量则系统自动关闭和停止任务。</li></ol><pre class="line-numbers language-java"><code class="language-java"> streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setTolerableCheckpointFailureNumber</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h5 id="3-保存机制-StateBackend-状态后端"><a href="#3-保存机制-StateBackend-状态后端" class="headerlink" title="3. 保存机制 StateBackend(状态后端)"></a>3. 保存机制 StateBackend(状态后端)</h5><p>默认情况下，State 会保存在 TaskManager 的<code>内存</code>中，<code>CheckPoint</code>会存储在 <code>JobManager</code>的内存中。<code>State</code>和 <code>CheckPoint</code>的存储位置取决于<code>StateBackend</code>的配置。Flink 一共提供 了 3 种 <code>StateBackend</code>。包括基于内存的 <code>MemoryStateBackend</code>、基于文件系统的<code>FsStateBackend</code>，以及基于 <code>RockDB</code> 作为存储介质的 <code>RocksDBState-Backend</code>。</p><h6 id="1-MemoryStateBackend"><a href="#1-MemoryStateBackend" class="headerlink" title="1. MemoryStateBackend"></a>1. MemoryStateBackend</h6><p>基于内存的状态管理具有非常<code>快速</code>和<code>高效</code>的特点，但也具有非常多的限制，最主要的就 是内存的容量限制，一旦存储的状态数据过多就会导致系统内存溢出等问题，从而影响整个 应用的正常运行。同时如果机器出现问题，整个主机内存中的状态数据都会丢失，进而无法 恢复任务中的状态数据。因此从数据安全的角度建议用户<code>尽可能地避免</code>在生产环境中使用 MemoryStateBackend。<br><img src="https://img-blog.csdnimg.cn/20200716211157372.png#pic_center" alt="在这里插入图片描述"></p><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">// 设定存储空间为10G</span>streamEnv<span class="token punctuation">.</span><span class="token function">setStateBackend</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MemoryStateBackend</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token operator">*</span><span class="token number">1024</span><span class="token operator">*</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">12</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h6 id="2-FsStateBackend"><a href="#2-FsStateBackend" class="headerlink" title="2. FsStateBackend"></a>2. FsStateBackend</h6><p>和 <code>MemoryStateBackend</code>有所不同，FsStateBackend 是<code>基于文件系统</code>的一种状态管理器， 这里的文件系统可以是本地文件系统，也可以是 HDFS 分布式文件系统。FsStateBackend 更适合任务状态非常大的情况，例如应用中含有时间范围非常长的窗口计算，或 Key/value State 状态数据量非常大的场景。<br>TaskManager仍然使用内存保存数据，但是进行CheckPoint的时候是<strong>将数据保存到FS中</strong>。<br><img src="https://img-blog.csdnimg.cn/20200716211942762.png#pic_center" alt="在这里插入图片描述"></p><pre class="line-numbers language-java"><code class="language-java"> streamEnv<span class="token punctuation">.</span><span class="token function">setStateBackend</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FsStateBackend</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop101:9000/checkpoint/cp1"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h6 id="3-RocksDBStateBackend"><a href="#3-RocksDBStateBackend" class="headerlink" title="3. RocksDBStateBackend"></a>3. RocksDBStateBackend</h6><p>RocksDBStateBackend 是 Flink 中内置的第三方状态管理器，和前面的状态管理器不同，RocksDBStateBackend 需要单独引入相关的依赖包到工程中。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flink-statebackend-rocksdb_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>12345<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RocksDBStateBackend 采用<code>异步</code>的方式进行状态数据的 <code>Snapshot</code>，任务中的状态数据首先被写入本地 RockDB 中，这样在 RockDB 仅会存储正在进行计算的热数据，而需要进行 CheckPoint 的时候，会把本地的数据直接复制到远端的 FileSystem 中。</p><p>与 FsStateBackend 相比，RocksDBStateBackend 在性能上要比 FsStateBackend 高一些，主要是因为借助于 RocksDB 在本地存储了最新热数据，然后通过异步的方式再同步到文件系 统中，但 <code>RocksDBStateBackend</code>和 <code>MemoryStateBackend</code>相比性能就会较弱一些。RocksDB 克服了 State 受内存限制的缺点，同时又能够持久化到远端文件系统中，推荐在生产中使用。</p><pre class="line-numbers language-java"><code class="language-java"> streamEnv<span class="token punctuation">.</span><span class="token function">setStateBackend</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">RocksDBStateBackend</span> <span class="token punctuation">(</span><span class="token string">"hdfs://hadoop101:9000/checkpoint/cp2"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/20200716212307419.png#pic_center" alt="在这里插入图片描述"></p><h6 id="4-全局配置-StateBackend"><a href="#4-全局配置-StateBackend" class="headerlink" title="4. 全局配置 StateBackend"></a>4. 全局配置 StateBackend</h6><p>以上的代码都是<code>单 job</code> 配置状态后端，也可以全局配置状态后端，需要修改 flink-conf.yaml 配置文件:</p><pre class="line-numbers language-java"><code class="language-java">state<span class="token punctuation">.</span>backend<span class="token operator">:</span> filesystemfilesystem 表示使用 FsStateBackend<span class="token punctuation">,</span> jobmanager 表示使用 MemoryStateBackend rocksdb 表示使用 RocksDBStateBackend。<span class="token operator">--</span><span class="token operator">-</span>flink<span class="token operator">-</span>conf<span class="token punctuation">.</span>yaml 配置文件中state<span class="token punctuation">.</span>checkpoints<span class="token punctuation">.</span>dir<span class="token operator">:</span> hdfs<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>hadoop101<span class="token operator">:</span><span class="token number">9000</span><span class="token operator">/</span>checkpoints<span class="token number">1234567</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>默认情况下，如果设置了 CheckPoint 选项，则 Flink 只保留最近成功生成的 1 个 CheckPoint，而当 Flink 程序失败时，可以通过最近的 CheckPoint 来进行恢复。但是，如果希望保留多个CheckPoint，并能够根据实际需要选择其中一个进行恢复，就会更加灵活。 添加如下配置，指定最多可以保存的 CheckPoint 的个数。</p><pre class="line-numbers language-java"><code class="language-java">state<span class="token punctuation">.</span>checkpoints<span class="token punctuation">.</span>num<span class="token operator">-</span>retained<span class="token operator">:</span> <span class="token number">2</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h5 id="4-Checkpoint案例"><a href="#4-Checkpoint案例" class="headerlink" title="4. Checkpoint案例"></a>4. Checkpoint案例</h5><p><code>案例</code>:设置 HDFS 文件系统的状态后端，取消 Job 之后再次恢复 Job。<br>使用WordCount案例来测试一下HDFS的状态后端，先运行一段时间Job，然后cancel，在重新启动，看看状态是否是连续的</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>state<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>runtime<span class="token punctuation">.</span>state<span class="token punctuation">.</span>filesystem<span class="token punctuation">.</span>FsStateBackend<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>CheckpointingMode<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>environment<span class="token punctuation">.</span>CheckpointConfig<span class="token punctuation">.</span>ExternalizedCheckpointCleanup<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject TestCheckPointByHDFS <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//使用WordCount案例来测试一下HDFS的状态后端，先运行一段时间Job，然后cancel，在重新启动，看看状态是否是连续的</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//开启CheckPoint并且设置一些参数</span>    streamEnv<span class="token punctuation">.</span><span class="token function">enableCheckpointing</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//每隔5秒开启一次CheckPoint</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setStateBackend</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">FsStateBackend</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop101:9000/checkpoint/cp1"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//存放检查点数据</span>    streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setCheckpointingMode</span><span class="token punctuation">(</span>CheckpointingMode<span class="token punctuation">.</span>EXACTLY_ONCE<span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setCheckpointTimeout</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">setMaxConcurrentCheckpoints</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span>getCheckpointConfig<span class="token punctuation">.</span><span class="token function">enableExternalizedCheckpoints</span><span class="token punctuation">(</span>ExternalizedCheckpointCleanup<span class="token punctuation">.</span>RETAIN_ON_CANCELLATION<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//终止job保留检查的数据</span>    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//2、导入隐式转换</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//3、读取数据,读取sock流中的数据</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//DataStream ==> spark 中Dstream</span>    <span class="token comment" spellcheck="true">//4、转换和处理数据</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//分组算子  : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//聚会累加算子</span>    <span class="token comment" spellcheck="true">//5、打印结果</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"结果"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//6、启动流计算程序</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"wordcount"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>打包上传到WebUI:<br><img src="https://img-blog.csdnimg.cn/20200722193339771.png#pic_center" alt="在这里插入图片描述"></p><p>在<code>nc -lk 8888</code> 输入若干单词。然后查找 WebUI 的输出。然后通过WebUI将任务取消。最后尝试将任务重启。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token punctuation">.</span>/flink run <span class="token operator">-</span>d <span class="token operator">-</span>s hdfs<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>hadoop101<span class="token operator">:</span><span class="token number">9000</span><span class="token operator">/</span>checkpoint<span class="token operator">/</span>cp1<span class="token operator">/</span>精确到跟meta数据同级目录 <span class="token operator">-</span>c com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>state<span class="token punctuation">.</span>CheckpointOnFsBackend <span class="token operator">/</span>home<span class="token operator">/</span>Flink<span class="token operator">-</span>Demo<span class="token operator">-</span><span class="token number">1.0</span><span class="token operator">-</span>SNAPSHOT<span class="token punctuation">.</span>jar<span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>也可以通过WebUI 重启，指定 MainClass跟 CheckPoint即可。此处关键在于CheckPoint路径要写对！</p><h5 id="5-SavePoint"><a href="#5-SavePoint" class="headerlink" title="5. SavePoint"></a>5. SavePoint</h5><p><strong>Savepoints 是检查点的一种特殊实现</strong>，底层实现其实也是使用 Checkpoints 的机制。 Savepoints 是用户以<code>手工命令</code>的方式触发 Checkpoint,并将结果持久化到指定的存储路径 中，其主要目的是帮助用户在升级和维护集群过程中保存系统中的状态数据，避免因为停机运维或者升级应用等正常终止应用的操作而导致系统无法恢复到原有的计算状态的情况，从而无法实现从端到端的 Excatly-Once 语义保证。</p><p><strong>配置 Savepoints 的存储路径</strong><br>在 flink-conf.yaml 中配置 SavePoint 存储的位置，设置后，如果要创建指定 Job 的 SavePoint，可以不用在手动执行命令时指定 SavePoint 的位置。</p><pre class="line-numbers language-java"><code class="language-java">state<span class="token punctuation">.</span>savepoints<span class="token punctuation">.</span>dir<span class="token operator">:</span> hdfs<span class="token operator">:</span><span class="token operator">/</span>hadoop101<span class="token operator">:</span><span class="token number">9000</span><span class="token operator">/</span>savepoints<span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>在代码中设置算子 ID</strong><br>为了能够在作业的不同版本之间以及 Flink 的不同版本之间顺利升级，<strong>强烈推荐程序员 通过手动给算子赋予 ID</strong>，这些 ID 将用于确定每一个算子的状态范围。如果不手动给各算子 指定 ID，则会由 Flink 自动给每个算子生成一个 ID。而这些自动生成的 ID 依赖于程序的结 构，并且对代码的更改是很敏感的。因此，强烈建议用户手动设置 ID。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>sowhat<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>state<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>StreamExecutionEnvironmentobject TestSavePoints <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//1、初始化Flink流计算的环境</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token comment" spellcheck="true">//修改并行度</span>    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//默认所有算子的并行度为1</span>    <span class="token comment" spellcheck="true">//2、导入隐式转换</span>    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//3、读取数据,读取sock流中的数据</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span><span class="token number">8888</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//DataStream ==> spark 中Dstream</span>    <span class="token punctuation">.</span><span class="token function">uid</span><span class="token punctuation">(</span><span class="token string">"socket001"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//4、转换和处理数据</span>    val result<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">uid</span><span class="token punctuation">(</span><span class="token string">"flatmap001"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">uid</span><span class="token punctuation">(</span><span class="token string">"map001"</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//分组算子  : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数</span>      <span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">uid</span><span class="token punctuation">(</span><span class="token string">"sum001"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//5、打印结果</span>    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"结果"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//6、启动流计算程序</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"wordcount"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>触发 SavePoint</p><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//先启动Job</span><span class="token punctuation">[</span>root<span class="token annotation punctuation">@hadoop101</span> bin<span class="token punctuation">]</span># <span class="token punctuation">.</span>/flink run <span class="token operator">-</span>c com<span class="token punctuation">.</span>bjsxt<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>state<span class="token punctuation">.</span>TestSavepoints <span class="token operator">-</span>d <span class="token operator">/</span>home<span class="token operator">/</span>Flink<span class="token operator">-</span>Demo<span class="token operator">-</span><span class="token number">1.0</span><span class="token operator">-</span>SNAPSHOT<span class="token punctuation">.</span>jar<span class="token punctuation">[</span>root<span class="token annotation punctuation">@hadoop101</span> bin<span class="token punctuation">]</span># <span class="token punctuation">.</span>/flink list 获取 job 对应ID<span class="token comment" spellcheck="true">//再取消Job </span><span class="token punctuation">[</span>root<span class="token annotation punctuation">@hadoop101</span> bin<span class="token punctuation">]</span># <span class="token punctuation">.</span>/flink savepoint 6ecb8cfda5a5200016ca6b01260b94ce <span class="token comment" spellcheck="true">// 触发SavePoint</span><span class="token punctuation">[</span>root<span class="token annotation punctuation">@hadoop101</span> bin<span class="token punctuation">]</span># <span class="token punctuation">.</span>/flink cancel 6ecb8cfda5a5200016ca6b01260b94ce<span class="token number">1234567</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>从 SavePoint 启动 Job</strong><br>大致方法跟上面的CheckPoint启动Job类似。</p><h5 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h5><p>若干个常用的状态算子大致如何存储的要了解。<br>CheckPoint的原理主要是<strong>图示</strong>，理解如何保证精准一致性的。<br>CheckPoint一般有基于内存的，基于HDFS的跟基于DB的，整体来说基于DB的把数据存储早DB中跟HDFS中是最好的。<br>SavePoint是手动触发的CheckPoint，一般方便线上迁移的功能等，并且尽量给每一个算子自定义一个UID，</p><h1 id="6-Window-窗口"><a href="#6-Window-窗口" class="headerlink" title="6. Window 窗口"></a>6. Window 窗口</h1><p><code>无界数据变为若干个有界数据</code>。Windows 计算是流式计算中非常常用的数据计算方式之一，通过按照固定时间或长度将数据流切分成不同的窗口，然后对数据进行相应的聚合运算，从而得到一定时间范围内的统计结果。例如统计最近 5 分钟内某基站的呼叫数，此时基站的数据在不断地产生，但是通过 5 分钟的窗口将数据限定在固定时间范围内，就可以对该范围内的有界数据执行聚合处理， 得出最近 5 分钟的基站的呼叫数量。</p><h3 id="1-Window分类"><a href="#1-Window分类" class="headerlink" title="1. Window分类"></a>1. Window分类</h3><h5 id="1-Global-Window-和-Keyed-Window"><a href="#1-Global-Window-和-Keyed-Window" class="headerlink" title="1. Global Window 和 Keyed Window"></a>1. Global Window 和 Keyed Window</h5><p>在运用窗口计算时，Flink根据上游数据集<strong>是否为KeyedStream类型</strong>，对应的Windows 也 会有所不同。</p><ul><li>Keyed Window: 上游数据集如果是 KeyedStream 类型，则调用 DataStream API 的<code>window()</code>方法，数据会根据 Key 在不同的 Task 实例中并行分别计算，最后得出针对每个 Key 统计的结果。</li><li>Global Window:如果是 Non-Keyed 类型，则调用 <code>WindowsAll()</code>方法，所有的<strong>数据都会在窗口算子中由到一个 Task 中计算</strong>，并得到全局统计结果。</li></ul><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//读取文件数据</span>val data <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">readTextFile</span><span class="token punctuation">(</span>getClass<span class="token punctuation">.</span><span class="token function">getResource</span><span class="token punctuation">(</span><span class="token string">"/station.log"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getPath<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line<span class="token operator">=</span><span class="token operator">></span><span class="token punctuation">{</span>var arr <span class="token operator">=</span>line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span> <span class="token keyword">new</span><span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>to Long<span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//Global Window </span>data<span class="token punctuation">.</span><span class="token function">windowAll</span><span class="token punctuation">(</span>自定义的WindowAssigner<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//Keyed Window</span>data<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>sid<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>自定义的WindowAssigner<span class="token punctuation">)</span><span class="token number">12345678910</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-Time-Window-和-Count-Window"><a href="#2-Time-Window-和-Count-Window" class="headerlink" title="2. Time Window 和 Count Window"></a>2. Time Window 和 Count Window</h5><p>基于业务数据的方面考虑，Flink 又支持两种类型的窗口，一种是基于时间的窗口叫<code>Time Window</code>。还有一种基于输入数据数量的窗口叫 <code>Count Window</code></p><h5 id="3-Time-Window-时间窗口"><a href="#3-Time-Window-时间窗口" class="headerlink" title="3. Time Window(时间窗口)"></a>3. Time Window(时间窗口)</h5><p>根据不同的业务场景，Time Window 也可以分为三种类型，分别是<code>滚动窗口</code>(Tumbling Window)、<code>滑动窗口</code>(Sliding Window)和<code>会话窗口</code>(Session Window)</p><ol><li>滚动窗口(Tumbling Window)<br>滚动窗口是根据固定时间进行切分，且窗口和窗口之间的元素<code>互不重叠</code>。这种类型的窗 口的最大特点是比较简单。只需要指定一个窗口长度(window size)。<br><img src="https://img-blog.csdnimg.cn/20200716230654378.png#pic_center" alt="在这里插入图片描述"></li></ol><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//每隔5秒统计每个基站的日志数量 </span>data<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>stationLog<span class="token operator">=</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">(</span>stationLog<span class="token punctuation">.</span>sid<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">timeWindow</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//.window(TumblingEventTimeWindows.of(Time.seconds(5))) 跟上面同样功能</span><span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//聚合</span><span class="token number">123456</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中时间间隔可以是 Time.milliseconds(x)、Time.seconds(x)或 Time.minutes(x)。</p><ol><li>滑动窗口(Sliding Window)<br>滑动窗口也是一种比较常见的窗口类型，其特点是在滚动窗口基础之上增加了窗口滑动时间(Slide Time)，且允许窗口数据发生重叠。当 Windows size 固定之后，窗口并不像 滚动窗口按照 Windows Size 向前移动，而是根据设定的 Slide Time 向前滑动。窗口之间的 数据重叠大小根据 Windows size 和 Slide time 决定，当 Slide time 小于 Windows size 便会发生窗口重叠，Slide size 大于 Windows size 就会出现窗口不连续，数据可能不能在 任何一个窗口内计算，Slide size 和 Windows size 相等时，Sliding Windows 其实就是 Tumbling Windows。<br><img src="https://img-blog.csdnimg.cn/20200716230931751.png#pic_center" alt="在这里插入图片描述"></li></ol><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//每隔3秒计算最近5秒内，每个基站的日志数量 </span>data<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>stationLog<span class="token operator">=</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">(</span>stationLog<span class="token punctuation">.</span>sid<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">timeWindow</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//.window(SlidingEventTimeWindows.of(Time.seconds(5),Time.seconds(3)))</span><span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token number">12345</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>会话窗口(Session Window)<br>会话窗口(Session Windows)主要是将某段时间内活跃度较高的数据聚合成一个窗口 进行计算，窗口的触发的条件是 <code>Session Gap</code>，是指<code>在规定的时间内如果没有数据活跃接入</code>， 则认为窗口结束，然后触发窗口计算结果。需要注意的是如果数据一直不间断地进入窗口， 也会导致窗口始终不触发的情况。与滑动窗口、滚动窗口不同的是，Session Windows 不需 要有固定 windows size 和 slide time，只需要定义 session gap，来规定不活跃数据的时 间上限即可。<br><img src="https://img-blog.csdnimg.cn/20200716231110572.png#pic_center" alt="在这里插入图片描述"></li></ol><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//3秒内如果没有数据进入，则计算每个基站的日志数量</span> data<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>stationLog<span class="token operator">=</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">(</span>stationLog<span class="token punctuation">.</span>sid<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>EventTimeSessionWindows<span class="token punctuation">.</span><span class="token function">withGap</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token number">123</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h5 id="4-Count-Window-数量窗口"><a href="#4-Count-Window-数量窗口" class="headerlink" title="4. Count Window(数量窗口)"></a>4. Count Window(数量窗口)</h5><p>Count Window 也有滚动窗口、滑动窗口等。由于使用比较少TODO，比如五条数据算一批次这样的统计。</p><h3 id="2-Window的API"><a href="#2-Window的API" class="headerlink" title="2. Window的API"></a>2. Window的API</h3><p>在以后的实际案例中 <code>Keyed Window</code>使用最多，所以我们需要掌握 Keyed Window 的算子， 在每个窗口算子中包含了 Windows Assigner、Windows Trigger(窗口触发器)、Evictor (数据剔除器)、Lateness(时延设定)、Output Tag(输出标签)以及 Windows Funciton 等组成部分，其中 Windows Assigner 和 Windows Funciton 是所有窗口算子<code>必须指定</code>的属性， 其余的属性都是根据实际情况选择指定。</p><pre class="line-numbers language-java"><code class="language-java">stream<span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">// 是Keyed类型数据集</span><span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//指定窗口分配器类型</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token function">trigger</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//指定触发器类型(可选)</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token function">evictor</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//指定evictor或者不指定(可选) </span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token function">allowedLateness</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//指定是否延迟处理数据(可选) </span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token function">sideOutputLateData</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//指定Output Lag(可选) </span><span class="token punctuation">.</span>reduce<span class="token operator">/</span>aggregate<span class="token operator">/</span>fold<span class="token operator">/</span><span class="token function">apply</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//指定窗口计算函数</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token function">getSideOutput</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//根据Tag输出数据(可选)</span><span class="token number">12345678</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>Windows Assigner: 指定窗口的类型，定义如何将数据流分配到一个或多个窗口。</li><li>Windows Trigger: 指定窗口触发的时机，定义窗口满足什么样的条件触发计算。</li><li>Evictor: 用于数据剔除。</li><li>allowedLateness: 标记是否处理迟到数据，当迟到数据到达窗口中是否触发计算。</li><li>Output Tag: 标记输出标签，然后在通过 getSideOutput 将窗口中的数据根据标签输出。</li><li>Windows Funciton: 定义窗口上数据处理的逻辑，例如对数据进行 sum 操作。</li></ul><h3 id="3-窗口聚合函数"><a href="#3-窗口聚合函数" class="headerlink" title="3. 窗口聚合函数"></a>3. 窗口聚合函数</h3><p>如果定义了 Window Assigner 之后，下一步就可以定义窗口内数据的计算逻辑，这也就是 Window Function 的定义。Flink 中提供了四种类型的 Window Function，分别为 <code>ReduceFunction</code>、<code>AggregateFunction</code> 以及 <code>ProcessWindowFunction</code>,<code>（sum 和 max)</code>等。 前三种类型的 Window Fucntion 按照计算原理的不同可以分为两大类：</p><ul><li>一类是<strong>增量</strong>聚合函数：对应有 <code>ReduceFunction</code>、<code>AggregateFunction</code>；</li><li>另一类是全量窗口函数，对应有 <code>ProcessWindowFunction</code>（还有 <code>WindowFunction</code>）。</li></ul><p>增量聚合函数计算性能较高，占用存储空间少，主要因为<strong>基于中间状态的计算结果</strong>，窗口中只维护中间结果状态值，不需要缓存原始数据。而全量窗口函数使用的代价相对较高， 性能比较弱，主要因为此时算子需要对所有属于该窗口的接入数据进行缓存，然后等到窗口触发的时候，对所有的原始数据进行汇总计算。</p><h5 id="1-ReduceFunction"><a href="#1-ReduceFunction" class="headerlink" title="1. ReduceFunction"></a>1. ReduceFunction</h5><p>Reduce要求输入跟输出类型要一样！这点切记。<br><code>需求</code>：每隔5秒统计每个基站的日志数量</p><pre class="line-numbers language-java"><code class="language-java">object TestReduceFunctionByWindow <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//每隔5秒统计每个基站的日志数量</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        val arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//开窗</span>    stream<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>log <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">(</span>log<span class="token punctuation">.</span>sid<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">timeWindow</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//开窗</span>      <span class="token punctuation">.</span><span class="token function">reduce</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span>t1<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t1<span class="token punctuation">.</span>_2 <span class="token operator">+</span> t2<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">123456789101112131415161718192021222324</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="2-AggregateFunction"><a href="#2-AggregateFunction" class="headerlink" title="2. AggregateFunction"></a>2. AggregateFunction</h5><p>和 ReduceFunction 相似，AggregateFunction 也是基于<strong>中间</strong>状态计算结果的增量计算 函数，但 AggregateFunction 在窗口计算上更加通用。AggregateFunction 接口相对 ReduceFunction 更加灵活，输入跟输出类型不要求完全一致，实现复杂度也相对较高。AggregateFunction 接口中定义了三个 需要复写的方法，其中 add()定义数据的添加逻辑，getResult 定义了根据 accumulator 计 算结果的逻辑，merge 方法定义合并 accumulator 的逻辑。初始化，<a href="https://blog.csdn.net/chilimei8516/article/details/100796930" target="_blank" rel="noopener">分区内如何处理</a>，分区间如何处理，最终如何输出。</p><p><code>需求</code>：每隔3秒计算最近5秒内，每个基站的日志数量</p><pre class="line-numbers language-java"><code class="language-java">object TestAggregatFunctionByWindow <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//每隔3秒计算最近5秒内，每个基站的日志数量</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        val arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//开窗</span>    val value<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> stream<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>log <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">(</span>log<span class="token punctuation">.</span>sid<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>SlidingProcessingTimeWindows<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//开窗，滑动窗口</span>      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">MyAggregateFunction</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">MyWindowFunction</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">// 到底是数字对应哪个基站</span>      <span class="token comment" spellcheck="true">// aggregate(增量函数，全量函数)</span>    value<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>       streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">/**    * 里面的add方法，是来一条数据执行一次，getResult在窗口结束的时候执行一次    * in,累加器acc,out    * https://blog.csdn.net/chilimei8516/article/details/100796930    */</span>  <span class="token keyword">class</span> <span class="token class-name">MyAggregateFunction</span> <span class="token keyword">extends</span> <span class="token class-name">AggregateFunction</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> Long<span class="token punctuation">,</span> Long<span class="token punctuation">]</span> <span class="token punctuation">{</span>    override def <span class="token function">createAccumulator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> Long <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true">//初始化一个累加器 acc，开始的时候为0</span>    <span class="token comment" spellcheck="true">// 分区内操作</span>    override def <span class="token function">add</span><span class="token punctuation">(</span>value<span class="token operator">:</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> accumulator<span class="token operator">:</span> Long<span class="token punctuation">)</span><span class="token operator">:</span> Long <span class="token operator">=</span> accumulator <span class="token operator">+</span> value<span class="token punctuation">.</span>_2    <span class="token comment" spellcheck="true">// 结果返回</span>    override def <span class="token function">getResult</span><span class="token punctuation">(</span>accumulator<span class="token operator">:</span> Long<span class="token punctuation">)</span><span class="token operator">:</span> Long <span class="token operator">=</span> accumulator    <span class="token comment" spellcheck="true">// 分区间操作</span>    override def <span class="token function">merge</span><span class="token punctuation">(</span>a<span class="token operator">:</span> Long<span class="token punctuation">,</span> b<span class="token operator">:</span> Long<span class="token punctuation">)</span><span class="token operator">:</span> Long <span class="token operator">=</span> a <span class="token operator">+</span> b  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">// WindowFunction 输入数据来自于AggregateFunction ，</span>  <span class="token comment" spellcheck="true">// 在窗口结束的时候先执行AggregateFunction对象的getResult，然后再执行apply</span>  <span class="token comment" spellcheck="true">// in,out,key,window   </span>  <span class="token keyword">class</span> <span class="token class-name">MyWindowFunction</span> <span class="token keyword">extends</span> <span class="token class-name">WindowFunction</span><span class="token punctuation">[</span>Long<span class="token punctuation">,</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">,</span> String<span class="token punctuation">,</span> TimeWindow<span class="token punctuation">]</span> <span class="token punctuation">{</span>    override def <span class="token function">apply</span><span class="token punctuation">(</span>key<span class="token operator">:</span> String<span class="token punctuation">,</span> window<span class="token operator">:</span> TimeWindow<span class="token punctuation">,</span> input<span class="token operator">:</span> Iterable<span class="token punctuation">[</span>Long<span class="token punctuation">]</span><span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>      out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> input<span class="token punctuation">.</span>iterator<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//next得到第一个值，迭代器中只有一个值</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="3-ProcessWindowFunction"><a href="#3-ProcessWindowFunction" class="headerlink" title="3. ProcessWindowFunction"></a>3. ProcessWindowFunction</h5><p>前面提到的<code>ReduceFunction</code>和 <code>AggregateFunction</code> 都是基于中间状态实现增量计算的 窗口函数，虽然已经满足绝大多数场景，但在某些情况下，统计更复杂的指标可能需要依赖于窗口中<strong>所有</strong>的数据元素，或需要操作窗口中的状态数据和窗口元数据，这时就需要使用到 <code>ProcessWindowsFunction</code>，<code>ProcessWindowsFunction</code>能够更加灵活地支持基于窗口全部数据元素的结果计算 ， 例如对整个窗口 数 据排序取TopN， 这样的需要就必须使用<code>ProcessWindowFunction</code>。</p><p><code>需求</code>：每隔5秒统计每个基站的日志数量</p><pre class="line-numbers language-java"><code class="language-java">object TestProcessWindowFunctionByWindow <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">//每隔5秒统计每个基站的日志数量</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val streamEnv<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>scala<span class="token punctuation">.</span>_    streamEnv<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//读取数据源</span>    val stream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>StationLog<span class="token punctuation">]</span> <span class="token operator">=</span> streamEnv<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"hadoop101"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>        var arr <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span>        <span class="token keyword">new</span> <span class="token class-name">StationLog</span><span class="token punctuation">(</span><span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">,</span> <span class="token function">arr</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>trim<span class="token punctuation">.</span>toLong<span class="token punctuation">)</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//开窗</span>    stream<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>log <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token punctuation">(</span>log<span class="token punctuation">.</span>sid<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">keyBy</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">// .timeWindow(Time.seconds(5))//开窗</span>      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span>TumblingProcessingTimeWindows<span class="token punctuation">.</span><span class="token function">of</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProcessWindowFunction</span><span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">,</span> String<span class="token punctuation">,</span> TimeWindow<span class="token punctuation">]</span> <span class="token punctuation">{</span>       <span class="token comment" spellcheck="true">//一个窗口结束的时候调用一次(一个分组执行一次)    in,out,key,windows</span>        override def <span class="token function">process</span><span class="token punctuation">(</span>key<span class="token operator">:</span> String<span class="token punctuation">,</span> context<span class="token operator">:</span> Context<span class="token punctuation">,</span> elements<span class="token operator">:</span> Iterable<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Long<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>          <span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"------------"</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true">//注意：整个窗口的数据保存到Iterable，里面有很多行数据。Iterable的size就是日志的总条数</span>          out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> elements<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    streamEnv<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>需求</code>：窗口函数读数据然后将数据写入到neo4j，感觉其实应该用 <a href="https://sowhat.blog.csdn.net/article/details/107323074" target="_blank" rel="noopener">自定的Sink</a> 更合适一些。</p><pre class="line-numbers language-java"><code class="language-java">object DealDataFromKafka <span class="token punctuation">{</span>  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>    val environment<span class="token operator">:</span> StreamExecutionEnvironment <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span>getExecutionEnvironment    environment<span class="token punctuation">.</span><span class="token function">setParallelism</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    val properties<span class="token operator">:</span> Properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"IP1:9092,IP2:9092"</span><span class="token punctuation">)</span>    properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"group.id"</span><span class="token punctuation">,</span> <span class="token string">"timer"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// 从最新数据开始读</span>    properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"auto.offset.reset"</span><span class="token punctuation">,</span> <span class="token string">"latest"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//val dataStream: DataStream[String] = environment.addSource(new FlinkKafkaConsumer011[String]("sowhat", new SimpleStringSchema(), properties))</span>    val dataStream<span class="token operator">:</span> DataStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> environment<span class="token punctuation">.</span><span class="token function">socketTextStream</span><span class="token punctuation">(</span><span class="token string">"IP"</span><span class="token punctuation">,</span> <span class="token number">8889</span><span class="token punctuation">)</span>    val winData<span class="token operator">:</span> AllWindowedStream<span class="token punctuation">[</span>String<span class="token punctuation">,</span> TimeWindow<span class="token punctuation">]</span> <span class="token operator">=</span> dataStream<span class="token punctuation">.</span><span class="token function">timeWindowAll</span><span class="token punctuation">(</span>Time<span class="token punctuation">.</span><span class="token function">seconds</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    var pre<span class="token operator">:</span> Int <span class="token operator">=</span> <span class="token number">0</span>    var tmp<span class="token operator">:</span> Int <span class="token operator">=</span> <span class="token number">0</span>    val timeWithHashCode<span class="token operator">:</span> DataStream<span class="token punctuation">[</span><span class="token punctuation">(</span>Int<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> winData<span class="token punctuation">.</span><span class="token function">process</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProcessAllWindowFunction</span><span class="token punctuation">[</span>String<span class="token punctuation">,</span> <span class="token punctuation">(</span>Int<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">,</span> TimeWindow<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      override def <span class="token function">process</span><span class="token punctuation">(</span>context<span class="token operator">:</span> Context<span class="token punctuation">,</span> elements<span class="token operator">:</span> Iterable<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">,</span> out<span class="token operator">:</span> Collector<span class="token punctuation">[</span><span class="token punctuation">(</span>Int<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{</span>        val driver<span class="token operator">:</span> Driver <span class="token operator">=</span> GraphDatabase<span class="token punctuation">.</span><span class="token function">driver</span><span class="token punctuation">(</span><span class="token string">"bolt://IP:9314"</span><span class="token punctuation">,</span> AuthTokens<span class="token punctuation">.</span><span class="token function">basic</span><span class="token punctuation">(</span><span class="token string">"neo4j"</span><span class="token punctuation">,</span> <span class="token string">"neo4j0fcredithc"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        val session<span class="token operator">:</span> Session <span class="token operator">=</span> driver<span class="token punctuation">.</span><span class="token function">session</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        elements<span class="token punctuation">.</span><span class="token function">foreach</span><span class="token punctuation">(</span>value <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>          tmp <span class="token operator">+=</span> <span class="token number">1</span>          var now<span class="token operator">:</span> Int <span class="token operator">=</span> value<span class="token punctuation">.</span>hashCode          now <span class="token operator">=</span> tmp          session<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span>s<span class="token string">"CREATE (a:Test {id:${now}, time:'${value}'})"</span><span class="token punctuation">)</span>          <span class="token keyword">if</span> <span class="token punctuation">(</span>pre <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            session<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span>s<span class="token string">"MATCH (begin:Test{id:${pre}}) ,(end:Test{id:${now}})   MERGE (begin)-[like:Time_Link]->(end)"</span><span class="token punctuation">)</span>          <span class="token punctuation">}</span>          out<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> s<span class="token string">" MATCH (begin:Test{id:${pre}}) ,(end:Test{id:${now}})   MERGE (begin)-[like:Time_Link]->(end)"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          pre <span class="token operator">=</span> now        <span class="token punctuation">}</span>        <span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">//        session.close()</span>        <span class="token comment" spellcheck="true">//        driver.close()</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span>    timeWithHashCode<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"HashCode With time:"</span><span class="token punctuation">)</span>    environment<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token string">"getData"</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token number">1234567891011121314151617181920212223242526272829303132333435363738394041</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="End"><a href="#End" class="headerlink" title="End"></a>End</h2><p>窗口的分类从不同的维度来说，</p><ol><li>上游是否为KeyedStream，不同数据集调用不同方法。</li><li>根据上游数据是时间窗口(滚动窗口、滑动窗口、会话窗口)还是数据量窗口。</li><li>窗口若干API调用方法，窗口的聚合函数(reduceFunction、AggregateFunction、ProcessWindowFunction、WindowFunction)。</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-初识-Flink&quot;&gt;&lt;a href=&quot;#1-初识-Flink&quot; class=&quot;headerlink&quot; title=&quot;1. 初识 Flink&quot;&gt;&lt;/a&gt;1. 初识 Flink&lt;/h1&gt;&lt;p&gt;在当前数据量激增的时代，各种业务场景都有大量的业务数据产生，对于这些不断
      
    
    </summary>
    
    
      <category term="Flink" scheme="https://dataquaner.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://dataquaner.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>【hive日常使用问题记录】Hive建表导致的ORC序列化错误</title>
    <link href="https://dataquaner.github.io/2020/09/13/hive-ri-chang-shi-yong-wen-ti-ji-lu-hive-jian-biao-dao-zhi-de-orc-xu-lie-hua-cuo-wu/"/>
    <id>https://dataquaner.github.io/2020/09/13/hive-ri-chang-shi-yong-wen-ti-ji-lu-hive-jian-biao-dao-zhi-de-orc-xu-lie-hua-cuo-wu/</id>
    <published>2020-09-13T08:00:00.000Z</published>
    <updated>2020-09-13T05:19:56.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p><strong>hive表在创建时候指定存储格式</strong></p><pre class="line-numbers language-sql"><code class="language-sql">STORED <span class="token keyword">AS</span> ORC tblproperties <span class="token punctuation">(</span><span class="token string">'orc.compress'</span><span class="token operator">=</span><span class="token string">'SNAPPY'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> <strong>当insert数据到表时抛出异常</strong></p><pre><code>Caused by: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.hive.ql.io.orc.OrcSerde$OrcSerdeRow    at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:98)    at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:743)    at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)    at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:97)    at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)    at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:115)    at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:169)    at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:561)</code></pre><p> <strong>此时查看表结构</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">desc</span> formatted persons_orc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​    <img src="https://img2018.cnblogs.com/blog/932932/201812/932932-20181218160535441-989917770.png" alt="img"></p><p><strong>可以看到SerDe Library 的格式是LazySimpleSerDe，序列化格式不是orc的,所以抛出异常</strong></p><h2 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h2><p><strong>这里将表的序列化方式修改为orc即可</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> persons_orc <span class="token keyword">SET</span> FILEFORMAT ORC<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>再看序列化格式已经是orc，使用insert(insert overwrite table persons_orc select * from persons;)插入数据可以ok</strong></p><p><img src="https://img2018.cnblogs.com/blog/932932/201812/932932-20181218160848723-111121235.png" alt="img"></p><p> 可以参考详细解释:<a href="http://www.imooc.com/article/252830" target="_blank" rel="noopener">http://www.imooc.com/article/252830</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述：&quot;&gt;&lt;a href=&quot;#问题描述：&quot; class=&quot;headerlink&quot; title=&quot;问题描述：&quot;&gt;&lt;/a&gt;问题描述：&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;hive表在创建时候指定存储格式&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;line-num
      
    
    </summary>
    
    
      <category term="-- Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="-- Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师面试资料汇总</title>
    <link href="https://dataquaner.github.io/2020/07/06/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20200705/"/>
    <id>https://dataquaner.github.io/2020/07/06/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20200705/</id>
    <published>2020-07-05T17:25:00.000Z</published>
    <updated>2020-07-05T16:21:35.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>[TOC]</p><h1 id="一-Hadoop篇"><a href="#一-Hadoop篇" class="headerlink" title="一. Hadoop篇"></a>一. Hadoop篇</h1><h2 id="1-并行计算模型MapReduce"><a href="#1-并行计算模型MapReduce" class="headerlink" title="1. 并行计算模型MapReduce"></a>1. 并行计算模型MapReduce</h2><h3 id="1-1-MapReduce-的工作原理"><a href="#1-1-MapReduce-的工作原理" class="headerlink" title="1.1 MapReduce 的工作原理"></a>1.1 MapReduce 的工作原理</h3><blockquote><p>​       MapReduce是一个基于集群的计算<strong>平台</strong>，是一个简化分布式编程的计算<strong>框架</strong>，是一个将分布式计算抽象为<strong>Map</strong>和<strong>Reduce</strong>两个阶段的编程<strong>模型</strong>。<em>（这句话记住了是可以用来装逼的）</em></p></blockquote><p><img src="http://dl.iteye.com/upload/attachment/0066/0130/e1090dee-ee98-30d1-ad55-2f88f774fa73.jpg" alt="img"></p><p><strong>执行步骤：</strong>切片&gt;分词&gt;映射&gt;分区&gt;排序&gt;聚合&gt;shuffle&gt;reduce</p><p>1）<strong>Map()阶段</strong></p><ul><li><p>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数</p></li><li><p>重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出</p></li><li><p>对输出的key、value进行分区</p></li><li><p>对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中</p></li></ul><p>2）<strong>Reduce阶段</strong></p><ul><li>多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上</li><li>对多个map的输出进行合并、排序。</li><li>重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出</li><li>把reduce的输出保存到文件中</li></ul><p><strong>特别说明：</strong></p><p><strong>切片</strong> 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理</p><p><strong>分词</strong>，<strong>映射</strong>，<strong>分区</strong>，<strong>排序</strong>，<strong>聚合</strong> 都属map阶段</p><p><strong>混洗</strong>  横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段</p><p><strong>规约</strong> 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义</p><p>分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容</p><p><strong>补充</strong></p><p><strong>切片：</strong>HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</p><p><strong>Map任务的数量</strong>：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。<strong>大多数情况下，理想的分片大小是一个HDFS块</strong></p><p><strong>Reduce任务的数量：</strong> <strong>最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</strong> 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</p><h3 id="1-2-MapReduce中shuffle工作流程及优化"><a href="#1-2-MapReduce中shuffle工作流程及优化" class="headerlink" title="1.2 MapReduce中shuffle工作流程及优化"></a>1.2 MapReduce中shuffle工作流程及优化</h3><blockquote><p>shuffle主要功能是把map task的输出结果有效地传送到reduce端。</p><p>简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。</p><p>前奏：</p><p><strong>1.</strong> 在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。</p><p><strong>2.</strong> 在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。前面我们知道这个job有3个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。</p></blockquote><p>主要工作流程map端分区，排序，溢写，拷贝，reduce端合并</p><h4 id="1）Map端shuffle"><a href="#1）Map端shuffle" class="headerlink" title="1）Map端shuffle"></a>1）Map端shuffle</h4><p><img src="https://img-blog.csdnimg.cn/20190705232803270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190705232832585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><ul><li><p><strong>分区Partition</strong></p><p>MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。 </p></li><li><p><strong>写入内存缓冲区：</strong> 在我们的例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入<strong>内存缓冲区</strong>中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。 这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为<strong>Spill</strong>，中文可译为<strong>溢写</strong>，字面意思很直观。</p></li><li><p><strong>溢写Spill：</strong> 这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。</p></li><li><p><strong>排序Sort：</strong> 当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。 </p></li><li><p><strong>合并Map端</strong>：在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节在于，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。</p></li><li><p><strong>CombineReduce端</strong>：在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。 </p></li></ul><h4 id="2）Reduce端shuffle"><a href="#2）Reduce端shuffle" class="headerlink" title="2）Reduce端shuffle"></a>2）Reduce端shuffle</h4><p><strong>1.</strong> <strong>Copy过程</strong>，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 </p><p><strong>2.</strong> <strong>Merge阶段</strong>。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。</p><p>增加combiner，压缩溢写的文件。</p><p><strong>3.</strong> <strong>Reducer的输入文件</strong>。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。至于怎样才能让这个文件出现在内存中，之后的<a href="http://langyu.iteye.com/blog/1341267" target="_blank" rel="noopener">性能优化篇</a>我再说。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。 </p><p><img src="https://img-blog.csdnimg.cn/20190705233112741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190705233139761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc0MDY4MA==,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="3）shuffle优化"><a href="#3）shuffle优化" class="headerlink" title="3）shuffle优化"></a>3）shuffle优化</h4><ul><li><p><strong>压缩</strong>：对数据进行压缩，减少写读数据量；</p></li><li><p><strong>减少不必要的排序</strong>：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；</p></li><li><p><strong>内存化</strong>：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；</p><p>补充</p><blockquote><p><strong>1. Map端</strong></p><p><strong>1) io.sort.mb</strong></p><p>用于map输出排序的内存缓冲区大小</p><p>类型：Int</p><p>默认：100mb</p><p>备注：如果能估算map输出大小，就可以合理设置该值来尽可能<strong>减少溢出写的次数</strong>，这对调优很有帮助。</p><p><strong>2) io.sort.spill.percent</strong></p><p>map输出排序时的spill阀值（即使用比例达到该值时，将缓冲区中的内容spill 到磁盘）</p><p>类型：float</p><p>默认：0.80</p><p><strong>3) io.sort.factor</strong></p><p>归并因子（归并时的最多合并的流数），map、reduce阶段都要用到</p><p>类型：Int</p><p>默认：10</p><p>备注：将此值增加到100是很常见的。</p><p><strong>4) min.num.spills.for.combine</strong></p><p>运行combiner所需的最少溢出写文件数（如果已指定combiner）</p><p>类型：Int</p><p>默认：3</p><p><strong>5) mapred.compress.map.output</strong></p><p>map输出是否压缩</p><p>类型：Boolean</p><p>默认：false</p><p>备注：如果map输出的数据量非常大，那么在写入磁盘时压缩数据往往是个很好的主意，因为这样会让写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量。</p><p><strong>6) mapred.map.output.compression.codec</strong></p><p>用于map输出的压缩编解码器</p><p>类型：Classname</p><p>默认：org.apache.hadoop.io.compress.DefaultCodec</p><p>备注：推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的 TeraSort作业，运行时间减少60%，且明显快于Zlib压缩。</p><p><strong>7) tasktracker.http.threads</strong></p><p>每个tasktracker的工作线程数，用于将map输出到reducer。</p><p>（注：这是集群范围的设置，不能由单个作业设置）</p><p>类型：Int</p><p>默认：40</p><p>备注：tasktracker开http服务的线程数。用于reduce拉取map输出数据，大集群可以将其设为40~50。</p><p><strong>2. reduce端</strong></p><p><strong>1) mapred.reduce.slowstart.completed.maps</strong></p><p>调用reduce之前，map必须完成的最少比例</p><p>类型：float</p><p>默认：0.05</p><p><strong>2) mapred.reduce.parallel.copies</strong></p><p>reducer在copy阶段同时从mapper上拉取的文件数</p><p>类型：int</p><p>默认：5</p><p><strong>3) mapred.job.shuffle.input.buffer.percent</strong></p><p>在shuffle的复制阶段，分配给map输出的缓冲区占堆空间的百分比</p><p>类型：float</p><p>默认：0.70</p><p><strong>4) mapred.job.shuffle.merge.percent</strong></p><p>map输出缓冲区（由mapred.job.shuffle.input.buffer.percent定义）使用比例阀值，当达到此阀值，缓冲区中的数据将会被归并然后spill 到磁盘。</p><p>类型：float</p><p>默认：0.66</p><p><strong>5) mapred.inmem.merge.threshold</strong></p><p>map输出缓冲区中文件数</p><p>类型：int</p><p>默认：1000</p><p>备注：0或小于0的数意味着没有阀值限制，溢出写将有mapred.job.shuffle.merge.percent单独控制。</p><p><strong>6) mapred.job.reduce.input.buffer.percent</strong></p><p>在reduce过程中，在内存中保存map输出的空间占整个堆空间的比例。</p><p>类型：float</p><p>默认：0.0</p><p>备注：reduce阶段开始时，内存中的map输出大小不能大于该值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存。然而，如果reducer需要的内存较少，则可以增加此值来最小化访问磁盘的次数，以提高reduce性能。</p></blockquote></li></ul><h2 id="2-分布式文件系统HDFS"><a href="#2-分布式文件系统HDFS" class="headerlink" title="2. 分布式文件系统HDFS"></a>2. 分布式文件系统HDFS</h2><h3 id="2-1-HDFS-的体系架构和读写流程"><a href="#2-1-HDFS-的体系架构和读写流程" class="headerlink" title="2.1 HDFS 的体系架构和读写流程"></a>2.1 HDFS 的体系架构和读写流程</h3><h4 id="1）体系架构"><a href="#1）体系架构" class="headerlink" title="1）体系架构"></a>1）体系架构</h4><p><img src="https://img-blog.csdn.net/20161114151112205" alt="img"></p><p><strong>采用Master-Slaver模式：</strong></p><ul><li><p>NameNode中心服务器（Master）:维护文件系统树、以及整棵树内的文件目录、负责整个数据集群的管理。</p></li><li><p>DataNode分布在不同的机架上（Slaver）：在客户端或者NameNode的调度下，存储并检索数据块，并且定期向NameNode发送所存储的数据块的列表。</p></li><li><p>客户端与NameNode获取元数据；与DataNode交互获取数据。</p></li><li><p>默认情况下，每个DataNode都保存了3个副本，其中两个保存在同一个机架的两个不同的节点上。另一个副本放在不同机架上的节点上。</p></li></ul><blockquote><p>补充</p><p>基本概念</p><p>机架：HDFS集群，由分布在多个机架上的大量DataNode组成，不同机架之间节点通过交换机通信，HDFS通过机架感知策略，使NameNode能够确定每个DataNode所属的机架ID，使用副本存放策略，来改进数据的可靠性、可用性和网络带宽的利用率。</p><p>数据块(block)：HDFS最基本的存储单元，默认为64M，用户可以自行设置大小。</p><p>元数据：指HDFS文件系统中，文件和目录的属性信息。HDFS实现时，采用了 镜像文件（Fsimage） + 日志文件（EditLog）的备份机制。文件的镜像文件中内容包括：修改时间、访问时间、数据块大小、组成文件的数据块的存储位置信息。目录的镜像文件内容包括：修改时间、访问控制权限等信息。日志文件记录的是：HDFS的更新操作。</p><p>NameNode启动的时候，会将镜像文件和日志文件的内容在内存中合并。把内存中的元数据更新到最新状态。</p><p>用户数据：HDFS存储的大部分都是用户数据，以数据块的形式存放在DataNode上。</p><p>在HDFS中，NameNode 和 DataNode之间使用TCP协议进行通信。DataNode每3s向NameNode发送一个心跳。每10次心跳后，向NameNode发送一个数据块报告自己的信息，通过这些信息，NameNode能够重建元数据，并确保每个数据块有足够的副本。</p></blockquote><h4 id="2）HDFS文件读流程"><a href="#2）HDFS文件读流程" class="headerlink" title="2）HDFS文件读流程"></a>2）HDFS文件读流程</h4><p><img src="https://img-blog.csdn.net/20161110132352177?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p>（1）客户端通过调用<strong>FileSystem</strong>的<strong>open</strong>方法获取需要读取的数据文件，对<strong>HDFS</strong>来说该<strong>FileSystem</strong>就是<strong>DistributeFileSystem</strong></p><p>（2）<strong>DistributeFileSystem</strong>通过<strong>RPC</strong>来调用<strong>NameNode</strong>，获取到要读的数据文件对应的<strong>bock</strong>存储在哪些<strong>NataNode</strong>之上</p><p>（3）客户端先到最佳位置（距离最近）的<strong>DataNode</strong>上调用<strong>FSDataInputStream</strong>的<strong>read</strong>方法，通过反复调用<strong>read</strong>方法，可以将数据从<strong>DataNode</strong>传递到客户端</p><p>（4）当读取完所有的数据之后，<strong>FSDataInputStream</strong>会关闭与<strong>DataNode</strong>的连接，然后寻找下一块的最佳位置，客户端只需要读取连续的流。</p><p>（5）一旦客户端完成读取操作之后，就对<strong>FSDataInputStream</strong>调用<strong>close</strong>方法来完成资源的关闭操作</p><h4 id="3）HDFS文件写操作"><a href="#3）HDFS文件写操作" class="headerlink" title="3）HDFS文件写操作"></a>3）HDFS文件写操作</h4><p><img src="https://img-blog.csdn.net/20161110132117505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p>（1）客户端通过调用<strong>DistributeFileSystem</strong>的<strong>create</strong>方法来创建一个文件</p><p>（2）<strong>DistributeFileSystem</strong>会对<strong>NameNode</strong>发起<strong>RPC</strong>请求，在文件系统的名称空间中创建一个新的文件，此时会进行各种检查，比如：检查要创建的文件是否已经存在，如果该文件不存在，<strong>NameNode</strong>就会为该文件创建一条元数据记录</p><p>（3）客户端调用<strong>FSDataOututStream</strong>的<strong>write</strong>方法将数据写到一个内部队列中。假设副本数为3，那么将队列中的数据写到3个副本对应的存储的DataNode上。</p><p>（4）<strong>FSDataOututStream</strong>内部维护着一个确认队列，当接收到所有<strong>DataNode</strong>确认写完的消息后，数据才会从确认队列中删除</p><p>（5）当客户端完成数据的写入后，会对数据流调用close方法来关闭相关资源</p><p>补充</p><blockquote><p>写入过程客户端奔溃怎么处理？（租约恢复）</p></blockquote><h3 id="2-2-HDFS-常用操作命令"><a href="#2-2-HDFS-常用操作命令" class="headerlink" title="2.2 HDFS 常用操作命令"></a>2.2 HDFS 常用操作命令</h3><h4 id="1）查看文件常用命令"><a href="#1）查看文件常用命令" class="headerlink" title="1）查看文件常用命令"></a>1）查看文件常用命令</h4><ul><li>命令格式<br>1.hdfs dfs -ls path 查看文件列表<br>2.hdfs dfs -lsr path 递归查看文件列表<br>3.hdfs dfs -du path 查看path下的磁盘情况，单位字节</li><li>使用示例<br>1.hdfs dfs -ls / 查看当前目录<br>2.hdfs dfs - lsr / 递归查看当前目录</li></ul><h4 id="2）创建文件夹"><a href="#2）创建文件夹" class="headerlink" title="2）创建文件夹"></a>2）创建文件夹</h4><ul><li>命令格式<br>hdfs dfs -mkdir path</li><li>使用用例<br>hdfs dfs -mkdir /user/iron<br>注：该命令可递归创建文件夹，不可重复创建，在Linux文件系统中不可见</li></ul><h4 id="3）创建文件"><a href="#3）创建文件" class="headerlink" title="3）创建文件"></a>3）创建文件</h4><ul><li>命令格式<br>hdfs dfs -touchz path</li><li>使用用例<br>hdfs dfs -touchz /user/iron/iron.txt<br>注：该命令不可递归创建文件即当该文件的上级目录不存在时无法创建该文件，可重复创建但会覆盖原有的内容</li></ul><h4 id="4）复制文件和目录"><a href="#4）复制文件和目录" class="headerlink" title="4）复制文件和目录"></a>4）复制文件和目录</h4><ul><li>命令格式<br>hdfs dfs -cp 源目录 目标目录</li><li>使用用例<br>hdfs dfs -cp /user/iron /user/iron01<br>注：该命令会将源目录的整个目录结构都复制到目标目录中<br>hdfs dfs -cp /user/iron/* /user/iron01<br>注：该命令只会将源目录中的文件及其文件夹都复制到目标目录中</li></ul><h4 id="5）移动文件和目录"><a href="#5）移动文件和目录" class="headerlink" title="5）移动文件和目录"></a>5）移动文件和目录</h4><ul><li>命令格式<br>hdfs dfs -mv 源目录 目标目录</li><li>使用用例<br>hdfs dfs -mv /user/iron /user/iron01</li></ul><h4 id="6）赋予权限"><a href="#6）赋予权限" class="headerlink" title="6）赋予权限"></a>6）赋予权限</h4><ul><li>命令格式<br>hdfs dfs -chmod [权限参数][拥有者][:[组]] path</li><li>使用用例<br>hdfs dfs -chmod 777 /user/*<br>注：该命令是将user目录下的所用文件及其文件夹（不包含子文件夹中的文件）赋予最高权限：读，写，执行<br>777表示该用户，该用户的同组用户，其他用户都具有最高权限</li></ul><h4 id="7）上传文件"><a href="#7）上传文件" class="headerlink" title="7）上传文件"></a>7）上传文件</h4><ul><li>命令格式<br>hdfs dfs -put 源文件夹 目标文件夹</li><li>使用用例<br>hdfs dfs -put /home/hadoop01/iron /user/iron01<br>注：该命令上传Linux文件系统中iron整个文件夹<br>hdfs dfs -put /home/hadoop01/iron/* /user/iron01<br>注：该命令上传Linux文件系统中iron文件夹中的所有文件（不包括文件夹）<br>类似命令：<br>hdfs dfs -copyFromLocal 源文件夹 目标文件夹 作用同put<br>hdfs dfs -moveFromLocal 源文件夹 目标文件夹 上传后删除本地</li></ul><h4 id="8）下载文件"><a href="#8）下载文件" class="headerlink" title="8）下载文件"></a>8）下载文件</h4><ul><li>命令格式<br>hdfs dfs -get源文件夹 目标文件夹</li><li>使用用例<br>hdfs dfs -get /user/iron01 /home/hadoop01/iron<br>注：该命令下载hdfs文件系统中的iron01整个文件夹到Linux文件系统中<br>hdfs dfs -get /user/iron01/* /home/hadoop01/iron<br>注：该命令下载hdfs文件系统中的iron01整个文件夹到Linux文件系统中（不包含文件夹）<br>类似命令<br>hdfs dfs -copyToLocal 源文件夹 目标文件夹 作用同get<br>hdfs dfs -moveToLocal 源文件夹 目标文件夹 get后删除源文件</li></ul><h4 id="9）查看文件内容"><a href="#9）查看文件内容" class="headerlink" title="9）查看文件内容"></a>9）查看文件内容</h4><ul><li>命令格式<br>hadoop fs -cat path 从头查看这个文件<br>hadoop fs -tail path 从尾部查看最后1K</li><li>使用用例<br>hadoop fs -cat /userjzl/home/book/1.txt<br>hadoop fs -tail /userjzl/home/book/1.txt</li></ul><h4 id="10）删除文件"><a href="#10）删除文件" class="headerlink" title="10）删除文件"></a>10）删除文件</h4><ul><li>命令格式<br>hdfs dfs -rm 目标文件<br>hdfs dfs -rmr 目标文件 递归删除（慎用）</li><li>使用用例<br>hdfs dfs -rm /user/test.txt 删除test.txt文件<br>hdfs dfs -rmr /user/testdir 递归删除testdir文件夹<br>注：rm不可以删除文件夹</li></ul><h2 id="3-通用资源管理器Yarn"><a href="#3-通用资源管理器Yarn" class="headerlink" title="3. 通用资源管理器Yarn"></a>3. 通用资源管理器Yarn</h2><h3 id="3-1-Yarn-的产生背景和架构"><a href="#3-1-Yarn-的产生背景和架构" class="headerlink" title="3.1 Yarn 的产生背景和架构"></a>3.1 Yarn 的产生背景和架构</h3><h3 id="3-2-Yarn-中的角色划分和各自的作用"><a href="#3-2-Yarn-中的角色划分和各自的作用" class="headerlink" title="3.2 Yarn 中的角色划分和各自的作用"></a>3.2 Yarn 中的角色划分和各自的作用</h3><h3 id="3-3-Yarn-的配置和常用的资源调度策略"><a href="#3-3-Yarn-的配置和常用的资源调度策略" class="headerlink" title="3.3 Yarn 的配置和常用的资源调度策略"></a>3.3 Yarn 的配置和常用的资源调度策略</h3><h3 id="3-4-Yarn-进行一次任务资源调度的过程"><a href="#3-4-Yarn-进行一次任务资源调度的过程" class="headerlink" title="3.4 Yarn 进行一次任务资源调度的过程"></a>3.4 Yarn 进行一次任务资源调度的过程</h3><h2 id="4-数据仓库工具Hive"><a href="#4-数据仓库工具Hive" class="headerlink" title="4. 数据仓库工具Hive"></a>4. 数据仓库工具Hive</h2><h4 id="4-1-Hive-和普通关系型数据库的区别"><a href="#4-1-Hive-和普通关系型数据库的区别" class="headerlink" title="4.1 Hive 和普通关系型数据库的区别"></a>4.1 Hive 和普通关系型数据库的区别</h4><ul><li>Hive和关系型数据库存储文件的系统不同, Hive使用的是HDFS(Hadoop的分布式文件系统),关系型数据则是服务器本地的文件系统。</li><li>Hive使用的计算模型是MapReduce,而关系型数据库则是自己设计的计算模型.</li><li>关系型数据库都是为实时查询业务设计的,而Hive则是为海量数据做挖掘而设计的,实时性差;实时性的区别导致Hive的应用场景和关系型数据库有很大区别。</li><li>Hive很容易扩展自己的存储能力和计算能力,这几是继承Hadoop的,而关系型数据库在这方面要比Hive差很多。</li></ul><h4 id="4-2-Hive内部表和外部表的区别"><a href="#4-2-Hive内部表和外部表的区别" class="headerlink" title="4.2 Hive内部表和外部表的区别"></a>4.2 Hive内部表和外部表的区别</h4><ul><li><strong>创建表时</strong>：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径， 不对数据的位置做任何改变。</li><li><strong>删除表时</strong>：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</li></ul><h4 id="4-3-Hive分区表和分桶表的区别"><a href="#4-3-Hive分区表和分桶表的区别" class="headerlink" title="4.3 Hive分区表和分桶表的区别"></a>4.3 Hive分区表和分桶表的区别</h4><p><strong>分区</strong>在HDFS上的表现形式是一个<strong>目录</strong>， <strong>分桶</strong>是一个单独的<strong>文件</strong></p><p><strong>分区</strong>: 细化数据管理，直接读对应目录，缩小mapreduce程序要扫描的数据量</p><p><strong>分桶</strong>： 1、提高join查询的效率（用分桶字段做连接字段）2、提高采样的效率</p><h4 id="4-4-Hive-支持哪些数据格式"><a href="#4-4-Hive-支持哪些数据格式" class="headerlink" title="4.4 Hive 支持哪些数据格式"></a>4.4 Hive 支持哪些数据格式</h4><p>可支持Text，SequenceFile，ParquetFile，ORC，RCFILE等</p><p><strong>补充</strong></p><blockquote><ul><li><p><strong>TextFile：</strong> TextFile文件不支持块压缩，默认格式，数据不做压缩，磁盘开销大，数据解析开销大。这边不做深入介绍。 </p></li><li><p><strong>RCFile：</strong> Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能，但是不支持模式演进。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</p></li><li><p><strong>ORCFile</strong>： 存储方式：数据按行分块 每块按照列存储 ，压缩快 快速列存取，效率比rcfile高,是rcfile的改良版本，相比RC能够更好的压缩，能够更快的查询，但还是不支持模式演进。</p></li><li><p><strong>Parquet：</strong> Parquet能够很好的压缩，有很好的查询性能，支持有限的模式演进。但是写速度通常比较慢。这中文件格式主要是用在Cloudera Impala上面的。</p></li></ul></blockquote><p><strong>性能对比</strong></p><ul><li><strong>读操作</strong></li></ul><img src="https://img-blog.csdn.net/20180206102745607?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl6enh5Y2o=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:50%;"><ul><li><p><strong>存储效率</strong></p><img src="https://img-blog.csdn.net/20180206102837722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenl6enh5Y2o=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:50%;"></li></ul><h4 id="4-5-Hive元数据库作用及存储内容"><a href="#4-5-Hive元数据库作用及存储内容" class="headerlink" title="4.5 Hive元数据库作用及存储内容"></a>4.5 Hive元数据库作用及存储内容</h4><p>​     本质上只是用来存储hive中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。为数据库创建的目录一般在hive数据仓库目录下</p><h4 id="4-6-HiveSQL-支持的几种排序区别"><a href="#4-6-HiveSQL-支持的几种排序区别" class="headerlink" title="4.6 HiveSQL 支持的几种排序区别"></a>4.6 HiveSQL 支持的几种排序区别</h4><p><strong>1）Order By：全局排序，只有一个Reducer</strong></p><ul><li><p>使用 ORDER BY 子句排序</p><p>ASC（ascend）: 升序（默认）</p><p>DESC（descend）: 降序</p></li><li><p>ORDER BY 子句在SELECT语句的结尾</p></li><li><p>案例实操 </p><ul><li><p>查询员工信息按工资升序排列</p><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp <span class="token keyword">order</span> <span class="token keyword">by</span> sal<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>查询员工信息按工资降序排列</p><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp <span class="token keyword">order</span> <span class="token keyword">by</span> sal <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><p><strong>2）Sort By：每个MapReduce内部排序</strong><br>      Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ul><li>设置reduce个数</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>查看设置reduce个数</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>根据部门编号降序查看员工信息</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp sort <span class="token keyword">by</span> deptno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>将查询结果导入到文件中（按照部门编号降序排序）</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/opt/module/datas/sortby-result'</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp sort <span class="token keyword">by</span> deptno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>3）Distribute By：分区排序</strong><br>      Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>案例实操：</p><ul><li>先按照部门编号分区，再按照员工编号降序排序。</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/opt/module/datas/distribute-result'</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> deptno sort <span class="token keyword">by</span> empno <span class="token keyword">desc</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：</p><p>​    distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。<br>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br><strong>4）Cluster By</strong><br>​      当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><ul><li>以下两种写法等价</li></ul><pre class="line-numbers language-sql"><code class="language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp cluster <span class="token keyword">by</span> deptno<span class="token punctuation">;</span>hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> deptno sort <span class="token keyword">by</span> deptno<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</span><span class="token comment" spellcheck="true">--cluster by  ：sort by 和 distribute by的组合</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-7-Hive-的动态分区"><a href="#4-7-Hive-的动态分区" class="headerlink" title="4.7 Hive 的动态分区"></a>4.7 Hive 的动态分区</h4><blockquote><p>​        往hive分区表中插入数据时，如果需要创建的分区很多，比如以表中某个字段进行分区存储，则需要复制粘贴修改很多sql去执行，效率低。因为hive是批处理系统，<strong>所以hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。</strong></p></blockquote><ul><li><p><strong>使用动态分区表必须配置的参数</strong></p><ul><li><code>set hive.exec.dynamic.partition =true</code>（默认false）,表示开启动态分区功能；</li><li><code>set hive.exec.dynamic.partition.mode = nonstrict</code>(默认strict),表示允许所有分区都是动态的，否则必须有静态分区字段；</li></ul></li><li><p><strong>动态分区相关调优参数</strong></p><ul><li><code>set  hive.exec.max.dynamic.partitions.pernode=100</code> （默认100，一般可以设置大一点，比如1000）； 表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。</li><li><code>set hive.exec.max.dynamic.partitions =1000</code>(默认值) ；  表示一个动态分区语句可以创建的最大动态分区个数，超出报错；</li><li><code>set hive.exec.max.created.files =10000</code>(默认) 全局可以创建的最大文件个数，超出报错。</li></ul></li></ul><h4 id="4-8-Hive-MapJoin"><a href="#4-8-Hive-MapJoin" class="headerlink" title="4.8 Hive MapJoin"></a>4.8 Hive MapJoin</h4><blockquote><p>​     MapJoin是Hive的一种优化操作，其适用于小表JOIN大表的场景，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率</p></blockquote><p><strong>使用</strong></p><p><strong>方法一：</strong></p><p>在Hive0.11前，必须使用MAPJOIN来标记显示地启动该优化操作，由于其需要将小表加载进内存所以要注意小表的大小</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token comment" spellcheck="true">/*+ MAPJOIN(smalltable)*/</span>  <span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">,</span><span class="token keyword">value</span><span class="token keyword">FROM</span> smalltable <span class="token keyword">JOIN</span> bigtable <span class="token keyword">ON</span> smalltable<span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> bigtable<span class="token punctuation">.</span><span class="token keyword">key</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>方法二</strong>：</p><p>在Hive0.11后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机</p><pre class="line-numbers language-sql"><code class="language-sql">hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>默认值为true，自动开启MAPJOIN优化</p><pre class="line-numbers language-sql"><code class="language-sql">hive<span class="token punctuation">.</span>mapjoin<span class="token punctuation">.</span>smalltable<span class="token punctuation">.</span>filesize<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中 </p><p>注意：使用默认启动该优化的方式如果出现默名奇妙的BUG(比如MAPJOIN并不起作用),就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化</p><pre class="line-numbers language-shell"><code class="language-shell">hive.auto.convert.join=false(关闭自动MAPJOIN转换操作)hive.ignore.mapjoin.hint=false(不忽略MAPJOIN标记)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> 对于以下查询是不支持使用方法二(MAPJOIN标记)来启动该优化的</p><pre class="line-numbers language-shell"><code class="language-shell">select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM  ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM    bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                    ) firstjoin                                                              JOIN                                                                   smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo)  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是，如果使用的是方法一即没有MAPJOIN标记则以上查询语句将会被作为两个MJ执行，进一步的，如果预先知道表大小是能够被加载进内存的，则可以通过以下属性来将两个MJ合并成一个MJ</p><pre class="line-numbers language-shell"><code class="language-shell">hive.auto.convert.join.noconditionaltask：Hive在基于输入文件大小的前提下将普通JOIN转换成MapJoin，并是否将多个MJ合并成一个hive.auto.convert.join.noconditionaltask.size：多个MJ合并成一个MJ时，其表的总的大小须小于该值，同时hive.auto.convert.join.noconditionaltask必须为true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="4-9-HQL-和-SQL-有哪些常见的区别"><a href="#4-9-HQL-和-SQL-有哪些常见的区别" class="headerlink" title="4.9 HQL 和 SQL 有哪些常见的区别"></a>4.9 HQL 和 SQL 有哪些常见的区别</h4><ul><li><p><strong>总体一致</strong>：Hive-sql与SQL基本上一样，因为当初的设计目的，就是让会SQL不会编程MapReduce的也能使用Hadoop进行处理数据。</p></li><li><p><strong>区别：</strong>Hive没有delete和update。</p><ul><li><p><strong>Hive不支持等值连接</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--SQL中对两表内联可以写成：</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dual <span class="token number">a</span><span class="token punctuation">,</span>dual <span class="token number">b</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">--Hive中应为</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dual <span class="token number">a</span> <span class="token keyword">join</span> dual <span class="token number">b</span> <span class="token keyword">on</span> <span class="token number">a</span><span class="token punctuation">.</span><span class="token keyword">key</span> <span class="token operator">=</span> <span class="token number">b</span><span class="token punctuation">.</span><span class="token keyword">key</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--而不是传统的格式：</span><span class="token keyword">SELECT</span> t1<span class="token number">.a1</span> <span class="token keyword">as</span> <span class="token number">c1</span><span class="token punctuation">,</span> t2<span class="token number">.b1</span> <span class="token keyword">as</span> c2FROM t1<span class="token punctuation">,</span> t2<span class="token keyword">WHERE</span> t1<span class="token number">.a2</span> <span class="token operator">=</span> t2<span class="token number">.b2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>分号字符</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如：</span><span class="token keyword">select</span> concat<span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span>concat<span class="token punctuation">(</span><span class="token string">';'</span><span class="token punctuation">,</span><span class="token keyword">key</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> dual<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--但HiveQL在解析语句时提示：</span>FAILED: Parse Error: line <span class="token number">0</span>:<span class="token operator">-</span><span class="token number">1</span> mismatched input <span class="token string">'&lt;EOF>'</span> expecting <span class="token punctuation">)</span> <span class="token operator">in</span> <span class="token keyword">function</span> specification<span class="token comment" spellcheck="true">--解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成：</span><span class="token keyword">select</span> concat<span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span>concat<span class="token punctuation">(</span><span class="token string">'\073'</span><span class="token punctuation">,</span><span class="token keyword">key</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> dual<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>IS [NOT] NULL</strong></p><pre class="line-numbers language-sql"><code class="language-sql"> <span class="token comment" spellcheck="true">--SQL中null代表空值, 值得警惕的是, </span> <span class="token comment" spellcheck="true">--在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>Hive不支持将数据插入现有的表或分区中</strong>        </p></li><li><p><strong>hive不支持INSERT INTO 表 Values（）, UPDATE, DELETE操作</strong></p></li><li><p><strong>hive支持嵌入mapreduce程序，来处理复杂的逻辑</strong><br>如：</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">FROM</span> <span class="token punctuation">(</span> <span class="token number">1</span><span class="token punctuation">.</span> MAP doctext <span class="token keyword">USING</span> <span class="token string">'python wc_mapper.py'</span> <span class="token keyword">AS</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> cnt<span class="token punctuation">)</span> <span class="token number">2</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> docs <span class="token number">3</span><span class="token punctuation">.</span> CLUSTER <span class="token keyword">BY</span> word <span class="token number">4</span><span class="token punctuation">.</span> <span class="token punctuation">)</span> <span class="token number">a</span> <span class="token number">5</span><span class="token punctuation">.</span> REDUCE word<span class="token punctuation">,</span> cnt <span class="token keyword">USING</span> <span class="token string">'python wc_reduce.py'</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--doctext: 是输入</span><span class="token comment" spellcheck="true">--word, cnt: 是map程序的输出</span><span class="token comment" spellcheck="true">--CLUSTER BY: 将wordhash后，又作为reduce程序的输入并且map程序、reduce程序可以单独使用，如：</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> <span class="token punctuation">(</span> <span class="token number">2</span><span class="token punctuation">.</span> <span class="token keyword">FROM</span> session_table <span class="token number">3</span><span class="token punctuation">.</span> <span class="token keyword">SELECT</span> sessionid<span class="token punctuation">,</span> tstamp<span class="token punctuation">,</span> <span class="token keyword">data</span> <span class="token number">4</span><span class="token punctuation">.</span> DISTRIBUTE <span class="token keyword">BY</span> sessionid SORT <span class="token keyword">BY</span> tstamp <span class="token number">5</span><span class="token punctuation">.</span> <span class="token punctuation">)</span> <span class="token number">a</span> <span class="token number">6</span><span class="token punctuation">.</span> REDUCE sessionid<span class="token punctuation">,</span> tstamp<span class="token punctuation">,</span> <span class="token keyword">data</span> <span class="token keyword">USING</span> <span class="token string">'session_reducer.sh'</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--DISTRIBUTE BY: 用于给reduce程序分配行数据</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h4 id="4-10-Hive开窗函数"><a href="#4-10-Hive开窗函数" class="headerlink" title="4.10 Hive开窗函数"></a>4.10 Hive开窗函数</h4><p>假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。</p><p><img src="https://pic3.zhimg.com/80/v2-008682fd90478af4fb84e88bccd480ee_1440w.jpg" alt="img"></p><p><strong>1. SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">/*前三次贷款的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token number">3</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token keyword">CURRENT</span> <span class="token keyword">ROW</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv1<span class="token punctuation">,</span><span class="token comment" spellcheck="true">/*历史所有贷款 累加到下一次贷款 的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token keyword">UNBOUNDED</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token number">1</span> <span class="token keyword">FOLLOWING</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv2<span class="token keyword">FROM</span> loan <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​      其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。</p><p>窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用：</p><ul><li>n PRECEDING : 前n行    n preceding</li><li>n FOLLOWING：后n行</li><li>CURRENT ROW ： 当前行</li></ul><p>如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为:</p><p>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.</p><p>窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。</p><p><strong>2. NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#将数据按name切分成10区，并返回属于第几个分区</span>NTILE<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>ROW_NUMBER<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f3</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>DENSE_RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f4</span><span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中第一个函数<a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">NTILE(10)</a>是将数据按name切分成10区，并返回属于第几个分区。</p><blockquote><p>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p><p>然后可以根据桶号，选取前或后 n分之几的数据。</p></blockquote><p>后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。</p><ul><li>ROW_NUMBER()返回的是一列连续的序号。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-0f2c6da71227f7840aea5257acf8d88b_1440w.png" alt="img"></p><ul><li>RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}.</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f0bd3985ddacd2f347151dacc24cce_1440w.png" alt="img"></p><ul><li>DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-ac60db4d8a9c658ddf17fb43f09f616d_1440w.png" alt="img"></p><p><strong>3. LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取上一笔贷款的日期,缺失默认填NULL</span>LAG<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> last_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取下一笔贷款的日期,缺失指定填'1970-1-1'</span>LEAD<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'1970-1-1'</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> next_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取最早一笔贷款的日期</span>FIRST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> first_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取新一笔贷款的日期</span>LAST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> latest_dt<span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/pelifymeng2/article/details/70313943" target="_blank" rel="noopener">LAG(n)</a>将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录</p><p><strong>4. GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdateGROUPING SETS<span class="token punctuation">(</span>substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> orderdate<span class="token punctuation">)</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。</p><p>比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5.</p><p>如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null.</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdate<span class="token keyword">with</span> CUBE<span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdate<span class="token keyword">with ROLLUP</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。</p><h4 id="4-11-HiveUDF-UDAF-UDTF函数"><a href="#4-11-HiveUDF-UDAF-UDTF函数" class="headerlink" title="4.11 HiveUDF UDAF UDTF函数"></a>4.11 HiveUDF UDAF UDTF函数</h4><p><strong>UDF：一进一出</strong></p><p><strong>实现方法</strong>：</p><ol><li><p>继承UDF类</p></li><li><p>重写evaluate方法</p></li><li><p>将该java文件编译成jar</p></li><li><p>在终端输入如下命令：</p><pre class="line-numbers language-powershell"><code class="language-powershell">hive> add jar test<span class="token punctuation">.</span>jar<span class="token punctuation">;</span>hive> create temporary <span class="token keyword">function</span> function_name as <span class="token string">'com.hrj.hive.udf.UDFClass'</span><span class="token punctuation">;</span>hive> <span class="token function">select</span> function_name<span class="token punctuation">(</span>t<span class="token punctuation">.</span>col1<span class="token punctuation">)</span> <span class="token keyword">from</span> table t<span class="token punctuation">;</span>hive> drop temporary <span class="token keyword">function</span> function_name<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p><strong>UDAF：多进一出</strong></p><p><strong>实现方法</strong>: </p><ol><li><p>用户的UDAF必须继承了<code>org.apache.hadoop.hive.ql.exec.UDAF；</code></p></li><li><p>用户的UDAF必须包含至少一个实现了<code>org.apache.hadoop.hive.ql.exec</code>的静态类，诸如实现了 UDAFEvaluator</p></li><li><p>一个计算函数必须实现的5个方法的具体含义如下：</p><ul><li><strong>init()：</strong>主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。</li><li><strong>iterate()：</strong>每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。</li><li><strong>terminatePartial()：</strong>Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。</li><li><strong>merge()：</strong>Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。</li><li><strong>terminate()：</strong>Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。</li></ul></li><li><p>部分聚集结果的数据类型和最终结果的数据类型可以不同。</p></li></ol><p><strong>UDTF：一进多出</strong></p><p><strong>实现方法</strong>：</p><ol><li><p>继承<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</code></p></li><li><p><strong>initialize()</strong>：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）</p></li><li><p><strong>process：</strong>初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列      可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数</p></li><li><p>最后close()方法调用，对需要清理的方法进行清理</p></li></ol><h4 id="4-12-Hive数据倾斜问题"><a href="#4-12-Hive数据倾斜问题" class="headerlink" title="4.12 Hive数据倾斜问题"></a>4.12 Hive数据倾斜问题</h4><p><strong>0. 什么是数据倾斜</strong></p><blockquote><p>​        对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。</p><p>来源百度百科</p></blockquote><p>​        对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><p><strong>1. 数据倾斜的现象</strong></p><p>​       多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。</p><p><strong>2. 数据倾斜的影响</strong></p><p>1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：<strong>Out Of Memory</strong>。</p><p>2）<strong>运行速度慢</strong> ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。</p><p><img src="https://pic1.zhimg.com/80/v2-b26e15f4b1c3ce2f78fba64397b6fd60_1440w.jpg" alt="img"></p><p><strong><em>一个经验结论是：一般情况下，OOM的原因都是数据倾斜\</em></strong></p><p><strong>3. 如何定位数据倾斜</strong></p><p>​         数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p><p><strong>原因</strong>： 查看任务-》查看Stage-》查看代码</p><p>​        某个task执行特别慢的情况</p><p>​        某个task莫名其妙内存溢出的情况</p><p>​        查看导致数据倾斜的key的数据分布情况</p><p><img src="https://pic1.zhimg.com/80/v2-b1b26a9b5e6a1d68d9aea4d1f2bc551c_1440w.jpg" alt="img"></p><p>也可从以下几种情况考虑：</p><p>1、是不是有OOM情况出现，一般是少数内存溢出的问题</p><p>2、是不是应用运行时间差异很大，总体时间很长</p><p>3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题</p><p>4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断</p><p>5、看看代码里面是否有一些导致Shuffle的算子出现</p><p><strong>4. 数据倾斜的几种典型情况（重点）</strong></p><ul><li>数据源中的数据分布不均匀，Spark需要频繁交互</li><li>数据集中的不同Key由于分区方式，导致数据倾斜</li><li>JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</li><li>聚合操作中，数据集中的数据分布不均匀（主要）</li><li>JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</li><li>JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</li><li>数据集中少数几个key数据量很大，不重要，其他数据均匀</li></ul><p>注意：</p><ul><li><p>需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题</p></li><li><p>只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以</p></li></ul><p><strong>5. 数据倾斜的处理方法</strong></p><p>​         发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。</p><p><strong>5.1 检查数据，找出异常的key</strong></p><p>​          如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。</p><p>选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"key"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​        如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况:</p><ul><li><p>null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。</p></li><li><p>无效数据，大量重复的测试数据或是对结果影响不大的有效数据。</p></li><li><p>有效数据，业务导致的正常数据分布。</p></li></ul><p><strong>解决办法</strong><br>  第1，2种情况，直接对数据进行过滤即可。</p><p>  第3种情况则需要进行一些特殊操作，常见的有以下几种做法。</p><ul><li><p>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。</p></li><li><p>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</p></li><li><p>使用reduceByKey 代替 groupByKey</p></li><li><p>使用map join。</p><p><strong>举例</strong>：<br>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下：</p><p>将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>对数据进行 reduceByKey(func)<br>将 key + 随机值 转成 key<br>再对数据进行 reduceByKey(func)<br>tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。</p><p>tips2: 单独处理异常数据时，可以配合使用Map Join解决</p></li></ul><p><strong>5.1.1</strong> 数据源中的数据分布不均匀，Spark需要频繁交互</p><p><strong>解决方案</strong>1：避免数据源的数据倾斜</p><p><strong>实现原理</strong>：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive或者Kafka中还是会发生数据倾斜。</p><p><strong>适用情况</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>总结</strong>：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题</p><p><strong>5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜</strong></p><p><strong>解决方案1</strong>：调整并行度</p><p><strong>实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。</p><p><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。</p><p><img src="https://pic4.zhimg.com/80/v2-9a1722a9ceb6fe125f7b36715f6dcfff_1440w.jpg" alt="img"></p><p><strong>总结</strong>：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效</p><p><strong>解决方案2</strong>：</p><p><strong>缓解数据倾斜**</strong>（自定义Partitioner）**</p><p><strong>适用场景</strong>：大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p><p><strong>解决方案</strong>： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p><p><strong>优势</strong>： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong>： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><p><strong>5.2 检查Spark运行过程相关操作</strong></p><p><strong>5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</strong></p><p><strong>解决方案</strong>：Reduce side Join转变为Map side Join</p><p><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。</p><p><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。</p><p><strong>5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）</strong></p><p><strong>解决方案</strong>：两阶段聚合（局部聚合+全局聚合）</p><p><strong>适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案</p><p><strong>实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案</p><p>将相同key的数据分拆处理</p><p><img src="https://pic3.zhimg.com/80/v2-495a5fed7eb38db37d2f0bd13c45a30e_1440w.jpg" alt="img"></p><p><strong>5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</strong></p><p><strong>解决方案</strong>：为倾斜key增加随机前/后缀</p><p><strong>适用场景</strong>：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong>：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong>：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong>：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>注意</strong>：具有倾斜Key的RDD数据集中，key的数量比较少</p><p><img src="https://pic4.zhimg.com/80/v2-248b0cead5e9fb8a7b1cec840dd61b2f_1440w.jpg" alt="img"></p><p><strong>5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</strong></p><p><strong>解决方案</strong>：随机前缀和扩容RDD进行join</p><p><strong>适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。</p><p><strong>实现思路</strong>：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><p>注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀）</p><p><img src="https://pic4.zhimg.com/80/v2-fa2211e3a343d7b68e83bfe83d67f0cb_1440w.jpg" alt="img"></p><p><strong>5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀</strong></p><p><strong>解决方案</strong>：过滤少数倾斜Key</p><p><strong>适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h4 id="4-13-HiveSQL-的优化（系统参数调整、SQL-语句优化）"><a href="#4-13-HiveSQL-的优化（系统参数调整、SQL-语句优化）" class="headerlink" title="4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化）"></a>4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化）</h4><ul><li><p><strong>Hive优化目标</strong></p></li><li><ul><li>在有限的资源下，执行效率更高</li></ul></li><li><p>常见问题</p></li><li><ul><li>数据倾斜</li><li>map数设置</li><li>reduce数设置</li><li>其他</li></ul></li><li><p><strong>Hive执行</strong></p></li><li><ul><li><p>HQL –&gt; Job –&gt; Map/Reduce</p></li><li><p>执行计划</p></li><li><ul><li>explain [extended] hql</li><li>样例</li><li>select col,count(1) from test2 group by col;</li><li>explain select col,count(1) from test2 group by col;</li></ul></li></ul></li><li><p><strong>Hive表优化</strong></p></li><li><ul><li><p>分区</p></li><li><ul><li>set hive.exec.dynamic.partition=true;</li><li>set hive.exec.dynamic.partition.mode=nonstrict;</li><li>静态分区</li><li>动态分区</li></ul></li><li><p>分桶</p></li><li><ul><li>set hive.enforce.bucketing=true;</li><li>set hive.enforce.sorting=true;</li></ul></li><li><p>数据</p></li><li><ul><li>相同数据尽量聚集在一起</li></ul></li></ul></li><li><p><strong>Hive Job优化</strong></p></li><li><ul><li><p><strong>并行化执行</strong></p></li><li><ul><li>每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间</li><li>set hive.exec.parallel= true;</li><li>set hive.exec.parallel.thread.numbe=8;</li></ul></li><li><p><strong>本地化执行</strong></p></li><li><ul><li>job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB)</li><li>job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4)</li><li>job的reduce数必须为0或者1</li><li>set hive.exec.mode.local.auto=true;</li><li>当一个job满足如下条件才能真正使用本地模式:</li></ul></li><li><p><strong>job合并输入小文件</strong></p></li><li><ul><li>set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</li><li>合并文件数由mapred.max.split.size限制的大小决定</li></ul></li><li><p><strong>job合并输出小文件**</strong></p></li><li><ul><li>set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件</li><li>set hive.merge.size.per.task=64000000;合并之后的文件大小</li></ul></li><li><p><strong>JVM重利用</strong></p></li><li><ul><li>set mapred.job.reuse.jvm.num.tasks=20;</li><li>JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。</li></ul></li><li><p>压缩数据</p></li><li><ul><li>set hive.exec.compress.output=true;</li><li>set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec;</li><li>set mapred.output.compression.type=BLOCK;</li><li>set hive.exec.compress.intermediate=true;</li><li>set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</li><li>set hive.intermediate.compression.type=BLOCK;</li><li>中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式</li><li>hive查询最终的输出也可以压缩</li></ul></li></ul></li><li><p><strong>Hive Map优化</strong></p></li><li><ul><li><p>set mapred.map.tasks =10; 无效</p></li><li><p>(1)默认map个数</p></li><li><ul><li>default_num=total_size/block_size;</li></ul></li><li><p>(2)期望大小</p></li><li><ul><li>goal_num=mapred.map.tasks;</li></ul></li><li><p>(3)设置处理的文件大小</p></li><li><ul><li>split_size=max(mapred.min.split.size,block_size);</li><li>split_num=total_size/split_size;</li></ul></li><li><p>(4)计算的map个数</p></li><li><ul><li>compute_map_num=min(split_num,max(default_num,goal_num))</li></ul></li><li><p>经过以上的分析，在设置map个数的时候，可以简答的总结为以下几点：</p></li><li><ul><li>增大mapred.min.split.size的值</li><li>如果想增加map个数，则设置mapred.map.tasks为一个较大的值</li><li>如果想减小map个数，则设置mapred.min.split.size为一个较大的值</li><li>情况1：输入文件size巨大，但不是小文件</li><li>情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。</li></ul></li><li><p>map端聚合</p></li><li><ul><li>set hive.map.aggr=true;</li></ul></li><li><p>推测执行</p></li><li><ul><li>mapred.map.tasks.apeculative.execution</li></ul></li></ul></li><li><p><strong>Hive Shuffle优化</strong></p></li><li><ul><li><p><strong>Map端</strong></p></li><li><ul><li>io.sort.mb</li><li>io.sort.spill.percent</li><li>min.num.spill.for.combine</li><li>io.sort.factor</li><li>io.sort.record.percent</li></ul></li><li><p><strong>Reduce端</strong></p></li><li><ul><li>mapred.reduce.parallel.copies</li><li>mapred.reduce.copy.backoff</li><li>io.sort.factor</li><li>mapred.job.shuffle.input.buffer.percent</li><li>mapred.job.shuffle.input.buffer.percent</li><li>mapred.job.shuffle.input.buffer.percent</li></ul></li></ul></li><li><p><strong>Hive Reduce优化</strong></p></li><li><ul><li><p><strong>需要reduce操作的查询</strong></p></li><li><ul><li>group by,join,distribute by,cluster by…</li><li>order by比较特殊,只需要一个reduce</li><li>sum,count,distinct…</li><li>聚合函数</li><li>高级查询</li></ul></li><li><p><strong>推测执行</strong></p></li><li><ul><li>mapred.reduce.tasks.speculative.execution</li><li>hive.mapred.reduce.tasks.speculative.execution</li></ul></li><li><p><strong>Reduce优化</strong></p></li><li><ul><li>numRTasks = min[maxReducers,input.size/perReducer]</li><li>maxReducers=hive.exec.reducers.max</li><li>perReducer = hive.exec.reducers.bytes.per.reducer</li><li>hive.exec.reducers.max 默认 ：999</li><li>hive.exec.reducers.bytes.per.reducer 默认:1G</li><li>set mapred.reduce.tasks=10;直接设置</li><li>计算公式</li></ul></li></ul></li><li><p><strong>Hive查询操作优化</strong></p></li><li><p><strong>join优化</strong></p></li><li><ul><li>关联操作中有一张表非常小</li><li>不等值的链接操作</li><li>set hive.auto.current.join=true;</li><li>hive.mapjoin.smalltable.filesize默认值是25mb</li><li>select <code>/*+mapjoin(A)*/</code> f.a,f.b from A t join B f on (f.a=t.a)</li><li>hive.optimize.skewjoin=true;如果是Join过程出现倾斜，应该设置为true</li><li>set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化</li><li>mapjoin</li><li>简单总结下,mapjoin的使用场景:</li></ul></li><li><p><strong>Bucket join</strong></p></li><li><ul><li>两个表以相同方式划分桶</li><li>两个表的桶个数是倍数关系</li><li>crete table order(cid int,price float) clustered by(cid) into 32 buckets;</li><li>crete table customer(id int,first string) clustered by(id) into 32 buckets;</li><li>select price from order t join customer s on t.cid=s.id</li></ul></li><li><p><strong>join 优化前</strong></p></li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> m<span class="token punctuation">.</span>cid<span class="token punctuation">,</span>u<span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token keyword">order</span> m <span class="token keyword">join</span> customer u <span class="token keyword">on</span> m<span class="token punctuation">.</span>cid<span class="token operator">=</span>u<span class="token punctuation">.</span>id <span class="token keyword">where</span> m<span class="token punctuation">.</span>dt<span class="token operator">=</span><span class="token string">'2013-12-12'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><strong>join优化后</strong></li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> m<span class="token punctuation">.</span>cid<span class="token punctuation">,</span>u<span class="token punctuation">.</span>id <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> cid <span class="token keyword">from</span> <span class="token keyword">order</span> <span class="token keyword">where</span> dt<span class="token operator">=</span><span class="token string">'2013-12-12'</span><span class="token punctuation">)</span>m <span class="token keyword">join</span> customer u <span class="token keyword">on</span> m<span class="token punctuation">.</span>cid<span class="token operator">=</span>u<span class="token punctuation">.</span>id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p><strong>group by 优化</strong></p><p><code>hive.groupby.skewindata=true;</code>如果是group by 过程出现倾斜 应该设置为true</p><p><code>set hive.groupby.mapaggr.checkinterval=100000;-</code>-这个是group的键对应的记录条数超过这个值则会进行优化</p></li></ul><ul><li><p><strong>count distinct 优化</strong></p><ul><li><p>优化前</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> id<span class="token punctuation">)</span> <span class="token keyword">from</span> tablename<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>优化后</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token keyword">distinct</span> id <span class="token keyword">from</span> tablename<span class="token punctuation">)</span> tmp<span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> id <span class="token keyword">from</span> tablename <span class="token keyword">group</span> <span class="token keyword">by</span> id<span class="token punctuation">)</span> tmp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>优化前</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> <span class="token number">c</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> <span class="token number">d</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>优化后</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">c</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">d</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span><span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">0</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">c</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">0</span> <span class="token keyword">as</span> <span class="token number">b</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token number">d</span> <span class="token keyword">from</span> test <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">d</span> <span class="token keyword">union</span> <span class="token keyword">all</span> <span class="token keyword">select</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">c</span><span class="token punctuation">,</span><span class="token boolean">null</span> <span class="token keyword">as</span> <span class="token number">d</span> <span class="token keyword">from</span> test<span class="token punctuation">)</span>tmp1 <span class="token keyword">group</span> <span class="token keyword">by</span> <span class="token number">a</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><p>#二. Spark篇</p><h2 id="1-SparkCore"><a href="#1-SparkCore" class="headerlink" title="1. SparkCore"></a>1. SparkCore</h2><h3 id="1-1-Spark工作原理"><a href="#1-1-Spark工作原理" class="headerlink" title="1.1 Spark工作原理"></a>1.1 Spark工作原理</h3><h4 id="1-Spark是什么"><a href="#1-Spark是什么" class="headerlink" title="1. Spark是什么"></a>1. Spark是什么</h4><p>Spark是一种通用分布式并行计算框架。和Mapreduce最大不同就是spark是基于内存的迭代式计算。</p><blockquote><p>Spark的Job处理的中间输出结果可以保存在内存中，从而不再需要读写HDFS，除此之外，一个MapReduce 在计算过程中只有map 和reduce 两个阶段，处理之后就结束了，而在Spark的计算模型中，可以分为n阶段，因为它内存迭代式的，我们在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。 </p><p>因此<strong>Spark能更好地适用于数据挖掘与<a href="http://lib.csdn.net/base/machinelearning" target="_blank" rel="noopener">机器学习</a>等需要迭代的MapReduce的<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="noopener">算法</a></strong>。其<strong>不仅实现了MapReduce的算子map 函数和reduce函数及计算模型</strong>，还提供更为丰富的<strong>算子</strong>，如filter、join、groupByKey等。是一个用来实现快速而同用的集群计算的平台。 </p></blockquote><h4 id="2-Spark工作原理框图"><a href="#2-Spark工作原理框图" class="headerlink" title="2. Spark工作原理框图"></a>2. Spark工作原理框图</h4><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cimage-20200627101139297.png" alt="image-20200627101139297"></p><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cimage-20200627101150890.png" alt="image-20200627101150890"></p><h5 id="第一层级"><a href="#第一层级" class="headerlink" title="第一层级"></a>第一层级</h5><p>工作流程</p><p>a. 构建Spark Application的运行环境（启动SparkContext）</p><p>b. SparkContext在初始化过程中分别创建DAGScheduler作业调度和TaskScheduler任务调度两级调度模块</p><p>c. SparkContext向资源管理器（可以是Standalone、Mesos、Yarn）申请运行Executor资源；</p><p>d. 由资源管理器分配资源并启动StandaloneExecutorBackend，executor，之后向SparkContext申请Task；</p><p>e. DAGScheduler将job 划分为多个stage,并将Stage提交给TaskScheduler;</p><p>g. Task在Executor上运行，运行完毕释放所有资源。</p><h5 id="第二层级"><a href="#第二层级" class="headerlink" title="第二层级"></a>第二层级</h5><p><strong>DAGScheduler作业调度生成过程</strong></p><p><strong>DAGScheduler</strong>是一个面向stage 的作业调度器。</p><blockquote><p>作业调度模块是<strong>基于任务阶段</strong>的高层调度模块，它为每个Spark作业计算具有依赖关系的多个调度阶段（通常根据<strong>shuffle</strong>来划分），然后为每个阶段构建出一组具体的任务（通常会考虑数据的本地性等），然后以TaskSets（任务组）的形式提交给任务调度模块来具体执行。</p></blockquote><p><strong>主要三大功能</strong></p><ol><li><p><strong>接受用户提交的job</strong>。将job根据类型划分为不同的stage，记录哪些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</p></li><li><p><strong>决定每个task的最佳位置</strong>，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给<strong>TaskScheduler</strong>；</p></li><li><p><strong>重新提交shuffle输出丢失的stage给taskScheduler；</strong></p></li></ol><p><strong>DAG如何将Job划分为多个stage</strong></p><p><img src="https://img-blog.csdn.net/20180909161915933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="图解"></p><p><strong>划分依据：**</strong>宽窄依赖**。何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p><p><strong>切割规则：</strong>从后往前，遇到宽依赖就切割stage；</p><blockquote><p> Spark任务会根据<strong>RDD</strong>之间的依赖关系，形成一个<strong>DAG有向无环图</strong>，<strong>DAG</strong>会提交给<strong>DAGScheduler</strong>，<strong>DAGScheduler</strong>会把<strong>DAG</strong>划分相互依赖的多个<strong>stage</strong>，划分依据就是<strong>宽窄依赖</strong>，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以<strong>taskSet</strong>的形式提交给<strong>TaskScheduler</strong>运行，stage是由一组并行的task组成。  </p><p> <strong>一旦driver程序中出现action，就会生成一个job，比如count等</strong>，</p><p> ​     向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。</p><p> ​        每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是<strong>宽依赖</strong>，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p></blockquote><h5 id="第三层级"><a href="#第三层级" class="headerlink" title="第三层级"></a>第三层级</h5><p><strong>谈谈spark中的宽窄依赖</strong></p><p>RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="1-2-Spark的shuffle原理和过程"><a href="#1-2-Spark的shuffle原理和过程" class="headerlink" title="1.2 Spark的shuffle原理和过程"></a>1.2 Spark的shuffle原理和过程</h3><h5 id="Shuffle过程框图"><a href="#Shuffle过程框图" class="headerlink" title="Shuffle过程框图"></a>Shuffle过程框图</h5><p><img src="https://yqfile.alicdn.com/278d5baeff935b9f4339f94fc85ccab67241927b.png" alt="img"></p><p>主要逻辑如下：</p><blockquote><p>1）首先每一个MapTask会根据ReduceTask的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</p><p>2）其次MapTask产生的结果会根据设置的partition算法填充到每个bucket中。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中。</p><p>当ReduceTask启动时，它会根据自己task的id和所依赖的Mapper的id从远端或本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</p><p>这里的bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等。</p></blockquote><p>Spark shuffle可以分为两部分：Shuffle Write 和 Shuffle Fetch</p><h5 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h5><blockquote><p>由于不要求数据有序，shuffle write 的任务很简单：<strong>将数据 partition 好，并持久化</strong>。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</p></blockquote><p>shuffle write 的任务很简单，那么实现也很简单：将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。图示如下：</p><p><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/shuffle-write-no-consolidation.png" alt="shuffle write"></p><p>上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数），缓冲区被称为 bucket，其大小为<code>spark.shuffle.file.buffer.kb</code> ，默认是 32KB（Spark 1.1 版本以前是 100KB）。</p><blockquote><p>其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。</p></blockquote><p>ShuffleMapTask 的执行过程很简单：先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由<code>partitioner.partition(record.getKey()))</code>决定。每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 <strong>FileSegment</strong>。之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。</p><p>这样的实现很简单，但有几个问题：</p><ol><li><strong>产生的 FileSegment 过多。</strong>每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。</li><li><strong>缓冲区占用内存空间大。</strong>每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M <em>R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores</em> R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了<code>cores * R * 32 KB</code>。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。</li></ol><p>目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图：</p><p><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/shuffle-write-consolidation.png" alt="shuffle-write-consolidation"></p><p>可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。先执行完的 ShuffleMapTask 形成 ShuffleBlocki，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlocki’，每个 ShuffleBlock 被称为 <strong>FileSegment</strong>。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过<code>spark.shuffle.consolidateFiles=true</code>来开启。</p><h5 id="Shuffle-Fetch"><a href="#Shuffle-Fetch" class="headerlink" title="Shuffle Fetch"></a>Shuffle Fetch</h5><p>先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey：<br><img src="https://spark-internals.books.yourtion.com/markdown/PNGfigures/reduceByKeyStage.png" alt="reduceByKey"></p><p>很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了：</p><ul><li>在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？</li><li>边 fetch 边处理还是一次性 fetch 完再处理？</li><li>fetch 来的数据存放到哪里？</li><li>怎么获得要 fetch 的数据的存放位置？</li></ul><blockquote><ul><li><p><strong>在什么时候 fetch？</strong>当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。理论上讲，一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 <code>spark.reducer.maxMbInFlight</code>，这里用 <strong>softBuffer</strong> 表示，默认大小为 48MB。一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。</p></li><li><p><strong>边 fetch 边处理还是一次性 fetch 完再处理？</strong>边 fetch 边处理。本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。<strong>那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？</strong>答案是使用可以 aggregate 的数据结构，比如 HashMap。每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 \ record，直接将其放进 HashMap 里面。如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 <code>func(hashMap.get(Key), Value)</code>，比如上面 WordCount 例子中的 func 就是 <code>hashMap.get(Key) ＋ Value</code>，并将 func 的结果重新 put(key) 到 HashMap 中去。这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。</p><pre class="line-numbers language-java"><code class="language-java">  <span class="token comment" spellcheck="true">// MapReduce</span>  <span class="token function">reduce</span><span class="token punctuation">(</span>K key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>V<span class="token operator">></span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>       result <span class="token operator">=</span> <span class="token function">process</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> values<span class="token punctuation">)</span>      <span class="token keyword">return</span> result      <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">// Spark</span>  <span class="token function">reduce</span><span class="token punctuation">(</span>K key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>V<span class="token operator">></span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>      result <span class="token operator">=</span> null       <span class="token keyword">for</span> <span class="token punctuation">(</span>V value <span class="token operator">:</span> values<span class="token punctuation">)</span>           result  <span class="token operator">=</span> <span class="token function">func</span><span class="token punctuation">(</span>result<span class="token punctuation">,</span> value<span class="token punctuation">)</span>      <span class="token keyword">return</span> result  <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。因此一些算法比如求平均数，在 process 里面很好实现，直接<code>sum(values)/values.length</code>，而在 Spark 中 func 可以实现<code>sum(values)</code>，但不好实现<code>/values.length</code>。更多的 func 将会在下面的章节细致分析。</p></li><li><p><strong>fetch 来的数据存放到哪里？</strong>刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。如果<code>spark.shuffle.spill = false</code>就只用内存。内存使用的是<code>AppendOnlyMap</code> ，类似 Java 的<code>HashMap</code>，内存＋磁盘使用的是<code>ExternalAppendOnlyMap</code>，如果内存空间不足时，<code>ExternalAppendOnlyMap</code>可以将 \ records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。<strong>使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？</strong>在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。</p></li><li><p><strong>怎么获得要 fetch 的数据的存放位置？</strong>在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 <code>MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)</code>，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。</p></li></ul></blockquote><h3 id="1-3-Spark的Stage划分及优化"><a href="#1-3-Spark的Stage划分及优化" class="headerlink" title="1.3 Spark的Stage划分及优化"></a>1.3 Spark的Stage划分及优化</h3><blockquote><p>窄依赖指父RDD的每一个分区最多被一个子RDD的分区所用，表现为</p><p> 一个父RDD的分区对应于一个子RDD的分区<br> 两个父RDD的分区对应于一个子RDD 的分区。<br> 宽依赖指子RDD的每个分区都要依赖于父RDD的所有分区，这是shuffle类操作</p></blockquote><p><strong>Stage:</strong></p><blockquote><p>一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。Stage的划分，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。</p><p>比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；</p><p>如果是rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage.</p><p>会根据RDD之间的依赖关系将DAG图划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中</p></blockquote><p><strong>Stage划分思路</strong></p><p>因此spark划分stage的整体思路是：<strong>从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。</strong></p><p>在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！</p><p>而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。</p><p><strong>总结</strong></p><p>map,filter为窄依赖，<br> groupbykey为宽依赖<br> 遇到一个宽依赖就分一个stage</p><h3 id="1-4-Spark和MapReduce的区别"><a href="#1-4-Spark和MapReduce的区别" class="headerlink" title="1.4 Spark和MapReduce的区别"></a>1.4 Spark和MapReduce的区别</h3><p><strong>整体对比概念</strong></p><p>Spark Shuffle 与MapReduce Shuffle的设计思想相同，但是实现细节优化方式不同。</p><blockquote><p><strong>1. 从逻辑角度来讲</strong>，Shuffle 过程就是一个 GroupByKey 的过程，两者没有本质区别。<br>只是 MapReduce 为了方便 GroupBy 存在于不同 partition 中的 key/value records，就<u>提前对 key 进行排序</u>。Spark 认为很多应用不需要对 key 排序，就默认没有在 GroupBy 的过程中对 key 排序。</p><p><strong>2. 从数据流角度讲，两者有差别。</strong><br>  MapReduce 只能从一个 Map Stage shuffle 数据，Spark 可以从多个 Map Stages shuffle 数据</p><p><strong>3 .Shuffle write/read 实现上有一些区别。</strong><br>   以前对 shuffle write/read 的分类是 <strong>sort-based</strong> 和 <strong>hash-based</strong>。MapReduce 可以说是 <strong>sort-based</strong>，shuffle write 和 shuffle read 过程都是基于key sorting 的 (buffering records + in-memory sort + on-disk external sorting)。早期的 Spark 是 hash-based，shuffle write 和 shuffle read 都使用 HashMap-like 的数据结构进行 aggregate (without key sorting)。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based (only sort partition id, without key sorting)，shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经“你中有我，我中有你”，界限已经不那么清晰。</p><p><strong>4. 从数据 fetch 与数据计算的重叠粒度来讲，两者有细微区别。</strong><br>   MapReduce 是<strong>粗粒度</strong>，reducer fetch 到的 records 先被放到 <code>shuffle buffer</code> 中休息，当 shuffle buffer 快满时，才对它们进行 combine()。而 Spark 是<strong>细粒度</strong>，可以即时将 fetch 到的 record 与 HashMap 中相同 key 的 record 进行 aggregate。</p></blockquote><blockquote><p><img src="https://pic4.zhimg.com/b5a8d3294a7c99f065896fee00f910e4_r.jpg" alt="img"></p><p>解说：<br>1、MapReduce在Map阶段完成之后数据会被写入到内存中的一个环形缓冲区（后续的分区/分组/排序在这里完成）；Spark的Map阶段完成之后直接输出到磁盘。<br>2、受第一步的影响，MapReduce输出的数据是有序的（针对单个Map数据来说）；Spark的数据是无序的（可以使用RDD算子达到排序的效果）。<br>3、MapReduce缓冲区的数据处理完之后会spill到磁盘形成一个文件，文件数量达到阈值之后将会进行merge操作，将多个小文件合并为一个大文件；Spark没有merge过程，一个Map中如果有对应多个Reduce的数据，则直接写多个磁盘文件。<br>4、MapReduce全部通过网络来获得数据；对于本地数据Spark可以直接读取</p></blockquote><h3 id="1-5-宽依赖与窄依赖区别"><a href="#1-5-宽依赖与窄依赖区别" class="headerlink" title="1.5 宽依赖与窄依赖区别"></a>1.5 宽依赖与窄依赖区别</h3><p>RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="1-6-Spark-RDD-原理"><a href="#1-6-Spark-RDD-原理" class="headerlink" title="1.6 Spark RDD 原理"></a>1.6 Spark RDD 原理</h3><h4 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1. RDD是什么"></a>1. RDD是什么</h4><blockquote><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合</p><ul><li><strong>Dataset</strong>：就是一个集合，用于存放数据的</li><li><strong>Destributed</strong>：分布式，可以并行在集群计算</li><li><strong>Resilient</strong>：表示弹性的，弹性表示<ul><li>RDD中的数据可以存储在内存或者磁盘中；</li><li>RDD中的分区是可以改变的；</li></ul></li></ul></blockquote><ol><li><strong>A list of partitions</strong>：一个分区列表，RDD中的数据都存储在一个分区列表中</li><li><strong>A function for computing each split</strong>：作用在每一个分区中的函数</li><li><strong>A list of dependencies on other RDDs</strong>：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的</li><li><strong>Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)</strong>：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向</li><li>可选项，数据本地性，数据位置最优</li></ol><h4 id="2-RDD操作"><a href="#2-RDD操作" class="headerlink" title="2. RDD操作"></a>2. RDD操作</h4><p>​        RDD创建后就可以在RDD上进行数据处理。RDD支持两种操作：<strong>转换</strong>（transformation），即从现有的数据集创建一个新的数据集；<strong>动作</strong>（action），即在数据集上进行计算后，返回一个值给Driver程序。</p><ul><li><strong>转换</strong>（transformation）</li></ul><blockquote><p>​        RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。</p><p>​        RDD中所有的Transformation都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的Action时，这些Transformation才会真正运行。</p></blockquote><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#### map(func)**</span>返回一个新的分布式数据集，该数据集由每一个输入元素经过func函数转换后组成<span class="token comment" spellcheck="true">#### **fitler(func)**</span>返回一个新的数据集，该数据集由经过func函数计算后返回值为true的输入元素组成<span class="token comment" spellcheck="true">#### **flatMap(func)**</span>类似于map，但是每一个输入元素可以被映射为<span class="token number">0</span>或多个输出元素（因此func返回一个序列，而不是单一元素）<span class="token comment" spellcheck="true">#### **mapPartitions(func)**</span>类似于map，但独立地在RDD上每一个分片上运行，因此在类型为T的RDD上运行时，func函数类型必须是Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token operator">=</span><span class="token operator">></span>Iterator<span class="token punctuation">[</span>U<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#### **mapPartitionsWithSplit(func)**</span>类似于mapPartitons，但func带有一个整数参数表示分片的索引值。因此在类型为T的RDD上运行时，func函数类型必须是<span class="token punctuation">(</span>Int<span class="token punctuation">,</span>Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">></span>Iterator<span class="token punctuation">[</span>U<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#### **sample(withReplacement,fraction,seed)**</span>根据fraction指定的比例对数据进行采样，可以选择是否用随机数进行替换，seed用于随机数生成器种子<span class="token comment" spellcheck="true">#### **union(otherDataSet)**</span>返回一个新数据集，新数据集是由原数据集和参数数据集联合而成<span class="token comment" spellcheck="true">#### **distinct([numTasks])**</span>返回一个包含原数据集中所有不重复元素的新数据集<span class="token comment" spellcheck="true">#### **groupByKey([numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">)</span>对的数据集。注意默认情况下，只有<span class="token number">8</span>个并行任务来操作，但是可以传入一个可选的numTasks参数来改变它<span class="token comment" spellcheck="true">#### **reduceByKey(func,[numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。与groupByKey类似，reduceByKey任务的个数是可以通过第二个可选参数来设置的<span class="token comment" spellcheck="true">#### **sortByKey([[ascending],numTasks])**</span>在一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>对数据集。升序或降序由ascending布尔参数决定<span class="token comment" spellcheck="true">#### **join(otherDataset0,[numTasks])**</span>在类型为<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>和<span class="token punctuation">(</span>K<span class="token punctuation">,</span>W<span class="token punctuation">)</span>数据集上调用，返回一个相同的key对应的所有元素在一起的<span class="token punctuation">(</span>K<span class="token punctuation">,</span><span class="token punctuation">(</span>V<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">)</span>数据集<span class="token comment" spellcheck="true">#### **cogroup(otherDataset,[numTasks])**</span>在类型为<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>和<span class="token punctuation">(</span>K<span class="token punctuation">,</span>W<span class="token punctuation">)</span>数据集上调用，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>V<span class="token punctuation">]</span><span class="token punctuation">,</span>Seq<span class="token punctuation">[</span>W<span class="token punctuation">]</span><span class="token punctuation">)</span>元祖的数据集。这个操作也可以称为groupwith<span class="token comment" spellcheck="true">#### **cartesain(ohterDataset)**</span>笛卡尔积，在类型为T和U类型的数据集上调用，返回一个<span class="token punctuation">(</span>T<span class="token punctuation">,</span>U<span class="token punctuation">)</span>对数据集<span class="token punctuation">(</span>两两的元素对<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>动作</strong>（action）</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#### **reduce(func)**</span>通过函数func<span class="token punctuation">(</span>接收两个参数，返回一个参数<span class="token punctuation">)</span>聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的并行运行<span class="token comment" spellcheck="true">#### **collect()**</span>在驱动程序中，以数组形式返回数据集中的所有元素。通常在使用filter或者其他操作返回一个足够小的数据子集后再使用会比较有用<span class="token comment" spellcheck="true">#### **count()**</span>返回数据集元素个数<span class="token comment" spellcheck="true">#### **first()**</span>返回数据集第一个元素<span class="token punctuation">(</span>类似于take<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#### **take(n)**</span>返回一个由数据集前n个元素组成的数组注意 这个操作目前并非并行执行，而是由驱动程序计算所有的元素<span class="token comment" spellcheck="true">#### **takeSample(withReplacement,num,seed)**</span>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否由随机数替换不足的部分，seed用户指定随机数生成器种子<span class="token comment" spellcheck="true">#### **saveAsTextFile(path)**</span>将数据集的元素以textfile的形式保存到本地文件系统—HDFS或者任何其他Hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行<span class="token comment" spellcheck="true">#### **saveAsSequenceFile(path)**</span>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是本地系统、HDFS或者任何其他的Hadoop支持的文件系统。这个只限于由key<span class="token operator">-</span>value对组成，并实现了Hadoop的Writable接口，或者可以隐式的转换为Writable的RDD<span class="token punctuation">(</span>Spark包括了基本类型转换，例如Int、Double、String等<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#### **countByKey()**</span>对<span class="token punctuation">(</span>K<span class="token punctuation">,</span>V<span class="token punctuation">)</span>类型的RDD有效，返回一个<span class="token punctuation">(</span>K<span class="token punctuation">,</span>Int<span class="token punctuation">)</span>对的map，表示每一个key对应的元素个数<span class="token comment" spellcheck="true">#### **foreach(func)**</span>在数据集的每一个元素上，运行函数func进行更新。通常用于边缘效果，例如更新一个叠加器，或者和外部存储系统进行交互，如HBase<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-RDD共享变量"><a href="#3-RDD共享变量" class="headerlink" title="3. RDD共享变量"></a>3. RDD共享变量</h4><p>在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：<strong>广播变量和累加器</strong>。</p><p>(1). <strong>广播变量（Broadcast Variables）</strong></p><p>– 广播变量缓存到各个节点的内存中，而不是每个 Task</p><p>– 广播变量被创建后，能在集群中运行的任何函数调用</p><p>– 广播变量是只读的，不能在被广播后修改</p><p>– 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本</p><p>val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量<br>(2). <strong>累加器</strong></p><p>​    累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值。</p><h4 id="4-RDD缓存"><a href="#4-RDD缓存" class="headerlink" title="4. RDD缓存"></a>4. RDD缓存</h4><p>Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 <strong>transformation</strong>自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。</p><p>​         Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。</p><p>​          Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li><p>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</p></li><li><p>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p></li><li><p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p></li><li><p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p></li><li><p>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</p></li><li><p>在不会使用cached RDD的时候，及时使用unpersist方法来释放它。</p></li></ul><h3 id="1-7-RDD有哪几种创建方式"><a href="#1-7-RDD有哪几种创建方式" class="headerlink" title="1.7 RDD有哪几种创建方式"></a>1.7 RDD有哪几种创建方式</h3><p>1) 使用程序中的集合创建rdd<br>2) 使用本地文件系统创建rdd<br>3) 使用hdfs创建rdd，<br>4) 基于数据库db创建rdd<br>5) 基于Nosql创建rdd，如hbase<br>6) 基于s3创建rdd，<br>7) 基于数据流，如socket创建rdd</p><h3 id="1-8-Spark的RDD-DataFrame和DataSet的区别"><a href="#1-8-Spark的RDD-DataFrame和DataSet的区别" class="headerlink" title="1.8 Spark的RDD DataFrame和DataSet的区别"></a>1.8 Spark的RDD DataFrame和DataSet的区别</h3><p><strong>RDD的优点：</strong></p><ol><li>相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。</li><li>面向对象编程，直接存储的java对象，类型转化也安全</li></ol><p><strong>RDD的缺点：</strong></p><ol><li>由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦</li><li>默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁</li></ol><p><strong>DataFrame的优点：</strong></p><ol><li><p>结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表</p></li><li><p>有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。</p></li><li><p>hive兼容，支持hql、udf等</p></li></ol><p><strong>DataFrame的缺点：</strong></p><ol><li>编译时不能类型转化安全检查，运行时才能确定是否有问题</li><li>对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象</li></ol><p><strong>DateSet的优点：</strong></p><ol><li><p>DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据</p></li><li><p>和RDD一样，支持自定义对象存储</p></li><li><p>和DataFrame一样，支持结构化数据的sql查询</p></li><li><p>采用堆外内存存储，gc友好</p></li><li><p>类型转化安全，代码友好</p><p>如此回答有3个坑（容易引起面试官追问）：</p><p>1）Spark shuffle 与 MapReduce shuffle（或者Spark 与 MR 的区别）</p><p>2）Spark内存模型 </p><p>3）对gc（垃圾回收）的了解</p></li></ol><h3 id="1-10-Spark-的通信机制"><a href="#1-10-Spark-的通信机制" class="headerlink" title="1.10 Spark 的通信机制"></a>1.10 Spark 的通信机制</h3><h4 id="分布式的通信方式"><a href="#分布式的通信方式" class="headerlink" title="分布式的通信方式"></a>分布式的通信方式</h4><ul><li>RPC</li><li>RMI</li><li>JMS</li><li>EJB</li><li>Web Serivice</li></ul><h4 id="通信框架Akka"><a href="#通信框架Akka" class="headerlink" title="通信框架Akka"></a>通信框架Akka</h4><p>​       Hadoop MR中的计算框架，jobTracker和TaskTracker间是由于通过<strong>heartbeat</strong>的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色的高效的<strong>Akka</strong>和<strong>netty</strong>通信系统</p><h3 id="1-11-Spark的数据容错机制"><a href="#1-11-Spark的数据容错机制" class="headerlink" title="1.11 Spark的数据容错机制"></a>1.11 Spark的数据容错机制</h3><p>一般而言，对于分布式系统，数据集的容错性通常有两种方式：</p><p>1） <strong>数据检查点（在Spark中对应Checkpoint机制）</strong>。</p><p>2） <strong>记录数据的更新（在Spark中对应Lineage血统机制</strong>）。</p><p>对于大数据分析而言，数据检查点操作成本较高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低，同时会消耗大量存储资源。</p><p>Spark选择记录更新的方式。但更新粒度过细时，记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列记录下来，以便恢复丢失的分区。</p><h4 id="Lineage（血统）机制"><a href="#Lineage（血统）机制" class="headerlink" title="Lineage（血统）机制"></a>Lineage（血统）机制</h4><p>​      每个RDD除了包含分区信息外，还包含它从父辈RDD变换过来的步骤，以及如何重建某一块数据的信息，因此RDD的这种容错机制又称“血统”（Lineage）容错。Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做以便恢复数据。</p><p>​       相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新计算和恢复丢失的数据分区。但这种数据模型粒度较粗，因此限制了Spark的应用场景。所以可以说Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能方面的提升。</p><p>​      RDD在Lineage容错方面采用如下两种依赖来保证容错方面的性能：</p><p><strong>窄依赖（Narrow Dependeny）</strong>：窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。其中，1个父RDD分区对应1个子RDD分区，可以分为如下两种情况：</p><p> 子RDD分区与父RDD分区一一对应（如map、filter等算子）。一个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。</p><p><strong>宽依赖（Wide Dependency，源码中称为Shuffle Dependency）：</strong></p><p>宽依赖是指一个父RDD分区对应多个子RDD分区，可以分为如下两种情况：</p><p>一个父RDD对应所有子RDD分区（未经协同划分的Join）。</p><p>一个父RDD对应多个RDD分区（非全部分区）（如groupByKey）。</p><p>窄依赖与宽依赖关系如图3-10所示。</p><p>从图3-10可以看出对依赖类型的划分：根据父RDD分区是对应一个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，对于需要重新计算的子分区而言，只需要父分区的一部分数据，因此其余数据的重算就导致了冗余计算。</p><p><img src="https://yqfile.alicdn.com/f319c81b2aaeb9f7b29dfeff3ef1cd19ec64ca9b.png" alt="f319c81b2aaeb9f7b29dfeff3ef1cd19ec64ca9b"></p><p>图3-10　两种依赖关系</p><p>对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，在通过重新计算恢复数据的情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。</p><p>窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点（前面调度机制中已讲过）。</p><p>依赖关系在lineage容错中的应用总结如下：</p><p>1）窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成，并且父RDD的计算结果进行hash并传到对应节点上之后，才能计算子RDD。</p><p>2）数据丢失时，对于窄依赖，只需要重新计算丢失的那一块数据来恢复；对于宽依赖，则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖时，需要在适当的时机设置数据检查点（checkpoint机制在下节讲述）。可见Spark在容错性方面要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。</p><p>在Spark容错机制中，如果一个节点宕机了，而且运算属于窄依赖，则只要重算丢失的父RDD分区即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。更深入地来说：在窄依赖关系中，当子RDD的分区丢失，重算其父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，因此不存在冗余计算。而在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区使用，其中有一部分数据对应的是其他不需要重新计算的子RDD分区中的数据，因此在宽依赖关系下，这样计算就会产生冗余开销，这也是宽依赖开销更大的原因。为了减少这种冗余开销，通常在Lineage血统链比较长，并且含有宽依赖关系的容错中使用Checkpoint机制设置检查点。</p><h4 id="Checkpoint（检查点）机制"><a href="#Checkpoint（检查点）机制" class="headerlink" title="Checkpoint（检查点）机制"></a>Checkpoint（检查点）机制</h4><p>通过上述分析可以看出Checkpoint的本质是将RDD写入Disk来作为检查点。这种做法是为了通过lineage血统做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p><h3 id="1-13-Spark性能调优"><a href="#1-13-Spark性能调优" class="headerlink" title="1.13 Spark性能调优"></a>1.13 Spark性能调优</h3><h4 id="1-常用参数说明"><a href="#1-常用参数说明" class="headerlink" title="1) 常用参数说明"></a>1) 常用参数说明</h4><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可</span><span class="token comment" spellcheck="true">--executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时处理大批量数据时容易内存不足，再多申请一点，如6G</span><span class="token comment" spellcheck="true">--num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等</span><span class="token comment" spellcheck="true">--executor-cores 2  : 每个executor内的核数，即每个executor中的任务task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce任务的并行度是executor数目*executor中的任务数</span>yarn集群中一般有资源申请上限，如，executor<span class="token operator">-</span>memory<span class="token operator">*</span>num<span class="token operator">-</span>executors <span class="token operator">&lt;</span> 400G 等，所以调试参数时要注意这一点—<span class="token operator">-</span>spark<span class="token punctuation">.</span><span class="token keyword">default</span><span class="token punctuation">.</span>parallelism <span class="token number">200</span> ： Spark作业的默认为<span class="token number">500</span><span class="token operator">~</span><span class="token number">1000</span>个比较合适<span class="token punctuation">,</span>如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num<span class="token operator">-</span>executors <span class="token operator">*</span> executor<span class="token operator">-</span>cores的<span class="token number">2</span><span class="token operator">~</span><span class="token number">3</span>倍比较合适。<span class="token comment" spellcheck="true">-- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor内存中能占的最大比例。默认值是0.6</span>—<span class="token operator">-</span>spark<span class="token punctuation">.</span>shuffle<span class="token punctuation">.</span>memoryFraction <span class="token number">0.2</span> ： 设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是<span class="token number">0.2</span>，如果shuffle聚合时使用的内存超出了这个<span class="token number">20</span><span class="token operator">%</span>的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能—<span class="token operator">-</span>spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor<span class="token operator">-</span>memory，所以会为executor额外预留一部分内存，spark<span class="token punctuation">.</span>yarn<span class="token punctuation">.</span>executor<span class="token punctuation">.</span>memoryOverhead即代表这部分内存<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-Spark常用编程建议"><a href="#2-Spark常用编程建议" class="headerlink" title="2) Spark常用编程建议"></a>2) Spark常用编程建议</h4><ol><li><p>避免创建重复的RDD，尽量复用同一份数据。</p></li><li><p>尽量避免使用shuffle类算子，因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子</p></li><li><p>用aggregateByKey和reduceByKey替代groupByKey,因为前两个是预聚合操作，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。</p></li><li><p>repartition适用于RDD[V], partitionBy适用于RDD[K, V]</p></li><li><p>mapPartitions操作替代普通map，foreachPartitions替代foreach</p></li><li><p>filter操作之后进行coalesce操作，可以减少RDD的partition数量</p></li><li><p>如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER</p></li><li><p>尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作</p></li><li><p>多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow</p></li><li><p>spark中的Group/join/XXXByKey等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数</p></li><li><p>尽量保证每轮Stage里每个task处理的数据量&gt;128M</p></li><li><p>如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲</p></li><li><p>2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD)</p></li><li><p>若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores=1， executor-memory=m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores=size/m(向上取整)</p></li><li><p>如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以hash join的方式实现，具体原理参考下一节的shuffle过程</p></li><li><p>如果需要在repartition重分区之后还要进行排序，可直接使用repartitionAndSortWithinPartitions，比分解操作效率高，因为它可以一边shuffle一边排序</p></li></ol><h4 id="3-shuffle性能优化"><a href="#3-shuffle性能优化" class="headerlink" title="3) shuffle性能优化"></a>3) shuffle性能优化</h4><p><strong>3.1 什么是shuffle操作</strong></p><p>spark中的shuffle操作功能：将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join操作，类似洗牌的操作。这些分布在各个存储节点上的数据重新打乱然后汇聚到不同节点的过程就是shuffle过程。</p><p><strong>3.2 哪些操作中包含shuffle操作</strong></p><p>RDD的特性是不可变的带分区的记录集合，Spark提供了Transformation和Action两种操作RDD的方式。Transformation是生成新的RDD，包括map, flatMap, filter, union, sample, join, groupByKey, cogroup, ReduceByKey, cros, sortByKey, mapValues等；Action只是返回一个结果，包括collect，reduce，count，save，lookupKey等</p><p>Spark所有的算子操作中是否使用shuffle过程要看计算后对应多少分区：</p><ul><li>若一个操作执行过程中，结果RDD的每个分区只依赖上一个RDD的同一个分区，即属于窄依赖，如map、filter、union等操作，这种情况是不需要进行shuffle的，同时还可以按照pipeline的方式，把一个分区上的多个操作放在同一个Task中进行</li><li>若结果RDD的每个分区需要依赖上一个RDD的全部分区，即属于宽依赖，如repartition相关操作（repartition，coalesce）、*ByKey操作（groupByKey，ReduceByKey，combineByKey、aggregateByKey等）、join相关操作（cogroup，join）、distinct操作，这种依赖是需要进行shuffle操作的</li></ul><p><strong>3.3 shuffle操作过程</strong></p><p>shuffle过程分为shuffle write和shuffle read两部分</p><ul><li>shuffle write： 分区数由上一阶段的RDD分区数控制，shuffle write过程主要是将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上（当前stage结束之后，每个task处理的数据按key进行分类，数据先写入内存缓冲区，缓冲区满，溢写spill到磁盘文件，最终相同key被写入同一个磁盘文件）创建的磁盘文件数量=当前stage中task数量*下一个stage的task数量</li><li>shuffle read：从上游stage的所有task节点上拉取属于自己的磁盘文件，每个read task会有自己的buffer缓冲，每次只能拉取与buffer缓冲相同大小的数据，然后聚合，聚合完一批后拉取下一批，边拉取边聚合。分区数由Spark提供的一些参数控制，如果这个参数值设置的很小，同时shuffle read的数据量很大，会导致一个task需要处理的数据非常大，容易发生JVM crash，从而导致shuffle数据失败，同时executor也丢失了，就会看到Failed to connect to host 的错误(即executor lost)。</li></ul><p>shuffle过程中，各个节点会通过shuffle write过程将相同key都会先写入本地磁盘文件中，然后其他节点的shuffle read过程通过网络传输拉取各个节点上的磁盘文件中的相同key。这其中大量数据交换涉及到的网络传输和文件读写操作是shuffle操作十分耗时的根本原因</p><p><strong>3.4 spark的shuffle类型</strong></p><p>参数spark.shuffle.manager用于设置ShuffleManager的类型。Spark1.5以后，该参数有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2以前的默认值，Spark1.2之后的默认值都是SortShuffleManager。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</p><p>由于SortShuffleManager默认会对数据进行排序，因此如果业务需求中需要排序的话，使用默认的SortShuffleManager就可以；但如果不需要排序，可以通过bypass机制或设置HashShuffleManager避免排序，同时也能提供较好的磁盘读写性能。</p><p>HashShuffleManager流程：</p><p><img src="https://pic3.zhimg.com/80/v2-ac4da726d0fd470fa4a058b750d79ba6_1440w.jpg" alt="img"></p><p>SortShuffleManager流程：</p><p><img src="https://pic2.zhimg.com/80/v2-6e277c1ca522102acf693b2fe6e36775_1440w.jpg" alt="img"></p><p><strong>3.5 如何开启bypass机制</strong></p><p>bypass机制通过参数spark.shuffle.sort.bypassMergeThreshold设置，默认值是200，表示当ShuffleManager是SortShuffleManager时，若shuffle read task的数量小于这个阈值（默认200）时，则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式写数据，但最后会将每个task产生的所有临时磁盘文件合并成一个文件，并创建索引文件。</p><p>这里给出的调优建议是，当使用SortShuffleManager时，如果的确不需要排序，可以将这个参数值调大一些，大于shuffle read task的数量。那么此时就会自动开启bypass机制，map-side就不会进行排序了，减少排序的性能开销，提升shuffle操作效率。但这种方式并没有减少shuffle write过程产生的磁盘文件数量，所以写的性能没有改变。</p><p><strong>3.6 HashShuffleManager优化建议</strong></p><p>如果使用HashShuffleManager，可以设置spark.shuffle.consolidateFiles参数。该参数默认为false，只有当使用HashShuffleManager且该参数设置为True时，才会开启consolidate机制，大幅度合并shuffle write过程产生的输出文件，对于shuffle read task 数量特别多的情况下，可以极大地减少磁盘IO开销，提升shuffle性能。参考社区同学给出的数据，consolidate性能比开启bypass机制的SortShuffleManager高出10% ~ 30%。</p><p><strong>3.7 shuffle调优建议</strong></p><p>除了上述的几个参数调优，shuffle过程还有一些参数可以提高性能：</p><pre class="line-numbers language-text"><code class="language-text">- spark.shuffle.file.buffer : 默认32M，shuffle Write阶段写文件时的buffer大小，若内存资源比较充足，可适当将其值调大一些（如64M），减少executor的IO读写次数，提高shuffle性能- spark.shuffle.io.maxRetries ： 默认3次，Shuffle Read阶段取数据的重试次数，若shuffle处理的数据量很大，可适当将该参数调大。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3.8 shuffle操作过程中的常见错误</p><p>SparkSQL中的shuffle错误：</p><pre class="line-numbers language-text"><code class="language-text">org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0org.apache.spark.shuffle.FetchFailedException:Failed to connect to hostname/192.168.xx.xxx:50268<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>RDD中的shuffle错误：</p><pre class="line-numbers language-text"><code class="language-text">WARN TaskSetManager: Lost task 17.1 in stage 4.1 (TID 1386, spark050013): java.io.FileNotFoundException: /data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/2f/temp_shuffle_e22e013a-5392-4edb-9874-a196a1dad97c (没有那个文件或目录)FetchFailed(BlockManagerId(6083b277-119a-49e8-8a49-3539690a2a3f-S155, spark050013, 8533), shuffleId=1, mapId=143, reduceId=3, message=org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/0e/shuffle_1_143_0.data, offset=997061, length=112503}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>处理shuffle类操作的注意事项：</p><ul><li>减少shuffle数据量：在shuffle前过滤掉不必要的数据，只选取需要的字段处理</li><li>针对SparkSQL和DataFrame的join、group by等操作：可以通过 spark.sql.shuffle.partitions控制分区数，默认设置为200，可根据shuffle的量以及计算的复杂度提高这个值，如2000等</li><li>RDD的join、group by、reduceByKey等操作：通过spark.default.parallelism控制shuffle read与reduce处理的分区数，默认为运行任务的core总数，官方建议为设置成运行任务的core的2~3倍</li><li>提高executor的内存：即spark.executor.memory的值</li><li>分析数据验证是否存在数据倾斜的问题：如空值如何处理，异常数据（某个key对应的数据量特别大）时是否可以单独处理，可以考虑自定义数据分区规则，如何自定义可以参考下面的join优化环节</li></ul><h4 id="4-join性能优化"><a href="#4-join性能优化" class="headerlink" title="4) join性能优化"></a>4) join性能优化</h4><p>Spark所有的操作中，join操作是最复杂、代价最大的操作，也是大部分业务场景的性能瓶颈所在。所以针对join操作的优化是使用spark必须要学会的技能。</p><p>spark的join操作也分为Spark SQL的join和Spark RDD的join。</p><p><strong>4.1 Spark SQL 的join操作</strong></p><p>4.1.1 Hash Join</p><p>Hash Join的执行方式是先将小表映射成Hash Table的方式，再将大表使用相同方式映射到Hash Table，在同一个hash分区内做join匹配。</p><p>hash join又分为broadcast hash join和shuffle hash join两种。其中Broadcast hash join，顾名思义，就是把小表广播到每一个节点上的内存中，大表按Key保存到各个分区中，小表和每个分区的大表做join匹配。这种情况适合一个小表和一个大表做join且小表能够在内存中保存的情况。如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-e738483e795e49c5c58a943c0d914e9f_1440w.jpg" alt="img"></p><p>当Hash Join不能适用的场景就需要Shuffle Hash Join了，Shuffle Hash Join的原理是按照join Key分区，key相同的数据必然分配到同一分区中，将大表join分而治之，变成小表的join，可以提高并行度。执行过程也分为两个阶段：</p><ul><li>shuffle阶段：分别将两个表按照join key进行分区，将相同的join key数据重分区到同一节点</li><li>hash join阶段：每个分区节点上的数据单独执行单机hash join算法</li></ul><p>Shuffle Hash Join的过程如下图所示：</p><p><img src="https://pic2.zhimg.com/80/v2-f1b93fcb9091521f24aa2c26ff31c771_1440w.jpg" alt="img"></p><p>4.1.2 Sort-Merge Join</p><p>SparkSQL针对两张大表join的情况提供了全新的算法——Sort-merge join，整个过程分为三个步骤：</p><ul><li>Shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式进行处理</li><li>sort阶段：对单个分区节点的两表数据，分别进行排序</li><li>merge阶段：对排好序的两张分区表数据执行join操作。分别遍历两个有序序列，遇到相同的join key就merge输出，否则继续取更小一边的key，即合并两个有序列表的方式。</li></ul><p>sort-merge join流程如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-63765c1a2359fce326413ac546cefd3e_1440w.jpg" alt="img"></p><p><strong>4.2 Spark RDD的join操作</strong></p><p>Spark的RDD join没有上面这么多的分类，但是面临的业务需求是一样的。如果是大表join小表的情况，则可以将小表声明为broadcast变量，使用map操作快速实现join功能，但又不必执行Spark core中的join操作。</p><p>如果是两个大表join，则必须依赖Spark Core中的join操作了。Spark RDD Join的过程可以自行阅读源码了解，这里只做一个大概的讲解。</p><p>spark的join过程中最核心的函数是cogroup方法，这个方法中会判断join的两个RDD所使用的partitioner是否一样，如果分区相同，即存在OneToOneDependency依赖，不用进行hash分区，可直接join；如果要关联的RDD和当前RDD的分区不一致时，就要对RDD进行重新hash分区，分到正确的分区中，即存在ShuffleDependency，需要先进行shuffle操作再join。因此提升join效率的一个思路就是使得两个RDD具有相同的partitioners。</p><p>所以针对Spark RDD的join操作的优化建议是：</p><ul><li>如果需要join的其中一个RDD比较小，可以直接将其存入内存，使用broadcast hash join</li><li>在对两个RDD进行join操作之前，使其使用同一个partitioners，避免join操作的shuffle过程</li><li>如果两个RDD其一存在重复的key也会导致join操作性能变低，因此最好先进行key值的去重处理</li></ul><p><strong>4.3 数据倾斜优化</strong></p><p>均匀数据分布的情况下，前面所说的优化建议就足够了。但存在数据倾斜时，仍然会有性能问题。主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。</p><p>shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。</p><p>4.3.1 分析数据分布</p><p>如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。</p><p>4.3.2 数据倾斜的解决方案</p><p>这里参考美团技术博客中给出的几个方案。</p><p>1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。</p><p>2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key</p><p>3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量</p><p>4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。</p><p>5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。</p><p>6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作</p><h4 id="5-其他错误总结"><a href="#5-其他错误总结" class="headerlink" title="5) 其他错误总结"></a>5) 其他错误总结</h4><p>(1) 报错信息</p><pre class="line-numbers language-text"><code class="language-text">java.lang.OutOfMemory, unable to create new native thread Caused by: java.lang.OutOfMemoryError: unable to create new native thread         at java.lang.Thread.start0(Native Method)         at java.lang.Thread.start(Thread.java:640) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>解决方案：</p><p>上面这段错误提示的本质是Linux操作系统无法创建更多进程，导致出错，并不是系统的内存不足。因此要解决这个问题需要修改Linux允许创建更多的进程，就需要修改Linux最大进程数</p><p>（2）报错信息</p><p>由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实就是将/tmp目录挂载到内存当中, 那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误<br>No Space Left on the device（Shuffle临时文件过多）</p><p>解决方案：</p><p>修改配置文件spark-env.sh,把临时文件引入到一个自定义的目录中去, 即:</p><p>export SPARK_LOCAL_DIRS=/home/utoken/datadir/spark/tmp</p><p>（3）报错信息</p><p>Worker节点中的work目录占用许多磁盘空间, 这些是Driver上传到worker的文件, 会占用许多磁盘空间</p><p>解决方案：</p><p>需要定时做手工清理work目录</p><p>（4）spark-shell提交Spark Application如何解决依赖库</p><p>解决方案：</p><p>利用–driver-class-path选项来指定所依赖的jar文件，注意的是–driver-class-path后如果需要跟着多个jar文件的话，jar文件之间使用冒号:来分割。</p><p>（5）内存不足或数据倾斜导致Executor Lost，shuffle fetch失败，Task重试失败等（spark-submit提交）</p><pre class="line-numbers language-text"><code class="language-text">TaskSetManager: Lost task 1.0 in stage 6.0 (TID 100, 192.168.10.37): java.lang.OutOfMemoryError: Java heap spaceINFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.37:57139 (size: 42.0 KB, free: 24.2 MB)INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.38:53816 (size: 42.0 KB, free: 24.2 MB)INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 102, 192.168.10.37, ANY, 2152 bytes)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>解决方案：</p><p>增加worker内存，或者相同资源下增加partition数目，这样每个task要处理的数据变少，占用内存变少</p><p>如果存在shuffle过程，设置shuffle read阶段的并行数</p><h2 id="2-SparkSQL"><a href="#2-SparkSQL" class="headerlink" title="2. SparkSQL"></a>2. SparkSQL</h2><h3 id="2-1-Spark-SQL-的原理和运行机制"><a href="#2-1-Spark-SQL-的原理和运行机制" class="headerlink" title="2.1 Spark SQL 的原理和运行机制"></a>2.1 Spark SQL 的原理和运行机制</h3><img src="https://pic2.zhimg.com/80/v2-9338f9e4a1d5ce568f47097f5b56f285_1440w.jpg" alt="img" style="zoom:50%;"><p>从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作</p><ul><li>Parser 解析 SQL，生成 Unresolved Logical Plan</li><li>由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan</li><li>Optimizer根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan</li><li>Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan</li><li>CBO 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan</li><li>Spark 以 DAG 的方法执行上述 Physical Plan</li><li>在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率</li></ul><p><strong>Parser</strong></p><p>Spark SQL 使用 Antlr 进行记法和语法解析，并生成 UnresolvedPlan。</p><p>当用户使用 SparkSession.sql(sqlText : String) 提交 SQL 时，SparkSession 最终会调用 SparkSqlParser 的 parsePlan 方法。该方法分两步</p><ul><li>使用 Antlr 生成的 SqlBaseLexer 对 SQL 进行词法分析，生成 CommonTokenStream</li><li>使用 Antlr 生成的 SqlBaseParser 进行语法分析，得到 LogicalPlan</li></ul><p><strong>Analyzer</strong></p><p>从 Analyzer 的构造方法可见</p><ul><li><p>Analyzer 持有一个 SessionCatalog 对象的引用</p></li><li><p>Analyzer 继承自 RuleExecutor[LogicalPlan]，因此可对 LogicalPlan 进行转换</p><p><strong>Optimizer</strong></p></li></ul><p>Spark SQL 目前的优化主要是基于规则的优化，即 RBO （Rule-based optimization）</p><ul><li>每个优化以 Rule 的形式存在，每条 Rule 都是对 Analyzed Plan 的等价转换</li><li>RBO 设计良好，易于扩展，新的规则可以非常方便地嵌入进 Optimizer</li><li>RBO 目前已经足够好，但仍然需要更多规则来 cover 更多的场景</li><li>优化思路主要是减少参与计算的数据量以及计算本身的代价</li></ul><p><strong>PushdownPredicate</strong><br>PushdownPredicate 是最常见的用于减少参与计算的数据量的方法。</p><p><strong>SparkPlanner</strong></p><p>得到优化后的 LogicalPlan 后，SparkPlanner 将其转化为 SparkPlan 即物理计划。</p><p>本例中由于 score 表数据量较小，Spark 使用了 BroadcastJoin。因此 score 表经过 Filter 后直接使用 BroadcastExchangeExec 将数据广播出去，然后结合广播数据对 people 表使用 BroadcastHashJoinExec 进行 Join。再经过 Project 后使用 HashAggregateExec 进行分组聚合。</p><p>至此，一条 SQL 从提交到<strong>解析</strong>、<strong>分析</strong>、<strong>优化</strong>以及执行的完整过程就介绍完毕。</p><h3 id="2-3-Spark-SQL-的优化策略"><a href="#2-3-Spark-SQL-的优化策略" class="headerlink" title="2.3 Spark SQL 的优化策略"></a>2.3 Spark SQL 的优化策略</h3><p><strong>1）内存列式存储与内存缓存表</strong><br> Spark SQL可以通过cacheTable将数据存储转换为列式存储，同时将数据加载到内存缓存。cacheTable相当于在分布式集群的内存物化视图，将数据缓存，这样迭代的或者交互式的查询不用再从HDFS读数据，直接从内存读取数据大大减少了I/O开销。列式存储的优势在于Spark SQL只需要读出用户需要的列，而不需要像行存储那样每次都将所有列读出，从而大大减少内存缓存数据量，更高效地利用内存数据缓存，同时减少网络传输和I/O开销。数据按照列式存储，由于是数据类型相同的数据连续存储，所以能够利用序列化和压缩减少内存空间的占用。</p><p> <strong>2）列存储压缩</strong><br> 为了减少内存和硬盘空间占用，Spark SQL采用了一些压缩策略对内存列存储数据进行压缩。Spark SQL的压缩方式要比Shark丰富很多，如它支持PassThrough、RunLengthEncoding、DictionaryEncoding、BooleanBitSet、IntDelta、LongDelta等多种压缩方式，这样能够大幅度减少内存空间占用、网络传输和I/O开销。</p><p> <strong>3）逻辑查询优化</strong><br> SparkSQL在逻辑查询优化（见图8-4）上支持列剪枝、谓词下压、属性合并等逻辑查询优化方法。列剪枝为了减少读取不必要的属性列、减少数据传输和计算开销，在查询优化器进行转换的过程中会优化列剪枝。<br> 下面介绍一个逻辑优化的例子。<br> SELECT Class FROM （SELECT ID，Name，Class  FROM STUDENT ） S WHERE S.ID=1</p><p>Catalyst将原有查询通过谓词下压，将选择操作ID=1优先执行，这样过滤大部分数据，通过属性合并将最后的投影只做一次，最终保留Class属性列。<br> <strong>4）Join优化</strong><br> Spark SQL深度借鉴传统数据库的查询优化技术的精髓，同时在分布式环境下调整和创新特定的优化策略。现在Spark SQL对Join进行了优化，支持多种连接算法，现在的连接算法已经比Shark丰富，而且很多原来Shark的元素也逐步迁移过来，如BroadcastHashJoin、BroadcastNestedLoopJoin、HashJoin、LeftSemiJoin，等等。<br> 下面介绍其中的一个Join算法。<br> BroadcastHashJoin将小表转化为广播变量进行广播，这样避免Shuffle开销，最后在分区内做Hash连接。这里使用的就是Hive中Map Side Join的思想，同时使用DBMS中的Hash连接算法做连接。 随着Spark SQL的发展，未来会有更多的查询优化策略加入进来，同时后续Spark SQL会支持像Shark Server一样的服务端和JDBC接口，兼容更多的持久化层，如NoSQL、传统的DBMS等。一个强有力的结构化大数据查询引擎正在崛起。</p><h2 id="3-SparkStreaming"><a href="#3-SparkStreaming" class="headerlink" title="3. SparkStreaming"></a>3. SparkStreaming</h2><p>3.1 原理剖析（源码级别）和运行机制</p><p>3.2 Spark Dstream 及其 API 操作</p><p>3.3 Spark Streaming 消费 Kafka 的两种方式</p><p>3.4 Spark 消费 Kafka 消息的 Offset 处理</p><p>3.5 窗口操作</p><h2 id="4-SparkMlib"><a href="#4-SparkMlib" class="headerlink" title="4. SparkMlib"></a>4. SparkMlib</h2><p>可实现聚类、分类、推荐等算法</p><h1 id="三-Flink"><a href="#三-Flink" class="headerlink" title="三. Flink"></a>三. Flink</h1><ul><li>Flink 集群的搭建</li><li>Flink 的架构原理</li><li>Flink 的编程模型</li><li>Flink 集群的 HA 配置</li><li>Flink DataSet 和 DataSteam API</li><li>序列化</li><li>Flink 累加器</li><li>状态 State 的管理和恢复</li><li>窗口和时间</li><li>并行度</li><li>Flink 和消息中间件 Kafka 的结合</li><li>Flink Table 和 SQL 的原理和用法</li></ul><h1 id="四-Kafka"><a href="#四-Kafka" class="headerlink" title="四. Kafka"></a>四. Kafka</h1><h2 id="1-Kafka-的设计"><a href="#1-Kafka-的设计" class="headerlink" title="1. Kafka 的设计"></a>1. Kafka 的设计</h2><p>Kafka 将消息以 topic 为单位进行归纳</p><p>将向 Kafka topic 发布消息的程序成为 producers.</p><p>将预订 topics 并消费消息的程序成为 consumer.</p><p>Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker.</p><p>producers 通过网络将消息发送到 Kafka 集群，集群向消费者提供消息</p><h2 id="2-数据传输的三种事务定义"><a href="#2-数据传输的三种事务定义" class="headerlink" title="2. 数据传输的三种事务定义"></a>2. 数据传输的三种事务定义</h2><p>数据传输的事务定义通常有以下三种级别：</p><p>（1）最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输</p><p>（2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.</p><p>（3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而</p><p>且仅仅被传输一次，这是大家所期望的</p><h2 id="3-Kafka-判断一个节点是否活着两大条件"><a href="#3-Kafka-判断一个节点是否活着两大条件" class="headerlink" title="3. Kafka 判断一个节点是否活着两大条件"></a>3. Kafka 判断一个节点是否活着两大条件</h2><p>（1）节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连</p><p>接</p><p>（2）如果节点是个 follower,他必须能及时的同步 leader 的写操作，延时不能太久</p><h2 id="4-Kafa-consumer-是否可以消费指定分区消息？"><a href="#4-Kafa-consumer-是否可以消费指定分区消息？" class="headerlink" title="4. Kafa consumer 是否可以消费指定分区消息？"></a>4. Kafa consumer 是否可以消费指定分区消息？</h2><p>​      Kafa consumer 消费消息时，向 broker 发出”fetch”请求去消费特定分区的消息，consumer</p><p>指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer 拥有</p><p>了 offset 的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的</p><h2 id="5-Kafka-消息是采用-Pull-模式or-Push-模式？"><a href="#5-Kafka-消息是采用-Pull-模式or-Push-模式？" class="headerlink" title="5. Kafka 消息是采用 Pull 模式or Push 模式？"></a>5. Kafka 消息是采用 Pull 模式or Push 模式？</h2><p>​          Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到</p><p>consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统</p><p>的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息</p><p>一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的</p><p>consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的</p><p>consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消</p><p>息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，</p><p>consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式</p><p>​          Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push</p><p>模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还</p><p>是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一</p><p>次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决</p><p>定这些策略</p><p>​          Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，</p><p>直到新消息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达</p><p>(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发</p><h2 id="6-Kafka-存储在硬盘上的消息格式是什么？"><a href="#6-Kafka-存储在硬盘上的消息格式是什么？" class="headerlink" title="6. Kafka 存储在硬盘上的消息格式是什么？"></a>6. Kafka 存储在硬盘上的消息格式是什么？</h2><p>消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和 CRC32</p><p>校验码。</p><ul><li>消息长度: 4 bytes (value: 1+4+n)</li><li>版本号: 1 byte</li><li>CRC 校验码: 4 bytes</li><li>具体的消息: n bytes</li></ul><h2 id="7-Kafka-高效文件存储设计特点"><a href="#7-Kafka-高效文件存储设计特点" class="headerlink" title="7. Kafka 高效文件存储设计特点"></a>7. Kafka 高效文件存储设计特点</h2><p>(1).Kafka 把 topic 中一个 parition 大文件分成多个小文件段，通过多个小文件段，就容易定</p><p>期清除或删除已经消费完文件，减少磁盘占用。</p><p>(2).通过索引信息可以快速定位 message 和确定 response 的最大大小。</p><p>(3).通过 index 元数据全部映射到 memory，可以避免 segment file 的 IO 磁盘操作。</p><p>(4).通过索引文件稀疏存储，可以大幅降低 index 文件元数据占用空间大小。</p><h2 id="8-Kafka-与传统消息系统之间有三个关键区别"><a href="#8-Kafka-与传统消息系统之间有三个关键区别" class="headerlink" title="8. Kafka 与传统消息系统之间有三个关键区别"></a>8. Kafka 与传统消息系统之间有三个关键区别</h2><p>(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留</p><p>(2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据</p><p>提升容错能力和高可用性</p><p>(3).Kafka 支持实时的流式处理</p><h2 id="9-Kafka-创建-Topic-时如何将分区放置到不同的-Broker-中"><a href="#9-Kafka-创建-Topic-时如何将分区放置到不同的-Broker-中" class="headerlink" title="9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中"></a>9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中</h2><ul><li>副本因子不能大于 Broker 的个数；</li><li>第一个分区（编号为 0）的第一个副本放置位置是随机从 brokerList 选择的；</li><li>其他分区的第一个副本放置位置相对于第 0 个分区依次往后移。也就是如果我们有 5 个Broker，5 个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个Broker 上，依次类推；</li><li>剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的</li></ul><h2 id="10-Kafka-新建的分区会在哪个目录下创建"><a href="#10-Kafka-新建的分区会在哪个目录下创建" class="headerlink" title="10. Kafka 新建的分区会在哪个目录下创建"></a>10. Kafka 新建的分区会在哪个目录下创建</h2><p>在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，</p><p>这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘</p><p>上用于提高读写性能。</p><p>当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。</p><p>如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个</p><p>目录下创建文件夹用于存放数据。</p><p>但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？</p><p>答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic</p><p>名+分区 ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就</p><p>是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁</p><p>盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。</p><h2 id="11-partition-的数据如何保存到硬盘"><a href="#11-partition-的数据如何保存到硬盘" class="headerlink" title="11. partition 的数据如何保存到硬盘"></a>11. partition 的数据如何保存到硬盘</h2><p>topic 中的多个 partition 以文件夹的形式保存到 broker，每个分区序号从 0 递增，</p><p>且消息有序</p><p>Partition 文件下有多个 segment（xxx.index，xxx.log）</p><p>segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为 1g</p><p>如果大小大于 1g 时，会滚动一个新的 segment 并且以上一个 segment 最后一条消息的偏移</p><p>量命名</p><h2 id="12-kafka-的-ack-机制"><a href="#12-kafka-的-ack-机制" class="headerlink" title="12. kafka 的 ack 机制"></a>12. kafka 的 ack 机制</h2><p>request.required.acks 有三个值 0 1 -1</p><p>0:生产者不会等待 broker 的 ack，这个延迟最低但是存储的保证最弱当 server 挂掉的时候</p><p>就会丢数据</p><p>1：服务端会等待 ack 值 leader 副本确认接收到消息后发送 ack 但是如果 leader 挂掉后他</p><p>不确保是否复制完成新 leader 也会导致数据丢失</p><p>-1：同样在 1 的基础上 服务端会等所有的 follower 的副本受到数据后才会受到 leader 发出</p><p>的 ack，这样数据不会丢失</p><h2 id="13-Kafka-的消费者如何消费数据"><a href="#13-Kafka-的消费者如何消费数据" class="headerlink" title="13. Kafka 的消费者如何消费数据"></a>13. Kafka 的消费者如何消费数据</h2><p>​       消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置</p><p>等到下次消费时，他会接着上次位置继续消费</p><h2 id="14-消费者负载均衡策略"><a href="#14-消费者负载均衡策略" class="headerlink" title="14. 消费者负载均衡策略"></a>14. 消费者负载均衡策略</h2><p>​       一个消费者组中的一个分片对应一个消费者成员，他能保证每个消费者成员都能访问，如</p><p>果组中成员太多会有空闲的成员</p><h2 id="15-数据有序"><a href="#15-数据有序" class="headerlink" title="15. 数据有序"></a>15. 数据有序</h2><p>​       一个消费者组里它的内部是有序的</p><p>​       消费者组与消费者组之间是无序的</p><h2 id="16-kafaka-生产数据时数据的分组策略"><a href="#16-kafaka-生产数据时数据的分组策略" class="headerlink" title="16. kafaka 生产数据时数据的分组策略"></a>16. kafaka 生产数据时数据的分组策略</h2><p>​        生产者决定数据产生到集群的哪个 partition 中</p><p>​        每一条消息都是以（key，value）格式</p><p>​        Key 是由生产者发送数据传入</p><p>​        所以生产者（key）决定了数据产生到集群的哪个 partition</p><h1 id="五-数据仓库"><a href="#五-数据仓库" class="headerlink" title="五. 数据仓库"></a>五. 数据仓库</h1><h2 id="5-1-数仓概念相关"><a href="#5-1-数仓概念相关" class="headerlink" title="5.1 数仓概念相关"></a>5.1 数仓概念相关</h2><h3 id="1-数据仓库、数据集市、数据库之间的区别"><a href="#1-数据仓库、数据集市、数据库之间的区别" class="headerlink" title="1. 数据仓库、数据集市、数据库之间的区别"></a>1. 数据仓库、数据集市、数据库之间的区别</h3><ul><li><p><strong>数据仓库 ：</strong>数据仓库是一个面向主题的、集成的、随时间变化的、但信息本身相对稳定的数据集合，用于对管理决策过程的支持。是企业级的，能为整个企业各个部门的运行提供决策支持手段；</p></li><li><p><strong>数据集市：</strong>则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。</p></li><li><p><strong>数据库：</strong>是一种软件，用来实现数据库逻辑过程，属于物理层；</p><blockquote><p>数据仓库是数据库概念的升级，从数据量来说，数据仓库要比数据库更庞大德多，主要用于数据挖掘和数据分析，辅助领导做决策</p><p>只是数据库内的数据时限要远远的长于操作型环境中的数据时限。在操作型环境中一般只保存有60<del>90天的数据，而在数据仓库中则要需要保存较长时限的数据（例如：5</del>10年），以适应DSS进行趋势分析的要求。</p></blockquote></li></ul><h3 id="2-OLAP、OLTP概念及用途"><a href="#2-OLAP、OLTP概念及用途" class="headerlink" title="2. OLAP、OLTP概念及用途"></a>2. OLAP、OLTP概念及用途</h3><ul><li><p><strong>OLAP:</strong>  即<code>On-Line Analysis Processing</code>在线分析处理。</p><blockquote><p><strong>OLAP的特点</strong>：联机分析处理的主要特点，是直接仿照用户的多角度思考模式，预先为用户组建<strong>多维</strong>的数据模型，维指的是用户的分析角度。</p></blockquote></li><li><p><strong>OLTP:</strong>  即<code>On-Line Transaction Processing</code>联机事务处理过程(OLTP)</p><blockquote><p>OLTP的特点：结构复杂、实时性要求高。</p></blockquote><p><strong>OLAP和OLTP区别</strong></p><blockquote><p>1、<strong>基本含义不同</strong>：OLTP是传统的关系型数据库的bai主要应用，主要是基本的、日常的事务处理，记du录即时的增、删、改、查，比如在银行存取一笔款，就是一个事务交易。OLAP即联机分析处理，是数据仓库的核心部心，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。典型的应用就是复杂的动态报表系统。</p><p>2、<strong>实时性要求不同</strong>：OLTP实时性要求高，OLTP 数据库旨在使事务应用程序仅写入所需的数据，以便尽快处理单个事务。OLAP的实时性要求不是很高，很多应用顶多是每天更新一下数据。</p><p>3、<strong>数据量不同</strong>：OLTP数据量不是很大，一般只读/写数十条记录，处理简单的事务。OLAP数据量大，因为OLAP支持的是动态查询，所以用户也许要通过将很多数据的统计后才能得到想要知道的信息，例如时间序列分析等等，所以处理的数据量很大。</p><p>4、<strong>用户和系统的面向性不同</strong>：OLTP是面向顾客的,用于事务和查询处理。OLAP是面向市场的,用于数据分析。</p><p>5、<strong>数据库设计不同</strong>：OLTP采用实体-联系ER模型和面向应用的数据库设计。OLAP采用星型或雪花模型和面向主题的数据库设计。</p></blockquote></li></ul><p><img src="https://iknow-pic.cdn.bcebos.com/80cb39dbb6fd526618590e1fa618972bd407364e?x-bce-process=image/resize,m_lfit,w_600,h_800,limit_1" alt="OLAP和OLTP区别"></p><h3 id="3-事实表、维度表、拉链表概念及区别"><a href="#3-事实表、维度表、拉链表概念及区别" class="headerlink" title="3. 事实表、维度表、拉链表概念及区别"></a>3. 事实表、维度表、拉链表概念及区别</h3><ul><li><strong>事实表：</strong>事实表其实质就是通过各种维度和一些指标值得组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。</li><li><strong>维度表：</strong>维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。</li><li><strong>拉链表：</strong>拉链表，它是一种维护<strong>历史状态</strong>，以及<strong>最新状态数据</strong>的一种表。拉链表也是分区表，有些不变的数据或者是已经达到状态终点的数据就会把它放在分区里面，分区字段一般为开始时间：start_date和结束时间：end_date。一般在该天有效的数据，它的end_date是大于等于该天的日期的。获取某一天全量的数据，可以通过表中的start_date和end_date来做筛选，选出固定某一天的数据。例如我想取截止到20190813的全量数据，其where过滤条件就是where start_date&lt;=’20190813’ and end_date&gt;=20190813。</li></ul><h3 id="4-全量表、增量表、快照表概念及区别"><a href="#4-全量表、增量表、快照表概念及区别" class="headerlink" title="4. 全量表、增量表、快照表概念及区别"></a>4. 全量表、增量表、快照表概念及区别</h3><ul><li><strong>全量表：</strong>全量表没有分区，表中的数据是前一天的所有数据，比如说今天是24号，那么全量表里面拥有的数据是23号的所有数据，每次往全量表里面写数据都会覆盖之前的数据，所以<strong>全量表不能记录历史的数据情况，只有截止到当前最新的、全量的数据</strong>。</li><li><strong>增量表：</strong>增量表，就是<strong>记录每天新增数据的表</strong>，比如说，从24号到25号新增了那些数据，改变了哪些数据，这些都会存储在增量表的25号分区里面。上面说的快照表的25号分区和24号分区（都是t+1，实际时间分别对应26号和25号），它两的数据相减就是实际时间25号到26号有变化的、增加的数据，也就相当于增量表里面25号分区的数据。</li><li><strong>快照表：</strong>那么要能查到历史数据情况又该怎么办呢？这个时候快照表就派上用途了，快照表是有时间分区的，每个分区里面的数据都是分区时间对应的前一天的所有全量数据，比如说当前数据表有3个分区，24号，25号，26号。其中，24号分区里面的数据就是从历史到23号的所有数据，25号分区里面的数据就是从历史到24号的所有数据，以此类推。</li></ul><h3 id="4-什么叫维度和度量值"><a href="#4-什么叫维度和度量值" class="headerlink" title="4. 什么叫维度和度量值"></a>4. 什么叫维度和度量值</h3><ul><li><p><strong>维度</strong>：说明数据，维度是指可指定不同值的对象的描述性<strong>属性或特征</strong>。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。 </p></li><li><p><strong>度量</strong>：事实表和维度交叉汇聚的点，度量和维度构成OLAP的主要概念，这里面对于在事实表或者一个多维立方体里面存放的数值型的、连续的字段，就是度量。这符合上面的意思，有标准，一个度量字段肯定是统一单位，例如元、户数。如果一个度量字段，其中的度量值可能是欧元又有可能是美元，那这个度量可没法汇总。在统一计量单位下，对不同维度的描述。</p></li></ul><h3 id="5-什么叫缓慢维度变化（Slowly-Changing-Dimensions，SCD"><a href="#5-什么叫缓慢维度变化（Slowly-Changing-Dimensions，SCD" class="headerlink" title="5. 什么叫缓慢维度变化（Slowly Changing Dimensions，SCD)"></a>5. 什么叫缓慢维度变化（Slowly Changing Dimensions，SCD)</h3><p>​       维度建模的数据仓库中，有一个概念叫Slowly Changing Dimensions，中文一般翻译成缓慢变化维，经常被简写为SCD。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随时间发生变化的维度我们一般称之为缓慢变化维，并且把处理维度表的历史变化信息的问题称为处理缓慢变化维的问题，有时也简称为处理SCD的问题。</p><p><strong>处理缓慢变化维的方法通常分为三种方式：</strong></p><ul><li><strong>第一种方式是直接覆盖原值</strong>。这样处理，最容易实现，但是没有保留历史数据，无法分析历史变化信息。第一种方式通常简称为“TYPE 1”。</li><li><strong>第二种方式是添加维度行</strong>。这样处理，需要<strong>代理键</strong>的支持。实现方式是当有维度属性发生变化时，生成一条新的维度记录，主键是新分配的代理键，通过自然键可以和原维度记录保持关联。第二种方式通常简称为“TYPE 2”。</li><li><strong>第三种方式是添加属性列</strong>。这种处理的实现方式是对于需要分析历史信息的属性添加一列，来记录该属性变化前的值，而本属性字段使用TYPE 1来直接覆盖。这种方式的优点是可以同时分析当前及前一次变化的属性值，缺点是只保留了最后一次变化信息。第三种方式通常简称为“TYPE 3”。</li></ul><h2 id="5-2-数仓分层设计"><a href="#5-2-数仓分层设计" class="headerlink" title="5.2 数仓分层设计"></a>5.2 数仓分层设计</h2><h3 id="1-数据仓库分为4层："><a href="#1-数据仓库分为4层：" class="headerlink" title="1. 数据仓库分为4层："></a>1. 数据仓库分为4层：</h3><ul><li>ODS层 （原始数据层）  BDM</li><li>DWD层 （明细数据层） FDM</li><li>DWS层 （服务数据层） GDM ADM</li><li>ADS层 （数据应用层）  APP</li></ul><h3 id="2-各层主要负责职责"><a href="#2-各层主要负责职责" class="headerlink" title="2. 各层主要负责职责"></a>2. 各层主要负责职责</h3><p><strong>ODS层（原始数据层</strong>）：存放原始数据，直接加载原始日志、数据，数据保存原貌不做处理。</p><p><strong>DWD层（明细数据层）</strong>：结构与粒度原始表保持一致，对ODS层数据进行清洗（去除空值、脏数据、超过极限范围的数据）</p><p><strong>DWS层 （服务数据层）</strong>：以DWD为基础，进行轻度汇总</p><p><strong>ADS层 （数据应用层）</strong>：为各种统计报表提供数据</p><h3 id="3-为什么要分层？"><a href="#3-为什么要分层？" class="headerlink" title="3. **为什么要分层？**"></a><strong>3. **为什么要分层</strong>？**</h3><ul><li><p><strong>空间换时间：</strong>通过建设多层次的数据模型供用户使用，避免用户直接使用操作型数据，可以更高效的访问数据</p></li><li><p><strong>把复杂问题简单化：一</strong>个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复</p></li><li><p><strong>便于处理业务的变化：</strong>随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知</p></li></ul><h3 id="4-数仓中每层表的建模？怎么建模？"><a href="#4-数仓中每层表的建模？怎么建模？" class="headerlink" title="4. 数仓中每层表的建模？怎么建模？"></a>4. 数仓中每层表的建模？怎么建模？</h3><p><strong>（1）ODS：</strong> 特点是保持原始数据的原貌，不作修改！</p><p>原始数据怎么建模，ODS就怎么建模！举例： 用户行为数据特征是一条记录就是一行！</p><p>ODS层表(line string) 业务数据，参考Sqoop导入的数据类型进行建模！</p><p>（2）<strong>DWD层</strong>：特点从ODS层，将数据进行ETL（清洗），轻度聚合，再展开明细！</p><ul><li>在展开明细时，对部分维度表进行降维操作</li></ul><blockquote><p>例如：将商品一二三级分类表，sku商品表，spu商品表，商品品牌表合并汇总为一张维度表！</p></blockquote><ul><li>对事实表，参考星型模型的建模策略，按照<strong>选择业务过程→声明粒度→确认维度→确认事实</strong>思路进行建模</li></ul><blockquote><p><strong>选择业务过程</strong>： 选择感兴趣的事实表<br><strong>声明粒度</strong>： 选择最细的粒度！可以由最细的粒度通过聚合的方式得到粗粒度！<br><strong>确认维度</strong>： 根据3w原则确认维度，挑选自己感兴趣的维度<br><strong>确认事实</strong>： 挑选感兴趣的度量字段，一般是从事实表中选取！</p></blockquote><ul><li>DWS层： 根据业务需求进行分主题建模！一般是建宽表！</li><li>DWT层： 根据业务需求进行分主题建模！一般是建宽表！</li><li>ADS层： 根据业务需求进行建模！</li></ul><h2 id="5-3-数仓建模"><a href="#5-3-数仓建模" class="headerlink" title="5.3 数仓建模"></a>5.3 数仓建模</h2><h3 id="1-维度建模概念、类型、过程"><a href="#1-维度建模概念、类型、过程" class="headerlink" title="1. 维度建模概念、类型、过程"></a>1. 维度建模概念、类型、过程</h3><p><strong>维度建模：</strong>维度建模是一种将数据结构化的逻辑设计方法，它将客观世界划分为<strong>度量</strong>和<strong>上下文</strong>。度量是常常是以数值形式出现，事实周围有上下文包围着，这种上下文被直观地分成独立的逻辑块，称之为<strong>维度</strong>。它与实体-关系建模有很大的区别，实体-关系建模是面向应用，遵循第三范式，以消除数据冗余为目标的设计技术。维度建模是面向分析，为了提高查询性能可以增加数据冗余，反规范化的设计技术。</p><p><strong>维度建模过程：</strong>确定业务流程-&gt;确定粒度-&gt;确定纬度-&gt;确定事实</p><blockquote><p>建模四步走：</p><p><strong>1.选取要建模的业务处理流程</strong></p><p>　　　　关注业务处理流程，而不是业务部门！</p><p><strong>2.定义业务处理的粒度</strong></p><p>　　　　“如何描述事实表的单个行？”</p><p><strong>3.选定用于每个事实表行的维度</strong></p><p>　　　　常见维度包括日期、产品等</p><p><strong>4.确定用于形成每个事实表行的数字型事实</strong></p><p>　　　　典型的事实包括订货量、支出额这样的可加性数据</p></blockquote><h3 id="2-星型模型和雪花模型概念、区别"><a href="#2-星型模型和雪花模型概念、区别" class="headerlink" title="2. 星型模型和雪花模型概念、区别"></a>2. 星型模型和雪花模型概念、区别</h3><p>​      在多维分析的商业智能解决方案中，<strong>根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。</strong>在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。</p><p>当所有维表都<strong>直接</strong>连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型，</p><img src="https://images2018.cnblogs.com/blog/1135580/201802/1135580-20180228173453995-1193173201.png" alt="img" style="zoom: 80%;"><p><strong>星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余</strong>，</p><p>　　如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。</p><p>　　当有<strong>一个或多个维表没有直接连接到事实表上</strong>，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。</p><p><img src="https://images2018.cnblogs.com/blog/1135580/201802/1135580-20180228173658849-1541605786.png" alt="img"></p><p>雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。</p><p>　　它的优点是 : <strong>通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。</strong></p><p>　　<strong>此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率（空间换易用与效率）。</strong></p><p><strong>1.数据优化</strong></p><p>　　雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。</p><p>　　相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。</p><p>　　<strong>2.业务模型</strong></p><p>　　主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。</p><p>　　在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。</p><p>　　<strong>3.性能</strong></p><p>　　第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。</p><p>而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。</p><p>　　<strong>4.ETL</strong></p><p>　　雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。</p><p>　　星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。</p><p>　　<strong>总结</strong></p><p>　　通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。</p><h2 id="5-4-数仓使用经验"><a href="#5-4-数仓使用经验" class="headerlink" title="5.4 数仓使用经验"></a>5.4 数仓使用经验</h2><h3 id="3-数据仓库系统的数据质量如何保证？方案？"><a href="#3-数据仓库系统的数据质量如何保证？方案？" class="headerlink" title="3. 数据仓库系统的数据质量如何保证？方案？"></a>3. 数据仓库系统的数据质量如何保证？方案？</h3><p>数据质量评估</p><ol><li><p>完整性</p></li><li><p>准确性</p></li><li><p>及时性</p></li><li><p>一致性</p></li></ol><h3 id="4-如何实现增量抽取？"><a href="#4-如何实现增量抽取？" class="headerlink" title="4. 如何实现增量抽取？"></a>4. 如何实现增量抽取？</h3><p>(主要采用时间戳方式，提供数据抽取和处理的性能)</p><h3 id="5-常见的数据治理方案"><a href="#5-常见的数据治理方案" class="headerlink" title="5. 常见的数据治理方案"></a>5. 常见的数据治理方案</h3><p>1）数据压缩</p><p>2）小文件合并</p><p>3）冷数据处理</p><h1 id="六-数据库"><a href="#六-数据库" class="headerlink" title="六. 数据库"></a>六. 数据库</h1><h2 id="6-1-基本概念"><a href="#6-1-基本概念" class="headerlink" title="6.1 基本概念"></a>6.1 基本概念</h2><h4 id="1-主键、外键、超键、候选键"><a href="#1-主键、外键、超键、候选键" class="headerlink" title="1. 主键、外键、超键、候选键"></a>1. 主键、外键、超键、候选键</h4><blockquote><p><strong>超键</strong>：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。</p><p><strong>候选键</strong>：是最小超键，即没有冗余元素的超键。</p><p><strong>主键</strong>：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。</p><p><strong>外键</strong>：在一个表中存在的另一个表的主键称此表的外键。</p></blockquote><h4 id="2-为什么用自增列作为主键"><a href="#2-为什么用自增列作为主键" class="headerlink" title="2. 为什么用自增列作为主键"></a>2. 为什么用自增列作为主键</h4><blockquote><p>如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引、</p><p>如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引、</p><p>如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的)。</p><p>数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）</p><p>如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页</p><p>如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。</p></blockquote><h4 id="3-触发器的作用？"><a href="#3-触发器的作用？" class="headerlink" title="3. 触发器的作用？"></a>3. 触发器的作用？</h4><blockquote><p>触发器是一种特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。</p></blockquote><h4 id="4-什么是存储过程？用什么来调用？"><a href="#4-什么是存储过程？用什么来调用？" class="headerlink" title="4. 什么是存储过程？用什么来调用？"></a>4. 什么是存储过程？用什么来调用？</h4><blockquote><p>存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。</p><p><strong>调用：</strong></p><p>1）可以用一个命令对象来调用存储过程。</p><p>2）可以供外部程序调用，比如：java程序。</p></blockquote><h4 id="5-存储过程的优缺点？"><a href="#5-存储过程的优缺点？" class="headerlink" title="5. 存储过程的优缺点？"></a>5. 存储过程的优缺点？</h4><blockquote><p><strong>优点：</strong></p><p>1）存储过程是预编译过的，执行效率高。</p><p>2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。</p><p>3）安全性高，执行存储过程需要有一定权限的用户。</p><p>4）存储过程可以重复使用，可减少数据库开发人员的工作量。</p><p><strong>缺点：</strong>移植性差</p></blockquote><h4 id="6-存储过程与函数的区别"><a href="#6-存储过程与函数的区别" class="headerlink" title="6. 存储过程与函数的区别"></a>6. 存储过程与函数的区别</h4><p><img src="https://i.imgur.com/ymE9HPJ.png" alt="img"></p><h4 id="7-什么叫视图？游标是什么？"><a href="#7-什么叫视图？游标是什么？" class="headerlink" title="7. 什么叫视图？游标是什么？"></a>7. 什么叫视图？游标是什么？</h4><blockquote><p><strong>视图：</strong></p><p>是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改会影响基本表。它使得我们获取数据更容易，相比多表查询。</p><p><strong>游标：</strong></p><p>是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。</p></blockquote><h4 id="8-视图的优缺点"><a href="#8-视图的优缺点" class="headerlink" title="8. 视图的优缺点"></a>8. 视图的优缺点</h4><blockquote><p><strong>优点：</strong></p><p>1对数据库的访问，因为视图可以有选择性的选取数据库里的一部分。</p><p>2)用户通过简单的查询可以从复杂查询中得到结果。</p><p>3)维护数据的独立性，试图可从多个表检索数据。</p><p>4)对于相同的数据可产生不同的视图。</p><p><strong>缺点：</strong></p><p>性能：查询视图时，必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，那么就无法更改数据</p></blockquote><h4 id="9-drop、truncate、-delete区别"><a href="#9-drop、truncate、-delete区别" class="headerlink" title="9. drop、truncate、 delete区别"></a>9. drop、truncate、 delete区别</h4><blockquote><p><strong>最基本：</strong></p><ul><li>drop直接删掉表。</li><li>truncate删除表中数据，再插入时自增长id又从1开始。</li><li>delete删除表中数据，可以加where字句。</li></ul><p>（1） DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。</p><p>（2） 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。</p><p>（3） 一般而言，drop &gt; truncate &gt; delete</p><p>（4） 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view</p><p>（5） TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。</p><p>（6） truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。</p><p>（7） delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。</p><p>（8） truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚。</p><p>（9） 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老师想触发trigger,还是用delete。</p><p>（10） Truncate table 表名 速度快,而且效率高,因为:?truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。</p><p>（11） TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。</p><p>（12） 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。</p></blockquote><h4 id="10-什么是临时表，临时表什么时候删除"><a href="#10-什么是临时表，临时表什么时候删除" class="headerlink" title="10. 什么是临时表，临时表什么时候删除?"></a>10. 什么是临时表，临时表什么时候删除?</h4><blockquote><p><strong>临时表可以手动删除：</strong><br>DROP TEMPORARY TABLE IF EXISTS temp_tb;</p><p><strong>临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。因此在不同的连接中可以创建同名的临时表，并且操作属于本连接的临时表。<br>创建临时表的语法与创建表语法类似，不同之处是增加关键字TEMPORARY，</strong></p><p>如：</p><p>CREATE TEMPORARY TABLE tmp_table (</p><p>NAME VARCHAR (10) NOT NULL,</p><p>time date NOT NULL<br>);</p><p>select * from tmp_table;</p></blockquote><h4 id="11-非关系型数据库和关系型数据库区别，优势比较"><a href="#11-非关系型数据库和关系型数据库区别，优势比较" class="headerlink" title="11. 非关系型数据库和关系型数据库区别，优势比较?"></a>11. 非关系型数据库和关系型数据库区别，优势比较?</h4><blockquote><p><strong>非关系型数据库的优势：</strong></p><ul><li><strong>性能：</strong>NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。</li><li><strong>可扩展性：</strong>同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。</li></ul><p><strong>关系型数据库的优势：</strong></p><ul><li><strong>复杂查询：</strong>可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。</li><li><strong>事务支持：</strong>使得对于安全性能很高的数据访问要求得以实现。</li></ul><p><strong>其他：</strong></p><p><strong>1.</strong>对于这两类数据库，对方的优势就是自己的弱势，反之亦然。</p><p><strong>2.</strong>NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能，比如MongoDB。</p><p><strong>3.</strong>对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国，比如Redis set nx。</p></blockquote><h4 id="12-数据库范式，根据某个场景设计数据表"><a href="#12-数据库范式，根据某个场景设计数据表" class="headerlink" title="12. 数据库范式，根据某个场景设计数据表?"></a>12. 数据库范式，根据某个场景设计数据表?</h4><blockquote><p><strong>第一范式:</strong>(确保每列保持原子性)所有字段值都是不可分解的原子值。</p><p>第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。<br>第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。<br>上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。</p><p><strong>第二范式:</strong>(确保表中的每列都和主键相关)在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。</p><p>第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。<br>比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键。</p><p><strong>第三范式:</strong>(确保每列都和主键列直接相关,而不是间接相关) 数据表中的每一列数据都和主键直接相关，而不能间接相关。</p><p>第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。<br>比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。</p><p><strong>BCNF:</strong>符合3NF，并且，主属性不依赖于主属性。</p><p>若关系模式属于第二范式，且每个属性都不传递依赖于键码，则R属于BC范式。<br>通常BC范式的条件有多种等价的表述：每个非平凡依赖的左边必须包含键码；每个决定因素必须包含键码。<br>BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。<br>还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。<br>一般，一个数据库设计符合3NF或BCNF就可以了。</p><p><strong>第四范式:</strong>要求把同一表内的多对多关系删除。</p><p><strong>第五范式:</strong>从最终结构重新建立原始结构。</p></blockquote><h4 id="13-什么是-内连接、外连接、交叉连接、笛卡尔积等"><a href="#13-什么是-内连接、外连接、交叉连接、笛卡尔积等" class="headerlink" title="13. 什么是 内连接、外连接、交叉连接、笛卡尔积等?"></a>13. 什么是 内连接、外连接、交叉连接、笛卡尔积等?</h4><blockquote><p><strong>内连接:</strong> 只连接匹配的行</p><p><strong>左外连接:</strong> 包含左边表的全部行（不管右边的表中是否存在与它们匹配的行），以及右边表中全部匹配的行</p><p><strong>右外连接:</strong> 包含右边表的全部行（不管左边的表中是否存在与它们匹配的行），以及左边表中全部匹配的行</p><p>例如1：<br>SELECT a.<em>,b.</em> FROM luntan LEFT JOIN usertable as b ON a.username=b.username</p><p>例如2：<br>SELECT a.<em>,b.</em> FROM city as a FULL OUTER JOIN user as b ON a.username=b.username</p><p><strong>全外连接:</strong> 包含左、右两个表的全部行，不管另外一边的表中是否存在与它们匹配的行。</p><p><strong>交叉连接:</strong> 生成笛卡尔积－它不使用任何匹配或者选取条件，而是直接将一个数据源中的每个行与另一个数据源的每个行都一一匹配</p><p>例如：<br>SELECT type,pub_name FROM titles CROSS JOIN publishers ORDER BY type</p><p><strong>注意：</strong></p><p>很多公司都只是考察是否知道其概念，但是也有很多公司需要不仅仅知道概念，还需要动手写sql,一般都是简单的连接查询，具体关于连接查询的sql练习，参见以下链接：</p><p><a href="https://www.nowcoder.com/ta/sql" target="_blank" rel="noopener">牛客网数据库SQL实战</a></p><p><a href="https://leetcode-cn.com/problemset/database/" target="_blank" rel="noopener">leetcode中文网站数据库练习</a></p><p><a href="http://www.baidu.com/" target="_blank" rel="noopener">我的另一篇文章，常用sql练习50题</a></p></blockquote><h4 id="14-varchar和char的使用场景"><a href="#14-varchar和char的使用场景" class="headerlink" title="14. varchar和char的使用场景?"></a>14. varchar和char的使用场景?</h4><blockquote></blockquote><blockquote><p><strong>1.</strong>char的长度是不可变的，而varchar的长度是可变的。</p><p>定义一个char[10]和varchar[10]。<br>如果存进去的是‘csdn’,那么char所占的长度依然为10，除了字符‘csdn’外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。</p><p><strong>2.</strong>char的存取数度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。<br>char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。<br>varchar是以空间效率为首位。</p><p><strong>3.</strong>char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。<br>varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。</p><p><strong>4.</strong>两者的存储数据都非unicode的字符数据。</p></blockquote><h4 id="15-SQL语言分类"><a href="#15-SQL语言分类" class="headerlink" title="15. SQL语言分类"></a>15. SQL语言分类</h4><blockquote><p><strong>SQL语言共分为四大类：</strong></p><ul><li>数据查询语言DQL</li><li>数据操纵语言DML</li><li>数据定义语言DDL</li><li>数据控制语言DCL。</li></ul><p><strong>1. 数据查询语言DQL</strong></p><p>数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：</p><p>SELECT<br>FROM<br>WHERE</p><p><strong>2 .数据操纵语言DML</strong></p><p>数据操纵语言DML主要有三种形式：</p><ol><li>插入：INSERT</li><li>更新：UPDATE</li><li>删除：DELETE</li></ol><p><strong>3. 数据定义语言DDL</strong></p><p>数据定义语言DDL用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等如：<br>CREATE TABLE/VIEW/INDEX/SYN/CLUSTER</p><p>表 视图 索引 同义词 簇</p><p>DDL操作是隐性提交的！不能rollback</p><p><strong>4. 数据控制语言DCL</strong></p><p>数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如：</p><ol><li>GRANT：授权。</li><li>ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚—ROLLBACK；回滚命令使数据库状态回到上次最后提交的状态。其格式为：<br>SQL&gt;ROLLBACK;</li><li>COMMIT [WORK]：提交。</li></ol><p>在数据库的插入、删除和修改操作时，只有当事务在提交到数据<br>库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看<br>到所做的事情，别人只有在最后提交完成后才可以看到。<br>提交数据有三种类型：显式提交、隐式提交及自动提交。下面分<br>别说明这三种类型。</p><p>(1) 显式提交<br>用COMMIT命令直接完成的提交为显式提交。其格式为：<br>SQL&gt;COMMIT；</p><p>(2) 隐式提交<br>用SQL命令间接完成的提交为隐式提交。这些命令是：<br>ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP，<br>EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。</p><p>(3) 自动提交<br>若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后，<br>系统将自动进行提交，这就是自动提交。其格式为：<br>SQL&gt;SET AUTOCOMMIT ON；</p><p>参考文章：<br><a href="https://www.cnblogs.com/study-s/p/5287529.html" target="_blank" rel="noopener">https://www.cnblogs.com/study-s/p/5287529.html</a></p></blockquote><h4 id="16-like-和-的区别"><a href="#16-like-和-的区别" class="headerlink" title="16. like %和-的区别"></a>16. like %和-的区别</h4><blockquote><p><strong>通配符的分类:</strong></p><p><strong>%百分号通配符:</strong>表示任何字符出现任意次数(可以是0次).</p><p><strong>_下划线通配符:</strong>表示只能匹配单个字符,不能多也不能少,就是一个字符.</p><p><strong>like操作符:</strong> LIKE作用是指示mysql后面的搜索模式是利用通配符而不是直接相等匹配进行比较.</p><p><strong>注意:</strong> 如果在使用like操作符时,后面的没有使用通用匹配符效果是和=一致的,SELECT * FROM products WHERE products.prod_name like ‘1000’;<br>只能匹配的结果为1000,而不能匹配像JetPack 1000这样的结果.</p><ul><li>%通配符使用: 匹配以”yves”开头的记录:(包括记录”yves”) SELECT <em>FROM products WHERE products.prod_name like ‘yves%’;<br>匹配包含”yves”的记录(包括记录”yves”) SELECT</em> FROM products WHERE products.prod_name like ‘%yves%’;<br>匹配以”yves”结尾的记录(包括记录”yves”,不包括记录”yves “,也就是yves后面有空格的记录,这里需要注意) SELECT * FROM products WHERE products.prod_name like ‘%yves’;</li><li><em>通配符使用: SELECT *FROM products WHERE products.prod_name like ‘_yves’; 匹配结果为: 像”yyves”这样记录.<br>SELECT\</em> FROM products WHERE products.prod*name like ‘yves**’; 匹配结果为: 像”yvesHe”这样的记录.(一个下划线只能匹配一个字符,不能多也不能少)</li></ul><p><strong>注意事项:</strong></p><ul><li>注意大小写,在使用模糊匹配时,也就是匹配文本时,mysql是可能区分大小的,也可能是不区分大小写的,这个结果是取决于用户对MySQL的配置方式.如果是区分大小写,那么像YvesHe这样记录是不能被”yves__”这样的匹配条件匹配的.</li><li>注意尾部空格,”%yves”是不能匹配”heyves “这样的记录的.</li><li>注意NULL,%通配符可以匹配任意字符,但是不能匹配NULL,也就是说SELECT * FROM products WHERE products.prod_name like ‘%;是匹配不到products.prod_name为NULL的的记录.</li></ul><p><strong>技巧与建议:</strong></p><p>正如所见， MySQL的通配符很有用。但这种功能是有代价的：通配符搜索的处理一般要比前面讨论的其他搜索所花时间更长。这里给出一些使用通配符要记住的技巧。</p><ul><li>不要过度使用通配符。如果其他操作符能达到相同的目的，应该 使用其他操作符。</li><li>在确实需要使用通配符时，除非绝对有必要，否则不要把它们用 在搜索模式的开始处。把通配符置于搜索模式的开始处，搜索起 来是最慢的。</li><li>仔细注意通配符的位置。如果放错地方，可能不会返回想要的数.</li></ul></blockquote><p>参考博文：<a href="https://blog.csdn.net/u011479200/article/details/78513632" target="_blank" rel="noopener">https://blog.csdn.net/u011479200/article/details/78513632</a></p><h4 id="17-count-、count-1-、count-column-的区别"><a href="#17-count-、count-1-、count-column-的区别" class="headerlink" title="17. count(*)、count(1)、count(column)的区别"></a>17. count(*)、count(1)、count(column)的区别</h4><blockquote><ul><li>count(*)对行的数目进行计算,包含NULL</li><li>count(column)对特定的列的值具有的行数进行计算,不包含NULL值。</li><li>count()还有一种使用方式,count(1)这个用法和count(*)的结果是一样的。</li></ul><p><strong>性能问题:</strong></p><p>1.任何情况下SELECT COUNT(*) FROM tablename是最优选择;</p><p>2.尽量减少SELECT COUNT(*) FROM tablename WHERE COL = ‘value’ 这种查询;</p><p>3.杜绝SELECT COUNT(COL) FROM tablename WHERE COL2 = ‘value’ 的出现。</p><ul><li>如果表没有主键,那么count(1)比count(*)快。</li><li>如果有主键,那么count(主键,联合主键)比count(*)快。</li><li>如果表只有一个字段,count(*)最快。</li></ul><p>count(1)跟count(主键)一样,只扫描主键。count(*)跟count(非主键)一样,扫描整个表。明显前者更快一些。</p></blockquote><h4 id="18-最左前缀原则"><a href="#18-最左前缀原则" class="headerlink" title="18. 最左前缀原则"></a>18. 最左前缀原则</h4><blockquote><p><strong>多列索引：</strong></p><p>ALTER TABLE people ADD INDEX lname_fname_age (lame,fname,age);</p><p>为了提高搜索效率，我们需要考虑运用多列索引,由于索引文件以B－Tree格式保存，所以我们不用扫描任何记录，即可得到最终结果。</p><p>注：在mysql中执行查询时，只能使用一个索引，如果我们在lname,fname,age上分别建索引,执行查询时，只能使用一个索引，mysql会选择一个最严格(获得结果集记录数最少)的索引。</p><p><strong>最左前缀原则：</strong>顾名思义，就是最左优先，上例中我们创建了lname_fname_age多列索引,相当于创建了(lname)单列索引，(lname,fname)组合索引以及(lname,fname,age)组合索引。</p></blockquote><h2 id="6-2-索引"><a href="#6-2-索引" class="headerlink" title="6.2 索引"></a>6.2 索引</h2><h4 id="1-什么是索引？"><a href="#1-什么是索引？" class="headerlink" title="1. 什么是索引？"></a>1. 什么是索引？</h4><blockquote><p><strong>何为索引：</strong></p><p>数据库索引，是数据库管理系统中一个排序的数据结构，索引的实现通常使用B树及其变种B+树。</p><p>在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。</p></blockquote><h4 id="2-索引的作用？它的优点缺点是什么？"><a href="#2-索引的作用？它的优点缺点是什么？" class="headerlink" title="2. 索引的作用？它的优点缺点是什么？"></a>2. 索引的作用？它的优点缺点是什么？</h4><blockquote><p><strong>索引作用：</strong></p><p>协助快速查询、更新数据库表中数据。</p><p>为表设置索引要付出代价的：</p><ul><li><p>一是增加了数据库的存储空间</p></li><li><p>二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动)。</p><h4 id="3-索引的优缺点？"><a href="#3-索引的优缺点？" class="headerlink" title="3.索引的优缺点？"></a><strong>3.索引的优缺点？</strong></h4></li></ul><p><strong>创建索引可以大大提高系统的性能（优点）：</strong></p><p>1.通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。</p><p>2.可以大大加快数据的检索速度，这也是创建索引的最主要的原因。</p><p>3.可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。</p><p>4.在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。</p><p>5.通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。</p><p><strong>增加索引也有许多不利的方面(缺点)：</strong></p><p>1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。</p><p>2.索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。</p><p>3.当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。</p></blockquote><h4 id="4-哪些列适合建立索引、哪些不适合建索引？"><a href="#4-哪些列适合建立索引、哪些不适合建索引？" class="headerlink" title="4. 哪些列适合建立索引、哪些不适合建索引？"></a>4. 哪些列适合建立索引、哪些不适合建索引？</h4><blockquote><p>索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。</p><p><strong>一般来说，应该在这些列上创建索引：</strong></p><p>（1）在经常需要搜索的列上，可以加快搜索的速度；</p><p>（2）在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构；</p><p>（3）在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度；</p><p>（4）在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的；</p><p>（5）在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；</p><p>（6）在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。</p><p><strong>对于有些列不应该创建索引：</strong></p><p>（1）对于那些在查询中很少使用或者参考的列不应该创建索引。</p><p>这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。</p><p>（2）对于那些只有很少数据值的列也不应该增加索引。</p><p>这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。</p><p>（3）对于那些定义为text, image和bit数据类型的列不应该增加索引。</p><p>这是因为，这些列的数据量要么相当大，要么取值很少。</p><p>(4)当修改性能远远大于检索性能时，不应该创建索引。</p><p>这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。</p></blockquote><h4 id="5-什么样的字段适合建索引"><a href="#5-什么样的字段适合建索引" class="headerlink" title="5. 什么样的字段适合建索引"></a>5. 什么样的字段适合建索引</h4><blockquote><p>唯一、不为空、经常被查询的字段</p><h4 id="6-MySQL-B-Tree索引和Hash索引的区别"><a href="#6-MySQL-B-Tree索引和Hash索引的区别" class="headerlink" title="6.MySQL B+Tree索引和Hash索引的区别?"></a><strong>6.MySQL B+Tree索引和Hash索引的区别?</strong></h4><p><strong>Hash索引和B+树索引的特点：</strong></p><ul><li>Hash索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位;</li><li>B+树索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问;</li></ul><p><strong>为什么不都用Hash索引而使用B+树索引？</strong></p><ol><li>Hash索引仅仅能满足”=”,”IN”和””查询，不能使用范围查询,因为经过相应的Hash算法处理之后的Hash值的大小关系，并不能保证和Hash运算前完全一样；</li><li>Hash索引无法被用来避免数据的排序操作，因为Hash值的大小关系并不一定和Hash运算前的键值完全一样；</li><li>Hash索引不能利用部分索引键查询，对于组合索引，Hash索引在计算Hash值的时候是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用；</li><li>Hash索引在任何时候都不能避免表扫描，由于不同索引键存在相同Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要回表查询数据；</li><li>Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。</li></ol><p><strong>补充：</strong></p><p>1.MySQL中，只有HEAP/MEMORY引擎才显示支持Hash索引。</p><p>2.常用的InnoDB引擎中默认使用的是B+树索引，它会实时监控表上索引的使用情况，如果认为建立哈希索引可以提高查询效率，则自动在内存中的“自适应哈希索引缓冲区”建立哈希索引（在InnoDB中默认开启自适应哈希索引），通过观察搜索模式，MySQL会利用index key的前缀建立哈希索引，如果一个表几乎大部分都在缓冲池中，那么建立一个哈希索引能够加快等值查询。<br>B+树索引和哈希索引的明显区别是：</p><p>3.如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据；</p><p>4.如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索；<br>同理，哈希索引没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）；</p><p>5.哈希索引也不支持多列联合索引的最左匹配规则；</p><p>6.B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。</p><p>7.在大多数场景下，都会有范围查询、排序、分组等查询特征，用B+树索引就可以了。</p></blockquote><h4 id="7-B树和B-树的区别"><a href="#7-B树和B-树的区别" class="headerlink" title="7. B树和B+树的区别"></a>7. B树和B+树的区别</h4><blockquote><ol><li>B树，每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为nul，叶子结点不包含任何关键字信息。<br><img src="https://i.imgur.com/RbzI0R8.jpg" alt="img"></li><li>B+树，所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大的顺序链接，所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (而B 树的非终节点也包含需要查找的有效信息)<br><img src="https://i.imgur.com/9VbnDME.jpg" alt="img"></li></ol></blockquote><h4 id="8-为什么说B-比B树更适合实际应用中操作系统的文件索引和数据库索引？"><a href="#8-为什么说B-比B树更适合实际应用中操作系统的文件索引和数据库索引？" class="headerlink" title="8. 为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？"></a>8. 为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？</h4><blockquote><p><strong>1.B+的磁盘读写代价更低</strong></p><p>B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。</p><p><strong>2.B+tree的查询效率更加稳定</strong></p><p>由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</p></blockquote><h4 id="9-聚集索引和非聚集索引区别"><a href="#9-聚集索引和非聚集索引区别" class="headerlink" title="9. 聚集索引和非聚集索引区别?"></a>9. 聚集索引和非聚集索引区别?</h4><blockquote><p><strong>聚合索引(clustered index):</strong></p><p>聚集索引<strong>表记录的排列顺序和索引的排列顺序一致，所以查询效率快，</strong>只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。<br>聚集索引类似于新华字典中用拼音去查找汉字，拼音检索表于书记顺序都是按照a~z排列的，就像相同的逻辑顺序于物理顺序一样，当你需要查找a,ai两个读音的字，或是想一次寻找多个傻(sha)的同音字时，也许向后翻几页，或紧接着下一行就得到结果了。</p><p><strong>非聚合索引(nonclustered index):</strong></p><p>非聚集索引<strong>指定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，</strong>两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。<br>非聚集索引类似在新华字典上通过偏旁部首来查询汉字，检索表也许是按照横、竖、撇来排列的，但是由于正文中是a~z的拼音顺序，所以就类似于逻辑地址于物理地址的不对应。同时适用的情况就在于分组，大数目的不同值，频繁更新的列中，这些情况即不适合聚集索引。</p><p><strong>根本区别：</strong></p><p>聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。</p></blockquote><h2 id="6-3-事务"><a href="#6-3-事务" class="headerlink" title="6.3 事务"></a>6.3 事务</h2><h4 id="1-什么是事务？"><a href="#1-什么是事务？" class="headerlink" title="1. 什么是事务？"></a>1. 什么是事务？</h4><blockquote><p>事务是对数据库中一系列操作进行统一的回滚或者提交的操作，主要用来保证数据的完整性和一致性。</p></blockquote><h4 id="2-事务四大特性（ACID）原子性、一致性、隔离性、持久性"><a href="#2-事务四大特性（ACID）原子性、一致性、隔离性、持久性" class="headerlink" title="2. 事务四大特性（ACID）原子性、一致性、隔离性、持久性?"></a>2. 事务四大特性（ACID）原子性、一致性、隔离性、持久性?</h4><blockquote><p><strong>原子性（Atomicity）:</strong><br>原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。</p><p><strong>一致性（Consistency）:</strong><br>事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。</p><p><strong>隔离性（Isolation）:</strong><br>隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。</p><p><strong>持久性（Durability）:</strong><br>持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</p></blockquote><h4 id="3-事务的并发-事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别"><a href="#3-事务的并发-事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别" class="headerlink" title="3. 事务的并发?事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别?"></a>3. 事务的并发?事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别?</h4><blockquote><p>从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题，然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行， 在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行， 事务的隔离级别可以通过隔离事务属性指定。<br><strong>事务的并发问题</strong></p><p><strong>1、脏读：</strong>事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据</p><p><strong>2、不可重复读：</strong>事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。</p><p><strong>3、幻读：</strong>幻读解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。</p><p>例如：事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作 这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。 而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有跟没有修改一样，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。<br><strong>小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。</strong></p><p><strong>事务的隔离级别</strong></p><p><img src="https://i.imgur.com/xAeWTSp.png" alt="img"></p><p><strong>读未提交：</strong>另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据脏读</p><p><strong>不可重复读：</strong>事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。</p><p><strong>可重复读：</strong>在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。但是，会有幻读现象</p><p><strong>串行化：</strong>最高的隔离级别，在这个隔离级别下，不会产生任何异常。并发的事务，就像事务是在一个个按照顺序执行一样</p></blockquote><p><strong>特别注意：</strong></p><blockquote><p>MySQL默认的事务隔离级别为repeatable-read</p><p>MySQL 支持 4 中事务隔离级别.</p><p>事务的隔离级别要得到底层数据库引擎的支持, 而不是应用程序或者框架的支持.</p><p>Oracle 支持的 2 种事务隔离级别：READ_COMMITED , SERIALIZABLE</p><p>SQL规范所规定的标准，不同的数据库具体的实现可能会有些差异</p><p><strong>MySQL中默认事务隔离级别是“可重复读”时并不会锁住读取到的行</strong></p><p><strong>事务隔离级别：</strong>未提交读时，写数据只会锁住相应的行。</p><p><strong>事务隔离级别为：</strong>可重复读时，写数据会锁住整张表。</p><p><strong>事务隔离级别为：</strong>串行化时，读写数据都会锁住整张表。</p><p>隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，鱼和熊掌不可兼得啊。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed，它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。</p></blockquote><h4 id="4-事务传播行为"><a href="#4-事务传播行为" class="headerlink" title="4. 事务传播行为"></a>4. 事务传播行为</h4><blockquote><p><strong>1.PROPAGATION_REQUIRED：</strong>如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。</p><p><strong>2.PROPAGATION_SUPPORTS：</strong>支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。</p><p><strong>3.PROPAGATION_MANDATORY：</strong>支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。</p><p><strong>4.PROPAGATION_REQUIRES_NEW：</strong>创建新事务，无论当前存不存在事务，都创建新事务。</p><p><strong>5.PROPAGATION_NOT_SUPPORTED：</strong>以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。</p><p><strong>6.PROPAGATION_NEVER：</strong>以非事务方式执行，如果当前存在事务，则抛出异常。</p><p><strong>7.PROPAGATION_NESTED：</strong>如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。</p></blockquote><h4 id="5-嵌套事务"><a href="#5-嵌套事务" class="headerlink" title="5. 嵌套事务"></a>5. 嵌套事务</h4><blockquote><p><strong>什么是嵌套事务？</strong></p><p>嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点就在于那个save point。看几个问题就明了了：</p><p><strong>如果子事务回滚，会发生什么？</strong></p><p>父事务会回滚到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。</p><p><strong>如果父事务回滚，会发生什么？</strong></p><p>父事务回滚，子事务也会跟着回滚！为什么呢，因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理。那么：</p><p><strong>事务的提交，是什么情况？</strong></p><p>是父事务先提交，然后子事务提交，还是子事务先提交，父事务再提交？答案是第二种情况，还是那句话，子事务是父事务的一部分，由父事务统一提交。</p></blockquote><p>参考文章：<a href="https://blog.csdn.net/liangxw1/article/details/51197560" target="_blank" rel="noopener">https://blog.csdn.net/liangxw1/article/details/51197560</a></p><h2 id="6-4-存储引擎"><a href="#6-4-存储引擎" class="headerlink" title="6.4 存储引擎"></a>6.4 存储引擎</h2><h4 id="1-MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别"><a href="#1-MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别" class="headerlink" title="1. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别?"></a>1. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别?</h4><blockquote><p><strong>两种存储引擎的大致区别表现在：</strong></p><p>1.<strong>InnoDB支持事务，MyISAM不支持，</strong> <strong>这一点是非常之重要。</strong>事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而MyISAM就不可以了。</p><p>2.MyISAM适合查询以及插入为主的应用。</p><p>3.InnoDB适合频繁修改以及涉及到安全性较高的应用。</p><p>4.InnoDB支持外键，MyISAM不支持。</p><p>5.从MySQL5.5.5以后，InnoDB是默认引擎。</p><p>6.InnoDB不支持FULLTEXT类型的索引。</p><p>7.InnoDB中不保存表的行数，如select count(<em>) from table时，InnoDB需要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count(</em>)语句包含where条件时MyISAM也需要扫描整个表。</p><p>8.对于自增长的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中可以和其他字段一起建立联合索引。</p><p>9.DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除，效率非常慢。MyISAM则会重建表。</p><p>10.InnoDB支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’。</p></blockquote><h4 id="2-MySQL存储引擎MyISAM与InnoDB如何选择"><a href="#2-MySQL存储引擎MyISAM与InnoDB如何选择" class="headerlink" title="2. MySQL存储引擎MyISAM与InnoDB如何选择"></a>2. MySQL存储引擎MyISAM与InnoDB如何选择</h4><blockquote><p>MySQL有多种存储引擎，每种存储引擎有各自的优缺点，可以择优选择使用：MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE。</p><p>虽然MySQL里的存储引擎不只是MyISAM与InnoDB这两个，但常用的就是两个。<br>关于MySQL数据库提供的两种存储引擎，MyISAM与InnoDB选择使用：</p></blockquote><ul><li>1.INNODB会支持一些关系数据库的高级功能，如事务功能和行级锁，MyISAM不支持。</li><li>2.MyISAM的性能更优，占用的存储空间少，所以，选择何种存储引擎，视具体应用而定。</li></ul><blockquote><p>如果你的应用程序一定要使用事务，毫无疑问你要选择INNODB引擎。但要注意，INNODB的行级锁是有条件的。在where条件没有使用主键时，照样会锁全表。比如DELETE FROM mytable这样的删除语句。</p><p>如果你的应用程序对查询性能要求较高，就要使用MyISAM了。MyISAM索引和数据是分开的，而且其索引是压缩的，可以更好地利用内存。所以它的查询性能明显优于INNODB。压缩后的索引也能节约一些磁盘空间。MyISAM拥有全文索引的功能，这可以极大地优化LIKE查询的效率。</p><p>有人说MyISAM只能用于小型应用，其实这只是一种偏见。如果数据量比较大，这是需要通过升级架构来解决，比如分表分库，而不是单纯地依赖存储引擎。</p><p>现在一般都是选用innodb了，主要是MyISAM的全表锁，读写串行问题，并发效率锁表，效率低，MyISAM对于读写密集型应用一般是不会去选用的。<br>MEMORY存储引擎</p><p>MEMORY是MySQL中一类特殊的存储引擎。它使用存储在内存中的内容来创建表，而且数据全部放在内存中。这些特性与前面的两个很不同。<br>每个基于MEMORY存储引擎的表实际对应一个磁盘文件。该文件的文件名与表名相同，类型为frm类型。该文件中只存储表的结构。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。值得注意的是，服务器需要有足够的内存来维持MEMORY存储引擎的表的使用。如果不需要了，可以释放内存，甚至删除不需要的表。</p><p>MEMORY默认使用哈希索引。速度比使用B型树索引快。当然如果你想用B型树索引，可以在创建索引时指定。</p><p>注意，MEMORY用到的很少，因为它是把数据存到内存中，如果内存出现异常就会影响数据。如果重启或者关机，所有数据都会消失。因此，基于MEMORY的表的生命周期很短，一般是一次性的。</p></blockquote><h4 id="3-MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景"><a href="#3-MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景" class="headerlink" title="3. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景?"></a>3. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景?</h4><blockquote><p><strong>事务处理上方面</strong></p></blockquote><ul><li>MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。</li><li>InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</li></ul><blockquote><p><strong>锁级别</strong></p></blockquote><ul><li>MyISAM：只支持表级锁，用户在操作MyISAM表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。</li><li>InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。</li></ul><blockquote><p><strong>关于存储引擎MyISAM和InnoDB的其他参考资料如下：</strong></p><p><a href="http://blog.csdn.net/lc0817/article/details/52757194" target="_blank" rel="noopener">MySQL存储引擎中的MyISAM和InnoDB区别详解</a></p><p><a href="https://www.cnblogs.com/kevingrace/p/5685355.html" target="_blank" rel="noopener">MySQL存储引擎之MyISAM和Innodb总结性梳理</a></p></blockquote><h2 id="6-5-优化"><a href="#6-5-优化" class="headerlink" title="6.5 优化"></a>6.5 优化</h2><h4 id="1-查询语句不同元素（where、jion、limit、group-by、having等等）执行先后顺序"><a href="#1-查询语句不同元素（where、jion、limit、group-by、having等等）执行先后顺序" class="headerlink" title="1. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序?"></a>1. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序?</h4><ul><li>1.查询中用到的关键词主要包含<strong>六个</strong>，并且他们的顺序依次为 <strong>select–from–where–group by–having–order by</strong></li></ul><blockquote><p><strong>其中select和from是必须的，其他关键词是可选的，这六个关键词的执行顺序 与sql语句的书写顺序并不是一样的，而是按照下面的顺序来执行</strong></p><p><strong>from:</strong>需要从哪个数据表检索数据</p></blockquote><blockquote><p><strong>where:</strong>过滤表中数据的条件</p><p><strong>group by:</strong>如何将上面过滤出的数据分组</p><p><strong>having:</strong>对上面已经分组的数据进行过滤的条件</p><p><strong>select:</strong>查看结果集中的哪个列，或列的计算结果</p><p><strong>order by :</strong>按照什么样的顺序来查看返回的数据</p></blockquote><ul><li>2.<strong>from后面的表关联，是自右向左解析 而where条件的解析顺序是自下而上的。</strong></li></ul><blockquote><p>也就是说，在写SQL语句的时候，尽量把数据量小的表放在最右边来进行关联（用小表去匹配大表），而把能筛选出小量数据的条件放在where语句的最左边 （用小表去匹配大表）</p><p>其他参考资源：<br><a href="http://www.cnblogs.com/huminxxl/p/3149097.html" target="_blank" rel="noopener">http://www.cnblogs.com/huminxxl/p/3149097.html</a></p></blockquote><h4 id="2-使用explain优化sql和索引"><a href="#2-使用explain优化sql和索引" class="headerlink" title="2. 使用explain优化sql和索引?"></a>2. 使用explain优化sql和索引?</h4><blockquote><p><strong>对于复杂、效率低的sql语句，我们通常是使用explain sql 来分析sql语句，这个语句可以打印出，语句的执行。这样方便我们分析，进行优化</strong></p><p><strong>table：</strong>显示这一行的数据是关于哪张表的</p><p><strong>type：</strong>这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、index和ALL</p><p><strong>all:</strong>full table scan ;MySQL将遍历全表以找到匹配的行；</p><p><strong>index:</strong> index scan; index 和 all的区别在于index类型只遍历索引；</p><p><strong>range：</strong>索引范围扫描，对索引的扫描开始于某一点，返回匹配值的行，常见与between ，等查询；</p><p><strong>ref：</strong>非唯一性索引扫描，返回匹配某个单独值的所有行，常见于使用非唯一索引即唯一索引的非唯一前缀进行查找；</p><p><strong>eq_ref：</strong>唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常用于主键或者唯一索引扫描；</p><p><strong>const，system：</strong>当MySQL对某查询某部分进行优化，并转为一个常量时，使用这些访问类型。如果将主键置于where列表中，MySQL就能将该查询转化为一个常量。</p><p><strong>possible_keys：</strong>显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句</p><p><strong>key：</strong> 实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MySQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MySQL忽略索引</p><p><strong>key_len：</strong>使用的索引的长度。在不损失精确性的情况下，长度越短越好</p><p><strong>ref：</strong>显示索引的哪一列被使用了，如果可能的话，是一个常数</p><p><strong>rows：</strong>MySQL认为必须检查的用来返回请求数据的行数</p><p><strong>Extra：</strong>关于MySQL如何解析查询的额外信息。将在表4.3中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思MySQL根本不能使用索引，结果是检索会很慢。</p></blockquote><h4 id="3-MySQL慢查询怎么解决"><a href="#3-MySQL慢查询怎么解决" class="headerlink" title="3. MySQL慢查询怎么解决?"></a>3. MySQL慢查询怎么解决?</h4><blockquote><ul><li>slow_query_log 慢查询开启状态。</li><li>slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）。</li><li>long_query_time 查询超过多少秒才记录。</li></ul></blockquote><h2 id="6-6-数据库锁"><a href="#6-6-数据库锁" class="headerlink" title="6.6 数据库锁"></a>6.6 数据库锁</h2><h4 id="1-mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决"><a href="#1-mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决" class="headerlink" title="1. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决?"></a>1. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决?</h4><blockquote><p><strong>MySQL有三种锁的级别：</strong>页级、表级、行级。</p><ul><li><strong>表级锁：</strong>开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。</li><li><strong>行级锁：</strong>开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。</li><li><strong>页面锁：</strong>开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般<br><strong>什么情况下会造成死锁?</strong></li></ul><p><strong>什么是死锁？</strong></p><p><strong>死锁:</strong> 是指两个或两个以上的进程在执行过程中。因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程。</p><p>表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。</p><p><strong>死锁的关键在于：</strong>两个(或以上)的Session加锁的顺序不一致。</p><p>那么对应的解决死锁问题的关键就是：让不同的session加锁有次序。</p><p><strong>死锁的解决办法?</strong></p><p>1.查出的线程杀死 kill<br>SELECT trx_MySQL_thread_id FROM information_schema.INNODB_TRX;</p><p>2.设置锁的超时时间<br>Innodb 行锁的等待时间，单位秒。可在会话级别设置，RDS 实例该参数的默认值为 50（秒）。</p><p>生产环境不推荐使用过大的 innodb_lock_wait_timeout参数值<br>该参数支持在会话级别修改，方便应用在会话级别单独设置某些特殊操作的行锁等待超时时间，如下：<br>set innodb_lock_wait_timeout=1000; —设置当前会话 Innodb 行锁等待超时时间，单位秒。</p><p>3.指定获取锁的顺序</p></blockquote><h4 id="2-有哪些锁（乐观锁悲观锁），select-时怎么加排它锁"><a href="#2-有哪些锁（乐观锁悲观锁），select-时怎么加排它锁" class="headerlink" title="2. 有哪些锁（乐观锁悲观锁），select 时怎么加排它锁?"></a>2. 有哪些锁（乐观锁悲观锁），select 时怎么加排它锁?</h4><blockquote><p><strong>悲观锁（Pessimistic Lock）:</strong></p><p><strong>悲观锁特点:</strong>先获取锁，再进行业务操作。</p><p>即“悲观”的认为获取锁是非常有可能失败的，因此要先确保获取锁成功再进行业务操作。通常所说的<strong>“一锁二查三更新”即指的是使用悲观锁。</strong>通常来讲在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select for update时会获取被select中的数据行的行锁，因此其他并发执行的select for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。</p><p><strong>补充：</strong><br>不同的数据库对select for update的实现和支持都是有所区别的，</p><ul><li>oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，MySQL就没有no wait这个选项。</li><li>MySQL还有个问题是select for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此如果在MySQL中用悲观锁务必要确定走了索引，而不是全表扫描。</li></ul><p><strong>乐观锁（Optimistic Lock）:</strong></p><p><strong>1.</strong>乐观锁，也叫乐观并发控制，它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，那么当前正在提交的事务会进行回滚。</p><p><strong>2.\</strong>*<em>乐观锁的特点先进行业务操作，不到万不得已不去拿锁。*</em>即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。<br>乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。</p><p><strong>3.</strong>一般的做法是<strong>在需要锁的数据上增加一个版本号，或者时间戳</strong>，</p><p><strong>实现方式举例如下：</strong></p><p><strong>乐观锁（给表加一个版本号字段）</strong> 这个并不是乐观锁的定义，给表加版本号，是<strong>数据库实现乐观锁的一种方式</strong>。</p><ol><li>SELECT data AS old_data, version AS old_version FROM …;</li><li>根据获取的数据进行业务操作，得到new_data和new_version</li><li>UPDATE SET data = new_data, version = new_version WHERE version = old_version</li></ol><p>if (updated row &gt; 0) {</p><p>// 乐观锁获取成功，操作完成</p><p>} else {</p><p>// 乐观锁获取失败，回滚并重试</p><p>}</p><p><strong>注意：</strong></p><ul><li>乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能</li><li>乐观锁还适用于一些比较特殊的场景，例如在业务操作过程中无法和数据库保持连接等悲观锁无法适用的地方。</li></ul><p><strong>总结：</strong><br>悲观锁和乐观锁是数据库用来保证数据并发安全防止更新丢失的两种方法，例子在select … for update前加个事务就可以防止更新丢失。悲观锁和乐观锁大部分场景下差异不大，一些独特场景下有一些差别，一般我们可以从如下几个方面来判断。</p><ul><li><strong>响应速度：</strong> 如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁。’</li><li><strong>冲突频率：</strong> 如果冲突频率非常高，建议采用悲观锁，保证成功率，如果冲突频率大，乐观锁会需要多次重试才能成功，代价比较大。</li><li><strong>重试代价：</strong> 如果重试代价大，建议采用悲观锁。</li></ul></blockquote><h1 id="七-操作系统"><a href="#七-操作系统" class="headerlink" title="七. 操作系统"></a>七. 操作系统</h1><h2 id="7-1-Linux"><a href="#7-1-Linux" class="headerlink" title="7.1 Linux"></a>7.1 Linux</h2><h3 id="1-文件管理"><a href="#1-文件管理" class="headerlink" title="1. 文件管理"></a>1. 文件管理</h3><h4 id="1-cat-命令：连接文件"><a href="#1-cat-命令：连接文件" class="headerlink" title="1. cat 命令：连接文件"></a>1. cat 命令：连接文件</h4><ul><li><p><strong>描述：</strong>cat 命令用于连接文件并打印到标准输出设备上。</p></li><li><p><strong>功能：</strong></p><pre class="line-numbers language-shell"><code class="language-shell"># 1.一次显示整个文件: cat filename# 2.从键盘创建一个文件:cat > filename# 3.将几个文件合并为一个文件:cat file1 file2 > file<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>示例：</strong></p><p>（1）把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里</p><pre class="line-numbers language-shell"><code class="language-shell">cat -n log2012.log log2013.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里</p><pre class="line-numbers language-shell"><code class="language-shell">cat -b log2012.log log2013.log log.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）使用 here doc 生成新文件</p><pre class="line-numbers language-shell"><code class="language-shell">cat >log.txt <<EOF>Hello>World>PWD=$(pwd)>EOFls -l log.txtcat log.txtHelloWorldPWD=/opt/soft/test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）反向列示</p><pre class="line-numbers language-shell"><code class="language-shell">tac log.txtPWD=/opt/soft/testWorldHello<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="2-chmod-命令-权限控制"><a href="#2-chmod-命令-权限控制" class="headerlink" title="2. chmod 命令: 权限控制"></a>2. chmod 命令: 权限控制</h4><p>Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。</p><p>用于改变 linux 系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。</p><p>每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。</p><p>以文件 log2012.log 为例：</p><pre class="line-numbers language-shell"><code class="language-shell">-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。</p><p>常用参数：</p><ul><li>-c 当发生改变时，报告处理信息</li><li>-R 处理指定目录以及其子目录下所有文件</li></ul><p>权限范围：</p><ul><li>u ：目录或者文件的当前的用户</li><li>g ：目录或者文件的当前的群组</li><li>o ：除了目录或者文件的当前用户或群组之外的用户或者群组</li><li>a ：所有的用户及群组</li></ul><p>权限代号：</p><ul><li>r ：读权限，用数字4表示</li><li>w ：写权限，用数字2表示</li><li>x ：执行权限，用数字1表示</li><li>- ：删除权限，用数字0表示</li><li>s ：特殊权限</li></ul><p>实例：</p><p>（1）增加文件 t.log 所有用户可执行权限</p><pre class="line-numbers language-shell"><code class="language-shell">chmod a+x t.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息</p><pre class="line-numbers language-shell"><code class="language-shell">chmod u=r t.log -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限</p><pre class="line-numbers language-shell"><code class="language-shell">chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）将 test 目录及其子目录所有文件添加可读权限</p><pre class="line-numbers language-shell"><code class="language-shell">chmod u+r,g+r,o+r -R text/ -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-chown-命令：更改拥有者权限"><a href="#3-chown-命令：更改拥有者权限" class="headerlink" title="3. chown 命令：更改拥有者权限"></a>3. chown 命令：更改拥有者权限</h4><p>chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。</p><ul><li>-c 显示更改的部分的信息</li><li>-R 处理指定目录及子目录</li></ul><p>实例：</p><p>（1）改变拥有者和群组 并显示改变信息</p><pre class="line-numbers language-shell"><code class="language-shell">chown -c mail:mail log2012.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）改变文件群组</p><pre class="line-numbers language-shell"><code class="language-shell">chown -c :mail t.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）改变文件夹及子文件目录属主及属组为 mail</p><pre class="line-numbers language-shell"><code class="language-shell">chown -cR mail: test/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-cp-命令：复制文件"><a href="#4-cp-命令：复制文件" class="headerlink" title="4. cp 命令：复制文件"></a>4. cp 命令：复制文件</h4><p>将源文件复制至目标文件，或将多个源文件复制至目标目录。</p><p>注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！</p><ul><li>-i 提示</li><li>-r 复制目录及目录内所有项目</li><li>-a 复制的文件与原文件时间一样</li></ul><p>实例：</p><p>（1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。</p><pre class="line-numbers language-shell"><code class="language-shell">cp -ai a.txt test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）为 a.txt 建议一个链接（快捷方式）</p><pre class="line-numbers language-shell"><code class="language-shell">cp -s a.txt link_a.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-find-命令：查找文件"><a href="#5-find-命令：查找文件" class="headerlink" title="5. find 命令：查找文件"></a>5. find 命令：查找文件</h4><p>用于在文件树中查找文件，并作出相应的处理。</p><p>命令格式：</p><pre class="line-numbers language-shell"><code class="language-shell">find pathname -options [-print -exec -ok ...]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>命令参数：</p><ul><li>pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。</li><li>-print： find命令将匹配的文件输出到标准输出。</li><li>-exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为’command’ { } ;，注意{ }和\；之间的空格。</li><li>-ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。</li></ul><p>命令选项：</p><ul><li>-name 按照文件名查找文件</li><li>-perm 按文件权限查找文件</li><li>-user 按文件属主查找文件</li><li>-group 按照文件所属的组来查找文件。</li><li>-type 查找某一类型的文件，诸如：</li></ul><ol><li>b - 块设备文件</li><li>d - 目录</li><li>c - 字符设备文件</li><li>l - 符号链接文件</li><li>p - 管道文件</li><li>f - 普通文件</li></ol><p>实例：</p><p>（1）查找 48 小时内修改过的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find -atime -2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在当前目录查找 以 .log 结尾的文件。 . 代表当前目录</p><pre class="line-numbers language-shell"><code class="language-shell">find ./ -name '*.log'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查找 /opt 目录下 权限为 777 的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find /opt -perm 777<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）查找大于 1K 的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find -size +1000c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查找等于 1000 字符的文件</p><pre class="line-numbers language-shell"><code class="language-shell">find -size 1000c <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>-exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。</p><h4 id="6-head-命令：显示文件开头"><a href="#6-head-命令：显示文件开头" class="headerlink" title="6. head 命令：显示文件开头"></a>6. head 命令：显示文件开头</h4><p>head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。</p><p>常用参数：</p><pre class="line-numbers language-shell"><code class="language-shell">-n<行数> 显示的行数（行数为复数表示从最后向前数）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例：</p><p>（1）显示 1.log 文件中前 20 行</p><pre class="line-numbers language-shell"><code class="language-shell">head 1.log -n 20<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）显示 1.log 文件前 20 字节</p><pre class="line-numbers language-shell"><code class="language-shell">head -c 20 log2014.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）显示 t.log最后 10 行</p><pre class="line-numbers language-shell"><code class="language-shell">head -n -10 t.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="7-less-命令：文件浏览"><a href="#7-less-命令：文件浏览" class="headerlink" title="7. less 命令：文件浏览"></a>7. less 命令：文件浏览</h4><p>less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。</p><p>常用命令参数：</p><ul><li>-i 忽略搜索时的大小写</li><li>-N 显示每行的行号</li><li>-o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来</li><li>-s 显示连续空行为一行</li><li>/字符串：向下搜索“字符串”的功能</li><li>?字符串：向上搜索“字符串”的功能</li><li>n：重复前一个搜索（与 / 或 ? 有关）</li><li>N：反向重复前一个搜索（与 / 或 ? 有关）</li><li>-x &lt;数字&gt; 将“tab”键显示为规定的数字空格</li><li>b 向后翻一页</li><li>d 向后翻半页</li><li>h 显示帮助界面</li><li>Q 退出less 命令</li><li>u 向前滚动半页</li><li>y 向前滚动一行</li><li>空格键 滚动一行</li><li>回车键 滚动一页</li><li>[pagedown]： 向下翻动一页</li><li>[pageup]： 向上翻动一页</li></ul><p>实例：</p><p>（1）ps 查看进程信息并通过 less 分页显示</p><pre class="line-numbers language-shell"><code class="language-shell">ps -aux | less -N<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看多个文件</p><pre class="line-numbers language-shell"><code class="language-shell">less 1.log 2.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以使用 n 查看下一个，使用 p 查看前一个。</p><h4 id="8-ln-命令：文件同步链接"><a href="#8-ln-命令：文件同步链接" class="headerlink" title="8. ln 命令：文件同步链接"></a>8. ln 命令：文件同步链接</h4><p>功能是为文件在另外一个位置建立一个同步的链接，当在不同目录需要该问题时，就不需要为每一个目录创建同样的文件，通过 ln 创建的链接（link）减少磁盘占用量。</p><p>链接分类：软件链接及硬链接</p><p>软链接：</p><p>1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式</p><p>2.软链接可以 跨文件系统 ，硬链接不可以</p><p>3.软链接可以对一个不存在的文件名进行链接</p><p>4.软链接可以对目录进行链接</p><p>硬链接:</p><p>1.硬链接，以文件副本的形式存在。但不占用实际空间。</p><p>2.不允许给目录创建硬链接</p><p>3.硬链接只有在同一个文件系统中才能创建</p><p>需要注意：</p><p>第一：ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化；</p><p>第二：ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。</p><p>第三：ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。</p><p>常用参数：</p><ol><li>-b 删除，覆盖以前建立的链接</li><li>-s 软链接（符号链接）</li><li>-v 显示详细处理过程</li></ol><p>实例：</p><p>（1）给文件创建软链接，并显示操作信息</p><pre class="line-numbers language-shell"><code class="language-shell">ln -sv source.log link.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）给文件创建硬链接，并显示操作信息</p><pre class="line-numbers language-shell"><code class="language-shell">ln -v source.log link1.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）给目录创建软链接</p><pre class="line-numbers language-shell"><code class="language-shell">ln -sv /opt/soft/test/test3 /opt/soft/test/test5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="9-locate-命令：快速查找档案"><a href="#9-locate-命令：快速查找档案" class="headerlink" title="9. locate 命令：快速查找档案"></a>9. locate 命令：快速查找档案</h4><p>​       locate 通过搜寻系统内建文档数据库达到快速找到档案，数据库由 updatedb 程序来更新，updatedb 是由 cron daemon 周期性调用的。默认情况下 locate 命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是 locate 所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb 每天会跑一次，可以由修改 crontab 来更新设定值 (etc/crontab)。</p><p>locate 与 find 命令相似，可以使用如 *、? 等进行正则匹配查找</p><p>常用参数：</p><ol><li>-l num（要显示的行数）</li><li>-f 将特定的档案系统排除在外，如将proc排除在外</li><li>-r 使用正则运算式做为寻找条件</li></ol><p>实例：</p><p>（1）查找和 pwd 相关的所有文件(文件名中包含 pwd）</p><pre class="line-numbers language-shell"><code class="language-shell">locate pwd<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）搜索 etc 目录下所有以 sh 开头的文件</p><pre class="line-numbers language-shell"><code class="language-shell">locate /etc/sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查找 /var 目录下，以 reason 结尾的文件</p><pre class="line-numbers language-shell"><code class="language-shell">locate -r '^/var.*reason$'（其中.表示一个字符，*表示任务多个；.*表示任意多个字符）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="10-more-命令：文件分页浏览"><a href="#10-more-命令：文件分页浏览" class="headerlink" title="10. more 命令：文件分页浏览"></a>10. more 命令：文件分页浏览</h4><p>​      功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。</p><p>命令参数：</p><ul><li>+n 从笫 n 行开始显示</li><li>-n 定义屏幕大小为n行</li><li>+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示</li><li>-c 从顶部清屏，然后显示</li><li>-d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能</li><li>-l 忽略Ctrl+l（换页）字符</li><li>-p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似</li><li>-s 把连续的多个空行显示为一行</li><li>-u 把文件内容中的下画线去掉</li></ul><p>常用操作命令：</p><ul><li>Enter 向下 n 行，需要定义。默认为 1 行</li><li>Ctrl+F 向下滚动一屏</li><li>空格键 向下滚动一屏</li><li>Ctrl+B 返回上一屏</li><li>= 输出当前行的行号</li><li>:f 输出文件名和当前行的行号</li><li>V 调用vi编辑器</li><li>!命令 调用Shell，并执行命令</li><li>q 退出more</li></ul><p>实例：</p><p>（1）显示文件中从第3行起的内容</p><pre class="line-numbers language-shell"><code class="language-shell">more +3 text.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在所列出文件目录详细信息，借助管道使每次显示 5 行</p><pre class="line-numbers language-shell"><code class="language-shell">ls -l | more -5<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>按空格显示下 5 行。</p><h4 id="11-mv-命令：文件移动"><a href="#11-mv-命令：文件移动" class="headerlink" title="11. mv 命令：文件移动"></a>11. mv 命令：文件移动</h4><p>移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。</p><p>当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。</p><p>实例：</p><p>（1）将文件 test.log 重命名为 test1.txt</p><pre class="line-numbers language-shell"><code class="language-shell">mv test.log test1.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中</p><pre class="line-numbers language-shell"><code class="language-shell">mv llog1.txt log2.txt log3.txt /test3<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖</p><pre class="line-numbers language-shell"><code class="language-shell">mv -i log1.txt log2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）移动当前文件夹下的所有文件到上一级目录</p><pre class="line-numbers language-shell"><code class="language-shell">mv * ../<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="12-rm-命令：文件删除"><a href="#12-rm-命令：文件删除" class="headerlink" title="12. rm 命令：文件删除"></a>12. rm 命令：文件删除</h4><p>​     删除一个目录中的一个或多个文件或目录，如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。</p><pre class="line-numbers language-shell"><code class="language-shell">rm [选项] 文件…<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例：</p><p>（1）删除任何 .log 文件，删除前逐一询问确认：</p><pre class="line-numbers language-shell"><code class="language-shell">rm -i *.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）删除 test 子目录及子目录中所有档案删除，并且不用一一确认：</p><pre class="line-numbers language-shell"><code class="language-shell">rm -rf test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）删除以 -f 开头的文件</p><pre class="line-numbers language-shell"><code class="language-shell">rm -- -f*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="13-tail-命令：显示文件末尾"><a href="#13-tail-命令：显示文件末尾" class="headerlink" title="13. tail 命令：显示文件末尾"></a>13. tail 命令：显示文件末尾</h4><p>用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。</p><p>常用参数：</p><ul><li>-f 循环读取（常用于查看递增的日志文件）</li><li>-n&lt;行数&gt; 显示行数（从后向前）</li></ul><p>（1）循环读取逐渐增加的文件内容</p><pre class="line-numbers language-shell"><code class="language-shell">ping 127.0.0.1 > ping.log &<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。</p><pre class="line-numbers language-shell"><code class="language-shell">tail -f ping.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（查看日志）</p><h4 id="14-touch-命令：修改时间属性"><a href="#14-touch-命令：修改时间属性" class="headerlink" title="14. touch 命令：修改时间属性"></a>14. touch 命令：修改时间属性</h4><p>Linux touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。</p><p>ls -l 可以显示档案的时间记录。</p><p>语法</p><pre class="line-numbers language-shell"><code class="language-shell">touch [-acfm][-d<日期时间>][-r<参考文件或目录>] [-t<日期时间>][--help][--version][文件或目录…]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><ul><li>a 改变档案的读取时间记录。</li><li>m 改变档案的修改时间记录。</li><li>c 假如目的档案不存在，不会建立新的档案。与 –no-create 的效果一样。</li><li>f 不使用，是为了与其他 unix 系统的相容性而保留。</li><li>r 使用参考档的时间记录，与 –file 的效果一样。</li><li>d 设定时间与日期，可以使用各种不同的格式。</li><li>t 设定档案的时间记录，格式与 date 指令相同。</li><li>–no-create 不会建立新档案。</li><li>–help 列出指令格式。</li><li>–version 列出版本讯息。</li></ul><p>实例</p><p>使用指令”touch”修改文件”testfile”的时间属性为当前系统时间，输入如下命令：</p><pre class="line-numbers language-shell"><code class="language-shell">$ touch testfile                #修改文件的时间属性 <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>首先，使用ls命令查看testfile文件的属性，如下所示：</p><pre class="line-numbers language-shell"><code class="language-shell">$ ls -l testfile                #查看文件的时间属性  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>#原来文件的修改时间为16:09</p><pre class="line-numbers language-shell"><code class="language-shell">-rw-r--r-- 1 hdd hdd 55 2011-08-22 16:09 testfile  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行指令”touch”修改文件属性以后，并再次查看该文件的时间属性，如下所示：</p><pre class="line-numbers language-shell"><code class="language-shell">$ touch testfile                #修改文件时间属性为当前系统时间  $ ls -l testfile                #查看文件的时间属性  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>#修改后文件的时间属性为当前系统时间</p><pre class="line-numbers language-shell"><code class="language-shell">-rw-r--r-- 1 hdd hdd 55 2011-08-22 19:53 testfile  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用指令”touch”时，如果指定的文件不存在，则将创建一个新的空白文件。例如，在当前目录下，使用该指令创建一个空白文件”file”，输入如下命令：</p><pre class="line-numbers language-shell"><code class="language-shell">$ touch file            #创建一个名为“file”的新的空白文件 <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="15-vim-命令：文本编辑器"><a href="#15-vim-命令：文本编辑器" class="headerlink" title="15. vim 命令：文本编辑器"></a>15. vim 命令：文本编辑器</h4><p>Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。</p><p>打开文件并跳到第 10 行：vim +10 filename.txt 。</p><p>打开文件跳到第一个匹配的行：vim +/search-term filename.txt 。</p><p>以只读模式打开文件：vim -R /etc/passwd 。</p><p>基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。</p><p>简单的说，我们可以将这三个模式想成底下的图标来表示：</p><img src="https://pic3.zhimg.com/80/v2-3545dde7a3b6e7d389b95f327b7508f2_1440w.jpg" alt="img" style="zoom:50%;"><h4 id="16-whereis-命令：搜索程序名"><a href="#16-whereis-命令：搜索程序名" class="headerlink" title="16. whereis 命令：搜索程序名"></a>16. whereis 命令：搜索程序名</h4><p>whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis 及 locate 都是基于系统内建的数据库进行搜索，因此效率很高，而find则是遍历硬盘查找文件。</p><p>常用参数：</p><ul><li>-b 定位可执行文件。</li><li>-m 定位帮助文件。</li><li>-s 定位源代码文件。</li><li>-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。</li></ul><p>实例：</p><p>（1）查找 locate 程序相关文件</p><pre class="line-numbers language-shell"><code class="language-shell">whereis locate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查找 locate 的源码文件</p><pre class="line-numbers language-shell"><code class="language-shell">whereis -s locate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查找 lcoate 的帮助文件</p><pre class="line-numbers language-shell"><code class="language-shell">whereis -m locate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="17-which-命令：查找文件"><a href="#17-which-命令：查找文件" class="headerlink" title="17. which 命令：查找文件"></a>17. which 命令：查找文件</h4><p>在 linux 要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索：</p><ul><li>which 查看可执行文件的位置。</li><li>whereis 查看文件的位置。</li><li>locate 配合数据库查看文件位置。</li><li>find 实际搜寻硬盘查询文件名称。</li></ul><p>which 是在 PATH 就是指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果。使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。</p><p>常用参数：</p><pre class="line-numbers language-shell"><code class="language-shell">-n 　指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>实例：</p><p>（1）查看 ls 命令是否存在，执行哪个</p><pre class="line-numbers language-shell"><code class="language-shell">which ls<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看 which</p><pre class="line-numbers language-shell"><code class="language-shell">which which<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查看 cd</p><pre class="line-numbers language-shell"><code class="language-shell">which cd（显示不存在，因为 cd 是内建命令，而 which 查找显示是 PATH 中的命令）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看当前 PATH 配置：</p><pre class="line-numbers language-shell"><code class="language-shell">echo $PATH<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>或使用 env 查看所有环境变量及对应值</p><h3 id="2-文档编辑"><a href="#2-文档编辑" class="headerlink" title="2. 文档编辑"></a>2. 文档编辑</h3><h4 id="1-grep-命令：文本搜索"><a href="#1-grep-命令：文本搜索" class="headerlink" title="1. grep 命令：文本搜索"></a>1. grep 命令：文本搜索</h4><p>强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。</p><p>grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。</p><p>命令格式：</p><pre class="line-numbers language-shell"><code class="language-shell">grep [option] pattern file|dir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>常用参数：</p><ul><li>-A n –after-context显示匹配字符后n行</li><li>-B n –before-context显示匹配字符前n行</li><li>-C n –context 显示匹配字符前后n行</li><li>-c –count 计算符合样式的列数</li><li>-i 忽略大小写</li><li>-l 只列出文件内容符合指定的样式的文件名称</li><li>-f 从文件中读取关键词</li><li>-n 显示匹配内容的所在文件中行数</li><li>-R 递归查找文件夹</li></ul><p>grep 的规则表达式:</p><ul><li>^  #锚定行的开始 如：’^grep’匹配所有以grep开头的行。</li><li>$ #锚定行的结束 如：’grep’匹配所有以grep结尾的行。</li><li>. #匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。</li><li>* #匹配零个或多个先前字符 如：’*grep’匹配所有一个或多个空格后紧跟grep的行。</li><li>.* #一起用代表任意字符。</li><li>[] #匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。</li><li>[^] #匹配一个不在指定范围内的字符，如：’[^A-FH-Z]rep’匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。</li><li>(..) #标记匹配字符，如’(love)‘，love被标记为1。</li><li>&lt; #锚定单词的开始，如:’&lt;grep’匹配包含以grep开头的单词的行。</li><li>&gt; #锚定单词的结束，如’grep&gt;‘匹配包含以grep结尾的单词的行。</li><li>x{m} #重复字符x，m次，如：’0{5}‘匹配包含5个o的行。</li><li>x{m,} #重复字符x,至少m次，如：’o{5,}‘匹配至少有5个o的行。</li><li>x{m,n} #重复字符x，至少m次，不多于n次，如：’o{5,10}‘匹配5–10个o的行。</li><li>\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：’G\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p。</li><li>\W #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。</li><li>\b #单词锁定符，如: ‘\bgrep\b’只匹配grep。</li></ul><p>实例：</p><p>（1）查找指定进程</p><pre class="line-numbers language-shell"><code class="language-shell">ps -ef | grep svn<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查找指定进程个数</p><pre class="line-numbers language-shell"><code class="language-shell">ps -ef | grep svn -c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）从文件中读取关键词</p><pre class="line-numbers language-shell"><code class="language-shell">cat test1.txt | grep -f key.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）从文件夹中递归查找以grep开头的行，并只列出文件</p><pre class="line-numbers language-shell"><code class="language-shell">grep -lR '^grep' /tmp<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）查找非x开关的行内容</p><pre class="line-numbers language-shell"><code class="language-shell">grep '^[^x]' test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（6）显示包含 ed 或者 at 字符的内容行</p><pre class="line-numbers language-shell"><code class="language-shell">grep -E 'ed|at' test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-wc-命令：文件统计"><a href="#2-wc-命令：文件统计" class="headerlink" title="2. wc 命令：文件统计"></a>2. wc 命令：文件统计</h4><p>wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出</p><p>命令格式：</p><pre class="line-numbers language-shell"><code class="language-shell">wc [option] file..<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>命令参数：</p><ul><li>-c 统计字节数</li><li>-l 统计行数</li><li>-m 统计字符数</li><li>-w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串</li></ul><p>实例：</p><p>（1）查找文件的 行数 单词数 字节数 文件名</p><pre class="line-numbers language-shell"><code class="language-shell">wc text.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>结果：</p><pre class="line-numbers language-shell"><code class="language-shell">7     8     70     test.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）统计输出结果的行数</p><pre class="line-numbers language-shell"><code class="language-shell">cat test.txt | wc -l<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-磁盘管理"><a href="#3-磁盘管理" class="headerlink" title="3. 磁盘管理"></a>3. 磁盘管理</h3><h4 id="1-cd-命令：切换目录"><a href="#1-cd-命令：切换目录" class="headerlink" title="1. cd 命令：切换目录"></a>1. cd 命令：切换目录</h4><p>cd(changeDirectory) 命令语法：</p><pre class="line-numbers language-shell"><code class="language-shell">cd [目录名]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>说明：切换当前目录至 dirName。</p><p>实例：</p><p>（1）进入要目录</p><pre class="line-numbers language-shell"><code class="language-shell">cd /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）进入 “home” 目录</p><pre class="line-numbers language-text"><code class="language-text">cd ~<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）进入上一次工作路径</p><pre class="line-numbers language-text"><code class="language-text">cd -<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）把上个命令的参数作为cd参数使用。</p><pre class="line-numbers language-shell"><code class="language-shell">cd !$<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-df-命令：显示磁盘空间"><a href="#2-df-命令：显示磁盘空间" class="headerlink" title="2. df 命令：显示磁盘空间"></a>2. df 命令：显示磁盘空间</h4><p>显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示：</p><ol><li>-a 全部文件系统列表</li><li>-h 以方便阅读的方式显示信息</li><li>-i 显示inode信息</li><li>-k 区块为1024字节</li><li>-l 只显示本地磁盘</li><li>-T 列出文件系统类型</li></ol><p>实例：</p><p>（1）显示磁盘使用情况</p><pre class="line-numbers language-shell"><code class="language-shell">df -l<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）以易读方式列出所有文件系统及其类型</p><pre class="line-numbers language-shell"><code class="language-shell">df -haT<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-du-命令：查看文件使用空间"><a href="#3-du-命令：查看文件使用空间" class="headerlink" title="3. du 命令：查看文件使用空间"></a>3. du 命令：查看文件使用空间</h4><p>du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看：</p><p>命令格式：</p><pre class="line-numbers language-text"><code class="language-text">du [选项] [文件]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>常用参数：</p><ul><li>-a 显示目录中所有文件大小</li><li>-k 以KB为单位显示文件大小</li><li>-m 以MB为单位显示文件大小</li><li>-g 以GB为单位显示文件大小</li><li>-h 以易读方式显示文件大小</li><li>-s 仅显示总计</li><li>-c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和</li></ul><p>实例：</p><p>（1）以易读方式显示文件夹内及子文件夹大小</p><pre class="line-numbers language-text"><code class="language-text">du -h scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）以易读方式显示文件夹内所有文件大小</p><pre class="line-numbers language-text"><code class="language-text">du -ah scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和</p><pre class="line-numbers language-text"><code class="language-text">du -hc test/ scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）输出当前目录下各个子目录所使用的空间</p><pre class="line-numbers language-text"><code class="language-text">du -hc --max-depth=1 scf/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-ls命令：查看文件夹文件"><a href="#4-ls命令：查看文件夹文件" class="headerlink" title="4. ls命令：查看文件夹文件"></a>4. ls命令：查看文件夹文件</h4><p>就是 list 的缩写，通过 ls 命令不仅可以查看 linux 文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)查看目录信息等等。</p><p>常用参数搭配：</p><ul><li>ls -a 列出目录所有文件，包含以.开始的隐藏文件</li><li>ls -A 列出除.及..的其它文件</li><li>ls -r 反序排列</li><li>ls -t 以文件修改时间排序</li><li>ls -S 以文件大小排序</li><li>ls -h 以易读大小显示</li><li>ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来</li></ul><p>实例：</p><p>(1) 按易读方式按时间反序排序，并显示文件详细信息</p><pre class="line-numbers language-text"><code class="language-text">ls -lhrt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(2) 按大小反序显示文件详细信息</p><pre class="line-numbers language-text"><code class="language-text">ls -lrS<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(3)列出当前目录中所有以”t”开头的目录的详细内容</p><pre class="line-numbers language-text"><code class="language-text">ls -l t*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(4) 列出文件绝对路径（不包含隐藏文件）</p><pre class="line-numbers language-text"><code class="language-text">ls | sed "s:^:`pwd`/:"<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(5) 列出文件绝对路径（包含隐藏文件）</p><pre class="line-numbers language-text"><code class="language-text">find $pwd -maxdepth 1 | xargs ls -ld<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-mkdir-命令：创建文件夹"><a href="#5-mkdir-命令：创建文件夹" class="headerlink" title="5. mkdir 命令：创建文件夹"></a>5. mkdir 命令：创建文件夹</h4><p>mkdir 命令用于创建文件夹。</p><p>可用选项：</p><ul><li>-m: 对新建目录设置存取权限，也可以用 chmod 命令设置;</li><li>-p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。</li></ul><p>实例：</p><p>（1）当前工作目录下创建名为 t的文件夹</p><pre class="line-numbers language-text"><code class="language-text">mkdir t<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建：</p><pre class="line-numbers language-text"><code class="language-text">mkdir -p /tmp/test/t1/t<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="6-pwd-命令：查看当前目录"><a href="#6-pwd-命令：查看当前目录" class="headerlink" title="6. pwd 命令：查看当前目录"></a>6. pwd 命令：查看当前目录</h4><p>pwd 命令用于查看当前工作目录路径。</p><p>实例：</p><p>（1）查看当前路径</p><pre class="line-numbers language-shell"><code class="language-shell">pw<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看软链接的实际路径</p><pre class="line-numbers language-shell"><code class="language-shell">pwd -P<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="7-rmdir-命令：删除空目录"><a href="#7-rmdir-命令：删除空目录" class="headerlink" title="7. rmdir 命令：删除空目录"></a>7. rmdir 命令：删除空目录</h4><p>从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。</p><p>注意：不能删除非空目录</p><p>实例：</p><p>（1）当 parent 子目录被删除后使它也成为空目录的话，则顺便一并删除：</p><pre class="line-numbers language-text"><code class="language-text">rmdir -p parent/child/child11<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-网络通讯"><a href="#4-网络通讯" class="headerlink" title="4. 网络通讯"></a>4. 网络通讯</h3><h4 id="1-ifconfig-命令：查看网络端口"><a href="#1-ifconfig-命令：查看网络端口" class="headerlink" title="1. ifconfig 命令：查看网络端口"></a>1. ifconfig 命令：查看网络端口</h4><p>ifconfig 用于查看和配置 Linux 系统的网络接口。</p><p>查看所有网络接口及其状态：ifconfig -a 。</p><p>使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。</p><h4 id="2-iptables-命令：开放端口"><a href="#2-iptables-命令：开放端口" class="headerlink" title="2. iptables 命令：开放端口"></a>2. iptables 命令：开放端口</h4><p>iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如</p><p>把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp –dport 80 -j REJECT 。</p><p>开启 80 端口，因为web对外都是这个端口</p><pre class="line-numbers language-text"><code class="language-text">iptables -A INPUT -p tcp --dport 80 -j ACCEP<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。</p><h4 id="3-netstat-命令：查看网络连接状态"><a href="#3-netstat-命令：查看网络连接状态" class="headerlink" title="3. netstat 命令：查看网络连接状态"></a>3. netstat 命令：查看网络连接状态</h4><p>Linux netstat命令用于显示网络状态。</p><p>利用netstat指令可让你得知整个Linux系统的网络情况。</p><p>语法</p><pre class="line-numbers language-text"><code class="language-text">netstat [-acCeFghilMnNoprstuvVwx][-A<网络类型>][--ip]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><ul><li>-a或–all 显示所有连线中的Socket。</li><li>-A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。</li><li>-c或–continuous 持续列出网络状态。</li><li>-C或–cache 显示路由器配置的快取信息。</li><li>-e或–extend 显示网络其他相关信息。</li><li>-F或–fib 显示FIB。</li><li>-g或–groups 显示多重广播功能群组组员名单。</li><li>-h或–help 在线帮助。</li><li>-i或–interfaces 显示网络界面信息表单。</li><li>-l或–listening 显示监控中的服务器的Socket。</li><li>-M或–masquerade 显示伪装的网络连线。</li><li>-n或–numeric 直接使用IP地址，而不通过域名服务器。</li><li>-N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。</li><li>-o或–timers 显示计时器。</li><li>-p或–programs 显示正在使用Socket的程序识别码和程序名称。</li><li>-r或–route 显示Routing Table。</li><li>-s或–statistice 显示网络工作信息统计表。</li><li>-t或–tcp 显示TCP传输协议的连线状况。</li><li>-u或–udp 显示UDP传输协议的连线状况。</li><li>-v或–verbose 显示指令执行过程。</li><li>-V或–version 显示版本信息。</li><li>-w或–raw 显示RAW传输协议的连线状况。</li><li>-x或–unix 此参数的效果和指定”-A unix”参数相同。</li><li>–ip或–inet 此参数的效果和指定”-A inet”参数相同。</li></ul><p><strong>实例</strong></p><p><strong>1）如何查看系统都开启了哪些端口？</strong></p><p><img src="https://pic3.zhimg.com/80/v2-df1430b4718867e521b68d27abfb700e_1440w.jpg" alt="img"></p><p><strong>2）如何查看网络连接状况？</strong></p><p><img src="https://pic2.zhimg.com/80/v2-5adefdeefa70d35ca5b59b8a9cc9acf5_1440w.jpg" alt="img"></p><p><strong>3）如何统计系统当前进程连接数？</strong></p><p>输入命令 <code>netstat -an | grep ESTABLISHED | wc -l</code> 。</p><p>输出结果 177 。一共有 177 连接数。</p><p><strong>4）用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？</strong></p><p>严格来说，这个题目考验的是对 awk 的使用。</p><p>首先，使用 <code>netstat -an|grep ESTABLISHED</code> 命令。结果如下：</p><p><img src="https://pic4.zhimg.com/80/v2-f7058b4652870858da60759619b9870b_1440w.jpg" alt="img"></p><h4 id="4-ping-命令：检测主机网络连接"><a href="#4-ping-命令：检测主机网络连接" class="headerlink" title="4. ping 命令：检测主机网络连接"></a>4. ping 命令：检测主机网络连接</h4><p>Linux ping命令用于检测主机。</p><p>执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。</p><p>指定接收包的次数</p><pre class="line-numbers language-text"><code class="language-text">ping -c 2 百度一下，你就知道<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-telnet-命令：远端登陆"><a href="#5-telnet-命令：远端登陆" class="headerlink" title="5.telnet 命令：远端登陆"></a>5.telnet 命令：远端登陆</h4><p>Linux telnet命令用于远端登入。</p><p>执行telnet指令开启终端机阶段作业，并登入远端主机。</p><p>语法</p><pre class="line-numbers language-text"><code class="language-text">telnet [-8acdEfFKLrx][-b<主机别名>][-e<脱离字符>][-k<域名>][-l<用户名称>][-n<记录文件>][-S<服务类型>][-X<认证形态>][主机名称或IP地址<通信端口>]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><pre class="line-numbers language-text"><code class="language-text">-8 允许使用8位字符资料，包括输入与输出。-a 尝试自动登入远端系统。-b<主机别名> 使用别名指定远端主机名称。-c 不读取用户专属目录里的.telnetrc文件。-d 启动排错模式。-e<脱离字符> 设置脱离字符。-E 滤除脱离字符。-f 此参数的效果和指定"-F"参数相同。-F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。-k<域名> 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。-K 不自动登入远端主机。-l<用户名称> 指定要登入远端主机的用户名称。-L 允许输出8位字符资料。-n<记录文件> 指定文件记录相关信息。-r 使用类似rlogin指令的用户界面。-S<服务类型> 设置telnet连线所需的IP TOS信息。-x 假设主机有支持数据加密的功能，就使用它。-X<认证形态> 关闭指定的认证形态。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实例</p><p>登录远程主机</p><pre class="line-numbers language-shell"><code class="language-shell"># 登录IP为 192.168.0.5 的远程主机telnet 192.168.0.5 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="5-系统管理"><a href="#5-系统管理" class="headerlink" title="5. 系统管理"></a>5. 系统管理</h3><h4 id="1-date-命令：显示系统日期"><a href="#1-date-命令：显示系统日期" class="headerlink" title="1. date 命令：显示系统日期"></a>1. date 命令：显示系统日期</h4><p>显示或设定系统的日期与时间。</p><p>命令参数：</p><ul><li>-d&lt;字符串&gt; 　显示字符串所指的日期与时间。字符串前后必须加上双引号。</li><li>-s&lt;字符串&gt; 　根据字符串来设置日期与时间。字符串前后必须加上双引号。</li></ul><p>-u 　显示GMT。</p><p>%H 小时(00-23)</p><p>%I 小时(00-12)</p><p>%M 分钟(以00-59来表示)</p><p>%s 总秒数。起算时间为1970-01-01 00:00:00 UTC。</p><p>%S 秒(以本地的惯用法来表示)</p><p>%a 星期的缩写。</p><p>%A 星期的完整名称。</p><p>%d 日期(以01-31来表示)。</p><p>%D 日期(含年月日)。</p><p>%m 月份(以01-12来表示)。</p><p>%y 年份(以00-99来表示)。</p><p>%Y 年份(以四位数来表示)。</p><p>实例：</p><p>（1）显示下一天</p><pre class="line-numbers language-text"><code class="language-text">date +%Y%m%d --date="+1 day"  //显示下一天的日期<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）-d参数使用</p><pre class="line-numbers language-text"><code class="language-text">date -d "nov 22"  今年的 11 月 22 日是星期三date -d '2 weeks' 2周后的日期date -d 'next monday' (下周一的日期)date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%ddate -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%ddate -d last-month +%Y%m(上个月是几月)date -d next-month +%Y%m(下个月是几月)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-free-命令：显示内存使用情况"><a href="#2-free-命令：显示内存使用情况" class="headerlink" title="2. free 命令：显示内存使用情况"></a>2. free 命令：显示内存使用情况</h4><p>显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。</p><p>命令参数：</p><ul><li>-b 以Byte显示内存使用情况</li><li>-k 以kb为单位显示内存使用情况</li><li>-m 以mb为单位显示内存使用情况</li><li>-g 以gb为单位显示内存使用情况</li><li>-s&lt;间隔秒数&gt; 持续显示内存</li><li>-t 显示内存使用总合</li></ul><p>实例：</p><p>（1）显示内存使用情况</p><pre class="line-numbers language-text"><code class="language-text">freefree -kfree -m<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）以总和的形式显示内存的使用信息</p><pre class="line-numbers language-text"><code class="language-text">free -t<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）周期性查询内存使用情况</p><pre class="line-numbers language-text"><code class="language-text">free -s 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-kill-命令：杀掉指定进程"><a href="#3-kill-命令：杀掉指定进程" class="headerlink" title="3. kill 命令：杀掉指定进程"></a>3. kill 命令：杀掉指定进程</h4><p>发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用”-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。</p><p>常用参数：</p><ul><li>-l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称</li><li>-a 当处理当前进程时，不限制命令名和进程号的对应关系</li><li>-p 指定kill 命令只打印相关进程的进程号，而不发送任何信号</li><li>-s 指定发送信号</li><li>-u 指定用户</li></ul><p>实例：</p><p>（1）先使用ps查找进程pro1，然后用kill杀掉</p><pre class="line-numbers language-text"><code class="language-text">kill -9 $(ps -ef | grep pro1)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-ps-命令：查看运行进程（快照）"><a href="#4-ps-命令：查看运行进程（快照）" class="headerlink" title="4. ps 命令：查看运行进程（快照）"></a>4. ps 命令：查看运行进程（快照）</h4><p>ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top</p><p>linux上进程有5种状态:</p><ol><li>运行(正在运行或在运行队列中等待)</li><li>中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号)</li><li>不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生)</li><li>僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放)</li><li>停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行)</li></ol><p>ps 工具标识进程的5种状态码:</p><ol><li>D 不可中断 uninterruptible sleep (usually IO)</li><li>R 运行 runnable (on run queue)</li><li>S 中断 sleeping</li><li>T 停止 traced or stopped</li><li>Z 僵死 a defunct (”zombie”) process</li></ol><p>命令参数：</p><ul><li>-A 显示所有进程</li><li>a 显示所有进程</li><li>-a 显示同一终端下所有进程</li><li>c 显示进程真实名称</li><li>e 显示环境变量</li><li>f 显示进程间的关系</li><li>r 显示当前终端运行的进程</li><li>-aux 显示所有包含其它使用的进程</li></ul><p>实例：</p><p>（1）显示当前所有进程环境变量及进程间关系</p><pre class="line-numbers language-text"><code class="language-text">ps -ef<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）显示当前所有进程</p><pre class="line-numbers language-text"><code class="language-text">ps -A<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）与grep联用查找某进程</p><pre class="line-numbers language-text"><code class="language-text">ps -aux | grep apache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）找出与 cron 与 syslog 这两个服务有关的 PID 号码</p><pre class="line-numbers language-text"><code class="language-text">ps aux | grep '(cron|syslog)'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-rpm-命令：管理套件"><a href="#5-rpm-命令：管理套件" class="headerlink" title="5. rpm 命令：管理套件"></a>5. rpm 命令：管理套件</h4><p>Linux rpm 命令用于管理套件。</p><p>rpm(redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。</p><p># 查看系统自带jdk</p><pre class="line-numbers language-text"><code class="language-text">rpm -qa | grep jdk<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 删除系统自带jdk</p><pre class="line-numbers language-text"><code class="language-text">rpm -e --nodeps 查看jdk显示的数据<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 安装jdk</p><pre class="line-numbers language-text"><code class="language-text">rpm -ivh jdk-7u80-linux-x64.rpm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="6-top-命令：显示当前进程信息（动态）"><a href="#6-top-命令：显示当前进程信息（动态）" class="headerlink" title="6. top 命令：显示当前进程信息（动态）"></a>6. top 命令：显示当前进程信息（动态）</h4><p>显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等</p><p>常用参数：</p><ul><li>-c 显示完整的进程命令</li><li>-s 保密模式</li><li>-p &lt;进程号&gt; 指定进程显示</li><li>-n &lt;次数&gt;循环显示次数</li></ul><p>实例：</p><p><img src="https://pic1.zhimg.com/80/v2-a229fdfac92fa6b7402522898d55899c_1440w.jpg" alt="img"></p><blockquote><p>前五行是当前系统情况整体的统计信息区。</p><p><strong>第一行，任务队列信息，同 uptime 命令的执行结果</strong>，具体参数说明情况如下：</p><p>14:06:23 — 当前系统时间</p><p>up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！）</p><p>2 users — 当前有2个用户登录系统</p><p>load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。</p><p>load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。</p><p><strong>第二行，Tasks — 任务（进程）</strong>，具体信息说明如下：</p><p>系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。</p><p><strong>第三行，cpu状态信息</strong>，具体属性说明如下：</p><ul><li>5.9%us — 用户空间占用CPU的百分比。</li><li>3.4% sy — 内核空间占用CPU的百分比。</li><li>0.0% ni — 改变过优先级的进程占用CPU的百分比</li><li>90.4% id — 空闲CPU百分比</li><li>0.0% wa — IO等待占用CPU的百分比</li><li>0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比</li><li>0.2% si — 软中断（Software Interrupts）占用CPU的百分比</li></ul><p>备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！</p><p><strong>第四行，内存状态</strong>，具体信息如下：</p><p>32949016k total — 物理内存总量（32GB）</p><p>14411180k used — 使用中的内存总量（14GB）</p><p>18537836k free — 空闲内存总量（18GB）</p><p>169884k buffers — 缓存的内存量 （169M）</p><p><strong>第五行，swap交换分区信息</strong>，具体信息说明如下：</p><p>32764556k total — 交换区总量（32GB）</p><p>0k used — 使用的交换区总量（0K）</p><p>32764556k free — 空闲交换区总量（32GB）</p><p>3612636k cached — 缓冲的交换区总量（3.6GB）</p><p><strong>第六行，空行</strong>。</p><p>第七行以下：各进程（任务）的状态监控，项目列信息说明如下：</p><p>PID — 进程id</p><p>USER — 进程所有者</p><p>PR — 进程优先级</p><p>NI — nice值。负值表示高优先级，正值表示低优先级</p><p>VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES</p><p>RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA</p><p>SHR — 共享内存大小，单位kb</p><p>S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程</p><p>%CPU — 上次更新到现在的CPU时间占用百分比</p><p>%MEM — 进程使用的物理内存百分比</p><p>TIME+ — 进程使用的CPU时间总计，单位1/100秒</p><p>COMMAND — 进程名称（命令名/命令行）</p></blockquote><h4 id="7-top-交互命令："><a href="#7-top-交互命令：" class="headerlink" title="7. top 交互命令："></a>7. top 交互命令：</h4><ul><li>h 显示top交互命令帮助信息</li><li>c 切换显示命令名称和完整命令行</li><li>m 以内存使用率排序</li><li>P 根据CPU使用百分比大小进行排序</li><li>T 根据时间/累计时间进行排序</li><li>W 将当前设置写入~/.toprc文件中</li><li>o或者O 改变显示项目的顺序</li></ul><h4 id="8-yum-命令：软件包管理器"><a href="#8-yum-命令：软件包管理器" class="headerlink" title="8. yum 命令：软件包管理器"></a>8. yum 命令：软件包管理器</h4><p>yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。</p><p>基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。</p><p>yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。</p><p>1.列出所有可更新的软件清单命令：yum check-update</p><p>2.更新所有软件命令：yum update</p><p>3.仅安装指定的软件命令：yum install <package_name></package_name></p><p>4.仅更新指定的软件命令：yum update <package_name></package_name></p><p>5.列出所有可安裝的软件清单命令：yum list</p><p>6.删除软件包命令：yum remove <package_name></package_name></p><p>7.查找软件包 命令：yum search</p><p>8.清除缓存命令:</p><p>yum clean packages: 清除缓存目录下的软件包</p><p>yum clean headers: 清除缓存目录下的 headers</p><p>yum clean oldheaders: 清除缓存目录下旧的 headers</p><p>yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers</p><p>实例</p><p>安装 pam-devel</p><pre class="line-numbers language-text"><code class="language-text">[root@www ~]# yum install pam-devel<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="6-备份压缩"><a href="#6-备份压缩" class="headerlink" title="6. 备份压缩"></a>6. 备份压缩</h3><h4 id="1-bzip2-命令：bz2文件压缩解压"><a href="#1-bzip2-命令：bz2文件压缩解压" class="headerlink" title="1. bzip2 命令：bz2文件压缩解压"></a>1. bzip2 命令：bz2文件压缩解压</h4><pre class="line-numbers language-text"><code class="language-text">创建 *.bz2 压缩文件：bzip2 test.txt 。解压 *.bz2 文件：bzip2 -d test.txt.bz2 。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="2-gzip-命令：gz文件压缩解压"><a href="#2-gzip-命令：gz文件压缩解压" class="headerlink" title="2. gzip 命令：gz文件压缩解压"></a>2. gzip 命令：gz文件压缩解压</h4><pre class="line-numbers language-text"><code class="language-text">创建一个 *.gz 的压缩文件：gzip test.txt 。解压 *.gz 文件：gzip -d test.txt.gz 。显示压缩的比率：gzip -l *.gz 。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="3-tar-命令：文件打包"><a href="#3-tar-命令：文件打包" class="headerlink" title="3. tar 命令：文件打包"></a>3. tar 命令：文件打包</h4><p>用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。</p><p>弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件</p><p>常用参数：</p><ol><li>-c 建立新的压缩文件</li><li>-f 指定压缩文件</li><li>-r 添加文件到已经压缩文件包中</li><li>-u 添加改了和现有的文件到压缩包中</li><li>-x 从压缩包中抽取文件</li><li>-t 显示压缩文件中的内容</li><li>-z 支持gzip压缩</li><li>-j 支持bzip2压缩</li><li>-Z 支持compress解压文件</li><li>-v 显示操作过程</li></ol><p><strong>有关 gzip 及 bzip2 压缩:</strong></p><p><strong>gzip 实例</strong>：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz</p><p>对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz</p><p><strong>bz2实例</strong>：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2</p><p>对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2</p><p>实例：</p><p>（1）将文件全部打包成 tar 包</p><pre class="line-numbers language-shell"><code class="language-shell">tar -cvf log.tar 1.log,2.log 或tar -cvf log.*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩</p><pre class="line-numbers language-shell"><code class="language-shell">tar -zcvf /tmp/etc.tar.gz /etc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的）</p><pre class="line-numbers language-shell"><code class="language-shell">tar -ztvf /tmp/etc.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）要压缩打包 /home, /etc ，但不要 /home/dmtsai</p><pre class="line-numbers language-shell"><code class="language-shell">tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-unzip-命令"><a href="#4-unzip-命令" class="headerlink" title="4. unzip 命令"></a>4. unzip 命令</h4><pre class="line-numbers language-text"><code class="language-text">解压 *.zip 文件：unzip test.zip 。查看 *.zip 文件的内容：unzip -l jasper.zip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="7-2-操作系统理论"><a href="#7-2-操作系统理论" class="headerlink" title="7.2 操作系统理论"></a>7.2 操作系统理论</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h4 id="基本特征"><a href="#基本特征" class="headerlink" title="基本特征"></a>基本特征</h4><h5 id="1-并发"><a href="#1-并发" class="headerlink" title="1. 并发"></a>1. 并发</h5><p>并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。</p><p>并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。</p><p>操作系统通过引入进程和线程，使得程序能够并发运行。</p><h5 id="2-共享"><a href="#2-共享" class="headerlink" title="2. 共享"></a>2. 共享</h5><p>共享是指系统中的资源可以被多个并发进程共同使用。</p><p>有两种共享方式：互斥共享和同时共享。</p><p>互斥共享的资源称为临界资源，例如打印机等，在同一时间只允许一个进程访问，需要用同步机制来实现对临界资源的访问。</p><h5 id="3-虚拟"><a href="#3-虚拟" class="headerlink" title="3. 虚拟"></a><strong>3. <strong>虚拟</strong></strong></h5><p>虚拟技术把一个物理实体转换为多个逻辑实体。</p><p>主要有两种虚拟技术：时分复用技术和空分复用技术。</p><p>多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片并快速切换。</p><p>虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页置换到内存中。</p><h5 id="4-异步"><a href="#4-异步" class="headerlink" title="4. 异步"></a>4. 异步</h5><p>异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。</p><h4 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h4><h5 id="1-进程管理"><a href="#1-进程管理" class="headerlink" title="1. 进程管理"></a>1. 进程管理</h5><p>进程控制、进程同步、进程通信、死锁处理、处理机调度等。</p><h5 id="2-内存管理"><a href="#2-内存管理" class="headerlink" title="2. 内存管理"></a><strong>2. <strong>内存管理</strong></strong></h5><p>内存分配、地址映射、内存保护与共享、虚拟内存等。</p><h5 id="3-文件管理"><a href="#3-文件管理" class="headerlink" title="3. 文件管理"></a><strong>3. <strong>文件管理</strong></strong></h5><p><a href="https://cloud.tencent.com/product/cfs?from=10680" target="_blank" rel="noopener">文件存储</a>空间的管理、目录管理、文件读写管理和保护等。</p><h5 id="4-设备管理"><a href="#4-设备管理" class="headerlink" title="4. 设备管理"></a><strong>4. <strong>设备管理</strong></strong></h5><p>完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。</p><p>主要包括缓冲管理、设备分配、设备处理、虛拟设备等。</p><h4 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h4><p>如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/pmt2abktte.jpeg?imageView2/2/w/1620" alt="img"></p><p>Linux 的系统调用主要有以下这些：</p><table><thead><tr><th align="left">Task</th><th align="left">Commands</th></tr></thead><tbody><tr><td align="left">进程控制</td><td align="left">fork(); exit(); wait();</td></tr><tr><td align="left">进程通信</td><td align="left">pipe(); shmget(); mmap();</td></tr><tr><td align="left">文件操作</td><td align="left">open(); read(); write();</td></tr><tr><td align="left">设备操作</td><td align="left">ioctl(); read(); write();</td></tr><tr><td align="left">信息维护</td><td align="left">getpid(); alarm(); sleep();</td></tr><tr><td align="left">安全</td><td align="left">chmod(); umask(); chown();</td></tr></tbody></table><h4 id="大内核和微内核"><a href="#大内核和微内核" class="headerlink" title="大内核和微内核"></a>大内核和微内核</h4><h5 id="1-大内核"><a href="#1-大内核" class="headerlink" title="1. 大内核"></a>1. 大内核</h5><p>大内核是将操作系统功能作为一个紧密结合的整体放到内核。</p><p>由于各模块共享信息，因此有很高的性能。</p><h5 id="2-微内核"><a href="#2-微内核" class="headerlink" title="2. 微内核"></a>2. 微内核</h5><p>由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。</p><p>在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。</p><p>因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/od2ypvw5gb.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="中断分类"><a href="#中断分类" class="headerlink" title="中断分类"></a>中断分类</h4><h5 id="1-外中断"><a href="#1-外中断" class="headerlink" title="1. 外中断"></a>1. 外中断</h5><p>由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。</p><h5 id="2-异常"><a href="#2-异常" class="headerlink" title="2. 异常"></a>2. 异常</h5><p>由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。</p><h5 id="3-陷入"><a href="#3-陷入" class="headerlink" title="3. 陷入"></a>3. 陷入</h5><p>在用户程序中使用系统调用。</p><h3 id="2-进程管理"><a href="#2-进程管理" class="headerlink" title="2. 进程管理"></a>2. 进程管理</h3><p><strong>进程与线程</strong></p><h5 id="1-进程"><a href="#1-进程" class="headerlink" title="1. 进程"></a>1. 进程</h5><p>进程是资源分配的基本单位。</p><p>进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。</p><p>下图显示了 4 个程序创建了 4 个进程，这 4 个进程可以并发地执行。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/hu0tt1z1tj.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="2-线程"><a href="#2-线程" class="headerlink" title="*2. *线程"></a>*<em>2. *</em>线程</h5><p>线程是独立调度的基本单位。</p><p>一个进程中可以有多个线程，它们共享进程资源。</p><p>QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/xagt3l9ulw.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="3-区别"><a href="#3-区别" class="headerlink" title="3. 区别"></a>3. 区别</h5><p>Ⅰ 拥有资源</p><p>进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。</p><p>Ⅱ 调度</p><p>线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。</p><p>Ⅲ 系统开销</p><p>由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。</p><p>Ⅳ 通信方面</p><p>线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。</p><p><strong>进程状态的切换</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/07gn4qy6s2.jpeg?imageView2/2/w/1620" alt="img"></p><ul><li>就绪状态（ready）：等待被调度</li><li>运行状态（running）</li><li>阻塞状态（waiting）：等待资源</li></ul><p>应该注意以下内容：</p><ul><li>只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。</li><li>阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。</li></ul><h4 id="进程调度算法"><a href="#进程调度算法" class="headerlink" title="进程调度算法"></a>进程调度算法</h4><p>不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。</p><h5 id="1-批处理系统"><a href="#1-批处理系统" class="headerlink" title="1. 批处理系统"></a>1. 批处理系统</h5><p>批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。</p><p><strong>1.1 先来先服务 first-come first-serverd（FCFS）</strong></p><p>按照请求的顺序进行调度。</p><p>有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。</p><p><strong>1.2 短作业优先 shortest job first（SJF）</strong></p><p>按估计运行时间最短的顺序进行调度。</p><p>长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。</p><p><strong>1.3 最短剩余时间优先 shortest remaining time next（SRTN）</strong></p><p>按估计剩余时间最短的顺序进行调度。</p><h5 id="2-交互式系统"><a href="#2-交互式系统" class="headerlink" title="2. 交互式系统"></a>2. 交互式系统</h5><p>交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。</p><p>2.1 时间片轮转</p><p>将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。</p><p>时间片轮转算法的效率和时间片的大小有很大关系：</p><ul><li>因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。</li><li>而如果时间片过长，那么实时性就不能得到保证。</li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/rczmx9j6f0.jpeg?imageView2/2/w/1620" alt="img"></p><p>2.2 优先级调度</p><p>为每个进程分配一个优先级，按优先级进行调度。</p><p>为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。</p><p>2.3 多级反馈队列</p><p>一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。</p><p>多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。</p><p>每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。</p><p>可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/55iy3nvaqt.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="3-实时系统"><a href="#3-实时系统" class="headerlink" title="3. 实时系统"></a>3. 实时系统</h5><p>实时系统要求一个请求在一个确定时间内得到响应。</p><p>分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。</p><h4 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h4><h5 id="1-临界区"><a href="#1-临界区" class="headerlink" title="1. 临界区"></a>1. 临界区</h5><p>对临界资源进行访问的那段代码称为临界区。</p><p>为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token comment" spellcheck="true">// entry section</span><span class="token comment" spellcheck="true">// critical section;</span><span class="token comment" spellcheck="true">// exit section</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h5 id="2-同步与互斥"><a href="#2-同步与互斥" class="headerlink" title="2. 同步与互斥"></a>2. 同步与互斥</h5><ul><li>同步：多个进程按一定顺序执行；</li><li>互斥：多个进程在同一时刻只有一个进程能进入临界区。</li></ul><h5 id="3-信号量"><a href="#3-信号量" class="headerlink" title="3. 信号量"></a>3. 信号量</h5><p>信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。</p><ul><li><strong>down</strong>  : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0；</li><li><strong>up</strong> ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。</li></ul><p>down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。</p><p>如果信号量的取值只能为 0 或者 1，那么就成为了  <strong>互斥量（Mutex）</strong> ，0 表示临界区已经加锁，1 表示临界区解锁。</p><pre class="line-numbers language-javascript"><code class="language-javascript">typedef int semaphore<span class="token punctuation">;</span>semaphore mutex <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">P1</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 临界区</span>    <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">P2</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 临界区</span>    <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> <strong>使用信号量实现生产者-消费者问题</strong>  </p><p>问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。</p><p>因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。</p><p>为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。</p><p>注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。</p><pre class="line-numbers language-javascript"><code class="language-javascript">#define N <span class="token number">100</span>typedef int semaphore<span class="token punctuation">;</span>semaphore mutex <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>semaphore empty <span class="token operator">=</span> N<span class="token punctuation">;</span>semaphore full <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">producer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>TRUE<span class="token punctuation">)</span> <span class="token punctuation">{</span>        int item <span class="token operator">=</span> <span class="token function">produce_item</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">insert_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">consumer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>TRUE<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">down</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        int item <span class="token operator">=</span> <span class="token function">remove_item</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">consume_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">up</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="4-管程"><a href="#4-管程" class="headerlink" title="4. 管程"></a>4. 管程</h5><p>使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易。</p><p>c 语言不支持管程，下面的示例代码使用了类 Pascal 语言来描述管程。示例代码的管程提供了 insert() 和 remove() 方法，客户端代码通过调用这两个方法来解决生产者-消费者问题。</p><pre class="line-numbers language-javascript"><code class="language-javascript">monitor ProducerConsumer    integer i<span class="token punctuation">;</span>    condition c<span class="token punctuation">;</span>    procedure <span class="token function">insert</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    begin        <span class="token comment" spellcheck="true">// ...</span>    end<span class="token punctuation">;</span>    procedure <span class="token function">remove</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    begin        <span class="token comment" spellcheck="true">// ...</span>    end<span class="token punctuation">;</span>end monitor<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>管程有一个重要特性：在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否者其它进程永远不能使用管程。</p><p>管程引入了  <strong>条件变量</strong>  以及相关的操作：<strong>wait()</strong> 和 <strong>signal()</strong> 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。</p><p><strong>使用管程实现生产者-消费者问题</strong> </p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token comment" spellcheck="true">// 管程</span>monitor ProducerConsumer    condition full<span class="token punctuation">,</span> empty<span class="token punctuation">;</span>    integer count <span class="token punctuation">:</span><span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    condition c<span class="token punctuation">;</span>    procedure <span class="token function">insert</span><span class="token punctuation">(</span>item<span class="token punctuation">:</span> integer<span class="token punctuation">)</span><span class="token punctuation">;</span>    begin        <span class="token keyword">if</span> count <span class="token operator">=</span> N then <span class="token function">wait</span><span class="token punctuation">(</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">insert_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>        count <span class="token punctuation">:</span><span class="token operator">=</span> count <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> count <span class="token operator">=</span> <span class="token number">1</span> then <span class="token function">signal</span><span class="token punctuation">(</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>    end<span class="token punctuation">;</span>    <span class="token keyword">function</span> remove<span class="token punctuation">:</span> integer<span class="token punctuation">;</span>    begin        <span class="token keyword">if</span> count <span class="token operator">=</span> <span class="token number">0</span> then <span class="token function">wait</span><span class="token punctuation">(</span>empty<span class="token punctuation">)</span><span class="token punctuation">;</span>        remove <span class="token operator">=</span> remove_item<span class="token punctuation">;</span>        count <span class="token punctuation">:</span><span class="token operator">=</span> count <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> count <span class="token operator">=</span> N <span class="token operator">-</span><span class="token number">1</span> then <span class="token function">signal</span><span class="token punctuation">(</span>full<span class="token punctuation">)</span><span class="token punctuation">;</span>    end<span class="token punctuation">;</span>end monitor<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 生产者客户端</span>procedure producerbegin    <span class="token keyword">while</span> <span class="token boolean">true</span> <span class="token keyword">do</span>    begin        item <span class="token operator">=</span> produce_item<span class="token punctuation">;</span>        ProducerConsumer<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>    endend<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 消费者客户端</span>procedure consumerbegin    <span class="token keyword">while</span> <span class="token boolean">true</span> <span class="token keyword">do</span>    begin        item <span class="token operator">=</span> ProducerConsumer<span class="token punctuation">.</span>remove<span class="token punctuation">;</span>        <span class="token function">consume_item</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>    endend<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信"></a>进程通信</h4><p>进程同步与进程通信很容易混淆，它们的区别在于：</p><ul><li>进程同步：控制多个进程按一定顺序执行；</li><li>进程通信：进程间传输信息。</li></ul><p>进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传输一些进程同步所需要的信息。</p><h5 id="1-管道"><a href="#1-管道" class="headerlink" title="1. 管道"></a>1. 管道</h5><p>管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。</p><pre class="line-numbers language-javascript"><code class="language-javascript">#include <span class="token operator">&lt;</span>unistd<span class="token punctuation">.</span>h<span class="token operator">></span>int <span class="token function">pipe</span><span class="token punctuation">(</span>int fd<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>它具有以下限制：</p><ul><li>只支持半双工通信（单向交替传输）；</li><li>只能在父子进程中使用。</li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/hx8g1045y3.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="2-FIFO"><a href="#2-FIFO" class="headerlink" title="2. FIFO"></a>2. FIFO</h5><p>也称为命名管道，去除了管道只能在父子进程中使用的限制。</p><pre class="line-numbers language-javascript"><code class="language-javascript">#include <span class="token operator">&lt;</span>sys<span class="token operator">/</span>stat<span class="token punctuation">.</span>h<span class="token operator">></span>int <span class="token function">mkfifo</span><span class="token punctuation">(</span><span class="token keyword">const</span> char <span class="token operator">*</span>path<span class="token punctuation">,</span> mode_t mode<span class="token punctuation">)</span><span class="token punctuation">;</span>int <span class="token function">mkfifoat</span><span class="token punctuation">(</span>int fd<span class="token punctuation">,</span> <span class="token keyword">const</span> char <span class="token operator">*</span>path<span class="token punctuation">,</span> mode_t mode<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/i73d40mmih.jpeg?imageView2/2/w/1620" alt="img"></p><h5 id="3-消息队列"><a href="#3-消息队列" class="headerlink" title="3. 消息队列"></a>3. <a href="https://cloud.tencent.com/product/cmq?from=10680" target="_blank" rel="noopener">消息队列</a></h5><p>相比于 FIFO，消息队列具有以下优点：</p><ul><li>消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难；</li><li>避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法；</li><li>读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。</li></ul><h5 id="4-信号量"><a href="#4-信号量" class="headerlink" title="4. 信号量"></a>4. 信号量</h5><p>它是一个计数器，用于为多个进程提供对共享数据对象的访问。</p><h5 id="5-共享存储"><a href="#5-共享存储" class="headerlink" title="5. 共享存储"></a>5. 共享存储</h5><p>允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。</p><p>需要使用信号量用来同步对共享存储的访问。</p><p>多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用使用内存的匿名段。</p><h5 id="6-套接字"><a href="#6-套接字" class="headerlink" title="6. 套接字"></a>6. 套接字</h5><p>与其它通信机制不同的是，它可用于不同机器间的进程通信。</p><h3 id="3-死锁"><a href="#3-死锁" class="headerlink" title="3. 死锁"></a>3. 死锁</h3><h5 id="必要条件"><a href="#必要条件" class="headerlink" title="必要条件"></a>必要条件</h5><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/t65ue2s1ya.jpeg?imageView2/2/w/1620" alt="img"></p><ul><li>互斥：每个资源要么已经分配给了一个进程，要么就是可用的。</li><li>占有和等待：已经得到了某个资源的进程可以再请求新的资源。</li><li>不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。</li><li>环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。</li></ul><h5 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h5><p>主要有以下四种方法：</p><ul><li>鸵鸟策略</li><li>死锁检测与死锁恢复</li><li>死锁预防</li><li>死锁避免</li></ul><h5 id="鸵鸟策略"><a href="#鸵鸟策略" class="headerlink" title="鸵鸟策略"></a>鸵鸟策略</h5><p>把头埋在沙子里，假装根本没发生问题。</p><p>因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。</p><p>当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。</p><p>大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。</p><h5 id="死锁检测与死锁恢复"><a href="#死锁检测与死锁恢复" class="headerlink" title="死锁检测与死锁恢复"></a>死锁检测与死锁恢复</h5><p>不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。</p><p><strong>1. 每种类型一个资源的死锁检测</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/pij0ss6sqm.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。</p><p>图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。</p><p>每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。</p><p><strong>2. 每种类型多个资源的死锁检测</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/ijj77i1mx1.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图中，有三个进程四个资源，每个数据代表的含义如下：</p><ul><li>E 向量：资源总量</li><li>A 向量：资源剩余量</li><li>C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量</li><li>R 矩阵：每个进程请求的资源数量</li></ul><p>进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。</p><p>算法总结如下：</p><p>每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。</p><ol><li>寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。</li><li>如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。</li><li>如果没有这样一个进程，算法终止。</li></ol><h6 id="3-死锁恢复"><a href="#3-死锁恢复" class="headerlink" title="3. 死锁恢复"></a>3. 死锁恢复</h6><ul><li>利用抢占恢复</li><li>利用回滚恢复</li><li>通过杀死进程恢复</li></ul><h5 id="死锁预防"><a href="#死锁预防" class="headerlink" title="死锁预防"></a>死锁预防</h5><p>在程序运行之前预防发生死锁。</p><p><strong>1. 破坏互斥条件</strong></p><p>例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。</p><p><strong>2. 破坏占有和等待条件</strong></p><p>一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。</p><p><strong>3. 破坏不可抢占条件</strong></p><p><strong>4. 破坏环路等待</strong></p><p>给资源统一编号，进程只能按编号顺序来请求资源。</p><h5 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h5><p>在程序运行时避免发生死锁。</p><p><strong>1. 安全状态</strong></p><p>图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。</p><p>定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。</p><p>安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。</p><p><strong>2. 单个资源的银行家算法</strong></p><p>一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/wxp7hahob2.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。</p><p><strong>3. 多个资源的银行家算法</strong></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/ree0vdeeyi.jpeg?imageView2/2/w/1620" alt="img"></p><p>上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。</p><p>检查一个状态是否安全的算法如下：</p><ul><li>查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。</li><li>假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。</li><li>重复以上两步，直到所有进程都标记为终止，则状态时安全的。</li></ul><p>如果一个状态不是安全的，需要拒绝进入这个状态。</p><h3 id="4-内存管理"><a href="#4-内存管理" class="headerlink" title="4. 内存管理"></a>4. 内存管理</h3><h4 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h4><p>虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。</p><p>为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。</p><p>从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/k7jtx5iwmc.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="分页系统地址映射"><a href="#分页系统地址映射" class="headerlink" title="分页系统地址映射"></a>分页系统地址映射</h4><p>内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。</p><p>一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。</p><p>下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/c2dnokrvmc.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="页面置换算法"><a href="#页面置换算法" class="headerlink" title="页面置换算法"></a>页面置换算法</h4><p>在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。</p><p>页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，当有新的缓存到达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据。</p><p>页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。</p><p><strong>1. 最佳</strong></p><blockquote><p>OPT, Optimal replacement algorithm</p></blockquote><p>所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。</p><p>是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。</p><p>举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列：</p><p>开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。</p><p><strong>2. 最近最久未使用</strong></p><blockquote><p>LRU, Least Recently Used</p></blockquote><p>虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。</p><p>为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。</p><p>因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/2x6e053pb1.jpeg?imageView2/2/w/1620" alt="img"></p><p><strong>3. 最近未使用</strong></p><blockquote><p>NRU, Not Recently Used</p></blockquote><p>每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类：</p><ul><li>R=0，M=0</li><li>R=0，M=1</li><li>R=1，M=0</li><li>R=1，M=1</li></ul><p>当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。</p><p>NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。</p><p><strong>4. 先进先出</strong></p><blockquote><p>FIFO, First In First Out</p></blockquote><p>选择换出的页面是最先进入的页面。</p><p>该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。</p><p><strong>5. 第二次机会算法</strong></p><p>FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改：</p><p>当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/tnlcns5fh1.jpeg?imageView2/2/w/1620" alt="img"></p><p><strong>6. 时钟</strong></p><blockquote><p>Clock</p></blockquote><p>第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面连接起来，再使用一个指针指向最老的页面。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/o6tw921hf2.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="分段"><a href="#分段" class="headerlink" title="分段"></a>分段</h4><p>虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。</p><p>下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/zfur3waatz.jpeg?imageView2/2/w/1620" alt="img"></p><p>分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1346475/bjksrn7rbd.jpeg?imageView2/2/w/1620" alt="img"></p><h4 id="段页式"><a href="#段页式" class="headerlink" title="段页式"></a>段页式</h4><p>程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统的共享和保护，又拥有分页系统的虚拟内存功能。</p><h4 id="分页与分段的比较"><a href="#分页与分段的比较" class="headerlink" title="分页与分段的比较"></a>分页与分段的比较</h4><ul><li>对程序员的透明性：分页透明，但是分段需要程序员显示划分每个段。</li><li>地址空间的维度：分页是一维地址空间，分段是二维的。</li><li>大小是否可以改变：页的大小不可变，段的大小可以动态改变。</li><li>出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。</li></ul><h1 id="八-计算机网络"><a href="#八-计算机网络" class="headerlink" title="八. 计算机网络"></a>八. 计算机网络</h1><h2 id="8-1-TCP和UDP的区别"><a href="#8-1-TCP和UDP的区别" class="headerlink" title="8.1 TCP和UDP的区别"></a>8.1 TCP和UDP的区别</h2><p>TCP<strong>面向连接（三次握手机制），通信前需要先建立连接；UDP面向无连接</strong>，通信前不需要建立连接；</p><p>TCP保障可靠传输（按序、无差错、不丢失、不重复）；UDP<strong>不保障可靠</strong>传输，使用最大努力交付；</p><p>TCP面向字节流的传输，UDP面向数据报的传输。</p><h2 id="8-2-TCP协议的三次握手-连接-和四次挥手-关闭"><a href="#8-2-TCP协议的三次握手-连接-和四次挥手-关闭" class="headerlink" title="8.2 TCP协议的三次握手(连接)和四次挥手(关闭)"></a>8.2 TCP协议的三次握手(连接)和四次挥手(关闭)</h2><p><strong>1. 三次握手过程</strong></p><p>形象理解：<br> 客户机：【how are you ?】<br> 服务器：【fine.And you?】<br> 客户机：【Fine.】</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cclip_image002.jpg" alt="IMG_256"></td></tr></tbody></table><p>具体过程：（SYN, (SYN+ACK), ACK) </p><p>（1）第一次握手：建立连接时，客户端A发送SYN包（SYN=j）到服务器B，并进入SYN_SEND状态，等待服务器B确认。</p><p>（2）第二次握手：服务器B收到SYN包，必须确认客户A的SYN（ACK=j+1），同时自己也发送一个SYN包（SYN=k），即SYN+ACK包，此时服务器B进入SYN_RECV状态。</p><p>（3）第三次握手：客户端A收到服务器B的SYN＋ACK包，向服务器B发送确认包ACK（ACK=k+1），此包发送完毕，客户端A和服务器B进入ESTABLISHED状态，完成三次握手。</p><p>完成三次握手，客户端与服务器开始传送数据。</p><p>确认号：其数值等于发送方的发送序号 +1(即接收方期望接收的下一个序列号)。</p><p><strong>2.**</strong>四次挥手过程**</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cclip_image004.jpg" alt="IMG_257"></td></tr></tbody></table><p>由于TCP连接是全双工的，一个TCP连接存在双向的读写通道，因此每个方向都必须单独进行关闭。TCP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。 </p><p>简单说来是 “先关读，后关写”，一共需要四个阶段。以客户机发起关闭连接为例：<br> 1.服务器读通道关闭<br> 2.客户机写通道关闭<br> 3.客户机读通道关闭<br> 4.服务器写通道关闭</p><p>关闭行为是在发起方数据发送完毕之后，给对方发出一个FIN（finish）数据段。直到接收到对方发送的FIN，且对方收到了接收确认ACK之后，双方的数据通信完全结束，过程中每次接收都需要返回确认数据段ACK。<br> 详细过程：<br> 第一阶段 客户机发送完数据之后，向服务器发送一个FIN数据段，序列号为i；<br> 1.服务器收到FIN(i)后，返回确认段ACK，序列号为i+1，关闭服务器读通道；<br> 2.客户机收到ACK(i+1)后，关闭客户机写通道；<br> （此时，客户机仍能通过读通道读取服务器的数据，服务器仍能通过写通道写数据）<br> 第二阶段 服务器发送完数据之后，向客户机发送一个FIN数据段，序列号为j；<br> 3.客户机收到FIN(j)后，返回确认段ACK，序列号为j+1，关闭客户机读通道；<br> 4.服务器收到ACK(j+1)后，关闭服务器写通道。</p><p>这是标准的TCP关闭两个阶段，服务器和客户机都可以发起关闭，完全对称。<br> FIN标识是通过发送最后一块数据时设置的，标准的例子中，服务器还在发送数据，所以要等到发送完的时候，设置FIN（此时可称为TCP连接处于半关闭状态，因为数据仍可从被动关闭一方向主动关闭方传送）。如果在服务器收到FIN(i)时，已经没有数据需要发送，可以在返回ACK(i+1)的时候就设置FIN(j)标识，这样就相当于可以合并第二步和第三步。</p><h2 id="8-3-TCP协议的通信状态"><a href="#8-3-TCP协议的通信状态" class="headerlink" title="8.3 TCP协议的通信状态"></a>8.3 TCP协议的通信状态</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="D:\2.我的工作\8.社招求职\2019JD\3.面试梳理\clip_image006.jpg" alt="IMG_258" style="zoom: 80%;"></td></tr></tbody></table><p>整个通信状态如图</p><p>说明：客户端和服务器均有6个状态。其中经常问到的两个状态是TIME_WAIT和CLOSE_WAIT.<br> 从图上可以发现，</p><p>TIME_WAIT状态是客户端【发起主动关闭的一方】<em>在四次挥手第二阶段</em>完成时，进入的状态。<br> CLOSE_WAIT状态是服务端【收到被动关闭的一方】<em>在四次挥手的第一阶段</em>完成时，进入的状态。</p><p>TIME_WAIT状态将持续2个MSL(Max Segment Lifetime),在Windows下默认为4分钟，即240秒。TIME_WAIT状态下的socket不能被回收使用. 具体现象是对于一个处理大量短连接的服务器,如果是由服务器主动关闭客户端的连接，将导致服务器端存在大量的处于TIME_WAIT状态的socket， 甚至比处于Established状态下的socket多的多,严重影响服务器的处理能力，甚至耗尽可用的socket，停止服务。</p><p>为什么需要TIME_WAIT？【保证Server最后收到了ACK】</p><p><img src="D:%5C2.%E6%88%91%E7%9A%84%E5%B7%A5%E4%BD%9C%5C8.%E7%A4%BE%E6%8B%9B%E6%B1%82%E8%81%8C%5C2019JD%5C3.%E9%9D%A2%E8%AF%95%E6%A2%B3%E7%90%86%5Cclip_image009.gif" alt="img"></p><p>原因有二：<br> <strong>一、保证TCP协议的全双工连接能够可靠关闭<br> 二、保证这次连接的重复数据段从网络中消失</strong></p><p>先说第一点，如果Client直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。</p><p>再说第二点，如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。</p><h2 id="8-4-网络编程时的同步、异步、阻塞、非阻塞"><a href="#8-4-网络编程时的同步、异步、阻塞、非阻塞" class="headerlink" title="8.4 网络编程时的同步、异步、阻塞、非阻塞"></a>8.4 网络编程时的同步、异步、阻塞、非阻塞</h2><p><strong>同步/异步主要针对C端:</strong><br> <strong>同步：</strong><br> 所谓同步，就是在c端发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。<br> 例如普通B/S模式（同步）：提交请求-&gt;等待服务器处理-&gt;处理完毕返回 这个期间客户端浏览器不能干任何事</p><p><strong>异步：</strong><br> 异步的概念和同步相对。当c端一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。<br> 例如 ajax请求（异步）: 请求通过事件触发-&gt;服务器处理（这是浏览器仍然可以作其他事情）-&gt;处理完毕</p><p><strong>阻塞/非阻塞主要针对S端:</strong><br> <strong>阻塞</strong><br> 阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。</p><p>有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。 例如，我们在socket中调用recv函数，如果缓冲区中没有数据，这个函数就会一直等待，直到有数据才返回。而此时，当前线程还会继续处理各种各样的消息。<br> 快递的例子：比如到你某个时候到A楼一层（假如是内核缓冲区）取快递，但是你不知道快递什么时候过来，你又不能干别的事，只能死等着。但你可以睡觉（进程处于休眠状态），因为你知道快递把货送来时一定会给你打个电话（假定一定能叫醒你）。</p><p><strong>非阻塞</strong><br> 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。<br> 还是等快递的例子：如果用忙轮询的方法，每隔5分钟到A楼一层(内核缓冲区）去看快递来了没有。如果没来，立即返回。而快递来了，就放在A楼一层，等你去取。</p><p>对象的阻塞模式和阻塞函数调用<br> 对象是否处于阻塞模式和函数是不是阻塞调用有很强的相关性，但是并不是一一对应的。阻塞对象上可以有非阻塞的调用方式，我们可以通过一定的API去轮询状 态，在适当的时候调用阻塞函数，就可以避免阻塞。而对于非阻塞对象，调用特殊的函数也可以进入阻塞调用。函数select就是这样的一个例子。</p><p>总结几句话就是：<br> <strong>同步/异步主要针对C端</strong><br> \1. 同步，就是我客户端（c端调用者）调用一个功能，该功能没有结束前，我（c端调用者）死等结果。<br> \2. 异步，就是我（c端调用者）调用一个功能，不需要知道该功能结果，该功能有结果后通知我（c端调用者）即回调通知。<br> 阻塞/非阻塞主要针对S端<br> \3. 阻塞， 就是调用我（s端被调用者，函数），我（s端被调用者，函数）没有接收完数据或者没有得到结果之前，我不会返回。<br> \4. 非阻塞， 就是调用我（s端被调用者，函数），我（s端被调用者，函数）立即返回，通过select通知调用者</p><h2 id="8-5-进程间通信方式"><a href="#8-5-进程间通信方式" class="headerlink" title="8.5 进程间通信方式"></a>8.5 进程间通信方式</h2><p><strong>1.**</strong>什么是进程间通信？**</p><p>由于不同的进程运行在各自不同的内存空间中,其中一个进程对于变量的修改另一方是无法感知的,因此,进程之间的消息传递不能通过变量或其他数据结构直接进行,只能通过进程间通信来完成.<br> 进程间通信是指不同进程间进行数据共享和数据交换.</p><p><strong>2.**</strong>进程通信的分类**</p><p>根据进程通信时信息量大小的不同,可以将进程通信划分为两大类型:<br> 控制信息的通信(低级通信)和大批数据信息的通信(高级通信).</p><p>低级通信主要用于进程之间的同步,互斥,终止和挂起等等控制信息的传递.</p><p>高级通信主要用于进程间数据块数据的交换和共享,常见的高级通信有管道,消息队列,共享内存等.</p><p><strong>进程通信的方式</strong></p><p>1)<strong>管道</strong>【管道分为有名管道和无名管道】</p><p>管道是一种半双工的通信方式,数据只能单向流动,而且只能在具有亲缘关系的进程间使用.进程的亲缘关系一般指的是父子关系.管道一般用于两个不同进程之间的通信.当一个进程创建了一个管道,并调用fork创建自己的一个子进程后,父进程关闭读管道端,子进程关闭写管道端,这样提供了两个进程之间数据流动的一种方式.</p><p>2)<strong>信号量</strong></p><p>信号量是一个计数器,可以用来控制多个线程对共享资源的访问.,它不是用于交换大批数据,而用于多线程之间的同步.它常作为一种锁机制,防止某进程在访问资源时其它进程也访问该资源.因此,主要作为进程间以及同一个进程内不同线程之间的同步手段.</p><p>3)<strong>信号</strong></p><p>信号是一种比较复杂的通信方式,用于通知接收进程某个事件已经发生.</p><p>4)<strong>消息队列</strong></p><p>消息队列是消息的链表,存放在内核中并由消息队列标识符标识.消息队列克服了信号传递信息少,管道只能承载无格式字节流以及缓冲区大小受限等特点.消息队列是UNIX下不同进程之间可实现共享资源的一种机制,UNIX允许不同进程将格式化的数据流以消息队列形式发送给任意进程.对消息队列具有操作权限的进程都可以使用msget完成对消息队列的操作控制.通过使用消息类型,进程可以按任何顺序读信息,或为消息安排优先级顺序.</p><p>5)<strong>共享内存</strong></p><p>共享内存就是映射一段能被其他进程所访问的内存,这段共享内存由一个进程创建,但多个进程都可以访问.共享内存是最快的IPC(进程间通信)方式,它是针对其它进程间通信方式运行效率低而专门设计的.它往往与其他通信机制,如信号量,配合使用,来实现进程间的同步与通信.</p><p>6)<strong>套接字(socket)</strong></p><p>套接口也是一种进程间通信机制,与其他通信机制不同的是,它可用于不同进程及其间进程的通信.</p><p><strong>3.**</strong>各种通信方式的优缺点**</p><p>无名管道简单方便．但局限于单向通信的工作方式．并且只能在创建它的进程及其子孙进程之间实现管道的共享：有名管道虽然可以提供给任意关系的进程使用．但是由于其长期存在于系统之中，使用不当容易出错．所以普通用户一般不建议使用。</p><p>消息队列可以不再局限于父子进程．而允许任意进程通过共享消息队列来实现进程间通信．并由系统调用函数来实现消息发送和接收之间的同步．从而使得用户在使用消息缓冲进行通信时不再需要考虑同步问题．使用方便，但是消息队列中信息的复制需要额外消耗CPU的时间．不适宜于信息量大或操作频繁的场合。<br> 因此．对于不同的应用问题，要根据问题本身的情况来选择进程间的通信方式。</p><h2 id="8-6-TCP的流量控制和拥塞控制"><a href="#8-6-TCP的流量控制和拥塞控制" class="headerlink" title="8.6 TCP的流量控制和拥塞控制"></a>8.6 TCP的流量控制和拥塞控制</h2><p><strong>流量控制【点对点】</strong><br> 所谓的流量控制就是让发送方的发送速率不要太快，让接收方来得及接受。利用<strong>滑动窗口</strong>机制可以很方便的在TCP连接上实现对发送方的流量控制。</p><p><strong>拥塞控制【网络资源】</strong><br> 在某段时间，若对网络中的某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变化，这种情况叫做拥塞。</p><p>所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能承受现有的网络负荷。<br> 流量控制往往指的是点对点通信量的控制，是个端到端的问题。流量控制所要做的就是控制发送端发送数据的速率，以便使接收端来得及接受。<br> 拥塞控制的四种算法，即<strong>慢开始（Slow-start)，拥塞避免（Congestion Avoidance)快重传（Fast Restrangsmit)和快回复（Fast Recovery）</strong></p><h1 id="九-大数据算法"><a href="#九-大数据算法" class="headerlink" title="九. 大数据算法"></a>九. 大数据算法</h1><h2 id="9-1-两个超大文件找共同出现的单词"><a href="#9-1-两个超大文件找共同出现的单词" class="headerlink" title="9.1 两个超大文件找共同出现的单词"></a>9.1 两个超大文件找共同出现的单词</h2><p><strong>1.题目描述</strong></p><p>给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?</p><p><strong>2.思考过程</strong> </p><p>（1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。</p><p>（2）但上述方法有一个明显问题，加载一个文件的数据需要50亿*64bytes = 320G远远大于4G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。</p><p>（3）针对上述问题，我们分治算法的思想。</p><blockquote><p><strong>step1</strong>：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,…,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计）</p><p><strong>step2</strong>：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,…,b999)（为什么要这样做? 文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中）</p><p>所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url）</p><p><strong>step3：</strong>因为每个hash大约300M，所以我们再可以采用（1）中的想法</p><p>最后对两个新的url文件做hadoop计数，reduce的结果中count为2的即是重复项。</p><p>也可用其他方法。</p></blockquote><h2 id="9-2-海量数据求-TopN"><a href="#9-2-海量数据求-TopN" class="headerlink" title="9.2 海量数据求 TopN"></a>9.2 海量数据求 TopN</h2><p><strong>问题描述</strong></p><p>有1亿个浮点数，如果找出期中最大的10000个？</p><p><strong>解题思路</strong></p><ul><li><p><strong>最容易想到的方法是将数据全部排序</strong>，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。</p></li><li><p><strong>第二种方法为局部淘汰法</strong>，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即10000。</p></li><li><p><strong>第三种方法是分治法</strong>，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100*10000个数据里面找出最大的10000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^64=4MB，一共需要101次这样的比较。</p></li><li><p><strong>第四种方法是Hash法</strong>。如果这1亿个数里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。</p></li><li><p><strong>第五种方法采用最小堆</strong>。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。</p></li></ul><h2 id="9-3-海量数据找出不重复的（整数）数据-分治-位图法"><a href="#9-3-海量数据找出不重复的（整数）数据-分治-位图法" class="headerlink" title="9.3 海量数据找出不重复的（整数）数据(分治+位图法)"></a>9.3 海量数据找出不重复的（整数）数据(分治+位图法)</h2><p><strong>题目描述</strong></p><p>在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。</p><p><strong>解答思路</strong></p><p><strong>方法一：分治法</strong></p><p>​      与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。</p><p><strong>方法二：位图法（bit-map）</strong></p><p>​      位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。</p><p>​       位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。</p><p>​       假设我们要对 [0,7] 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：</p><p><code>0 0 0 0 0 0 0 0</code><br>然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：</p><p>0 0 0 0 1 0 1 0<br>依次遍历，结束后，位数组是这样的：</p><p>0 1 1 0 1 1 1 0<br>每个为 1 的位，它的下标都表示了一个数：</p><p>for i in range(8):<br>    if bits[i] == 1:<br>        print(i)<br>1<br>2<br>3<br>这样我们其实就已经实现了排序。</p><p>对于<strong>整数相关的算法</strong>的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。</p><p>那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：</p><ul><li><p>00 表示这个数字没出现过；<br>01 表示这个数字出现过一次（即为题目所找的不重复整数）；<br>10 表示这个数字出现了多次。</p><p>那么这 232 个整数，总共所需内存为 <code>232*2b=1GB</code>。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：</p></li></ul><p>遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。</p><p>方法总结<br><strong>判断数字是否重复的问题，位图法是一种非常高效的方法。</strong></p><h2 id="9-4-布隆过滤器"><a href="#9-4-布隆过滤器" class="headerlink" title="9.4 布隆过滤器"></a>9.4 布隆过滤器</h2><p><strong>什么是布隆过滤器</strong></p><p>本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。</p><p>相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。</p><p><strong>实现原理</strong></p><h4 id="HashMap-的问题"><a href="#HashMap-的问题" class="headerlink" title="HashMap 的问题"></a>HashMap 的问题</h4><p>讲述布隆过滤器的原理之前，我们先思考一下，通常你判断某个元素是否存在用的是什么？应该蛮多人回答 HashMap 吧，确实可以将值映射到 HashMap 的 Key，然后可以在 O(1) 的时间复杂度内返回结果，效率奇高。但是 HashMap 的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了。</p><p>还比如说你的数据集存储在远程服务器上，本地服务接受输入，而数据集非常大不可能一次性读进内存构建 HashMap 的时候，也会存在问题。</p><p><strong>布隆过滤器数据结构</strong></p><p>布隆过滤器是一个 bit 向量或者说 bit 数组，长这样：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-07e149c32a2608fa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>​       如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并对每个生成的哈希值指向的 bit 位置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成了哈希值 1、4、7，则上图转变为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-12449becdb038afd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>Ok，我们现在再存一个值 “tencent”，如果哈希函数返回 3、4、8 的话，图继续变为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-802577f6332d76b4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>​        值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit 位，因此它被覆盖了。现在我们如果想查询 “dianping” 这个值是否存在，哈希函数返回了 1、5、8三个值，结果我们发现 5 这个 bit 位上的值为 0，说明没有任何一个值映射到这个 bit 位上，因此我们可以很确定地说 “dianping” 这个值不存在。而当我们需要查询 “baidu” 这个值是否存在的话，那么哈希函数必然会返回 1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。</p><p>这是为什么呢？答案跟简单，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值 “taobao” 即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断 “taobao” 这个值存在。</p><h4 id="支持删除么"><a href="#支持删除么" class="headerlink" title="支持删除么"></a>支持删除么</h4><p>​     传统的布隆过滤器并不支持删除操作。但是名为 Counting Bloom filter 的变种可以用来测试元素计数个数是否绝对小于某个阈值，它支持元素删除。可以参考文章 <a href="https://links.jianshu.com/go?to=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%3A%2F%2Fcloud.tencent.com%2Fdeveloper%2Farticle%2F1136056" target="_blank" rel="noopener">Counting Bloom Filter 的原理和实现</a></p><h4 id="如何选择哈希函数个数和布隆过滤器长度"><a href="#如何选择哈希函数个数和布隆过滤器长度" class="headerlink" title="如何选择哈希函数个数和布隆过滤器长度"></a>如何选择哈希函数个数和布隆过滤器长度</h4><p>​       很显然，过小的布隆过滤器很快所有的 bit 位均为 1，那么查询任何值都会返回“可能存在”，起不到过滤的目的了。布隆过滤器的长度会直接影响误报率，布隆过滤器越长其误报率越小。</p><p>​        另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置位 1 的速度越快，且布隆过滤器的效率越低；但是如果太少的话，那我们的误报率会变高。</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-76dccfbdc9d7bdb1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>如何选择适合业务的 k 和 m 值呢，这里直接贴一个公式：</p><p><img src="https:////upload-images.jianshu.io/upload_images/2785001-675967d74620371f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/349/format/webp" alt="img"></p><p>​       k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。<br> 至于如何推导这个公式，我在知乎发布的<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F43263751" target="_blank" rel="noopener">文章</a>有涉及，感兴趣可以看看，不感兴趣的话记住上面这个公式就行了。</p><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><p>​       常见的适用常见有，利用布隆过滤器减少磁盘 IO 或者网络请求，因为一旦一个值必定不存在的话，我们可以不用进行后续昂贵的查询请求。</p><p>​       另外，既然你使用布隆过滤器来加速查找和判断是否存在，那么性能很低的哈希函数不是个好选择，推荐 MurmurHash、Fnv 这些。</p><h4 id="大Value拆分"><a href="#大Value拆分" class="headerlink" title="大Value拆分"></a>大Value拆分</h4><p>​      Redis 因其支持 setbit 和 getbit 操作，且纯内存性能高等特点，因此天然就可以作为布隆过滤器来使用。但是布隆过滤器的不当使用极易产生大 Value，增加 Redis 阻塞风险，因此生成环境中建议对体积庞大的布隆过滤器进行拆分。</p><p>​      拆分的形式方法多种多样，但是本质是不要将 Hash(Key) 之后的请求分散在多个节点的多个小 bitmap 上，而是应该拆分成多个小 bitmap 之后，对一个 Key 的所有哈希函数都落在这一个小 bitmap 上</p><h2 id="9-5-bit-map"><a href="#9-5-bit-map" class="headerlink" title="9.5 bit-map"></a>9.5 bit-map</h2><p><strong>Bit-map的基本思想</strong></p><p>　　32位机器上，对于一个整型数，比如int a=1 在内存中占32bit位，这是为了方便计算机的运算。但是对于某些应用场景而言，这属于一种巨大的浪费，因为我们可以用对应的32bit位对应存储十进制的0-31个数，而这就是Bit-map的基本思想。<strong>Bit-map算法利用这种思想处理大量数据的排序、查询以及去重。Bitmap在用户群做交集和并集运算的时候也有极大的便利</strong>。</p><blockquote><p>在此我用一个简单的例子来详细介绍BitMap算法的原理。</p><p>假设我们要对0-7内的5个元素(4,7,2,5,3)进行排序(这里假设元素没有重复)。我们可以使用BitMap算法达到排序目的。要表示8个数，我们需要8个byte。</p><p>1.首先我们开辟一个字节(8byte)的空间，将这些空间的所有的byte位都设置为0</p><p>2.然后便利这5个元素，第一个元素是4，因为下边从0开始，因此我们把第五个字节的值设置为1</p><p>3.然后再处理剩下的四个元素，最终8个字节的状态如下图</p><p>　             　<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE3LmNuYmxvZ3MuY29tL2Jsb2cvMTI3MTA3My8yMDE3MTEvMTI3MTA3My0yMDE3MTEyMzE2MTUyNTgyMS03MTg5MTg0ODIucG5n?x-oss-process=image/format,png" alt="img"></p><p>4.现在我们遍历一次bytes区域，把值为1的byte的位置输出(2,3,4,5,7)，这样便达到了排序的目的 </p><p>从上面的例子我们可以看出，BitMap算法的思想还是比较简单的，关键的问题是如何确定10进制的数到2进制的映射图</p></blockquote><p><strong>MAP映射：</strong></p><p>　　假设需要排序或则查找的数的总数N=100000000，BitMap中1bit代表一个数字，1个int = 4Bytes = 4*8bit = 32 bit,那么N个数需要N/32 int空间。所以我们需要申请内存空间的大小为int a[1 + N/32]，其中：a[0]在内存中占32为可以对应十进制数0-31，依次类推：</p><p>　　a[0]—————————–&gt; 0-31</p><p>　　a[1]——————————&gt; 32-63</p><p>　　a[2]——————————-&gt; 64-95</p><p>　　a[3]——————————–&gt; 96-127</p><p>　　………………………………………………</p><p>　　那么十进制数如何转换为对应的bit位，下面介绍用位移将十进制数转换为对应的bit位:</p><p>　　<strong>1.求十进制数在对应数组a中的下标</strong></p><p>　　十进制数0-31，对应在数组a[0]中，32-63对应在数组a[1]中，64-95对应在数组a[2]中………，使用数学归纳分析得出结论：对于一个十进制数n，其在数组a中的下标为：a[n/32]</p><p>　　<strong>2.求出十进制数在对应数a[i]中的下标</strong></p><p>　　例如十进制数1在a[0]的下标为1，十进制数31在a[0]中下标为31，十进制数32在a[1]中下标为0。 在十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32求得在对应数组a[i]中的下标。</p><p>　　<strong>3.位移</strong></p><p>　　对于一个十进制数n,对应在数组a[n/32][n%32]中，但数组a毕竟不是一个二维数组，我们通过移位操作实现置1</p><p>　　a[n/32] |= 1 &lt;&lt; n % 32<br>　　移位操作：<br>　　a[n&gt;&gt;5] |= 1 &lt;&lt; (n &amp; 0x1F)</p><p>　　n &amp; 0x1F 保留n的后五位 相当于 n % 32 求十进制数在数组a[i]中的下标。</p><h2 id="9-6-字典树"><a href="#9-6-字典树" class="headerlink" title="9.6 字典树"></a>9.6 字典树</h2><p>Trie树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。字典树（Trie）可以保存一些 字符串 -&gt; 值 的对应关系。基本上，它跟 Java 的 HashMap 功能相同，都是 key-value 映射，只不过 Trie 的 key 只能是字符串。</p><p><strong>Trie的核心思想是空间换时间</strong>。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。</p><p>Trie 的强大之处就在于它的时间复杂度。它的插入和查询时间复杂度都为 O(k) ，其中 k 为 key 的长度，与 Trie 中保存了多少个元素无关。Hash 表号称是 O(1) 的，但在计算 hash 的时候就肯定会是 O(k) ，而且还有碰撞之类的问题；</p><p><strong>Trie 的缺点是空间消耗很高</strong>。</p><p>典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：<strong>最大限度地减少无谓的字符串比较，查询效率比哈希表高</strong>。</p><p><strong>Trie树的基本性质：</strong></p><p>（1）根节点不包含字符，除根节点意外每个节点只包含一个字符。<br>（2）从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。<br>（3）每个节点的所有子节点包含的字符串不相同。<br>（4）如果字符的种数为n，则每个结点的出度为n，这也是空间换时间的体现，浪费了很多的空间。<br>（5）插入查找的复杂度为O(n)，n为字符串长度。</p><p><strong>基本思想（以字母树为例）：</strong></p><p><strong>1、插入过程</strong></p><p>对于一个单词，从根开始，沿着单词的各个字母所对应的树中的节点分支向下走，直到单词遍历完，将最后的节点标记为红色，表示该单词已插入Trie树。</p><p><strong>2、查询过程</strong></p><p>同样的，从根开始按照单词的字母顺序向下遍历trie树，一旦发现某个节点标记不存在或者单词遍历完成而最后的节点未标记为红色，则表示该单词不存在，若最后的节点标记为红色，表示该单词存在。</p><p><strong>字典树的数据结构</strong></p><p>一般可以按下面步骤构建：</p><p>利用串构建一个字典树，这个字典树保存了串的公共前缀信息，因此可以降低查询操作的复杂度。</p><p>下面以英文单词构建的字典树为例，这棵Trie树中每个结点包括26个孩子结点，因为总共有26个英文字母(假设单词都是小写字母组成)。</p><p>则可声明包含Trie树的结点信息的结构体:</p><pre class="line-numbers language-c++"><code class="language-c++">typedef struct Trie_node{    int count;                    // 统计单词前缀出现的次数    struct Trie_node* next[26];   // 指向各个子树的指针    bool exist;                   // 标记该结点处是否构成单词  }TrieNode , *Trie;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中next是一个指针数组，存放着指向各个孩子结点的指针。</p><p>其中next是一个指针数组，存放着指向各个孩子结点的指针。</p><p>如给出字符串”abc”,”ab”,”bd”,”dda”，根据该字符串序列构建一棵Trie树。则构建的树如下:</p><h2 id="9-7-倒排索引"><a href="#9-7-倒排索引" class="headerlink" title="9.7 倒排索引"></a>9.7 倒排索引</h2><p>倒排索引是目前搜索引擎公司对搜索引擎最常用的存储方式，也是搜索引擎的核心内容，在搜索引擎的实际应用中，有时需要按照关键字的某些值查找记录，所以是按照关键字建立索引，这个索引就被称为倒排索引。</p><p>首先你要明确，索引这东西，一般是用于提高查询效率的。举个最简单的例子，已知有5个文本文件，需要我们去查某个单词位于哪个文本文件中，最直观的做法就是挨个加载每个文本文件中的单词到内存中，然后用for循环遍历一遍数组，直到找到这个单词。这种做法就是正向索引的思路。</p><p>正向索引的这种查询效率也不需要我多吐槽了。倒排索引的思路其实也并不难。再举一个例子，有两段文本</p><p>D1：Hello, conan!</p><p>D2：Hello, hattori!</p><p>第一步，找到所有的单词</p><p>Hello、conan、hattori</p><p>第二步，找到包含这些单词的文本位置</p><p>Hello（D1，D2）</p><p>conan（D1）</p><p>hattori（D2）</p><p>我们将单词作为Hash表的Key，将所在的文本位置作为Hash表的Value保存起来。</p><p>当我们要查询某个单词的所在位置时，只需要根据这张Hash表就可以迅速的找到目标文档。</p><p>结合之前的说的正向索引，不难发现。正向索引是通过文档去查找单词，反向索引则是通过单词去查找文档。</p><p>倒排索引的优点还包括在处理复杂的多关键字查询时，可在倒排表中先完成查询的并、交等逻辑运算，得到结果后再对记录进行存取，这样把对文档的查询转换为地址集合的运算，从而提高查找速度。</p><h1 id="十-数据结构和算法"><a href="#十-数据结构和算法" class="headerlink" title="十. 数据结构和算法"></a>十. 数据结构和算法</h1><h2 id="10-1-数组"><a href="#10-1-数组" class="headerlink" title="10.1 数组"></a>10.1 数组</h2><p>连续子数组的最大和</p><p>调整数组顺序使奇数位于偶数前面</p><h2 id="10-2-链表"><a href="#10-2-链表" class="headerlink" title="10.2 链表"></a>10.2 链表</h2><h4 id="1-链表删除"><a href="#1-链表删除" class="headerlink" title="1. 链表删除"></a>1. 链表删除</h4><h5 id="删除链表中的节点"><a href="#删除链表中的节点" class="headerlink" title="删除链表中的节点"></a><a href="https://leetcode-cn.com/problems/delete-node-in-a-linked-list/" target="_blank" rel="noopener">删除链表中的节点</a></h5><p>核心代码：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">deleteNode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>               node<span class="token punctuation">.</span>val <span class="token operator">=</span> node<span class="token punctuation">.</span>next<span class="token punctuation">.</span>val        node<span class="token punctuation">.</span>next <span class="token operator">=</span> node<span class="token punctuation">.</span>next<span class="token punctuation">.</span>next<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>[python 垃圾回收机制</p><blockquote><p><strong>引用计数</strong></p><p>Python语言默认采用的垃圾收集机制是『引用计数法 Reference Counting』，该算法最早George E. Collins在1960的时候首次提出，50年后的今天，该算法依然被很多编程语言使用，『引用计数法』的原理是：每个对象维护一个<code>ob_ref</code>字段，用来记录该对象当前被引用的次数，每当新的引用指向该对象时，它的引用计数ob_ref加1，每当该对象的引用失效时计数ob_ref减1，一旦对象的引用计数为0，该对象立即被回收，对象占用的内存空间将被释放。它的缺点是需要额外的空间维护引用计数，这个问题是其次的，不过最主要的问题是它不能解决对象的“循环引用”，因此，也有很多语言比如Java并没有采用该算法做来垃圾的收集机制。</p><p><strong>标记清除</strong></p><p>『标记清除（Mark—Sweep）』算法是一种基于<strong>追踪回收</strong>（tracing GC）技术实现的垃圾回收算法。它分为两个阶段：第一阶段是<strong>标记阶段</strong>，GC会把所有的『活动对象』打上标记，<strong>第二阶段是把那些没有标记的对象『非活动对象』进行回收</strong>。那么GC又是如何判断哪些是活动对象哪些是非活动对象的呢？</p><p>对象之间通过引用（指针）连在一起，构成一个<strong>有向图</strong>，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。</p><p><img src="https://foofish.net/images/mark-sweep.svg" alt="img"></p><p>在上图中，我们把小黑圈视为全局变量，也就是把它作为root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而4和5不可达，那么1、2、3就是活动对象，4和5是非活动对象会被GC回收。</p><p>标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些<strong>容器对象</strong>，比如<strong>list</strong>、<strong>dict</strong>、<strong>tuple</strong>，<strong>instance</strong>等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的缺点：<strong>清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象。</strong></p><h3 id="分代回收"><a href="#分代回收" class="headerlink" title="分代回收"></a>分代回收</h3><p><strong>分代回收是一种以空间换时间的操作方式</strong>，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象</p></blockquote></li></ul><hr><h5 id="奇偶链表"><a href="#奇偶链表" class="headerlink" title="奇偶链表"></a>奇偶链表</h5><p>核心代码</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">oddEvenList</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> ListNode<span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token operator">not</span> head<span class="token punctuation">:</span><span class="token keyword">return</span> head        odd <span class="token operator">=</span> head        even_head <span class="token operator">=</span> even <span class="token operator">=</span> head<span class="token punctuation">.</span>next        <span class="token keyword">while</span> odd<span class="token punctuation">.</span>next <span class="token operator">and</span> even<span class="token punctuation">.</span>next<span class="token punctuation">:</span>            odd<span class="token punctuation">.</span>next <span class="token operator">=</span> odd<span class="token punctuation">.</span>next<span class="token punctuation">.</span>next            even<span class="token punctuation">.</span>next <span class="token operator">=</span> even<span class="token punctuation">.</span>next<span class="token punctuation">.</span>next            odd<span class="token punctuation">,</span>even <span class="token operator">=</span> odd<span class="token punctuation">.</span>next<span class="token punctuation">,</span>even<span class="token punctuation">.</span>next        odd<span class="token punctuation">.</span>next <span class="token operator">=</span> even_head        <span class="token keyword">return</span> head<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>知识点：</p><h4 id="2-链表合并"><a href="#2-链表合并" class="headerlink" title="2. 链表合并"></a>2. 链表合并</h4><h5 id="1-2-1-合并两个有序链表"><a href="#1-2-1-合并两个有序链表" class="headerlink" title="1.2.1 合并两个有序链表"></a>1.2.1 <a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/" target="_blank" rel="noopener">合并两个有序链表</a></h5><p>核心代码</p><pre class="line-numbers language-python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="3-链表反转"><a href="#3-链表反转" class="headerlink" title="3. 链表反转"></a>3. 链表反转</h4><h5 id="1-3-1-K-个一组翻转链表"><a href="#1-3-1-K-个一组翻转链表" class="headerlink" title="1.3.1  K 个一组翻转链表"></a><a href="https://leetcode-cn.com/problems/reverse-nodes-in-k-group/" target="_blank" rel="noopener">1.3.1  K 个一组翻转链表</a></h5><p><strong>解题答案</strong>：</p><p>​     <strong>Python</strong>版</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#官方题解：</span><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 翻转一个子链表，并且返回新的头与尾</span>    <span class="token keyword">def</span> <span class="token function">reverse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">,</span> tail<span class="token punctuation">:</span> ListNode<span class="token punctuation">)</span><span class="token punctuation">:</span>        prev <span class="token operator">=</span> tail<span class="token punctuation">.</span>next        p <span class="token operator">=</span> head        <span class="token keyword">while</span> prev <span class="token operator">!=</span> tail<span class="token punctuation">:</span>            nex <span class="token operator">=</span> p<span class="token punctuation">.</span>next            p<span class="token punctuation">.</span>next <span class="token operator">=</span> prev            prev <span class="token operator">=</span> p            p <span class="token operator">=</span> nex        <span class="token keyword">return</span> tail<span class="token punctuation">,</span> head    <span class="token keyword">def</span> <span class="token function">reverseKGroup</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">,</span> k<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> ListNode<span class="token punctuation">:</span>        hair <span class="token operator">=</span> ListNode<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        hair<span class="token punctuation">.</span>next <span class="token operator">=</span> head        pre <span class="token operator">=</span> hair        <span class="token keyword">while</span> head<span class="token punctuation">:</span>            tail <span class="token operator">=</span> pre            <span class="token comment" spellcheck="true"># 查看剩余部分长度是否大于等于 k</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>                tail <span class="token operator">=</span> tail<span class="token punctuation">.</span>next                <span class="token keyword">if</span> <span class="token operator">not</span> tail<span class="token punctuation">:</span>                    <span class="token keyword">return</span> hair<span class="token punctuation">.</span>next            nex <span class="token operator">=</span> tail<span class="token punctuation">.</span>next            head<span class="token punctuation">,</span> tail <span class="token operator">=</span> self<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span>head<span class="token punctuation">,</span> tail<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 把子链表重新接回原链表</span>            pre<span class="token punctuation">.</span>next <span class="token operator">=</span> head            tail<span class="token punctuation">.</span>next <span class="token operator">=</span> nex            pre <span class="token operator">=</span> tail            head <span class="token operator">=</span> tail<span class="token punctuation">.</span>next        <span class="token keyword">return</span> hair<span class="token punctuation">.</span>next<span class="token comment" spellcheck="true">#复杂度分析</span>时间复杂度：O<span class="token punctuation">(</span>n<span class="token punctuation">)</span>，其中 n为链表的长度。head 指针会在O<span class="token punctuation">(</span>N<span class="token operator">/</span>K<span class="token punctuation">)</span>个结点上停留，每次停留需要进行一次 O<span class="token punctuation">(</span>k<span class="token punctuation">)</span>的翻转操作。空间复杂度：O<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>，我们只需要建立常数个变量。<span class="token comment" spellcheck="true">#网友解法：</span>复杂度分析    时间复杂度：O<span class="token punctuation">(</span>N<span class="token punctuation">)</span>。    空间复杂度：O<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#时间最优解法</span><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">reverseKGroup</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">:</span> ListNode<span class="token punctuation">,</span> k<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> ListNode<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 判断链表长度是否大于K</span>        cur <span class="token operator">=</span> head        <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token operator">not</span> cur<span class="token punctuation">:</span> <span class="token keyword">return</span> head            cur <span class="token operator">=</span> cur<span class="token punctuation">.</span>next        <span class="token comment" spellcheck="true"># 反转头部的长度为k的子链表</span>        pre <span class="token operator">=</span> head        itera <span class="token operator">=</span> head<span class="token punctuation">.</span>next        <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>k<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            next <span class="token operator">=</span> itera<span class="token punctuation">.</span>next            itera<span class="token punctuation">.</span>next <span class="token operator">=</span> pre            pre <span class="token operator">=</span> itera            itera <span class="token operator">=</span> next        <span class="token comment" spellcheck="true"># 反转链表的剩余部分</span>        head<span class="token punctuation">.</span>next <span class="token operator">=</span> self<span class="token punctuation">.</span>reverseKGroup<span class="token punctuation">(</span>cur<span class="token punctuation">,</span> k<span class="token punctuation">)</span>        <span class="token keyword">return</span> pre   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h5><ul><li><p>反转链表操作</p><pre class="line-numbers language-python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h4 id="4-链表相交"><a href="#4-链表相交" class="headerlink" title="4. 链表相交"></a>4. 链表相交</h4><h5 id="1-4-1-相交链表"><a href="#1-4-1-相交链表" class="headerlink" title="1.4.1 相交链表"></a><a href="https://leetcode-cn.com/problems/intersection-of-two-linked-lists/" target="_blank" rel="noopener">1.4.1 相交链表</a></h5><p>核心代码</p><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">-</span><span class="token operator">-</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-链表旋转"><a href="#5-链表旋转" class="headerlink" title="5. 链表旋转"></a>5. 链表旋转</h4><h5 id="1-5-1-旋转链表"><a href="#1-5-1-旋转链表" class="headerlink" title="1.5.1 旋转链表"></a><a href="https://leetcode-cn.com/problems/rotate-list/" target="_blank" rel="noopener">1.5.1 旋转链表</a></h5><p>核心代码</p><h2 id="10-3-字符串"><a href="#10-3-字符串" class="headerlink" title="10.3 字符串"></a>10.3 字符串</h2><h4 id="1-字符串比较"><a href="#1-字符串比较" class="headerlink" title="1. 字符串比较"></a>1. 字符串比较</h4><h5 id="比较字符串最小字母出现频次"><a href="#比较字符串最小字母出现频次" class="headerlink" title="比较字符串最小字母出现频次"></a><a href="https://leetcode-cn.com/problems/compare-strings-by-frequency-of-the-smallest-character/" target="_blank" rel="noopener">比较字符串最小字母出现频次</a></h5><h5 id="比较版本号"><a href="#比较版本号" class="headerlink" title="比较版本号"></a><a href="https://leetcode-cn.com/problems/compare-version-numbers/" target="_blank" rel="noopener">比较版本号</a></h5><p>核心代码：</p><pre class="line-numbers language-python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-字符串连接"><a href="#2-字符串连接" class="headerlink" title="2. 字符串连接"></a>2. 字符串连接</h4><h5 id=""><a href="#" class="headerlink" title=""></a></h5><h4 id="3-字符串匹配"><a href="#3-字符串匹配" class="headerlink" title="3. 字符串匹配"></a>3. 字符串匹配</h4><h5 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a><a href="https://leetcode-cn.com/problems/pattern-matching-lcci/" target="_blank" rel="noopener">模式匹配</a></h5><h5 id="数组中的字符串匹配"><a href="#数组中的字符串匹配" class="headerlink" title="数组中的字符串匹配"></a><a href="https://leetcode-cn.com/problems/string-matching-in-an-array/" target="_blank" rel="noopener">数组中的字符串匹配</a></h5><h5 id="通配符匹配"><a href="#通配符匹配" class="headerlink" title="通配符匹配"></a><a href="https://leetcode-cn.com/problems/wildcard-matching/" target="_blank" rel="noopener">通配符匹配</a></h5><h5 id="驼峰式匹配"><a href="#驼峰式匹配" class="headerlink" title="驼峰式匹配"></a><a href="https://leetcode-cn.com/problems/camelcase-matching/" target="_blank" rel="noopener">驼峰式匹配</a></h5><h5 id="正则表达式匹配"><a href="#正则表达式匹配" class="headerlink" title="正则表达式匹配"></a><a href="https://leetcode-cn.com/problems/regular-expression-matching/" target="_blank" rel="noopener">正则表达式匹配</a></h5><h5 id="重复叠加字符串匹配"><a href="#重复叠加字符串匹配" class="headerlink" title="重复叠加字符串匹配"></a><a href="https://leetcode-cn.com/problems/repeated-string-match/" target="_blank" rel="noopener">重复叠加字符串匹配</a></h5><h4 id="4-字符串反转"><a href="#4-字符串反转" class="headerlink" title="4. 字符串反转"></a>4. 字符串反转</h4><h5 id="反转字符串中的单词-III"><a href="#反转字符串中的单词-III" class="headerlink" title="反转字符串中的单词 III"></a><a href="https://leetcode-cn.com/problems/reverse-words-in-a-string-iii/" target="_blank" rel="noopener">反转字符串中的单词 III</a></h5><h5 id="仅仅反转字母"><a href="#仅仅反转字母" class="headerlink" title="仅仅反转字母"></a><a href="https://leetcode-cn.com/problems/reverse-only-letters/" target="_blank" rel="noopener">仅仅反转字母</a></h5><h5 id="反转字符串中的元音字母"><a href="#反转字符串中的元音字母" class="headerlink" title="反转字符串中的元音字母"></a><a href="https://leetcode-cn.com/problems/reverse-vowels-of-a-string/" target="_blank" rel="noopener">反转字符串中的元音字母</a></h5><h5 id="反转字符串"><a href="#反转字符串" class="headerlink" title="反转字符串"></a><a href="https://leetcode-cn.com/problems/reverse-string/" target="_blank" rel="noopener">反转字符串</a></h5><h5 id="反转字符串-II"><a href="#反转字符串-II" class="headerlink" title="反转字符串 II"></a><a href="https://leetcode-cn.com/problems/reverse-string-ii/" target="_blank" rel="noopener">反转字符串 II</a></h5><h4 id="5-字符串旋转"><a href="#5-字符串旋转" class="headerlink" title="5. 字符串旋转"></a>5. 字符串旋转</h4><h5 id="旋转数字"><a href="#旋转数字" class="headerlink" title="旋转数字"></a><a href="https://leetcode-cn.com/problems/rotated-digits/" target="_blank" rel="noopener">旋转数字</a></h5><h5 id="II-左旋转字符串"><a href="#II-左旋转字符串" class="headerlink" title="II. 左旋转字符串"></a><a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/" target="_blank" rel="noopener">II. 左旋转字符串</a></h5><h4 id="6-字符串子串"><a href="#6-字符串子串" class="headerlink" title="6. 字符串子串"></a>6. 字符串子串</h4><h5 id="子串能表示从-1-到-N-数字的二进制串"><a href="#子串能表示从-1-到-N-数字的二进制串" class="headerlink" title="子串能表示从 1 到 N 数字的二进制串"></a><a href="https://leetcode-cn.com/problems/binary-string-with-substrings-representing-1-to-n/" target="_blank" rel="noopener">子串能表示从 1 到 N 数字的二进制串</a></h5><h5 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">最长回文子串</a></h5><h5 id="定长子串中元音的最大数目"><a href="#定长子串中元音的最大数目" class="headerlink" title="定长子串中元音的最大数目"></a><a href="https://leetcode-cn.com/problems/maximum-number-of-vowels-in-a-substring-of-given-length/" target="_blank" rel="noopener">定长子串中元音的最大数目</a></h5><h5 id="串联所有单词的子串"><a href="#串联所有单词的子串" class="headerlink" title=" 串联所有单词的子串"></a><a href="https://leetcode-cn.com/problems/substring-with-concatenation-of-all-words/" target="_blank" rel="noopener"> 串联所有单词的子串</a></h5><h5 id="单字符重复子串的最大长度"><a href="#单字符重复子串的最大长度" class="headerlink" title="单字符重复子串的最大长度"></a><a href="https://leetcode-cn.com/problems/swap-for-longest-repeated-character-substring/" target="_blank" rel="noopener">单字符重复子串的最大长度</a></h5><h5 id="子串的最大出现次数"><a href="#子串的最大出现次数" class="headerlink" title="子串的最大出现次数"></a><a href="https://leetcode-cn.com/problems/maximum-number-of-occurrences-of-a-substring/" target="_blank" rel="noopener">子串的最大出现次数</a></h5><h2 id="10-4-二叉树"><a href="#10-4-二叉树" class="headerlink" title="10.4 二叉树"></a>10.4 二叉树</h2><h4 id="1-二叉树遍历"><a href="#1-二叉树遍历" class="headerlink" title="1. 二叉树遍历"></a>1. 二叉树遍历</h4><h5 id="二叉树的层序遍历"><a href="#二叉树的层序遍历" class="headerlink" title="二叉树的层序遍历"></a><a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/" target="_blank" rel="noopener">二叉树的层序遍历</a></h5><h4 id="2-二叉树路径"><a href="#2-二叉树路径" class="headerlink" title="2. 二叉树路径"></a>2. 二叉树路径</h4><h5 id="二叉树中的最大路径和"><a href="#二叉树中的最大路径和" class="headerlink" title="二叉树中的最大路径和"></a><a href="https://leetcode-cn.com/problems/binary-tree-maximum-path-sum/" target="_blank" rel="noopener">二叉树中的最大路径和</a></h5><h4 id="3-二叉树翻转"><a href="#3-二叉树翻转" class="headerlink" title="3. 二叉树翻转"></a>3. 二叉树翻转</h4><h5 id="翻转等价二叉树"><a href="#翻转等价二叉树" class="headerlink" title=" 翻转等价二叉树"></a><a href="https://leetcode-cn.com/problems/flip-equivalent-binary-trees/" target="_blank" rel="noopener"> 翻转等价二叉树</a></h5><h4 id="4-二叉树搜索"><a href="#4-二叉树搜索" class="headerlink" title="4. 二叉树搜索"></a>4. 二叉树搜索</h4><h5 id="二叉树的最大深度"><a href="#二叉树的最大深度" class="headerlink" title="二叉树的最大深度"></a><a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/" target="_blank" rel="noopener">二叉树的最大深度</a></h5><h5 id="二叉搜索树中的搜索"><a href="#二叉搜索树中的搜索" class="headerlink" title="二叉搜索树中的搜索"></a><a href="https://leetcode-cn.com/problems/search-in-a-binary-search-tree/" target="_blank" rel="noopener">二叉搜索树中的搜索</a></h5><h4 id="5-二叉树子树"><a href="#5-二叉树子树" class="headerlink" title="5. 二叉树子树"></a>5. 二叉树子树</h4><h5 id="出现次数最多的子树元素和"><a href="#出现次数最多的子树元素和" class="headerlink" title="出现次数最多的子树元素和"></a><a href="https://leetcode-cn.com/problems/most-frequent-subtree-sum/" target="_blank" rel="noopener">出现次数最多的子树元素和</a></h5><h2 id="10-5-堆"><a href="#10-5-堆" class="headerlink" title="10.5 堆"></a>10.5 堆</h2><h2 id="10-6-动态规划"><a href="#10-6-动态规划" class="headerlink" title="10.6 动态规划"></a>10.6 动态规划</h2><h4 id="1-连续子数组最值"><a href="#1-连续子数组最值" class="headerlink" title="1. 连续子数组最值"></a>1. 连续子数组最值</h4><h5 id="连续子数组的最大和"><a href="#连续子数组的最大和" class="headerlink" title="连续子数组的最大和"></a><a href="https://leetcode-cn.com/problems/lian-xu-zi-shu-zu-de-zui-da-he-lcof/" target="_blank" rel="noopener">连续子数组的最大和</a></h5><h5 id="最长重复子数组"><a href="#最长重复子数组" class="headerlink" title="最长重复子数组"></a><a href="https://leetcode-cn.com/problems/maximum-length-of-repeated-subarray/" target="_blank" rel="noopener">最长重复子数组</a></h5><h5 id="K个逆序对数组"><a href="#K个逆序对数组" class="headerlink" title="K个逆序对数组"></a><a href="https://leetcode-cn.com/problems/k-inverse-pairs-array/" target="_blank" rel="noopener">K个逆序对数组</a></h5><h5 id="最长递增子序列的个数"><a href="#最长递增子序列的个数" class="headerlink" title="最长递增子序列的个数"></a><a href="https://leetcode-cn.com/problems/number-of-longest-increasing-subsequence/" target="_blank" rel="noopener">最长递增子序列的个数</a></h5><h5 id="最长的斐波那契子序列的长度"><a href="#最长的斐波那契子序列的长度" class="headerlink" title="[最长的斐波那契子序列的长度]"></a>[最长的斐波那契子序列的长度]</h5><h2 id="10-7-二分查找"><a href="#10-7-二分查找" class="headerlink" title="10.7 二分查找"></a>10.7 二分查找</h2><h4 id="1-数组二分查找"><a href="#1-数组二分查找" class="headerlink" title="1. 数组二分查找"></a>1. 数组二分查找</h4><h5 id="分割数组的最大值"><a href="#分割数组的最大值" class="headerlink" title="分割数组的最大值"></a><a href="https://leetcode-cn.com/problems/split-array-largest-sum/" target="_blank" rel="noopener">分割数组的最大值</a></h5><h5 id="两个数组的交集"><a href="#两个数组的交集" class="headerlink" title="两个数组的交集"></a><a href="https://leetcode-cn.com/problems/intersection-of-two-arrays/" target="_blank" rel="noopener">两个数组的交集</a></h5><h5 id="寻找两个正序数组的中位数"><a href="#寻找两个正序数组的中位数" class="headerlink" title="寻找两个正序数组的中位数"></a><a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/" target="_blank" rel="noopener">寻找两个正序数组的中位数</a></h5><h5 id="最长重复子数组-1"><a href="#最长重复子数组-1" class="headerlink" title="最长重复子数组"></a><a href="https://leetcode-cn.com/problems/maximum-length-of-repeated-subarray/" target="_blank" rel="noopener">最长重复子数组</a></h5><h5 id="长度最小的子数组"><a href="#长度最小的子数组" class="headerlink" title="长度最小的子数组"></a><a href="https://leetcode-cn.com/problems/minimum-size-subarray-sum/" target="_blank" rel="noopener">长度最小的子数组</a></h5><h4 id="2-堆二分查找"><a href="#2-堆二分查找" class="headerlink" title="2. 堆二分查找"></a>2. 堆二分查找</h4><h5 id="有序矩阵中第K小的元素"><a href="#有序矩阵中第K小的元素" class="headerlink" title="有序矩阵中第K小的元素"></a><a href="https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/" target="_blank" rel="noopener">有序矩阵中第K小的元素</a></h5><h5 id="第-K-个最小的素数分数"><a href="#第-K-个最小的素数分数" class="headerlink" title="第 K 个最小的素数分数"></a><a href="https://leetcode-cn.com/problems/k-th-smallest-prime-fraction/" target="_blank" rel="noopener">第 K 个最小的素数分数</a></h5><h5 id="找出第-k-小的距离对"><a href="#找出第-k-小的距离对" class="headerlink" title="找出第 k 小的距离对"></a><a href="https://leetcode-cn.com/problems/find-k-th-smallest-pair-distance/" target="_blank" rel="noopener">找出第 k 小的距离对</a></h5><h4 id="3-二叉树二分查找"><a href="#3-二叉树二分查找" class="headerlink" title="3. 二叉树二分查找"></a>3. 二叉树二分查找</h4><h5 id="二叉搜索树中第K小的元素"><a href="#二叉搜索树中第K小的元素" class="headerlink" title="二叉搜索树中第K小的元素"></a><a href="https://leetcode-cn.com/problems/kth-smallest-element-in-a-bst/" target="_blank" rel="noopener">二叉搜索树中第K小的元素</a></h5><h5 id="完全二叉树的节点个数"><a href="#完全二叉树的节点个数" class="headerlink" title="[完全二叉树的节点个数]("></a>[完全二叉树的节点个数](</h5><h2 id="10-8-排序"><a href="#10-8-排序" class="headerlink" title="10.8 排序"></a>10.8 排序</h2><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;一-Hadoop篇&quot;&gt;&lt;a href=&quot;#一-Hadoop篇&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="Job" scheme="https://dataquaner.github.io/categories/Job/"/>
    
    
      <category term="Job" scheme="https://dataquaner.github.io/tags/Job/"/>
    
  </entry>
  
  <entry>
    <title>Spark面试问题梳理：选择题</title>
    <link href="https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/"/>
    <id>https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/</id>
    <published>2020-06-21T06:35:00.000Z</published>
    <updated>2020-06-21T12:27:36.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Spark-的四大组件下面哪个不是-D"><a href="#1-Spark-的四大组件下面哪个不是-D" class="headerlink" title="1. Spark 的四大组件下面哪个不是 (D )"></a>1. <strong>Spark 的四大组件下面哪个不是 (D )</strong></h2><p>A.Spark Streaming    B. Mlib </p><p>C Graphx    D.Spark R</p><h2 id="2-下面哪个端口不是-spark-自带服务的端口-C"><a href="#2-下面哪个端口不是-spark-自带服务的端口-C" class="headerlink" title="2. 下面哪个端口不是 spark 自带服务的端口 (C )"></a>2. 下面哪个端口不是 spark 自带服务的端口 (C )</h2><p>A.8080 B.4040 C.8090 D.18080</p><p>备注：8080：spark集群web ui端口，4040：sparkjob监控端口，18080：jobhistory端口</p><h2 id="3-spark-1-4-版本的最大变化-B"><a href="#3-spark-1-4-版本的最大变化-B" class="headerlink" title="3. spark 1.4 版本的最大变化 (B )"></a><strong>3. spark 1.4 版本的最大变化 (B )</strong></h2><p>A spark sql Release 版本  B .引入 Spark R </p><p>C DataFrame D.支持动态资源分配</p><h2 id="4-Spark-Job-默认的调度模式-A"><a href="#4-Spark-Job-默认的调度模式-A" class="headerlink" title="4. Spark Job 默认的调度模式 (A )"></a>4. Spark Job 默认的调度模式 (A )</h2><p>A FIFO   B FAIR   </p><p>C 无   D 运行时指定</p><blockquote><p>备注：Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。</p></blockquote><h2 id="5-哪个不是本地模式运行的条件-D"><a href="#5-哪个不是本地模式运行的条件-D" class="headerlink" title="5.哪个不是本地模式运行的条件 ( D)"></a>5.哪个不是本地模式运行的条件 ( D)</h2><p>A spark.localExecution.enabled=true  </p><p>B 显式指定本地运行</p><p>C finalStage 无父 Stage</p><p>D partition默认值</p><blockquote><p>备注：【问题】Spark在windows能跑集群模式吗？</p><p>我认为是可以的，但是需要详细了解cmd命令行的写法。目前win下跑spark的单机模式是没有问题的。</p></blockquote><blockquote><p>【关键点】spark启动机制容易被windows的命令行cmd坑</p><p>　　1、带空格、奇怪字符的安装路径，cmd不能识别。最典型的坑就是安装在Program Files文件夹下的程序，因为Program和Files之间有个空格，所以cmd竟不能识别。之前就把JDK安装在了Program Files下面，然后启动spark的时候，总是提示我找不到JDK。我明明配置了环境变量了啊？这就是所谓了《已经配置环境变量，spark 仍然找不到Java》的错误问题。至于奇怪的字符，如感叹号!，我经常喜欢用来将重要的文件夹排在最前面，但cmd命令提示符不能识别。</p><p>　　2、是否需要配置hadoop的路径的问题——答案是需要用HDFS或者yarn就配，不需要用则不需配置。目前大多数的应用场景里面，Spark大规模集群基本安装在Linux服务器上，而自己用windows跑spark的情景，则大多基于学习或者实验性质，如果我们所要读取的数据文件从本地windows系统的硬盘读取（比如说d:\data\ml.txt），基本上不需要配置hadoop路径。我们都知道，在编spark程序的时候，可以指定spark的启动模式，而启动模式有这么三中（以python代码举例）：</p><p>　　　（2.1）本地情况，conf = SparkConf().setMaster(“local[*]”) ——&gt;也就是拿本机的spark来跑程序</p><p>　　　（2.2）远程情况，conf = SparkConf().setMaster(“spark://remotehost:7077”) ——&gt;远程spark主机</p><p>　　　（2.3）yarn情况，conf = SparkConf().setMaster(“yarn-client”) ——&gt;远程或本地 yarn集群代理spark</p><p>针对这3种情况，配置hadoop安装路径都有什么作用呢？（2.1）本地的情况，直接拿本机安装的spark来运行spark程序（比如d:\spark-1.6.2），则配不配制hadoop路径取决于是否需要使用hdfs。java程序的情况就更为简单，只需要导入相应的hadoop的jar包即可，是否配置hadoop路径并不重要。（2.2）的情况大体跟（2.1）的情况相同，虽然使用的远程spark，但如果使用本地数据，则运算的元数据也是从本地上传到远程spark集群的，无需配置hdfs。而（2.3）的情况就大不相同，经过我搜遍baidu、google、bing引擎，均没找到SparkConf直接配置远程yarn地址的方法，唯一的一个帖子介绍可以使用yarn://remote:8032的形式，则会报错“无法解析 地址”。查看Spark的官方说明，Spark其实是通过hadoop路径下的etc\hadoop文件夹中的配置文件来寻找yarn集群的。因此，需要使用yarn来运行spark的情况，在spark那配置好hadoop的目录就尤为重要。后期经过虚拟机的验证，表明，只要windows本地配置的host地址等信息与linux服务器端相同（注意应更改hadoop-2/etc/hadoop 下各种文件夹的配置路径，使其与windows本地一致），是可以直接在win下用yarn-client提交spark任务到远程集群的。</p><p>3、是否需要配置环境变量的问题，若初次配置，可以考虑在IDE里面配置，或者在程序本身用setProperty函数进行配置。因为配置windows下的hadoop、spark环境是个非常头疼的问题，有可能路径不对而导致无法找到相应要调用的程序。待实验多次成功率提高以后，再直接配置windows的全局环境变量不迟。</p><p>　　4、使用Netbeans这个IDE的时候，有遇到Netbeans不能清理构建的问题。原因，极有可能是导入了重复的库，spark里面含有hadoop包，记得检查冲突。同时，在清理构建之前，记得重新编译一遍程序，再进行清理并构建。</p><p>　　５、经常遇到WARN YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources资源不足无法运行的问题，添加conf.set(“spark.executor.memory”, “512m”);语句进行资源限制。先前在虚拟机跑spark，由于本身机子性能不高，给虚拟机设置的内存仅仅2G，导致hadoop和spark双开之后系统资源严重不足。因此可以缩小每个executor的运算规模。其他资源缺乏问题的解决方法参考<a href="http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html</a></p></blockquote><h2 id="6-下面哪个不是-RDD-的特点-C"><a href="#6-下面哪个不是-RDD-的特点-C" class="headerlink" title="6.下面哪个不是 RDD 的特点 (C )"></a>6.下面哪个不是 RDD 的特点 (C )</h2><p>A. 可分区   B 可序列化   C 可修改   D 可持久化</p><h2 id="7-关于广播变量，下面哪个是错误的-D"><a href="#7-关于广播变量，下面哪个是错误的-D" class="headerlink" title="7. 关于广播变量，下面哪个是错误的 (D )"></a>7. 关于广播变量，下面哪个是错误的 (D )</h2><p>A 任何函数调用    B 是只读的  </p><p>C 存储在各个节点    D 存储在磁盘或 HDFS</p><h2 id="8-关于累加器，下面哪个是错误的-D"><a href="#8-关于累加器，下面哪个是错误的-D" class="headerlink" title="8. 关于累加器，下面哪个是错误的 (D )"></a>8. 关于累加器，下面哪个是错误的 (D )</h2><p>A 支持加法 B 支持数值类型 </p><p>C 可并行 D 不支持自定义类型</p><h2 id="9-Spark-支持的分布式部署方式中哪个是错误的-D"><a href="#9-Spark-支持的分布式部署方式中哪个是错误的-D" class="headerlink" title="9.Spark 支持的分布式部署方式中哪个是错误的 (D )"></a>9.Spark 支持的分布式部署方式中哪个是错误的 (D )</h2><p>A standalone B spark on mesos  </p><p>C spark on YARN D Spark on local</p><h2 id="10-Stage-的-Task-的数量由什么决定-A"><a href="#10-Stage-的-Task-的数量由什么决定-A" class="headerlink" title="10.Stage 的 Task 的数量由什么决定 (A )"></a>10.Stage 的 Task 的数量由什么决定 (A )</h2><p>A Partition B Job C Stage D TaskScheduler</p><h2 id="11-下面哪个操作是窄依赖-B"><a href="#11-下面哪个操作是窄依赖-B" class="headerlink" title="11.下面哪个操作是窄依赖 (B )"></a>11.下面哪个操作是窄依赖 (B )</h2><p>A join B filter </p><p>C group D sort</p><h2 id="12-下面哪个操作肯定是宽依赖-C"><a href="#12-下面哪个操作肯定是宽依赖-C" class="headerlink" title="12.下面哪个操作肯定是宽依赖 (C )"></a>12.下面哪个操作肯定是宽依赖 (C )</h2><p>A map B flatMap </p><p>C reduceByKey D sample</p><h2 id="13-spark-的-master-和-worker-通过什么方式进行通信的？-D"><a href="#13-spark-的-master-和-worker-通过什么方式进行通信的？-D" class="headerlink" title="13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )"></a>13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )</h2><p>A http B nio C netty D Akka</p><blockquote><p>备注：从spark1.3.1之后，netty完全代替 了akka</p><p>一直以来，基于Akka实现的RPC通信框架是Spark引以为豪的主要特性，也是与Hadoop等分布式计算框架对比过程中一大亮点，但是时代和技术都在演化，从Spark1.3.1版本开始，为了解决大数据块（如shuffle）的传输问题，Spark引入了Netty通信框架，到了1.6.0版本，Netty居然完全取代了Akka，承担Spark内部所有的RPC通信以及数据流传输。</p><p>那么Akka又是什么东西？从Akka出现背景来说，它是基于Actor的RPC通信系统，它的核心概念也是Message，它是基于协程的，性能不容置疑；基于scala的偏函数，易用性也没有话说，但是它毕竟只是RPC通信，无法适用大的package/stream的数据传输，这也是Spark早期引入Netty的原因。</p><p>那么Netty为什么可以取代Akka？首先不容置疑的是Akka可以做到的，Netty也可以做到，但是Netty可以做到，Akka却无法做到，原因是啥？在软件栈中，Akka相比Netty要Higher一点，它专门针对RPC做了很多事情，而Netty相比更加基础一点，可以为不同的应用层通信协议（RPC，FTP，HTTP等）提供支持，在早期的Akka版本，底层的NIO通信就是用的Netty；其次一个优雅的工程师是不会允许一个系统中容纳两套通信框架，恶心！最后，虽然Netty没有Akka协程级的性能优势，但是Netty内部高效的Reactor线程模型，无锁化的串行设计，高效的序列化，零拷贝，内存池等特性也保证了Netty不会存在性能问题。</p><p>那么Spark是怎么用Netty来取代Akka呢？一句话，利用偏函数的特性，基于Netty“仿造”出一个简约版本的Actor模型！！</p></blockquote><h2 id="14-默认的存储级别-A"><a href="#14-默认的存储级别-A" class="headerlink" title="14. 默认的存储级别 (A )"></a>14. 默认的存储级别 (A )</h2><p>A MEMORY_ONLY B MEMORY_ONLY_SER</p><p>C MEMORY_AND_DISK D MEMORY_AND_DISK_SER</p><pre class="line-numbers language-scala"><code class="language-scala">备注：<span class="token comment" spellcheck="true">//不会保存任务数据 </span><span class="token keyword">val</span> NONE <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//直接将RDD的partition保存在该节点的Disk上 </span><span class="token keyword">val</span> DISK_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//直接将RDD的partition保存在该节点的Disk上,在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> DISK_ONLY_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition对应的原生的Java Object保存在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //那么这些partition将不会缓存,并且需要的时候被重新计算,默认缓存的级别 </span><span class="token keyword">val</span> MEMORY_ONLY <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition对应的原生的Java Object保存在JVM中,在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> MEMORY_ONLY_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_ONLY_SER <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_ONLY_SER_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition反序列化后的对象存储在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //超出的partition将被保存在Disk上,并且在需要时读取 </span><span class="token keyword">val</span> MEMORY_AND_DISK <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//在其他节点上保存一个相同的备份 </span><span class="token keyword">val</span> MEMORY_AND_DISK_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_AND_DISK_SER <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span> <span class="token keyword">val</span> MEMORY_AND_DISK_SER_2 <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//将RDD的partition序列化后存储在Tachyon中 </span><span class="token keyword">val</span> OFF_HEAP <span class="token operator">=</span> <span class="token keyword">new</span> StorageLevel<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="15-spark-deploy-recoveryMode-不支持那种-D"><a href="#15-spark-deploy-recoveryMode-不支持那种-D" class="headerlink" title="15 spark.deploy.recoveryMode 不支持那种 (D )"></a>15 spark.deploy.recoveryMode 不支持那种 (D )</h2><p>A.ZooKeeper B. FileSystem </p><p>D NONE D Hadoop</p><h2 id="16-下列哪个不是-RDD-的缓存方法-C"><a href="#16-下列哪个不是-RDD-的缓存方法-C" class="headerlink" title="16.下列哪个不是 RDD 的缓存方法 (C )"></a>16.下列哪个不是 RDD 的缓存方法 (C )</h2><p>A persist() B Cache() </p><p>C Memory()</p><h2 id="17-Task-运行在下来哪里个选项中-Executor-上的工作单元-C"><a href="#17-Task-运行在下来哪里个选项中-Executor-上的工作单元-C" class="headerlink" title="17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )"></a>17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )</h2><p>A Driver program B. spark master </p><p>C.worker node D Cluster manager</p><h2 id="18-hive-的元数据存储在-derby-和-MySQL-中有什么区别-B"><a href="#18-hive-的元数据存储在-derby-和-MySQL-中有什么区别-B" class="headerlink" title="18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )"></a>18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )</h2><p>A.没区别 B.多会话</p><p>C.支持网络环境 D数据库的区别</p><pre class="line-numbers language-sql"><code class="language-sql">备注：  Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。内置的derby主要问题是并发性能很差，可以理解为单线程操作。Derby还有一个特性。更换目录执行操作，会找不到相关表等<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="19-DataFrame-和-RDD-最大的区别-B"><a href="#19-DataFrame-和-RDD-最大的区别-B" class="headerlink" title="19.DataFrame 和 RDD 最大的区别 (B )"></a>19.DataFrame 和 RDD 最大的区别 (B )</h2><p>A.科学统计支持 B.多了 schema </p><p>C.存储方式不一样 D.外部数据源支持</p><blockquote><p>备注：</p><p><strong>上图直观体现了RDD与DataFrame的区别</strong>：左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><p><strong>提升执行效率</strong>： RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。</p><p><strong>减少数据读取</strong>：分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。</p></blockquote><blockquote><p>上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。</p><p>  对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过（例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200）。</p><p>  此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。</p><p> 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><p>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。</p><p>对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。</p></blockquote><ul><li><p><strong>RDD和Dataset</strong></p><p>​    DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。</p><p>​    DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为Spark</p><p>SQl类型，然而RDD依赖于运行时反射机制。</p></li><li><p><strong>DataFrame和Dataset</strong></p><p>​    Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。因此具有如下三个特点：</p><p>​    DataSet可以在编译时检查类型</p><p>并且是面向对象的编程接口。</p></li></ul><h2 id="20-Master-的-ElectedLeader-事件后做了哪些操作-D"><a href="#20-Master-的-ElectedLeader-事件后做了哪些操作-D" class="headerlink" title="20.Master 的 ElectedLeader 事件后做了哪些操作 (D )"></a>20.Master 的 ElectedLeader 事件后做了哪些操作 (D )</h2><p>A. 通知 driver B.通知 worker </p><p>C.注册 application D.直接 ALIVE</p><h2 id="34-cache后面能不能接其他算子-它是不是action操作？"><a href="#34-cache后面能不能接其他算子-它是不是action操作？" class="headerlink" title="34.cache后面能不能接其他算子,它是不是action操作？"></a>34.cache后面能不能接其他算子,它是不是action操作？</h2><p>答：cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。</p><p>cache不是action操作</p><h2 id="35-reduceByKey是不是action？"><a href="#35-reduceByKey是不是action？" class="headerlink" title="35.reduceByKey是不是action？"></a>35.reduceByKey是不是action？</h2><p>答：不是，很多人都会以为是action，reduce rdd是action</p><h2 id="36-数据本地性是在哪个环节确定的？"><a href="#36-数据本地性是在哪个环节确定的？" class="headerlink" title="36.数据本地性是在哪个环节确定的？"></a>36.数据本地性是在哪个环节确定的？</h2><p>具体的task运行在那他机器上，dag划分stage的时候确定的</p><h2 id="37-RDD的弹性表现在哪几点？"><a href="#37-RDD的弹性表现在哪几点？" class="headerlink" title="37.RDD的弹性表现在哪几点？"></a>37.RDD的弹性表现在哪几点？</h2><p>1）自动的进行内存和磁盘的存储切换；</p><p>2）基于Lingage的高效容错；</p><p>3）task如果失败会自动进行特定次数的重试；</p><p>4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；</p><p>5）checkpoint和persist，数据计算之后持久化缓存</p><p>6）数据调度弹性，DAG TASK调度和资源无关</p><p>7）数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par</p><h2 id="38-常规的容错方式有哪几种类型？"><a href="#38-常规的容错方式有哪几种类型？" class="headerlink" title="38.常规的容错方式有哪几种类型？"></a>38.常规的容错方式有哪几种类型？</h2><p>1）.数据检查点,会发生拷贝，浪费资源</p><p>2）.记录数据的更新，每次更新都会记录下来，比较复杂且比较消耗性能</p><h2 id="39-RDD通过Linage（记录数据更新）的方式为何很高效？"><a href="#39-RDD通过Linage（记录数据更新）的方式为何很高效？" class="headerlink" title="39.RDD通过Linage（记录数据更新）的方式为何很高效？"></a>39.RDD通过Linage（记录数据更新）的方式为何很高效？</h2><p>1）lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且rDD</p><p>之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就</p><p>产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条</p><p>将复杂计算链条存储下来，计算的时候从后往前回溯</p><p>900步是上一个stage的结束，要么就checkpoint</p><p>2）记录原数据，是每次修改都记录，代价很大</p><p>如果修改一个集合，代价就很小，官方说rdd是</p><p>粗粒度的操作，是为了效率，为了简化，每次都是</p><p>操作数据集合，写或者修改操作，都是基于集合的</p><p>rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的</p><p>也可以是细粒度，读可以读其中的一条条的记录。</p><p>3）简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景</p><p>如网络爬虫，现实世界中，大多数写是粗粒度的场景</p><h2 id="40-RDD有哪些缺陷？"><a href="#40-RDD有哪些缺陷？" class="headerlink" title="40.RDD有哪些缺陷？"></a>40.RDD有哪些缺陷？</h2><p>1）不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的</p><p>所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是</p><p>说可以一条条的读</p><p>2）不支持增量迭代计算，Flink支持</p><h2 id="41-说一说Spark程序编写的一般步骤？"><a href="#41-说一说Spark程序编写的一般步骤？" class="headerlink" title="41.说一说Spark程序编写的一般步骤？"></a>41.说一说Spark程序编写的一般步骤？</h2><p>答：初始化，资源，数据源，并行化，rdd转化，action算子打印输出结果或者也可以存至相应的数据存储介质，具体的可看下图：</p><p>file:///E:/%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6/qq19B99AF2399E52F466CC3CF7E3B24ED5/069fa7b471f54e038440faf63233acce/640.webp</p><h2 id="42-Spark有哪两种算子？"><a href="#42-Spark有哪两种算子？" class="headerlink" title="42. Spark有哪两种算子？"></a>42. Spark有哪两种算子？</h2><p>答：Transformation（转化）算子和Action（执行）算子。</p><h2 id="43-Spark提交你的jar包时所用的命令是什么？"><a href="#43-Spark提交你的jar包时所用的命令是什么？" class="headerlink" title="43. Spark提交你的jar包时所用的命令是什么？"></a>43. Spark提交你的jar包时所用的命令是什么？</h2><p>答：spark-submit。</p><h2 id="44-Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？"><a href="#44-Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？" class="headerlink" title="44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？"></a>44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？</h2><p>答：在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h2 id="45-你所理解的Spark的shuffle过程？"><a href="#45-你所理解的Spark的shuffle过程？" class="headerlink" title="45. 你所理解的Spark的shuffle过程？"></a>45. 你所理解的Spark的shuffle过程？</h2><p>答：从下面三点去展开</p><p>1）shuffle过程的划分</p><p>2）shuffle的中间结果如何存储</p><p>3）shuffle的数据如何拉取过来</p><p>可以参考这篇博文：<a href="http://www.cnblogs.com/jxhd1/p/6528540.html" target="_blank" rel="noopener">http://www.cnblogs.com/jxhd1/p/6528540.html</a></p><blockquote><p><strong>Shuffle后续优化方向</strong>：通过上面的介绍，我们了解到，Shuffle过程的主要存储介质是磁盘，尽量的减少IO是Shuffle的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，Shuffle过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有“土豪”内存节点，减少Shuffle IO的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW计算引擎运行地更好。</p><p><strong>MapReduce Shuffle后续优化方向</strong>：压缩：对数据进行压缩，减少写读数据量；</p><p>减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；<br>内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；<br>网络框架：netty的性能据说要占优了；<br><strong>本节点上的数据不走网络框架</strong>：对于本节点上的Map输出，Reduce直接去读吧，不需要绕道网络框架。<br>Spark Shuffle后续优化方向：Spark作为MapReduce的进阶架构，对于Shuffle过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是Spark的Shuffle对于我们来说在一些方面还是需要优化的。</p><p>压缩：对数据进行压缩，减少写读数据量；<br><strong>内存化</strong>：Spark历史版本中是有这样设计的：Map写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。</p></blockquote><h2 id="46-你如何从Kafka中获取数据？"><a href="#46-你如何从Kafka中获取数据？" class="headerlink" title="46. 你如何从Kafka中获取数据？"></a>46. 你如何从Kafka中获取数据？</h2><p><strong>1) 基于Receiver的方式</strong></p><p>这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。</p><p><strong>2) 基于Direct的方式</strong></p><p>这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据</p><h2 id="47-对于Spark中的数据倾斜问题你有什么好的方案？"><a href="#47-对于Spark中的数据倾斜问题你有什么好的方案？" class="headerlink" title="47. 对于Spark中的数据倾斜问题你有什么好的方案？"></a>47. 对于Spark中的数据倾斜问题你有什么好的方案？</h2><p>1）前提是定位数据倾斜，是OOM了，还是任务执行缓慢，看日志，看WebUI</p><p>2)解决方法，有多个方面</p><p>· 避免不必要的shuffle，如使用广播小表的方式，将reduce-side-join提升为map-side-join</p><p>·分拆发生数据倾斜的记录，分成几个部分进行，然后合并join后的结果</p><p>·改变并行度，可能并行度太少了，导致个别task数据压力大</p><p>·两阶段聚合，先局部聚合，再全局聚合</p><p>·自定义paritioner，分散key的分布，使其更加均匀</p><p>详细解决方案参考博文《Spark数据倾斜优化方法》</p><h2 id="48-RDD创建有哪几种方式？"><a href="#48-RDD创建有哪几种方式？" class="headerlink" title="48.RDD创建有哪几种方式？"></a>48.RDD创建有哪几种方式？</h2><p>1).使用程序中的集合创建rdd</p><p>2).使用本地文件系统创建rdd</p><p>3).使用hdfs创建rdd，</p><p>4).基于数据库db创建rdd</p><p>5).基于Nosql创建rdd，如hbase</p><p>6).基于s3创建rdd，</p><p>7).基于数据流，如socket创建rdd</p><p>如果只回答了前面三种，是不够的，只能说明你的水平还是入门级的，实践过程中有很多种创建方式。</p><h2 id="49-Spark并行度怎么设置比较合适"><a href="#49-Spark并行度怎么设置比较合适" class="headerlink" title="49.Spark并行度怎么设置比较合适"></a>49.Spark并行度怎么设置比较合适</h2><p>答：spark并行度，每个core承载2<del>4个partition,如，32个core，那么64</del>128之间的并行度，也就是</p><p>设置64~128个partion，并行读和数据规模无关，只和内存使用量和cpu使用</p><p>时间有关</p><h2 id="50-Spark中数据的位置是被谁管理的？"><a href="#50-Spark中数据的位置是被谁管理的？" class="headerlink" title="50.Spark中数据的位置是被谁管理的？"></a>50.Spark中数据的位置是被谁管理的？</h2><p>答：每个数据分片都对应具体物理位置，数据的位置是被blockManager，无论</p><p>数据是在磁盘，内存还是tacyan，都是由blockManager管理</p><p>答：Spark中的数据本地性有三种：</p><p>a.PROCESS_LOCAL是指读取缓存在本地节点的数据</p><p>b.NODE_LOCAL是指读取本地节点硬盘数据</p><p>c.ANY是指读取非本地节点数据</p><p>通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。</p><h2 id="52-rdd有几种操作类型？"><a href="#52-rdd有几种操作类型？" class="headerlink" title="52.rdd有几种操作类型？"></a>52.rdd有几种操作类型？</h2><p>1）transformation，rdd由一种转为另一种rdd</p><p>2）action，</p><p>3）cronroller，crontroller是控制算子,cache,persist，对性能和效率的有很好的支持</p><p>三种类型，不要回答只有2中操作</p><h2 id="53-Spark如何处理不能被序列化的对象？"><a href="#53-Spark如何处理不能被序列化的对象？" class="headerlink" title="53.Spark如何处理不能被序列化的对象？"></a>53.Spark如何处理不能被序列化的对象？</h2><p>将不能序列化的内容封装成object</p><h2 id="54-collect功能是什么，其底层是怎么实现的？"><a href="#54-collect功能是什么，其底层是怎么实现的？" class="headerlink" title="54.collect功能是什么，其底层是怎么实现的？"></a>54.collect功能是什么，其底层是怎么实现的？</h2><p>答：driver通过collect把集群中各个节点的内容收集过来汇总成结果，collect返回结果是Array类型的，collect把各个节点上的数据抓过来，抓过来数据是Array型，collect对Array抓过来的结果进行合并，合并后Array中只有一个元素，是tuple类型（KV类型的）的。</p><h2 id="55-Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"><a href="#55-Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？" class="headerlink" title="55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"></a>55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？</h2><p>答：</p><p>1）因为输入数据有很多task，尤其是有很多小文件的时候，有多少个输入block就会有多少个task启动；</p><p>2）spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。</p><p>3）参数可以通过spark_home/conf/spark-default.conf配置文件设置:</p><p>spark.sql.shuffle.partitions 50 spark.default.parallelism 10</p><p>第一个是针对spark sql的task数量</p><p>第二个是非spark sql程序设置生效</p><h2 id="56-为什么Spark-Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生"><a href="#56-为什么Spark-Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生" class="headerlink" title="56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?"></a>56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?</h2><p>答：会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑</p><p>否则很容易出现长时间分配不到资源，job一直不能运行的情况。</p><h2 id="57-map与flatMap的区别"><a href="#57-map与flatMap的区别" class="headerlink" title="57.map与flatMap的区别"></a>57.map与flatMap的区别</h2><blockquote><p>map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象</p><p>flatMap：对RDD每个元素转换，然后再扁平化</p><p>将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组</p><p>对象，会抛弃值为null的值</p></blockquote><h2 id="58-列举你常用的action？"><a href="#58-列举你常用的action？" class="headerlink" title="58.列举你常用的action？"></a>58.列举你常用的action？</h2><p>collect，reduce,take,count,saveAsTextFile等</p><h2 id="59-Spark为什么要持久化，一般什么场景下要进行persist操作？"><a href="#59-Spark为什么要持久化，一般什么场景下要进行persist操作？" class="headerlink" title="59.Spark为什么要持久化，一般什么场景下要进行persist操作？"></a>59.Spark为什么要持久化，一般什么场景下要进行persist操作？</h2><blockquote><p>为什么要进行持久化？</p><p>spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤</p><p>只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。</p></blockquote><blockquote><p>以下场景会使用persist</p><p>1）某个步骤计算非常耗时，需要进行persist持久化</p><p>2）计算链条非常长，重新恢复要算很多步骤，很好使，persist</p><p>3）checkpoint所在的rdd要持久化persist，</p><p>lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前</p><p>要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。</p><p>4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</p><p>5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</p><p>60.为什么要进行序列化</p><p>序列化可以减少数据的体积，减少存储空间，高效存储和传输数据，不好的是使用的时候要反序列化，非常消耗CPU</p></blockquote><h2 id="61-介绍一下join操作优化经验？"><a href="#61-介绍一下join操作优化经验？" class="headerlink" title="61.介绍一下join操作优化经验？"></a>61.介绍一下join操作优化经验？</h2><blockquote><p>答：join其实常见的就分为两类： map-side join 和  reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。</p><p>备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。</p></blockquote><h2 id="62-介绍一下cogroup-rdd实现原理，你在什么场景下用过这个rdd？"><a href="#62-介绍一下cogroup-rdd实现原理，你在什么场景下用过这个rdd？" class="headerlink" title="62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？"></a>62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？</h2><blockquote><p>答：cogroup的函数实现:这个实现根据两个要进行合并的两个RDD操作,生成一个CoGroupedRDD的实例,这个RDD的返回结果是把相同的key中两个RDD分别进行合并操作,最后返回的RDD的value是一个Pair的实例,这个实例包含两个Iterable的值,第一个值表示的是RDD1中相同KEY的值,第二个值表示的是RDD2中相同key的值.由于做cogroup的操作,需要通过partitioner进行重新分区的操作,因此,执行这个流程时,需要执行一次shuffle的操作(如果要进行合并的两个RDD的都已经是shuffle后的rdd,同时他们对应的partitioner相同时,就不需要执行shuffle,)，</p><p>场景：表关联查询</p></blockquote><p>63下面这段代码输出结果是什么？</p><hr><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">def</span> joinRdd<span class="token punctuation">(</span>sc<span class="token operator">:</span>SparkContext<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token keyword">val</span> name<span class="token operator">=</span> Array<span class="token punctuation">(</span>Tuple2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">"spark"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">"tachyon"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token string">"hadoop"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">val</span> score<span class="token operator">=</span> Array<span class="token punctuation">(</span>Tuple2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">,</span>Tuple2<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">val</span> namerdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">val</span> scorerdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>score<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">val</span> result <span class="token operator">=</span> namerdd<span class="token punctuation">.</span>join<span class="token punctuation">(</span>scorerdd<span class="token punctuation">)</span><span class="token punctuation">;</span>result <span class="token punctuation">.</span>collect<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span>答案<span class="token operator">:</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>Spark<span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">(</span>tachyon<span class="token punctuation">,</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">(</span>hadoop<span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Spark-的四大组件下面哪个不是-D&quot;&gt;&lt;a href=&quot;#1-Spark-的四大组件下面哪个不是-D&quot; class=&quot;headerlink&quot; title=&quot;1. Spark 的四大组件下面哪个不是 (D )&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;Spark 的四
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark面试问题梳理</title>
    <link href="https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/"/>
    <id>https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/</id>
    <published>2020-06-21T06:35:00.000Z</published>
    <updated>2020-06-21T12:25:46.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题一：Spark中的RDD是什么，有哪些特性？"><a href="#问题一：Spark中的RDD是什么，有哪些特性？" class="headerlink" title="问题一：Spark中的RDD是什么，有哪些特性？"></a><strong>问题一：Spark中的RDD是什么，有哪些特性？</strong></h2><h3 id="1-RDD是什么？"><a href="#1-RDD是什么？" class="headerlink" title="1.RDD是什么？"></a><strong>1.RDD是什么？</strong></h3><blockquote><p><strong>RDD</strong>（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合</p></blockquote><ul><li><strong>Dataset</strong>：就是一个集合，用于存放数据的</li><li><strong>Destributed</strong>：分布式，可以并行在集群计算</li><li><strong>Resilient</strong>：表示弹性的，弹性表示<ul><li>RDD中的数据可以存储在内存或者磁盘中；</li><li>RDD中的分区是可以改变的；</li></ul></li></ul><h3 id="2-五大特性："><a href="#2-五大特性：" class="headerlink" title="2. 五大特性："></a><strong>2. 五大特性：</strong></h3><ol><li><strong>A list of partitions</strong>：一个分区列表，RDD中的数据都存储在一个分区列表中</li><li><strong>A function for computing each split</strong>：作用在每一个分区中的函数</li><li><strong>A list of dependencies on other RDDs</strong>：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的</li><li><strong>Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)</strong>：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向</li><li>可选项，数据本地性，数据位置最优</li></ol><h2 id="问题二：-概述一下spark中的常用算子区别（map-mapPartitions，foreach，foreachPatition）"><a href="#问题二：-概述一下spark中的常用算子区别（map-mapPartitions，foreach，foreachPatition）" class="headerlink" title="问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）"></a>问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）</h2><p>常用算子：</p><ul><li><strong>map</strong>：用于遍历RDD，将函数应用于每一个元素，返回新的RDD（transformation算子）</li><li><strong>foreach</strong>：用于遍历RDD，将函数应用于每一个元素，无返回值（action算子）</li><li><strong>mapPatitions</strong>：用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子）</li><li><strong>foreachPatition</strong>：用于遍历操作RDD中的每一个分区，无返回值（action算子）</li></ul><p><strong>总结</strong>：一般使用<strong>mapPatitions</strong>和<strong>foreachPatition</strong>算子比map和foreach更加高效，推荐使用</p><h2 id="问题三：-谈谈spark中的宽窄依赖："><a href="#问题三：-谈谈spark中的宽窄依赖：" class="headerlink" title="问题三：.谈谈spark中的宽窄依赖："></a><strong>问题三：.谈谈spark中的宽窄依赖</strong>：</h2><p>答：RDD和它的父RDD的关系有两种类型：<strong>窄依赖</strong>和<strong>宽依赖</strong></p><ul><li><strong>宽依赖</strong>：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生</li><li><strong>窄依赖</strong>：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生</li></ul><p>区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示：</p><p><img src="https://img-blog.csdn.net/20180909161853157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题四：spark中如何划分stage："><a href="#问题四：spark中如何划分stage：" class="headerlink" title="问题四：spark中如何划分stage："></a>问题四：spark中如何划分stage：</h2><ul><li><strong>概念</strong>：</li></ul><blockquote><p>​       Spark任务会根据<strong>RDD</strong>之间的依赖关系，形成一个<strong>DAG有向无环图</strong>，<strong>DAG</strong>会提交给<strong>DAGScheduler</strong>，<strong>DAGScheduler</strong>会把<strong>DAG</strong>划分相互依赖的多个<strong>stage</strong>，划分依据就是<strong>宽窄依赖</strong>，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以<strong>taskSet</strong>的形式提交给<strong>TaskScheduler</strong>运行，stage是由一组并行的task组成。</p><p>​       Spark程序中可以因为不同的action触发众多的job，一个程序中可以有很多的job，每一个job是由一个或者多个stage构成的，后面的stage依赖于前面的stage，也就是说只有前面依赖的stage计算完毕后，后面的stage才会运行；</p><p>​        stage 的划分标准就是宽依赖：何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；</p><p>​       切割规则：从后往前，遇到宽依赖就切割stage；</p></blockquote><ul><li><strong>图解</strong>：</li></ul><p><img src="https://img-blog.csdn.net/20180909161915933?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><ul><li><strong>计算格式</strong>：pipeline管道计算模式，piepeline只是一种计算思想，一种模式。</li></ul><blockquote><p>​        spark的pipeline管道计算模式相当于执行了一个高阶函数，也就是说来一条数据然后计算一条数据，会把所有的逻辑走完，然后落地，而MapReduce是1+1=2，2+1=3这样的计算模式，也就是计算完落地，然后再计算，然后再落地到磁盘或者内存，最后数据是落在计算节点上，按reduce的hash分区落地。管道计算模式完全基于内存计算，所以比MapReduce快的原因。</p><p>管道中的RDD何时落地：shuffle write的时候，对RDD进行持久化的时候。</p><p>​    stage的task的并行度是由stage的最后一个RDD的分区数来决定的，一般来说，一个partition对应一个task，但最后reduce的时候可以手动改变reduce的个数，也就是改变最后一个RDD的分区数，也就改变了并行度。例如：reduceByKey(<em>+</em>,3)</p></blockquote><ul><li><strong>优化</strong>：提高stage的并行度：reduceByKey(<em>+</em>,patition的个数) ，join(<em>+</em>,patition的个数)</li></ul><h2 id="问题五：DAGScheduler分析："><a href="#问题五：DAGScheduler分析：" class="headerlink" title="问题五：DAGScheduler分析："></a>问题五：DAGScheduler分析：</h2><p>答：</p><ul><li><p><strong>概述</strong>：<strong>DAGScheduler</strong>是一个面向stage 的调度器；</p></li><li><p><strong>主要入参</strong>：</p><ul><li>dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,resultHandler, localProperties.get)</li><li>rdd： final RDD；</li><li>cleanedFunc： 计算每个分区的函数；</li><li>resultHander： 结果侦听器；</li></ul></li><li><p><strong>主要功能</strong>：</p><ol><li><p>接受用户提交的job；</p></li><li><p>将job根据类型划分为不同的stage，记录那些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</p></li><li><p>决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给<strong>TaskScheduler</strong>；</p></li><li><p>重新提交<strong>shuffle</strong>输出丢失的stage给taskScheduler；</p></li></ol></li></ul><p>注：一个stage内部的错误不是由shuffle输出丢失造成的，DAGScheduler是不管的，由TaskScheduler负责尝试重新提交task执行。</p><h2 id="问题六：Job的生成："><a href="#问题六：Job的生成：" class="headerlink" title="问题六：Job的生成："></a><strong>问题六：Job的生成：</strong></h2><p>​        一旦driver程序中出现action，就会生成一个job，比如count等，向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。</p><p>​        每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是<strong>宽依赖</strong>，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p><h2 id="问题七：有向无环图："><a href="#问题七：有向无环图：" class="headerlink" title="问题七：有向无环图："></a><strong>问题七：有向无环图：</strong></h2><p>​    <strong>DAG</strong>，有向无环图，简单的来说，就是一个由顶点和有方向性的边构成的图中，从任意一个顶点出发，没有任意一条路径会将其带回到出发点的顶点位置，为每个spark job计算具有依赖关系的多个stage任务阶段，通常根据shuffle来划分stage，如reduceByKey,groupByKey等涉及到shuffle的transformation就会产生新的stage ，然后将每个stage划分为具体的一组任务，以TaskSets的形式提交给底层的任务调度模块来执行，其中不同stage之前的RDD为宽依赖关系，TaskScheduler任务调度模块负责具体启动任务，监控和汇报任务运行情况。</p><h2 id="问题八：RDD是什么以及它的分类："><a href="#问题八：RDD是什么以及它的分类：" class="headerlink" title="问题八：RDD是什么以及它的分类："></a><strong>问题八：RDD是什么以及它的分类：</strong></h2><p><img src="https://img-blog.csdn.net/20180909162040887?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题九：RDD的操作"><a href="#问题九：RDD的操作" class="headerlink" title="问题九：RDD的操作"></a><strong>问题九：RDD的操作</strong></h2><p><img src="https://img-blog.csdn.net/20180909162041368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/2018090916204272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162101528?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162110376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/2018090916212499?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162136156?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163215275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163232304?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163244483?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909163257291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题十-RDD缓存："><a href="#问题十-RDD缓存：" class="headerlink" title="问题十: RDD缓存："></a><strong>问题十: RDD缓存</strong>：</h2><p>​        Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 <strong>transformation</strong>自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。</p><p>​         Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。</p><p>​          Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li><p>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</p></li><li><p>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p></li><li><p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p></li><li><p>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</p></li><li><p>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</p></li><li><p>在不会使用cached RDD的时候，及时使用unpersist方法来释放它。</p></li></ul><h2 id="问题十一：RDD共享变量："><a href="#问题十一：RDD共享变量：" class="headerlink" title="问题十一：RDD共享变量："></a>问题十一：RDD共享变量：</h2><p>​       在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。</p><p>(1). <strong>广播变量（Broadcast Variables）</strong></p><p>– 广播变量缓存到各个节点的内存中，而不是每个 Task</p><p>– 广播变量被创建后，能在集群中运行的任何函数调用</p><p>– 广播变量是只读的，不能在被广播后修改</p><p>– 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本</p><p>val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量<br>(2). <strong>累加器</strong></p><p>​    累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值</p><h2 id="问题十二：spark-submit的时候如何引入外部jar包："><a href="#问题十二：spark-submit的时候如何引入外部jar包：" class="headerlink" title="问题十二：spark-submit的时候如何引入外部jar包："></a>问题十二：spark-submit的时候如何引入外部jar包：</h2><p>在通过spark-submit提交任务时，可以通过添加配置参数来指定 </p><p>–driver-class-path 外部jar包<br>–jars 外部jar包</p><h2 id="问题十三：spark如何防止内存溢出："><a href="#问题十三：spark如何防止内存溢出：" class="headerlink" title="问题十三：spark如何防止内存溢出："></a>问题十三：spark如何防止内存溢出：</h2><ul><li><p>driver端的内存溢出 </p><ul><li><strong>可以增大driver的内存参数</strong>：<code>spark.driver.memory (default 1g)</code><br>这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。</li><li><strong>map过程产生大量对象导致内存溢出</strong><br>这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。<br>面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区， 不会有shuffle的过程。</li></ul></li><li><p>数据不平衡导致内存溢出 </p><pre><code> 数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。</code></pre></li><li><p>shuffle后内存溢出 </p><pre><code>  shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。</code></pre></li><li><p>standalone模式下资源分配不均匀导致内存溢出</p><pre><code>  在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</code></pre><p>​      这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()<br>rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。</p></li></ul><h2 id="问题十四：spark中cache和persist的区别："><a href="#问题十四：spark中cache和persist的区别：" class="headerlink" title="问题十四：spark中cache和persist的区别："></a>问题十四：spark中cache和persist的区别：</h2><p><strong>cache</strong>：缓存数据，默认是缓存在内存中，其本质还是调用persist<br><strong>persist</strong>: 缓存数据，有丰富的数据缓存策略。数据可以保存在内存也可以保存在磁盘中，使用的时候指定对应的缓存级别就可以了。</p><h2 id="问题十五：spark中的数据倾斜的现象，原因，后果："><a href="#问题十五：spark中的数据倾斜的现象，原因，后果：" class="headerlink" title="问题十五：spark中的数据倾斜的现象，原因，后果："></a>问题十五：spark中的数据倾斜的现象，原因，后果：</h2><p><strong>(1)、数据倾斜的现象</strong><br>        多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。<br><strong>(2)、数据倾斜的原因</strong> </p><ul><li><p>数据问题<br>1、key本身分布不均衡（包括大量的key为空）<br>2、key的设置不合理</p></li><li><p>spark使用问题<br>1、shuffle时的并发度不够<br>2、计算方式有误</p></li></ul><p><strong>(3)、数据倾斜的后果</strong> </p><ol><li><p>spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）</p></li><li><p>过多的数据在同一个task中运行，将会把executor撑爆。</p></li></ol><h2 id="问题十六：spark数据倾斜的处理："><a href="#问题十六：spark数据倾斜的处理：" class="headerlink" title="问题十六：spark数据倾斜的处理："></a>问题十六：spark数据倾斜的处理：</h2><p>发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首<strong>先要检查数据本身</strong>，是否存在异常数据。</p><ol><li><p><strong>数据问题造成的数据倾斜</strong><br><strong>找出异常的key</strong><br>如果任务长时间卡在最后最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个。<br>比如:</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>“key”<span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span><span class="token operator">+</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><p>   如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况: </p><p>1、null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。<br>2、无效数据，大量重复的测试数据或是对结果影响不大的有效数据。<br>3、有效数据，业务导致的正常数据分布。</p><p><strong>解决办法</strong> </p><p>第1，2种情况，直接对数据进行过滤即可（因为该数据对当前业务不会产生影响）。<br>第3种情况则需要进行一些特殊操作，常见的有以下几种做法<br>(1) <strong>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作</strong>。<br>(2) <strong>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</strong><br>(3) 使用reduceByKey 代替 groupByKey(reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义.)<br>(4) 使用<strong>map join</strong>。</p><p><strong>案例</strong> </p><p>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:<br>(1) 将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>(2) 对数据进行 reduceByKey(func)<br>(3) 将 key + 随机值 转成 key<br>(4) 再对数据进行 reduceByKey(func)</p><p><strong>案例操作流程分析</strong>： </p><p>假设说有倾斜的Key，我们给所有的Key加上一个随机数，然后进行reduceByKey操作；此时同一个Key会有不同的随机数前缀，在进行reduceByKey操作的时候原来的一个非常大的倾斜的Key就分而治之变成若干个更小的Key，不过此时结果和原来不一样，怎么破？进行map操作，目的是把随机数前缀去掉，然后再次进行reduceByKey操作。（当然，如果你很无聊，可以再次做随机数前缀），这样我们就可以把原本倾斜的Key通过分而治之方案分散开来，最后又进行了全局聚合<br>注意1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。<br>注意2: 单独处理异常数据时，可以配合使用Map Join解决。</p><p>2.<strong>spark使用不当造成的数据倾斜</strong></p><ul><li><p><strong>提高shuffle并行度</strong><br>dataFrame和sparkSql可以设置spark.sql.shuffle.partitions参数控制shuffle的并发度，默认为200。<br>rdd操作可以设置spark.default.parallelism控制并发度，默认参数由不同的Cluster Manager控制。</p><ul><li><strong>局限性:</strong> 只是让每个task执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些key的大      小非常大，即使一个task单独执行它，也会受到数据倾斜的困扰。</li></ul></li><li><p><strong>使用map join 代替reduce join</strong></p><pre><code>  在小表不是特别大(取决于你的executor大小)的情况下使用，可以使程序避免shuffle的过程，自然也就没有数据倾斜的困扰了.（详细见http://blog.csdn.net/lsshlsw/article/details/50834858、http://blog.csdn.net/lsshlsw/article/details/48694893）</code></pre><ul><li><p><strong>局限性</strong>: 因为是先将小数据发送到每个executor上，所以数据量不能太大。</p><p>​    </p><h2 id="问题十七：spark中map-side-join关联优化："><a href="#问题十七：spark中map-side-join关联优化：" class="headerlink" title="问题十七：spark中map-side-join关联优化："></a>问题十七：spark中map-side-join关联优化：</h2></li></ul></li></ul><p>​      将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。</p><p>如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。</p><p>何时使用：<strong>在海量数据中匹配少量特定数据</strong></p><p>原理：reduce-side-join 的缺陷在于会将key相同的数据发送到同一个partition中进行运算，大数据集的传输需要长时间的IO，同时任务并发度收到限制，还可能造成数据倾斜。</p><p>reduce-side-join 运行图如下</p><p><img src="https://img-blog.csdn.net/20180909162441742?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>map-side-join 运行图如下：</p><p><img src="https://img-blog.csdn.net/20180909162409221?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>将少量的数据转化为Map进行广播，广播会将此 Map 发送到每个节点中，如果不进行广播，每个task执行时都会去获取该Map数据，造成了性能浪费。对大数据进行遍历，使用mapPartition而不是map，因为mapPartition是在每个partition中进行操作，因此可以减少遍历时新建broadCastMap.value对象的空间消耗，同时匹配不到的数据也不会返回。</p><h2 id="问题十八：kafka整合sparkStreaming问题："><a href="#问题十八：kafka整合sparkStreaming问题：" class="headerlink" title="问题十八：kafka整合sparkStreaming问题："></a>问题十八：kafka整合sparkStreaming问题：</h2><p><strong>(1)、如何实现sparkStreaming读取kafka中的数据</strong><br>可以这样说：在kafka0.10版本之前有二种方式与sparkStreaming整合，一种是基于<strong>receiver</strong>，一种是<strong>direct</strong>,然后分别阐述这2种方式分别是什么 </p><ul><li><p><strong>receiver</strong>：是采用了kafka高级api,利用receiver接收器来接受kafka topic中的数据，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据，kafka中topic的偏移量是保存在zk中的。<br>基本使用：还有几个需要注意的点： </p><p>​       在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度.<br>​         对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。<br>在默认配置下，这种方式可能会因为底层的失败而丢失数据. 因为receiver一直在接收数据,在其已经通知zookeeper数据接收完成但是还没有处理的时候,executor突然挂掉(或是driver挂掉通知executor关闭),缓存在其中的数据就会丢失. 如果希望做到高可靠, 让数据零丢失,如果我们启用了Write Ahead Logs(spark.streaming.receiver.writeAheadLog.enable=true）该机制会同步地将接收到的Kafka数据写入分布式文件系统(比如HDFS)上的预写日志中. 所以, 即使底层节点出现了失败, 也可以使用预写日志中的数据进行恢复. 复制到文件系统如HDFS，那么storage level需要设置成 StorageLevel.MEMORY_AND_DISK_SER，也就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER)</p></li></ul><ul><li><strong>direct</strong>: 在spark1.3之后，引入了Direct方式。不同于Receiver的方式，Direct方式没有receiver这一层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，之后根据设定的maxRatePerPartition来处理每个batch。（设置spark.streaming.kafka.maxRatePerPartition=10000。限制每秒钟从topic的每个partition最多消费的消息条数）</li></ul><p><strong>(2) 对比这2中方式的优缺点：</strong></p><ul><li><p>采用receiver方式：这种方式可以保证数据不丢失，但是无法保证数据只被处理一次，WAL实现的是At-least-once语义（至少被处理一次），如果在写入到外部存储的数据还没有将offset更新到zookeeper就挂掉,这些数据将会被反复消费. 同时,降低了程序的吞吐量。</p></li><li><p>采用direct方式:    相比Receiver模式而言能够确保机制更加健壮. 区别于使用Receiver来被动接收数据, Direct模式会周期性地主动查询Kafka, 来获得每个topic+partition的最新的offset, 从而定义每个batch的offset的范围. 当处理数据的job启动时, 就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 </p><pre><code>**优点**： **1、简化并行读取** 如果要读取多个partition, 不需要创建多个输入DStream然后对它们进行union操作. Spark会创建跟Kafka partition一样多的RDD partition, 并且会并行从Kafka中读取数据. 所以在Kafka partition和RDD partition之间, 有一个一对一的映射关系.**2、高性能** 如果要保证零数据丢失, 在基于receiver的方式中, 需要开启WAL机制. 这种方式其实效率低下, 因为数据实际上被复制了两份, Kafka自己本身就有高可靠的机制, 会对数据复制一份, 而这里又会复制一份到WAL中. 而基于direct的方式, 不依赖Receiver, 不需要开启WAL机制, 只要Kafka中作了数据的复制, 那么就可以通过Kafka的副本进行恢复.**3、一次且仅一次的事务机制** 基于receiver的方式, 是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的. 这是消费Kafka数据的传统方式. 这种方式配合着WAL机制可以保证数据零丢失的高可靠性, 但是却无法保证数据被处理一次且仅一次, 可能会处理两次. 因为Spark和ZooKeeper之间可能是不同步的. 基于direct的方式, 使用kafka的简单api, Spark Streaming自己就负责追踪消费的offset, 并保存在checkpoint中. Spark自己一定是同步的, 因此可以保证数据是消费一次且仅消费一次。不过需要自己完成将offset写入zk的过程,在官方文档中都有相应介绍. </code></pre><pre class="line-numbers language-scala"><code class="language-scala"><span class="token operator">-</span><span class="token operator">*</span>简单代码实例： messages<span class="token punctuation">.</span>foreachRDD<span class="token punctuation">(</span>rdd<span class="token keyword">=></span><span class="token punctuation">{</span> <span class="token keyword">val</span> message <span class="token operator">=</span> rdd<span class="token punctuation">.</span>map<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//对数据进行一些操作 </span>message<span class="token punctuation">.</span>map<span class="token punctuation">(</span>method<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//更新zk上的offset (自己实现) </span>updateZKOffsets<span class="token punctuation">(</span>rdd<span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>sparkStreaming程序自己消费完成后，自己主动去更新zk上面的偏移量。也可以将zk中的偏移量保存在mysql或者redis数据库中，下次重启的时候，直接读取mysql或者redis中的偏移量，获取到上次消费的偏移量，接着读取数据。</p></li></ul><h2 id="问题十九：利用scala语言进行排序"><a href="#问题十九：利用scala语言进行排序" class="headerlink" title="问题十九：利用scala语言进行排序"></a>问题十九：利用scala语言进行排序</h2><p><strong>1.冒泡</strong>：</p><p><img src="https://img-blog.csdn.net/20180909162545316?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><strong>2.快读排序</strong>：</p><p><img src="https://img-blog.csdn.net/20180909162606931?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180909162624563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x3ajg3OTUyNTkzMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="问题二十：spark-master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？"><a href="#问题二十：spark-master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？" class="headerlink" title="问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？"></a>问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？</h2><p>​        Spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括worker,master,application,executors.standby节点要从zk中获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的，另外，master切换需要注意两点：</p><p>​    1. 在master切换的过程中，所有的已经在运行的程序皆正常运行，因为spark application在运行前就已经通过cluster manager获得了计算资源，所以在运行时job本身的调度和处理master是没有任何关系的；</p><p>​    2. 在master的切换过程中唯一的影响是不能提交新的job，一方面不能提交新的应用程序给集群，因为只有Active master才能接受新的程序的提交请求，另外一方面，已经运行的程序也不能action操作触发新的job提交请求。</p><p><strong>问题二十一：spark master HA主从切换过程不会影响集群已有的作业运行，为什么？</strong></p><p>答：因为程序在运行之前，已经向集群申请过资源，这些资源已经提交给driver了，也就是说已经分配好资源了，这是粗粒度分配，一次性分配好资源后不需要再关心资源分配，在运行时让driver和executor自动交互，弊端是如果资源分配太多，任务运行完不会很快释放，造成资源浪费，这里不适用细粒度分配的原因是因为任务提交太慢。</p><h2 id="问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？"><a href="#问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？" class="headerlink" title="问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？"></a>问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？</h2><p><strong>1.粗粒度</strong>：启动时就分配好资源，程序启动，后续具体使用就使用分配好的资源，不需要再分配资源。好处：作业特别多时，资源复用率较高，使用粗粒度。缺点：容易资源浪费，如果一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度。如果有999个资源闲置在那里，会造成资源大量浪费。</p><p><strong>2.细粒度</strong>：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。</p><h2 id="问题二十三：driver的功能是什么："><a href="#问题二十三：driver的功能是什么：" class="headerlink" title="问题二十三：driver的功能是什么："></a><strong>问题二十三：driver的功能是什么</strong>：</h2><p>1.一个spark作业运行时包括一个driver进程，也就是作业的主进程，具有main函数，并且有sparkContext的实例，是程序的入口；</p><p><strong>2.功能</strong>：负责向集群申请资源，向master注册信息，负责了作业的调度，负责了作业的解析，生成stage并调度task到executor上，包括DAGScheduler，TaskScheduler。</p><h2 id="问题二十四：spark的有几种部署模式，每种模式特点？"><a href="#问题二十四：spark的有几种部署模式，每种模式特点？" class="headerlink" title="问题二十四：spark的有几种部署模式，每种模式特点？"></a>问题二十四：spark的有几种部署模式，每种模式特点？</h2><p><strong>1）本地模式</strong></p><p>Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类</p><p>·  local：只启动一个executor</p><p>·  local[k]:启动k个executor</p><p>·  local：启动跟cpu数目相同的 executor</p><p><strong>2)standalone模式</strong></p><p>分布式部署集群， 自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础，</p><p><strong>3)Spark on yarn模式</strong></p><p>分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端</p><p><strong>4）Spark On Mesos模式。</strong>官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序：</p><p><strong>1)   粗粒度模式</strong>（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p><p><strong>2)   细粒度模式（Fine-grained Mode）</strong>：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。</p><h2 id="问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a><strong>问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</strong></h2><p><strong>1）Spark core</strong>：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。</p><p><strong>2）SparkStreaming</strong>是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。</p><p><strong>3）Spark sql</strong>：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析</p><p><strong>4）BlinkDB</strong> ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。</p><p><strong>5）MLBase</strong>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib，MLI、ML Optimizer和MLRuntime。</p><p><strong>6）GraphX</strong>是Spark中用于图和图并行计算</p><h2 id="问题二十六：spark中worker-的主要工作是什么？"><a href="#问题二十六：spark中worker-的主要工作是什么？" class="headerlink" title="问题二十六：spark中worker 的主要工作是什么？"></a><strong>问题二十六：spark中worker 的主要工作是什么？</strong></h2><p><strong>主要功能</strong>：管理当前节点内存，CPU的使用情况，接受master发送过来的资源指令，通过executorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务，需要注意的是：</p><p><strong>1.worker</strong>会不会汇报当前信息给master？worker心跳给master主要只有workid，不会以心跳的方式发送资源信息给master，这样master就知道worker是否存活，只有故障的时候才会发送资源信息；</p><p><strong>2.worker</strong>不会运行代码，具体运行的是executor，可以运行具体application斜的业务逻辑代码，操作代码的节点，不会去运行代码。</p><h2 id="问题二十七：简单说一下hadoop和spark的shuffle相同和差异？"><a href="#问题二十七：简单说一下hadoop和spark的shuffle相同和差异？" class="headerlink" title="问题二十七：简单说一下hadoop和spark的shuffle相同和差异？"></a><strong>问题二十七：简单说一下hadoop和spark的shuffle相同和差异？</strong></h2><p><strong>1）从 high-level 的角度来看，两者并没有大的差别</strong>。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</p><p><strong>2）从 low-level 的角度来看，两者差别不小。</strong> Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。</p><p><strong>3）从实现角度来看，两者也有不少差别。</strong> Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。</p><p>如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？ </p><p>Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</p><h2 id="问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别"><a href="#问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别" class="headerlink" title="问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别"></a>问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别</h2><p><strong>两者都是用mr模型来进行并行计算:</strong></p><p>1) hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。 </p><p>2) spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 </p><p>3) hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 </p><p>spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。</p><h2 id="问题二十九：RDD机制？"><a href="#问题二十九：RDD机制？" class="headerlink" title="问题二十九：RDD机制？"></a>问题二十九：RDD机制？</h2><p>rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 </p><p>所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。 </p><p>rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。</p><h2 id="问题三十：spark有哪些组件？"><a href="#问题三十：spark有哪些组件？" class="headerlink" title="问题三十：spark有哪些组件？"></a><strong>问题三十：spark有哪些组件？</strong></h2><p>答：主要有如下组件：</p><p>1）master：管理集群和节点，不参与计算。 </p><p>2）worker：计算节点，进程本身不参与计算，和master汇报。 </p><p>3）Driver：运行程序的main方法，创建spark context对象。 </p><p>4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 </p><p>5）client：用户提交程序的入口。</p><h2 id="问题三十一：spark工作机制？"><a href="#问题三十一：spark工作机制？" class="headerlink" title="问题三十一：spark工作机制？"></a><strong>问题三十一：spark工作机制？</strong></h2><p>答：用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 </p><p>执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。 task scheduler会将stage划分为task set分发到各个节点的executor中执行。</p><h2 id="问题三十二：spark的优化怎么做？"><a href="#问题三十二：spark的优化怎么做？" class="headerlink" title="问题三十二：spark的优化怎么做？"></a><strong>问题三十二：spark的优化怎么做？</strong></h2><p>答： spark调优比较复杂，但是大体可以分为三个方面来进行，</p><p><strong>1）平台层面的调优：</strong>防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet，</p><p><strong>2）应用程序层面的调优</strong>：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等，</p><p><strong>3）JVM层面的调优</strong>：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等</p><p>​       序列化在分布式系统中扮演着重要的角色，优化Spark程序时，首当其冲的就是对序列化方式的优化。Spark为使用者提供两种序列化方式：</p><ul><li>Java serialization: 默认的序列化方式。</li></ul><ul><li>Kryo serialization: 相较于 Java serialization 的方式，速度更快，空间占用更小，但并不支持所有的序列化格式，同时使用的时候需要注册class。spark-sql中默认使用的是kyro的序列化方式。<br>可以在spark-default.conf设置全局参数，也可以代码中初始化时对SparkConf设置 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) ，该参数会同时作用于机器之间数据的shuffle操作以及序列化rdd到磁盘，内存。<br>Spark不将Kyro设置成默认的序列化方式是因为它需要对类进行注册，官方强烈建议在一些网络数据传输很大的应用中使用kyro序列化。</li></ul><p>如果你要序列化的对象比较大，可以增加参数spark.kryoserializer.buffer所设置的值。</p><p>如果你没有注册需要序列化的class，Kyro依然可以照常工作，但会存储每个对象的全类名(full class name)，这样的使用方式往往比默认的 Java serialization 还要浪费更多的空间。</p><p>可以设置 spark.kryo.registrationRequired 参数为 true，使用kyro时如果在应用中有类没有进行注册则会报错：</p><p>如上这个错误需要添加</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题一：Spark中的RDD是什么，有哪些特性？&quot;&gt;&lt;a href=&quot;#问题一：Spark中的RDD是什么，有哪些特性？&quot; class=&quot;headerlink&quot; title=&quot;问题一：Spark中的RDD是什么，有哪些特性？&quot;&gt;&lt;/a&gt;&lt;strong&gt;问题一：Sp
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://dataquaner.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>HiveSQL优化 hive参数版总结</title>
    <link href="https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/"/>
    <id>https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/</id>
    <published>2020-06-11T06:50:00.000Z</published>
    <updated>2020-06-11T10:22:08.086Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。</strong></p></blockquote><h2 id="1-Hive优化目标"><a href="#1-Hive优化目标" class="headerlink" title="1. Hive优化目标"></a><strong>1. Hive优化目标</strong></h2><ul><li><p>在有限的资源下，执行效率更高</p></li><li><p>常见问题</p><ul><li><p>数据倾斜</p></li><li><p>map数设置</p></li><li><p>reduce数设置</p></li><li><p>其他</p></li></ul></li></ul><h2 id="2-Hive执行优化"><a href="#2-Hive执行优化" class="headerlink" title="2. Hive执行优化"></a><strong>2. Hive执行优化</strong></h2><ul><li><p><code>HQL --&gt; Job --&gt; Map/Reduce</code></p></li><li><p>执行计划<code>explain [extended] hql</code></p><p>样例</p><p><code>select col,count(1) from test2 group by col;</code></p><p><code>explain select col,count(1) from test2 group by col;</code></p></li></ul><h2 id="3-Hive表优化"><a href="#3-Hive表优化" class="headerlink" title="3. Hive表优化"></a><strong>3. Hive表优化</strong></h2><ul><li><p>分区</p><p><code>set hive.exec.dynamic.partition=true;</code></p><p><code>set hive.exec.dynamic.partition.mode=nonstrict;</code></p><p>​    静态分区</p><p>​    动态分区</p></li><li><p>分桶</p><p>  <code>set hive.enforce.bucketing=true;</code></p><p>  <code>set hive.enforce.sorting=true;</code></p></li><li><p>数据</p><p>相同数据尽量聚集在一起</p></li></ul><h2 id="4-Hive-Job优化"><a href="#4-Hive-Job优化" class="headerlink" title="4. Hive Job优化"></a><strong>4. Hive Job优化</strong></h2><h3 id="并行化执行"><a href="#并行化执行" class="headerlink" title="并行化执行"></a>并行化执行</h3><p>每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间</p><ul><li><code>set hive.exec.parallel= true;</code></li><li><code>set hive.exec.parallel.thread.numbe=8;</code></li></ul><h3 id="本地化执行"><a href="#本地化执行" class="headerlink" title="本地化执行"></a>本地化执行</h3><ul><li>​    job的输入数据大小必须小于参数:<code>hive.exec.mode.local.auto.inputbytes.max(默认128MB)</code></li><li>​    job的map数必须小于参数:<code>hive.exec.mode.local.auto.tasks.max(默认4)</code></li><li>​    job的reduce数必须为0或者1</li><li>​    <code>set hive.exec.mode.local.auto=true;</code></li></ul><p>​       当一个job满足如上条件才能真正使用本地模式:</p><h3 id="job合并输入小文件"><a href="#job合并输入小文件" class="headerlink" title="job合并输入小文件"></a>job合并输入小文件</h3><ul><li><code>set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</code></li><li>合并文件数由mapred.max.split.size限制的大小决定</li></ul><h3 id="job合并输出小文件"><a href="#job合并输出小文件" class="headerlink" title="job合并输出小文件"></a>job合并输出小文件</h3><ul><li><p><code>set hive.merge.smallfiles.avgsize=256000000;</code>当输出文件平均小于该值，启动新job合并文件</p></li><li><p><code>set hive.merge.size.per.task=64000000;</code>合并之后的文件大小</p></li></ul><h3 id="JVM重利用"><a href="#JVM重利用" class="headerlink" title="JVM重利用"></a>JVM重利用</h3><ul><li><p><code>set mapred.job.reuse.jvm.num.tasks=20;</code></p><p>JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。</p></li></ul><h3 id="压缩数据"><a href="#压缩数据" class="headerlink" title="压缩数据"></a>压缩数据</h3><p><code>set hive.exec.compress.output=true;</code></p><p><code>set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec;</code></p><p><code>set mapred.output.compression.type=BLOCK;</code></p><p><code>set hive.exec.compress.intermediate=true;</code></p><p><code>set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</code></p><p><code>set hive.intermediate.compression.type=BLOCK;</code></p><p>中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式</p><p>hive查询最终的输出也可以压缩</p><h2 id="5-Hive-Map优化"><a href="#5-Hive-Map优化" class="headerlink" title="5. Hive Map优化"></a><strong>5. Hive Map优化</strong></h2><p><code>set mapred.map.tasks =10;</code> 无效</p><p> <strong>(1) 默认map个数</strong></p><p><code>default_num = total_size / block_size;</code></p><p>如果想增加map个数，则设置<code>mapred.map.tasks</code>为一个较大的值</p><p>如果想减小map个数，则设置<code>mapred.min.split.size</code>为一个较大的值</p><p>情况1：输入文件size巨大，但不是小文件</p><p>情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于<code>blockSize</code>。这种情况通过增大<code>mapred.min.split.size</code>不可行，需要使用<code>combineFileInputFormat</code>将多个input path合并成一个<code>InputSplit</code>送给mapper处理，从而减少<code>mapper</code>的数量。</p><h2 id="6-Hive-Shuffle优化"><a href="#6-Hive-Shuffle优化" class="headerlink" title="6. Hive Shuffle优化"></a><strong>6. Hive Shuffle优化</strong></h2><ul><li>Map端</li></ul><p>​    <code>io.sort.mb</code></p><p>​    <code>io.sort.spill.percent</code></p><p>​    <code>min.num.spill.for.combine</code></p><p>​    <code>io.sort.factor</code></p><p>​    <code>io.sort.record.percent</code></p><ul><li><p>Reduce端</p><p><code>mapred.reduce.parallel.copies</code></p><p><code>mapred.reduce.copy.backoff</code></p><p><code>io.sort.factor</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p><p><code>mapred.job.shuffle.input.buffer.percent</code></p></li></ul><h2 id="7-Hive-Reduce优化"><a href="#7-Hive-Reduce优化" class="headerlink" title="7. Hive Reduce优化"></a><strong>7. Hive Reduce优化</strong></h2><ul><li><p>需要reduce操作的查询</p><p>group by,</p><p>join,</p><p>distribute by,</p><p>cluster by…</p><p>order by 比较特殊,只需要一个reduce</p></li><li><p>sum,count,distinct…</p></li><li><p>聚合函数</p><p>高级查询</p></li><li><p>推测执行</p><p>mapred.reduce.tasks.speculative.execution</p><p>hive.mapred.reduce.tasks.speculative.execution</p></li><li><p>Reduce优化</p><p>numRTasks = min[maxReducers,input.size/perReducer]</p><p>maxReducers=hive.exec.reducers.max</p><p>perReducer = hive.exec.reducers.bytes.per.reducer</p><p>hive.exec.reducers.max 默认 ：999</p><p>hive.exec.reducers.bytes.per.reducer 默认:1G</p></li><li><p>set mapred.reduce.tasks=10;直接设置</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。&lt;/strong&gt;&lt;/p&gt;
&lt;/blo
      
    
    </summary>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>6.Hadoop面试系列之UDF</title>
    <link href="https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/"/>
    <id>https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/</id>
    <published>2020-06-10T14:57:19.767Z</published>
    <updated>2020-06-10T14:57:19.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-开发步骤"><a href="#1-开发步骤" class="headerlink" title="1. 开发步骤"></a>1. <strong>开发步骤</strong></h2><p>​       UDF简称自定义函数，它是Hive函数库的扩展，自定义函数UDF在MapReduce执行阶段发挥作用。开发步骤如下：</p><blockquote><p>1）  给hive.ql.exec.UDF包开发一个自定义函数类，从UDF继承。自定义函数类实现evaluate方法。</p><p>2）  在FunctionRegistry类中注册开发的自定义函数类。</p><p>3）  打包发布至Hive客户端。</p></blockquote><h3 id="1-1-开发工具"><a href="#1-1-开发工具" class="headerlink" title="1.1 开发工具"></a><strong>1.1 开发工具</strong></h3><p>​      Eclipse是一款开源的、基于Java的可扩展开发平台。Hadoop开发人员可通过在Eclipse上面开发UDF。</p><h3 id="1-2-UDF函数案例"><a href="#1-2-UDF函数案例" class="headerlink" title="1.2 UDF函数案例"></a>1.2 UDF函数案例</h3><h4 id="1）开发UDF函数类"><a href="#1）开发UDF函数类" class="headerlink" title="1）开发UDF函数类"></a>1）开发UDF函数类</h4><p>文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHelloWorld.java</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">.</span>UDF<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">UDFHelloWorld</span> <span class="token keyword">extends</span> <span class="token class-name">UDF</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> String <span class="token function">evaluate</span><span class="token punctuation">(</span>String str<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>str <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> null<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> <span class="token string">"HelloWorld "</span> <span class="token operator">+</span> str<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        helloUDF uf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">helloUDF</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//Text t = new Text("gfsg");</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>uf<span class="token punctuation">.</span><span class="token function">evaluate</span><span class="token punctuation">(</span><span class="token string">"nihao"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2）UDF类注册，注册方法"><a href="#2）UDF类注册，注册方法" class="headerlink" title="2）UDF类注册，注册方法"></a>2）UDF类注册，注册方法</h4><p>文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>UDFHelloWorld<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*** FunctionRegistry.*/</span><span class="token keyword">public</span> <span class="token keyword">final</span> <span class="token keyword">class</span> <span class="token class-name">FunctionRegistry</span> <span class="token punctuation">{</span>    <span class="token keyword">static</span> <span class="token punctuation">{</span>        <span class="token function">registerGenericUDF</span><span class="token punctuation">(</span><span class="token string">"concat"</span><span class="token punctuation">,</span> GenericUDFConcat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"substr"</span><span class="token punctuation">,</span> UDFSubstr<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"substring"</span><span class="token punctuation">,</span> UDFSubstr<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"space"</span><span class="token punctuation">,</span> UDFSpace<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"repeat"</span><span class="token punctuation">,</span> UDFRepeat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"ascii"</span><span class="token punctuation">,</span> UDFAscii<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"lpad"</span><span class="token punctuation">,</span> UDFLpad<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"rpad"</span><span class="token punctuation">,</span> UDFRpad<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerUDF</span><span class="token punctuation">(</span><span class="token string">"Hello"</span><span class="token punctuation">,</span> UDFHelloWorld<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">registerGenericUDF</span><span class="token punctuation">(</span><span class="token string">"size"</span><span class="token punctuation">,</span> GenericUDFSize<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3）Jar包发布路径"><a href="#3）Jar包发布路径" class="headerlink" title="3）Jar包发布路径"></a>3）Jar包发布路径</h4><p>发布路径：/opt/boh/hive/lib/hive-exec-0.12.0-cdh5.0.0.jar</p><p>上传至hadoop集群执行脚本的hive客户端。</p><h3 id="1-3Hive-UDF函数"><a href="#1-3Hive-UDF函数" class="headerlink" title="1.3Hive UDF函数"></a>1.3Hive UDF函数</h3><h4 id="1-3-1UDF函数列表"><a href="#1-3-1UDF函数列表" class="headerlink" title="1.3.1UDF函数列表"></a>1.3.1UDF函数列表</h4><ul><li>函数清单及其功能</li></ul><p><code>TO_DATE(string date,'format')</code></p><ul><li>格式化所需要的日期</li></ul><p><code>ADD_MONTHS(Timestamp date,int n)</code></p><ul><li>增加月数</li></ul><p><code>date_tostring(Timestamp date,'format')</code></p><ul><li>转换Date类型为指定格式字符串</li></ul><p><code>MONTHS_BETWEEN（Timestamp date1，Timestamp date2）</code></p><ul><li>返回两个日期之间的月数</li></ul><p><code>f_age(string identityId)</code></p><ul><li>验证身份证合法性并返回性别年龄</li></ul><p><code>f_checkidcard(string identityId)</code></p><ul><li>验证身份证合法性</li></ul><h4 id="1-3-2-UDF函数说明"><a href="#1-3-2-UDF函数说明" class="headerlink" title="1.3.2 UDF函数说明"></a>1.3.2 UDF函数说明</h4><ul><li>TO_DATE函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">Select</span>  to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">09</span> <span class="token number">11</span>:<span class="token number">11</span>:<span class="token number">11</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>ADD_MONTHS函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> add_months<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">10</span><span class="token operator">-</span><span class="token number">09</span> <span class="token number">11</span>:<span class="token number">11</span>:<span class="token number">11</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>date_tostring函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> date_tostring<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'YYYY-MM-DD'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">2014</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">09</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>MONTHS_BETWEEN函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> MONTHS_BETWEEN<span class="token punctuation">(</span>to_date<span class="token punctuation">(</span><span class="token string">'20140909111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>to_date<span class="token punctuation">(</span><span class="token string">'20140706111111'</span><span class="token punctuation">,</span><span class="token string">'YYYYMMDDHH24miss'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span>返回结果：<span class="token number">2.096774193548387</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>f_age函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> f_age<span class="token punctuation">(</span><span class="token string">'511024198710148199'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">127</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>f_checkidcard函数</li></ul><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> f_checkidcard<span class="token punctuation">(</span><span class="token string">'511024198710148199'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> test<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql">返回结果：<span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-开发步骤&quot;&gt;&lt;a href=&quot;#1-开发步骤&quot; class=&quot;headerlink&quot; title=&quot;1. 开发步骤&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;开发步骤&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;​       UDF简称自定义函数，它是Hive函数库的扩展，自定义
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hive开窗函数梳理</title>
    <link href="https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/"/>
    <id>https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/</id>
    <published>2020-06-10T06:35:00.000Z</published>
    <updated>2020-06-10T11:15:10.879Z</updated>
    
    <content type="html"><![CDATA[<p>本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。</p><p>假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。</p><p><img src="https://pic3.zhimg.com/80/v2-008682fd90478af4fb84e88bccd480ee_1440w.jpg" alt="img"></p><p><strong>1.SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">/*前三次贷款的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token number">3</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token keyword">CURRENT</span> <span class="token keyword">ROW</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv1<span class="token punctuation">,</span><span class="token comment" spellcheck="true">/*历史所有贷款 累加到下一次贷款 的金额之和*/</span><span class="token function">SUM</span><span class="token punctuation">(</span>amount<span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate <span class="token keyword">ROWS</span> <span class="token operator">BETWEEN</span> <span class="token keyword">UNBOUNDED</span> <span class="token keyword">PRECEDING</span> <span class="token operator">AND</span> <span class="token number">1</span> <span class="token keyword">FOLLOWING</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> pv2<span class="token keyword">FROM</span> loan <span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。</p><p>窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用：</p><ul><li>n PRECEDING : 前n行    n preceding</li><li>n FOLLOWING：后n行</li><li>CURRENT ROW ： 当前行</li></ul><p>如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为:</p><p>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.</p><p>窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。</p><p><strong>2.NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#将数据按name切分成10区，并返回属于第几个分区</span>NTILE<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>ROW_NUMBER<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f3</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将数据按照name分区，并按照orderdate排序，返回排序序号</span>DENSE_RANK<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span> <span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> <span class="token number">f4</span><span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中第一个函数<a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">NTILE(10)</a>是将数据按name切分成10区，并返回属于第几个分区。</p><blockquote><p>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p><p>   然后可以根据桶号，选取前或后 n分之几的数据。</p></blockquote><p>后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。</p><ul><li>ROW_NUMBER()返回的是一列连续的序号。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-0f2c6da71227f7840aea5257acf8d88b_1440w.png" alt="img"></p><ul><li>RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}.</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f0bd3985ddacd2f347151dacc24cce_1440w.png" alt="img"></p><ul><li>DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}.</li></ul><p><img src="https://pic2.zhimg.com/80/v2-ac60db4d8a9c658ddf17fb43f09f616d_1440w.png" alt="img"></p><p><strong>3.LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> <span class="token operator">*</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取上一笔贷款的日期,缺失默认填NULL</span>LAG<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> last_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取下一笔贷款的日期,缺失指定填'1970-1-1'</span>LEAD<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'1970-1-1'</span><span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> next_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取最早一笔贷款的日期</span>FIRST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> first_dt<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#取新一笔贷款的日期</span>LAST_VALUE<span class="token punctuation">(</span>orderdate<span class="token punctuation">)</span> <span class="token keyword">OVER</span><span class="token punctuation">(</span><span class="token keyword">PARTITION</span> <span class="token keyword">BY</span> name <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> orderdate<span class="token punctuation">)</span> <span class="token keyword">AS</span> latest_dt<span class="token keyword">FROM</span> loan<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/pelifymeng2/article/details/70313943" target="_blank" rel="noopener">LAG(n)</a>将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录</p><p><strong>4.GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制</strong></p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">SELECT</span> A<span class="token punctuation">,</span>B<span class="token punctuation">,</span>C<span class="token keyword">FROM</span> loan<span class="token comment" spellcheck="true">#分别按照月份和日进行分区</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span>orderdateGROUPING SETS<span class="token punctuation">(</span>substring<span class="token punctuation">(</span>orderdate<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> orderdate<span class="token punctuation">)</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> GROUPING__ID<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。</p><p>比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5.</p><p>如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null.</p><pre class="line-numbers language-text"><code class="language-text">SELECT A,B,CFROM loan#分别按照月份和日进行分区GROUP BY substring(orderdate,1,7),orderdatewith CUBEORDER BY GROUPING__ID; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。</p><pre class="line-numbers language-text"><code class="language-text">SELECT A,B,CFROM loan#分别按照月份和日进行分区GROUP BY substring(orderdate,1,7),orderdatewith ROLLUPORDER BY GROUPING__ID; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。&lt;/p&gt;
&lt;p&gt;假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-008
      
    
    </summary>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
      <category term="开窗函数" scheme="https://dataquaner.github.io/tags/%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop核心知识之MapReduce原理</title>
    <link href="https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/"/>
    <id>https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/</id>
    <published>2020-06-08T13:14:00.000Z</published>
    <updated>2020-06-08T14:31:06.843Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>MapReduce是一个基于集群的计算<strong>平台</strong>，是一个简化分布式编程的计算<strong>框架</strong>，是一个将分布式计算抽象为Map和Reduce两个阶段的编程<strong>模型</strong>。<em>（这句话记住了是可以用来装逼的）</em></p></blockquote><h2 id="1-MapReduce工作流程"><a href="#1-MapReduce工作流程" class="headerlink" title="1.MapReduce工作流程"></a>1.MapReduce工作流程</h2><p>0)   用户提交任务 （含数据）</p><p>1)    集群首先对输入数据源进行<strong>切片</strong></p><p>2)    master 调度 worker 执行 map 任务</p><p>3)    worker 读取输入源片段</p><p>4)    worker 执行 map 任务，将任务输出保存在本地<br>5)    master 调度 worker 执行 reduce 任务，reduce worker 读取 map 任务的输出文件</p><p>6） 执行 reduce 任务，将任务输出保存到 HDFS</p><p><img src="https://img-blog.csdnimg.cn/20181215100217729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjIzMTM3Mw==,size_16,color_FFFFFF,t_70" alt="MapReduce工作流程"></p><p>由上至下依次执行</p><table><thead><tr><th>过程</th><th>过程描述</th></tr></thead><tbody><tr><td></td><td>用户提交任务job 给集群</td></tr><tr><td>切片</td><td>集群查找源数据 对源数据做基本处理</td></tr><tr><td>分词(每行执行一次map函数)</td><td>集群(yarn的appliction)分配map任务节点worker</td></tr><tr><td>映射</td><td>其中间数据(存在本地)</td></tr><tr><td>分区(partition)</td><td>中间数据</td></tr><tr><td>排序 (或二次排序)</td><td>中间数据</td></tr><tr><td>聚合(combine有无key聚合后key无序)</td><td>中间数据  分组(group) 发生在排序后混洗前(个人理解)</td></tr><tr><td>混洗(shuffle后key有序)</td><td>混洗横跨mapper和reducer，其发生在mapper的输出和reducer的输入阶段</td></tr><tr><td>规约(reduce)</td><td>集群(yarn的appliction)分配reduce任务节点worker</td></tr></tbody></table><p><img src="https://upload-images.jianshu.io/upload_images/12652505-f3e65e7fc499b579.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1172/format/webp" alt="MapReduce工作原理"></p><p>下面针对具体过程详细介绍：</p><h2 id="2-切片split"><a href="#2-切片split" class="headerlink" title="2.切片split"></a>2.切片split</h2><p>​       HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</p><p><strong>Map任务的数量</strong>：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。<strong>大多数情况下，理想的分片大小是一个HDFS块</strong></p><p><strong>Reduce任务的数量：</strong> <strong>最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</strong> 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</p><h2 id="3-Map-阶段"><a href="#3-Map-阶段" class="headerlink" title="3.Map()阶段"></a>3.Map()阶段</h2><blockquote><ol><li><p>读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数</p></li><li><p>重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出</p></li><li><p>对输出的key、value进行分区</p></li><li><p>对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中</p></li></ol></blockquote><h2 id="4-Reduce阶段"><a href="#4-Reduce阶段" class="headerlink" title="4. Reduce阶段"></a>4. Reduce阶段</h2><blockquote><p>多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上</p></blockquote><blockquote><p>对多个map的输出进行合并、排序。</p></blockquote><blockquote><p>重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出</p></blockquote><blockquote><p>把reduce的输出保存到文件中</p></blockquote><p>特别说明：</p><blockquote><p>切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理</p><p>分词，映射，分区，排序，聚合 都属map阶段</p><p>混洗  横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段</p><p>规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义</p><p>分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容</p></blockquote><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>执行步骤：</p><p>1）map任务处理——&gt;切片</p><ul><li>读取输入文件内容，解析成key、value对，输入文件的每一行，就是一个key、value对，对应调用一次map函数。</li><li>写自己的逻辑，对输入的key、value（k1,v1）处理，转换成新的key、value(k2,v2)输出。</li></ul><p>2）reduce任务处理——&gt;计算</p><ul><li>在reduce之前，有一个shuffle的过程对多个map任务的输出进行合并、排序、分组等操作。</li><li>写reduce函数自己的逻辑，对输入的key、value（k2,{v2,…}）处理，转换成新的key、value(k3,v3)输出。</li><li>把reduce的输出保存到文件中。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;MapReduce是一个基于集群的计算&lt;strong&gt;平台&lt;/strong&gt;，是一个简化分布式编程的计算&lt;strong&gt;框架&lt;/strong&gt;，是一个将分布式计算抽象为Map和Reduce两个阶段的编程&lt;strong&gt;模型&lt;/strong&gt;。&lt;em
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="https://dataquaner.github.io/tags/MapReduce/"/>
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>hadoop shell命令</title>
    <link href="https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/"/>
    <id>https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/</id>
    <published>2020-06-08T10:40:00.000Z</published>
    <updated>2020-06-08T11:13:27.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><p>FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 </p><p>所有的的FS shell命令使用URI路径作为参数。URI格式是<code>scheme://authority/path</code>。</p><p>对HDFS文件系统，scheme是hdfs，</p><p>对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。</p><p>一个HDFS文件或目录比如<code>/parent/child</code>可以表示成<code>hdfs://namenode:namenodeport/parent/child</code>，或者更简单的<code>/parent/child</code>（假设你配置文件中的默认值是<code>namenode:namenodeport</code>）。</p><p>大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。</p><h2 id="1-hadoop-fs-命令列表"><a href="#1-hadoop-fs-命令列表" class="headerlink" title="1. hadoop fs 命令列表"></a>1. hadoop fs 命令列表</h2><p><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#FS+Shell" target="_blank" rel="noopener">FS Shell</a></p><ul><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#cat" target="_blank" rel="noopener">cat</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chgrp" target="_blank" rel="noopener">chgrp</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chmod" target="_blank" rel="noopener">chmod</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#chown" target="_blank" rel="noopener">chown</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#copyFromLocal" target="_blank" rel="noopener">copyFromLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#copyToLocal" target="_blank" rel="noopener">copyToLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#cp" target="_blank" rel="noopener">cp</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#du" target="_blank" rel="noopener">du</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#dus" target="_blank" rel="noopener">dus</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#expunge" target="_blank" rel="noopener">expunge</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#get" target="_blank" rel="noopener">get</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#getmerge" target="_blank" rel="noopener">getmerge</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#ls" target="_blank" rel="noopener">ls</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#lsr" target="_blank" rel="noopener">lsr</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#mkdir" target="_blank" rel="noopener">mkdir</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#movefromLocal" target="_blank" rel="noopener">movefromLocal</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#mv" target="_blank" rel="noopener">mv</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#put" target="_blank" rel="noopener">put</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#rm" target="_blank" rel="noopener">rm</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#rmr" target="_blank" rel="noopener">rmr</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#setrep" target="_blank" rel="noopener">setrep</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#stat" target="_blank" rel="noopener">stat</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#tail" target="_blank" rel="noopener">tail</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test" target="_blank" rel="noopener">test</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#text" target="_blank" rel="noopener">text</a></li><li><a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#touchz" target="_blank" rel="noopener">touchz</a></li></ul><p>特别说明： </p><blockquote><ul><li><code>hadoop fs</code>：通用的文件系统命令，针对任何系统，比如本地文件、HDFS文件、HFTP文件、S3文件系统等。</li><li><code>hadoop dfs</code>：特定针对HDFS的文件系统的相关操作，但是已经不推荐使用。</li><li><code>hdfs dfs</code>：与hadoop dfs类似，同样是针对HDFS文件系统的操作，替代hadoop dfs。</li></ul></blockquote><h2 id="2-各命令使用说明"><a href="#2-各命令使用说明" class="headerlink" title="2. 各命令使用说明"></a>2. 各命令使用说明</h2><ul><li><p><code>cat</code><br>使用方法：<code>hadoop fs -cat URI [URI …]</code></p><p>​    将路径指定文件的内容输出到<code>stdout</code>。</p><p>示例：    </p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2hadoop fs -cat file:///file3 /user/hadoop/file4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>返回值：</p><pre><code>  成功返回0，失败返回-1。</code></pre></li><li><p><code>chgrp</code><br>使用方法：<code>hadoop fs -chgrp [-R] GROUP URI [URI …]</code> </p><p>​        Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. </p><p>​        改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>chmod</code><br>使用方法：<code>hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …]</code></p><p>​      改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>chown</code><br>使用方法：<code>hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</code></p><p>​       改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。</p></li><li><p><code>copyFromLocal</code><br>使用方法：<code>hadoop fs -copyFromLocal URI</code></p><p>​       除了限定源路径是一个本地文件外，和<code>put</code>命令相似。</p></li><li><p><code>copyToLocal</code><br>使用方法：<code>hadoop fs -copyToLocal [-ignorecrc] [-crc] URI</code></p><p>​       除了限定目标路径是一个本地文件外，和get命令类似。</p></li><li><p><code>cp</code><br>使用方法：<code>hadoop fs -cp URI [URI …]</code></p><p>​     将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​     成功返回0，失败返回-1。</p><ul><li><p><code>du</code><br>使用方法：<code>hadoop fs -du URI [URI …]</code></p><p>​      显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>示例：    </p><pre class="line-numbers language-SJELL"><code class="language-SJELL">hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回值：</p><pre><code> 成功返回0，失败返回-1。</code></pre></li><li><p><code>dus</code><br>使用方法：<code>hadoop fs -dus</code></p><p>​    显示文件的大小。</p></li><li><p><code>expunge</code><br>使用方法：<code>hadoop fs -expunge</code></p></li><li><p>清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。</p></li><li><p><code>get</code><br>使用方法：<code>hadoop fs -get [-ignorecrc] [-crc]</code></p></li><li><p>​      复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。</p></li><li><p>示例：</p><pre class="line-numbers language-SHELL"><code class="language-SHELL">hadoop fs -get /user/hadoop/file localfilehadoop fs -get hdfs://host:port/user/hadoop/file localfile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​     成功返回0，失败返回-1。</p><ul><li><p>getmerge<br>使用方法：<code>hadoop fs -getmerge [addnl]</code></p></li><li><p>接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。</p></li><li><p>ls<br>使用方法：<code>hadoop fs -ls</code></p><p>​      如果是文件，则按照如下格式返回文件信息：<br>​          文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>​      如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>​         目录名</p><p>​         修改日期 修改时间 权限 用户ID 组ID<br>示例：       </p><pre class="line-numbers language-SHELL"><code class="language-SHELL">hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回值：</p><pre><code>    成功返回0，失败返回-1。</code></pre></li><li><p>mkdir<br>使用方法：hadoop fs -mkdir</p><p>​    接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。</p><p>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​    成功返回0，失败返回-1。</p><ul><li><p><code>movefromLocal</code><br>使用方法：<code>dfs -moveFromLocal</code></p><p> 输出一个”not implemented“信息。</p></li><li><p>mv<br>使用方法：<code>hadoop fs -mv URI [URI …]</code></p><p>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  ​    成功返回0，失败返回-1。</p><ul><li><p><code>put</code><br>使用方法：hadoop fs -put …</p><p>​      从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -put localfile /user/hadoop/hadoopfilehadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdirhadoop fs -put localfile hdfs://host:port/hadoop/hadoopfilehadoop fs -put - hdfs://host:port/hadoop/hadoopfile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>从标准输入中读取输入。<br>返回值：</p><p>​    成功返回0，失败返回-1。</p></li><li><p><code>rm</code><br>使用方法：<code>hadoop fs -rm URI [URI …]</code></p><p>​    删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  成功返回0，失败返回-1。</p><ul><li><p><code>rmr</code><br>使用方法：<code>hadoop fs -rmr URI [URI …]</code></p></li><li><p>delete的递归版本。<br>示例：</p><pre class="line-numbers language-shell"><code class="language-shell">hadoop fs -rmr /user/hadoop/dirhadoop fs -rmr hdfs://host:port/user/hadoop/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>  返回值：</p><p>  成功返回0，失败返回-1。</p><ul><li><p>setrep<br>使用方法：<code>hadoop fs -setrep [-R]</code></p><p>   改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。</p></li><li><p>示例：</p><p><code>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</code><br>返回值：</p><p>   成功返回0，失败返回-1。</p></li><li><p>stat<br>使用方法：hadoop fs -stat URI [URI …]</p><p>   返回指定路径的统计信息。</p></li><li><p>示例：</p></li><li><p><code>hadoop fs -stat path</code><br>返回值：<br>  成功返回0，失败返回-1。</p></li><li><p>tail<br>使用方法：<code>hadoop fs -tail [-f] URI</code></p><p>  将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。</p></li><li><p>示例：</p><p><code>hadoop fs -tail pathname</code><br>返回值：<br>   成功返回0，失败返回-1。</p></li><li><p>test<br>使用方法：hadoop fs -test -[ezd] URI</p><p>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。</p></li><li><p>示例：</p><p><code>hadoop fs -test -e filename</code><br>text<br>使用方法：hadoop fs -text</p><p>   将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</p></li><li><p>touchz<br>使用方法：<code>hadoop fs -touchz URI [URI …]</code></p><p>   创建一个0字节的空文件。</p></li><li><p>示例：</p><p><code>hadoop -touchz pathname</code><br>返回值：<br>成功返回0，失败返回-1。</p></li></ul><p>更多详细信息访问：<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;p&gt;FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 &lt;/p&gt;
&lt;p&gt;所有的的FS
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="https://dataquaner.github.io/tags/Hadoop/"/>
    
      <category term="Shell" scheme="https://dataquaner.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>【Hive日常问题】导入数据成功，查询显示NULL</title>
    <link href="https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/"/>
    <id>https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/</id>
    <published>2020-05-06T13:40:00.000Z</published>
    <updated>2020-05-06T11:17:26.029Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><strong>问题描述</strong></h2><p>hive导入数据成功，但是查询结果为NULL：</p><pre class="line-numbers language-powershell"><code class="language-powershell">load <span class="token keyword">data</span> local inpath <span class="token string">'/user/hive/student.txt'</span> into table hive_test<span class="token punctuation">.</span>students<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-powershell"><code class="language-powershell">Loading <span class="token keyword">data</span> to table hive_test<span class="token punctuation">.</span>studentsOK<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> hive_test<span class="token punctuation">.</span>students<span class="token punctuation">;</span>OK<span class="token boolean">NULL</span>    <span class="token boolean">NULL</span><span class="token boolean">NULL</span>    <span class="token boolean">NULL</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格：</p><p>OK<br>NULL    NULL<br>NULL    NULL<br>查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格：</p><pre class="line-numbers language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> students<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> students<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span> <span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">' '</span> <span class="token keyword">LINES</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">'\n'</span> STORED <span class="token keyword">AS</span> TEXTFILE<span class="token punctuation">;</span>OK<span class="token number">1</span>    sun<span class="token number">2</span>    lin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;hive导入数据成功，但是查询结果为NULL：&lt;/p&gt;
&lt;pre class=&quot;line-
      
    
    </summary>
    
    
      <category term="Data Question" scheme="https://dataquaner.github.io/categories/Data-Question/"/>
    
    
      <category term="Hive" scheme="https://dataquaner.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Python高级特性之切片</title>
    <link href="https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/"/>
    <id>https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/</id>
    <published>2020-04-25T14:14:00.000Z</published>
    <updated>2020-04-25T10:10:47.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Python可切片对象的索引方式"><a href="#1-Python可切片对象的索引方式" class="headerlink" title="1. Python可切片对象的索引方式"></a>1. Python可切片对象的索引方式</h2><p>​       包括：正索引和负索引两部分，如下图所示，以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例：</p><p><img src="https:////upload-images.jianshu.io/upload_images/14029140-3da45bbfe1029df4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/464/format/webp" alt="img"></p><h2 id="2-Python切片操作的一般方式"><a href="#2-Python切片操作的一般方式" class="headerlink" title="2. Python切片操作的一般方式"></a>2. Python切片操作的一般方式</h2><p>​       一个完整的切片表达式包含两个“:”，用于分隔三个参数(start_index、end_index、step)。</p><ul><li>当只有一个“:”时，默认第三个参数step=1；</li><li>当一个“:”也没有时，start_index=end_index，表示切取start_index指定的那个元素。</li></ul><p>​       切片操作基本表达式：object[start_index:end_index:step]</p><ul><li>step：正负数均可，其绝对值大小决定了切取数据时的‘‘步长”，而正负号决定了“切取方向”，正表示“从左往右”取值，负表示“从右往左”取值。当step省略时，默认为1，即从左往右以步长1取值。“切取方向非常重要！”“切取方向非常重要！”“切取方向非常重要！”，重要的事情说三遍！</li><li>start_index：表示起始索引（包含该索引对应值）；该参数省略时，表示从对象“端点”开始取值，至于是从“起点”还是从“终点”开始，则由step参数的正负决定，step为正从“起点”开始，为负从“终点”开始。</li><li>end_index：表示终止索引（不包含该索引对应值）；该参数省略时，表示一直取到数据“端点”，至于是到“起点”还是到“终点”，同样由step参数的正负决定，step为正时直到“终点”，为负时直到“起点”。</li></ul><h2 id="3-Python切片操作详细例子"><a href="#3-Python切片操作详细例子" class="headerlink" title="3. Python切片操作详细例子"></a>3. Python切片操作详细例子</h2><p>​      以下示例均以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="1-切取单个元素"><a href="#1-切取单个元素" class="headerlink" title="1. 切取单个元素"></a>1. 切取单个元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">0</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">6</span>当索引只有一个数时，表示切取某一个元素。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-切取完整对象"><a href="#2-切取完整对象" class="headerlink" title="2. 切取完整对象"></a>2. 切取完整对象</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#从左往右</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#从左往右</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#从右往左</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-start-index和end-index全为正（-）索引的情况"><a href="#3-start-index和end-index全为正（-）索引的情况" class="headerlink" title="3. start_index和end_index全为正（+）索引的情况"></a>3. start_index和end_index全为正（+）索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，start_index<span class="token operator">=</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token number">6</span>同样表示从左往右取值。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>输出为空列表，说明没取到数据。step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，决定了从右往左取值，而start_index<span class="token operator">=</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token number">6</span>决定了从左往右取值，两者矛盾，所以为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>同样输出为空列表。step<span class="token operator">=</span><span class="token number">1</span>，决定了从左往右取值，而start_index<span class="token operator">=</span><span class="token number">6</span>到end_index<span class="token operator">=</span><span class="token number">2</span>决定了从右往左取值，两者矛盾，所以为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值<span class="token number">0</span>开始一直取到end_index<span class="token operator">=</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值<span class="token number">9</span>开始一直取到end_index<span class="token operator">=</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从start_index<span class="token operator">=</span><span class="token number">6</span>开始，一直取到“终点”值<span class="token number">9</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从start_index<span class="token operator">=</span><span class="token number">6</span>开始，一直取到“起点”<span class="token number">0</span>。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-start-index和end-index全为负（-）索引的情况"><a href="#4-start-index和end-index全为负（-）索引的情况" class="headerlink" title="4. start_index和end_index全为负（-）索引的情况"></a>4. start_index和end_index全为负（-）索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，而start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>决定了从右往左取值，两者矛盾，所以为空。索引<span class="token operator">-</span><span class="token number">1</span>在<span class="token operator">-</span><span class="token number">6</span>的右边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>同样是从右往左取值。索引<span class="token operator">-</span><span class="token number">1</span>在<span class="token number">6</span>的右边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，而start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>同样是从左往右取值。索引<span class="token operator">-</span><span class="token number">6</span>在<span class="token operator">-</span><span class="token number">1</span>的左边（如上图）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从“起点”开始一直取到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从“终点”开始一直取到end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>（该点不包括）。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token number">1</span>，从左往右取值，从start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>开始，一直取到“终点”。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>，从右往左取值，从start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>开始，一直取到“起点”。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="5-start-index和end-index正（-）负（-）混合索引的情况"><a href="#5-start-index和end-index正（-）负（-）混合索引的情况" class="headerlink" title="5. start_index和end_index正（+）负（-）混合索引的情况"></a>5. start_index和end_index正（+）负（-）混合索引的情况</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>的左边，因此从左往右取值，而step<span class="token operator">=</span><span class="token number">1</span>同样决定了从左往右取值，因此结果正确<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">6</span>的左边，因此从左往右取值，但step<span class="token operator">=</span><span class="token operator">-</span>则决定了从右往左取值，两者矛盾，因此为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token number">6</span>的右边，因此从右往左取值，但step<span class="token operator">=</span><span class="token number">1</span>则决定了从左往右取值，两者矛盾，因此为空。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>start_index<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>在end_index<span class="token operator">=</span><span class="token number">6</span>的右边，因此从右往左取值，而step<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>同样决定了从右往左取值，因此结果正确。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="6-多层切片操作"><a href="#6-多层切片操作" class="headerlink" title="6. 多层切片操作"></a>6. 多层切片操作</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>相当于：a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span>理论上可无限次多层切片操作，只要上一次返回的是非空可切片对象即可。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="7-切片操作的三个参数可以用表达式"><a href="#7-切片操作的三个参数可以用表达式" class="headerlink" title="7. 切片操作的三个参数可以用表达式"></a>7. 切片操作的三个参数可以用表达式</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>即：a<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="8-其他对象的切片操作"><a href="#8-其他对象的切片操作" class="headerlink" title="8. 其他对象的切片操作"></a>8. 其他对象的切片操作</h3><p>​       前面的切片操作以list对象为例进行说明，但实际上可进行切片操作的数据类型还有很多，包括元组、字符串等等。</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>元组的切片操作<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token string">'ABCDEFG'</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token string">'ACEG'</span>字符串的切片操作<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token function">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token function">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token number">87</span><span class="token number">90</span><span class="token number">93</span><span class="token number">96</span><span class="token number">99</span>就是利用<span class="token function">range</span><span class="token punctuation">(</span><span class="token punctuation">)</span>函数生成<span class="token number">1</span><span class="token operator">-</span><span class="token number">99</span>的整数，然后从start_index<span class="token operator">=</span><span class="token number">2</span>（即<span class="token number">3</span>）开始以step<span class="token operator">=</span><span class="token number">3</span>取值，直到终点，再在新序列中取最后五个数。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-常用切片操作"><a href="#4-常用切片操作" class="headerlink" title="4. 常用切片操作"></a>4. 常用切片操作</h2><h3 id="1-取偶数位置"><a href="#1-取偶数位置" class="headerlink" title="1.取偶数位置"></a>1.取偶数位置</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-取奇数位置"><a href="#2-取奇数位置" class="headerlink" title="2.取奇数位置"></a>2.取奇数位置</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-拷贝整个对象"><a href="#3-拷贝整个对象" class="headerlink" title="3.拷贝整个对象"></a>3.拷贝整个对象</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#41946376</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#41921864</span>或<span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">.</span><span class="token function">copy</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#39783752</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#39759176</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>需要注意的是：<strong>[:]和.copy()都属于“浅拷贝”，只拷贝最外层元素，内层嵌套元素则通过引用方式共享，而非独立分配内存</strong>，如果需要彻底拷贝则需采用“深拷贝”方式，如下例所示：</p><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'a={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">9</span> <span class="token comment" spellcheck="true">#修改b的最外层元素，将1变成9</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>b<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'D'</span> <span class="token comment" spellcheck="true">#修改b的内嵌层元素</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'a={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'b={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'id(a)={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'id(b)={}'</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#原始a</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#b修改内部元素A为D后，a中的A也变成了D，说明共享内部嵌套元素，但外部元素1没变。</span>b<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#修改后的b</span><span class="token function">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">38669128</span><span class="token function">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">38669192</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-修改单个元素"><a href="#4-修改单个元素" class="headerlink" title="4.修改单个元素"></a>4.修改单个元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="5-在某个位置插入元素"><a href="#5-在某个位置插入元素" class="headerlink" title="5.在某个位置插入元素"></a>5.在某个位置插入元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">,</span><span class="token string">'C'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-替换一部分元素"><a href="#6-替换一部分元素" class="headerlink" title="6.替换一部分元素"></a>6.替换一部分元素</h3><pre class="line-numbers language-ruby"><code class="language-ruby"><span class="token operator">></span><span class="token operator">></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ul><li>start_index、end_index、step三者可同为正、同为负，或正负混合。但必须遵循一个原则，即：当start_index表示的实际位置在end_index的左边时，从左往右取值，此时step必须是正数（同样表示从左往右）；当start_index表示的实际位置在end_index的右边时，表示从右往左取值，此时step必须是负数（同样表示从右往左），即两者的取值顺序必须相同。</li><li>当start_index或end_index省略时，取值的起始索引和终止索引由step的正负来决定，这种情况不会有取值方向矛盾（即不会返回空列表[]），但正和负取到的结果顺序是相反的，因为一个向左一个向右。</li><li>step的正负是必须要考虑的，尤其是当step省略时。比如a[-1:]，很容易就误认为是从“终点”开始一直取到“起点”，即a[-1:]= [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]，但实际上a[-1:]=[9]（注意不是9），原因在于step省略时step=1表示从左往右取值，而起始索引start_index=-1本身就是对象的最右边元素了，再往右已经没数据了，因此结果只含有9一个元素。</li><li>需要注意：“取单个元素（不带“:”）”时，返回的是对象的某个元素，其类型由元素本身的类型决定，而与母对象无关，如上面的a[0]=0、a[-4]=6，元素0和6都是“数值型”，而母对象a却是“list”型；“取连续切片（带“:”）”时，返回结果的类型与母对象相同，哪怕切取的连续切片只包含一个元素，如上面的a[-1:]=[9]，返回的是一个只包含元素“9”的list，而非数值型“9”。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Python可切片对象的索引方式&quot;&gt;&lt;a href=&quot;#1-Python可切片对象的索引方式&quot; class=&quot;headerlink&quot; title=&quot;1. Python可切片对象的索引方式&quot;&gt;&lt;/a&gt;1. Python可切片对象的索引方式&lt;/h2&gt;&lt;p&gt;​   
      
    
    </summary>
    
    
      <category term="Python" scheme="https://dataquaner.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://dataquaner.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode数组系列之#88：合并两个有序数组</title>
    <link href="https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/"/>
    <id>https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/</id>
    <published>2020-04-25T08:16:16.000Z</published>
    <updated>2020-04-25T11:02:08.816Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目：合并两个有序数组"><a href="#题目：合并两个有序数组" class="headerlink" title="题目：合并两个有序数组"></a>题目：合并两个有序数组</h3><h3 id="难度：Easy"><a href="#难度：Easy" class="headerlink" title="难度：Easy"></a>难度：Easy</h3><h3 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h3><blockquote><h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>​        给你两个有序整数数组 nums1 和 nums2，请你将 nums2 合并到 nums1 中，使 nums1 成为一个有序数组。</p><h4 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h4><ul><li>初始化 nums1 和 nums2 的元素数量分别为 m 和 n 。</li><li>你可以假设 nums1 有足够的空间（空间大小大于或等于 m + n）来保存 nums2 中的元素。</li></ul><h4 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h4><p>​    输入:<br>​        nums1 = [1,2,3,0,0,0], m = 3<br>​        nums2 = [2,5,6],           n = 3</p><p>​     输出: </p><p>​        [1,2,2,3,5,6]</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/merge-sorted-array" target="_blank" rel="noopener">https://leetcode-cn.com/problems/merge-sorted-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="方法一：合并后排序"><a href="#方法一：合并后排序" class="headerlink" title="方法一：合并后排序"></a>方法一：合并后排序</h4><h5 id="直觉"><a href="#直觉" class="headerlink" title="直觉"></a>直觉</h5><p>​      最朴素的解法就是将两个数组合并之后再排序。</p><p>​      该算法只需要一行(Java是2行)，时间复杂度较差，为O((n + m)log(n + m))。这是由于这种方法没有利用两个数组本身已经有序这一点。</p><h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><h6 id="Python版"><a href="#Python版" class="headerlink" title="Python版"></a>Python版</h6><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> m<span class="token punctuation">:</span> int<span class="token punctuation">,</span> nums2<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Do not return anything, modify nums1 in-place instead.        """</span>         nums1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> sorted<span class="token punctuation">(</span>nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span> <span class="token operator">+</span> nums2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="Java版"><a href="#Java版" class="headerlink" title="Java版"></a>Java版</h5><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums1<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums2<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>nums2<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">;</span>    Arrays<span class="token punctuation">.</span><span class="token function">sort</span><span class="token punctuation">(</span>nums1<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h6 id="Scala版"><a href="#Scala版" class="headerlink" title="Scala版"></a>Scala版</h6><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">object</span> Solution <span class="token punctuation">{</span>    <span class="token keyword">def</span> merge<span class="token punctuation">(</span>nums1<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> m<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">,</span> nums2<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> n<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h5><ul><li>时间复杂度 : <em>O</em>((<em>n</em>+<em>m</em>)log(<em>n</em>+<em>m</em>))</li><li>空间复杂度 : O(1)</li></ul><h4 id="方法二：双指针-从前往后"><a href="#方法二：双指针-从前往后" class="headerlink" title="方法二：双指针 / 从前往后"></a>方法二：双指针 / 从前往后</h4><h5 id="直觉-1"><a href="#直觉-1" class="headerlink" title="直觉"></a>直觉</h5><p>​       一般而言，对于有序数组可以通过双指针法达到O(n+m)的时间复杂度。</p><p>最直接的算法实现是将指针p1 置为 nums1的开头， p2为 nums2的开头，在每一步将最小值放入输出数组中。</p><p>​      由于 nums1 是用于输出的数组，需要将nums1中的前m个元素放在其他地方，也就需要O(m) 的空间复杂度。</p><p><img src="https://pic.leetcode-cn.com/992f95361c37ad06deadb6f14a9970d0184fd47330365400dd1d6f7be239e0ff-image.png" alt="image.png"></p><h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><h5 id="Python版-1"><a href="#Python版-1" class="headerlink" title="Python版"></a>Python版</h5><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> nums2<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums1: List[int]        :type m: int        :type nums2: List[int]        :type n: int        :rtype: void Do not return anything, modify nums1 in-place instead.        """</span>        <span class="token comment" spellcheck="true"># Make a copy of nums1.</span>        nums1_copy <span class="token operator">=</span> nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span>         nums1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Two get pointers for nums1_copy and nums2.</span>        p1 <span class="token operator">=</span> <span class="token number">0</span>         p2 <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment" spellcheck="true"># Compare elements from nums1_copy and nums2</span>        <span class="token comment" spellcheck="true"># and add the smallest one into nums1.</span>        <span class="token keyword">while</span> p1 <span class="token operator">&lt;</span> m <span class="token operator">and</span> p2 <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>             <span class="token keyword">if</span> nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">:</span>                 nums1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">]</span><span class="token punctuation">)</span>                p1 <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">)</span>                p2 <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># if there are still elements to add</span>        <span class="token keyword">if</span> p1 <span class="token operator">&lt;</span> m<span class="token punctuation">:</span>             nums1<span class="token punctuation">[</span>p1 <span class="token operator">+</span> p2<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums1_copy<span class="token punctuation">[</span>p1<span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> p2 <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>            nums1<span class="token punctuation">[</span>p1 <span class="token operator">+</span> p2<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a><strong>复杂度分析</strong></h5><ul><li>时间复杂度 : O(n + m)</li><li>空间复杂度 : O(m)</li></ul><h4 id="方法三-双指针-从后往前"><a href="#方法三-双指针-从后往前" class="headerlink" title="方法三 : 双指针 / 从后往前"></a>方法三 : 双指针 / 从后往前</h4><h5 id="直觉-2"><a href="#直觉-2" class="headerlink" title="直觉"></a>直觉</h5><p>​       方法二已经取得了最优的时间复杂度O(n + m)，但需要使用额外空间。这是由于在从头改变nums1的值时，需要把nums1中的元素存放在其他位置。</p><p>​       如果我们从结尾开始改写 nums1 的值又会如何呢？这里没有信息，因此不需要额外空间。</p><p>这里的指针 p 用于追踪添加元素的位置。</p><p><img src="https://pic.leetcode-cn.com/57c1daae7dab21c175f0a3acc18e4535aecde350c5100832bd2fdb0e4279180e-image.png" alt="img"></p><p><img src="https://pic.leetcode-cn.com/bac9fc86e104b5fa65f144e0604e0f4ffe4585efac12c1942b618be1c70363ca-image.png" alt="img"></p><h5 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h5><p>Python版</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> m<span class="token punctuation">,</span> nums2<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :type nums1: List[int]        :type m: int        :type nums2: List[int]        :type n: int        :rtype: void Do not return anything, modify nums1 in-place instead.        """</span>        <span class="token comment" spellcheck="true"># two get pointers for nums1 and nums2</span>        p1 <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token number">1</span>        p2 <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># set pointer for nums1</span>        p <span class="token operator">=</span> m <span class="token operator">+</span> n <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># while there are still elements to compare</span>        <span class="token keyword">while</span> p1 <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">and</span> p2 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span>                p2 <span class="token operator">-=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                nums1<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span>  nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span>                p1 <span class="token operator">-=</span> <span class="token number">1</span>            p <span class="token operator">-=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># add missing elements from nums2</span>        nums1<span class="token punctuation">[</span><span class="token punctuation">:</span>p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums2<span class="token punctuation">[</span><span class="token punctuation">:</span>p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Java版</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums1<span class="token punctuation">,</span> <span class="token keyword">int</span> m<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums2<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// two get pointers for nums1 and nums2</span>    <span class="token keyword">int</span> p1 <span class="token operator">=</span> m <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> p2 <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// set pointer for nums1</span>    <span class="token keyword">int</span> p <span class="token operator">=</span> m <span class="token operator">+</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// while there are still elements to compare</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>p1 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>p2 <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">// compare two elements from nums1 and nums2 </span>      <span class="token comment" spellcheck="true">// and add the largest one in nums1 </span>      nums1<span class="token punctuation">[</span>p<span class="token operator">--</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>nums1<span class="token punctuation">[</span>p1<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums2<span class="token punctuation">[</span>p2<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">?</span> nums2<span class="token punctuation">[</span>p2<span class="token operator">--</span><span class="token punctuation">]</span> <span class="token operator">:</span> nums1<span class="token punctuation">[</span>p1<span class="token operator">--</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// add missing elements from nums2</span>    System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>nums2<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nums1<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> p2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h5><ul><li>时间复杂度 : O(n + m)</li><li>空间复杂度 : O(1)</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;题目：合并两个有序数组&quot;&gt;&lt;a href=&quot;#题目：合并两个有序数组&quot; class=&quot;headerlink&quot; title=&quot;题目：合并两个有序数组&quot;&gt;&lt;/a&gt;题目：合并两个有序数组&lt;/h3&gt;&lt;h3 id=&quot;难度：Easy&quot;&gt;&lt;a href=&quot;#难度：Easy&quot; c
      
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://dataquaner.github.io/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://dataquaner.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>数据倾斜问题总结</title>
    <link href="https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/"/>
    <id>https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/</id>
    <published>2020-04-22T14:14:00.000Z</published>
    <updated>2020-04-22T14:35:55.073Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-什么是数据倾斜"><a href="#0-什么是数据倾斜" class="headerlink" title="0. 什么是数据倾斜"></a>0. 什么是数据倾斜</h2><blockquote><p>​        对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。</p><p>来源百度百科</p></blockquote><p>​        对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p><h2 id="1-数据倾斜的现象"><a href="#1-数据倾斜的现象" class="headerlink" title="1. 数据倾斜的现象"></a>1. 数据倾斜的现象</h2><p>​       多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。</p><h2 id="2-数据倾斜的影响"><a href="#2-数据倾斜的影响" class="headerlink" title="2. 数据倾斜的影响"></a>2. 数据倾斜的影响</h2><p>1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：<strong>Out Of Memory</strong>。</p><p>2）<strong>运行速度慢</strong> ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。</p><p><img src="https://pic1.zhimg.com/80/v2-b26e15f4b1c3ce2f78fba64397b6fd60_1440w.jpg" alt="img"></p><p><strong><em>一个经验结论是：一般情况下，OOM的原因都是数据倾斜\</em></strong></p><h2 id="3-如何定位数据倾斜"><a href="#3-如何定位数据倾斜" class="headerlink" title="3. 如何定位数据倾斜"></a>3. 如何定位数据倾斜</h2><p>​         数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</p><p><strong>原因</strong>： 查看任务-》查看Stage-》查看代码</p><p>​        某个task执行特别慢的情况</p><p>​        某个task莫名其妙内存溢出的情况</p><p>​        查看导致数据倾斜的key的数据分布情况</p><p><img src="https://pic1.zhimg.com/80/v2-b1b26a9b5e6a1d68d9aea4d1f2bc551c_1440w.jpg" alt="img"></p><p>也可从以下几种情况考虑：</p><p>1、是不是有OOM情况出现，一般是少数内存溢出的问题</p><p>2、是不是应用运行时间差异很大，总体时间很长</p><p>3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题</p><p>4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断</p><p>5、看看代码里面是否有一些导致Shuffle的算子出现</p><h2 id="4-数据倾斜的几种典型情况（重点）"><a href="#4-数据倾斜的几种典型情况（重点）" class="headerlink" title="4. 数据倾斜的几种典型情况（重点）"></a><strong>4. 数据倾斜的几种典型情况（重点）</strong></h2><ul><li>数据源中的数据分布不均匀，Spark需要频繁交互</li><li>数据集中的不同Key由于分区方式，导致数据倾斜</li><li>JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</li><li>聚合操作中，数据集中的数据分布不均匀（主要）</li><li>JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</li><li>JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</li><li>数据集中少数几个key数据量很大，不重要，其他数据均匀</li></ul><p>注意：</p><ul><li><p>需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题</p></li><li><p>只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以</p></li></ul><h2 id="5-数据倾斜的处理方法"><a href="#5-数据倾斜的处理方法" class="headerlink" title="5. 数据倾斜的处理方法"></a>5. 数据倾斜的处理方法</h2><p>​         发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。</p><h3 id="5-1-检查数据，找出异常的key"><a href="#5-1-检查数据，找出异常的key" class="headerlink" title="5.1 检查数据，找出异常的key"></a>5.1 检查数据，找出异常的key</h3><p>​          如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。</p><p>选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个</p><pre class="line-numbers language-scala"><code class="language-scala">df<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"key"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>k<span class="token keyword">=></span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>_2<span class="token punctuation">,</span>k<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​        如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><p>经过分析，倾斜的数据主要有以下三种情况:</p><ul><li><p>null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。</p></li><li><p>无效数据，大量重复的测试数据或是对结果影响不大的有效数据。</p></li><li><p>有效数据，业务导致的正常数据分布。</p></li></ul><p><strong>解决办法</strong><br>  第1，2种情况，直接对数据进行过滤即可。</p><p>  第3种情况则需要进行一些特殊操作，常见的有以下几种做法。</p><ul><li><p>隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。</p></li><li><p>对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。</p></li><li><p>使用reduceByKey 代替 groupByKey</p></li><li><p>使用map join。</p><p><strong>举例</strong>：<br>如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下：</p><p>将原始的 key 转化为 key + 随机值(例如Random.nextInt)<br>对数据进行 reduceByKey(func)<br>将 key + 随机值 转成 key<br>再对数据进行 reduceByKey(func)<br>tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。</p><p>tips2: 单独处理异常数据时，可以配合使用Map Join解决</p></li></ul><h4 id="5-1-1-数据源中的数据分布不均匀，Spark需要频繁交互"><a href="#5-1-1-数据源中的数据分布不均匀，Spark需要频繁交互" class="headerlink" title="5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互"></a><strong>5.1.1</strong> 数据源中的数据分布不均匀，Spark需要频繁交互</h4><p><strong>解决方案</strong>1：避免数据源的数据倾斜</p><p><strong>实现原理</strong>：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。</p><p><strong>方案优点</strong>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点</strong>：治标不治本，Hive或者Kafka中还是会发生数据倾斜。</p><p><strong>适用情况</strong>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>总结</strong>：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题</p><h4 id="5-1-2-数据集中的不同Key由于分区方式，导致数据倾斜"><a href="#5-1-2-数据集中的不同Key由于分区方式，导致数据倾斜" class="headerlink" title="5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜"></a><strong>5.1.2</strong> 数据集中的不同Key由于分区方式，导致数据倾斜</h4><p><strong>解决方案1</strong>：调整并行度</p><p><strong>实现原理</strong>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。</p><p><strong>方案优点</strong>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点</strong>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>实践经验</strong>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。</p><p><img src="https://pic4.zhimg.com/80/v2-9a1722a9ceb6fe125f7b36715f6dcfff_1440w.jpg" alt="img"></p><p><strong>总结</strong>：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效</p><p><strong>解决方案2</strong>：</p><p><strong>缓解数据倾斜**</strong>（自定义Partitioner）**</p><p><strong>适用场景</strong>：大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p><p><strong>解决方案</strong>： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p><p><strong>优势</strong>： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p><p><strong>劣势</strong>： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><h3 id="5-2-检查Spark运行过程相关操作"><a href="#5-2-检查Spark运行过程相关操作" class="headerlink" title="5.2 检查Spark运行过程相关操作"></a>5.2 检查Spark运行过程相关操作</h3><h4 id="5-2-1-JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）"><a href="#5-2-1-JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）" class="headerlink" title="5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）"></a>5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）</h4><p><strong>解决方案</strong>：Reduce side Join转变为Map side Join</p><p><strong>方案适用场景</strong>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。</p><p><strong>方案实现原理</strong>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><p><strong>方案优点</strong>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点</strong>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。</p><h4 id="5-2-2-聚合操作中，数据集中的数据分布不均匀（主要）"><a href="#5-2-2-聚合操作中，数据集中的数据分布不均匀（主要）" class="headerlink" title="5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）"></a>5.2.2  聚合操作中，数据集中的数据分布不均匀（主要）</h4><p><strong>解决方案</strong>：两阶段聚合（局部聚合+全局聚合）</p><p><strong>适用场景</strong>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案</p><p><strong>实现原理</strong>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>优点</strong>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>缺点</strong>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案</p><p>将相同key的数据分拆处理</p><p><img src="https://pic3.zhimg.com/80/v2-495a5fed7eb38db37d2f0bd13c45a30e_1440w.jpg" alt="img"></p><h4 id="5-2-3-JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀"><a href="#5-2-3-JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀" class="headerlink" title="5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀"></a><strong>5.2.3</strong> JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀</h4><p><strong>解决方案</strong>：为倾斜key增加随机前/后缀</p><p><strong>适用场景</strong>：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong>：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><strong>优势</strong>：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong>：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>注意</strong>：具有倾斜Key的RDD数据集中，key的数量比较少</p><p><img src="https://pic4.zhimg.com/80/v2-248b0cead5e9fb8a7b1cec840dd61b2f_1440w.jpg" alt="img"></p><h4 id="5-2-4-JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀"><a href="#5-2-4-JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀" class="headerlink" title="5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀"></a><strong>5.2.4</strong> JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀</h4><p><strong>解决方案</strong>：随机前缀和扩容RDD进行join</p><p><strong>适用场景</strong>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。</p><p><strong>实现思路</strong>：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>优点</strong>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>缺点</strong>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>实践经验</strong>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><p>注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀）</p><p><img src="https://pic4.zhimg.com/80/v2-fa2211e3a343d7b68e83bfe83d67f0cb_1440w.jpg" alt="img"></p><h4 id="5-2-5-数据集中少数几个key数据量很大，不重要，其他数据均匀"><a href="#5-2-5-数据集中少数几个key数据量很大，不重要，其他数据均匀" class="headerlink" title="5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀"></a><strong>5.2.5</strong> 数据集中少数几个key数据量很大，不重要，其他数据均匀</h4><p><strong>解决方案</strong>：过滤少数倾斜Key</p><p><strong>适用场景</strong>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>优点</strong>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>缺点</strong>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>实践经验</strong>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-什么是数据倾斜&quot;&gt;&lt;a href=&quot;#0-什么是数据倾斜&quot; class=&quot;headerlink&quot; title=&quot;0. 什么是数据倾斜&quot;&gt;&lt;/a&gt;0. 什么是数据倾斜&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;​        对于集群系统，一般缓存是分布式的，即
      
    
    </summary>
    
    
      <category term="Data Question" scheme="https://dataquaner.github.io/categories/Data-Question/"/>
    
    
      <category term="数据倾斜" scheme="https://dataquaner.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
    
  </entry>
  
  <entry>
    <title>1.数据开发工程师面试题目必知必会</title>
    <link href="https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/"/>
    <id>https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/</id>
    <published>2020-04-21T10:59:38.680Z</published>
    <updated>2020-04-21T10:59:38.680Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面试题目梳理"><a href="#面试题目梳理" class="headerlink" title="面试题目梳理"></a>面试题目梳理</h2><h3 id="一-数据结构和算法-LeetCode"><a href="#一-数据结构和算法-LeetCode" class="headerlink" title="一. 数据结构和算法 LeetCode"></a>一. 数据结构和算法 <a href="https://leetcode-cn.com/problemset/all/" target="_blank" rel="noopener">LeetCode</a></h3><ol><li>合并数组</li><li>二元查找树转双向链表</li><li>二叉树层次遍历</li><li>堆 最小堆</li><li>排序算法</li><li>动态规划</li><li>青蛙跳台阶</li><li>贪心算法</li><li>字符串转换成整数</li><li>链表中倒数第K个结点</li><li><ol start="11"><li>二维数组中的查找</li></ol></li><li><ol start="12"><li>替换空格</li></ol></li><li><ol start="13"><li>从尾到头打印链表</li></ol></li><li><ol start="14"><li>重建二叉树</li></ol></li><li>用两个栈实现队列</li><li>斐波那契数列及变形题</li><li>二进制中1的个数</li><li>在O(1)时间删除链表结点</li><li>调整数组顺序使奇数位于偶数前面</li><li>反转链表</li><li>合并两个排序的链表</li><li>树的子结构</li><li>二叉树的镜像</li><li>顺时针打印矩阵</li><li>栈的压入、弹出序列</li><li>二叉搜索树的后序遍历序列</li><li>二叉树中和为某一值的路径</li><li>数组中出现次数超过一半的数字</li><li>最小的k个数</li><li>连续子数组的最大和</li><li>第一个只出现一次的字符</li><li>两个链表的第一个公共结点</li><li>链表中环的入口结点</li><li>二叉树的镜像</li><li>跳台阶</li><li>变态跳台阶</li><li>矩形覆盖</li><li>从上往下打印二叉树</li><li>二叉搜索树的第K个结点</li></ol><h3 id="二-计算平台"><a href="#二-计算平台" class="headerlink" title="二. 计算平台"></a>二. 计算平台</h3><h4 id="1-Hadoop"><a href="#1-Hadoop" class="headerlink" title="1. Hadoop"></a>1. Hadoop</h4><ol><li><p>数据倾斜问题</p></li><li><p>hive开窗函数</p></li><li><p>hive UDF UDAF<br>Mapreduce原理</p></li><li><p>MR的Shuffle过程</p></li><li><p>Yarn的工作机制，以及MR Job提交运行过程</p></li><li><p>MapReduce1的工作机制和过程</p></li><li><p>HDFS写入过程</p></li><li><p>Fsimage 与 EditLog定义及合并过程</p></li><li><p>HDFS读过程</p></li><li><p>HDFS简介</p></li><li><p>在向HDFS中写数据的时候，当写某一副本时出错怎么处理？</p></li><li><p>namenode的HA实现</p></li><li><p>简述联邦HDFS</p></li><li><p>HDFS源码解读–create()</p></li><li><p>NameNode高可用中editlog同步的过程</p></li><li><p>HDFS写入过程客户端奔溃怎么处理？（租约恢复）</p></li></ol><h4 id="2-Hive"><a href="#2-Hive" class="headerlink" title="2. Hive"></a>2. Hive</h4><ol><li>Hive内部表与外部表的区别</li><li>Hive与传统数据库的区别</li><li>Hiverc文件</li><li>Hive分区</li><li>Hive分区过多有何坏处以及分区时的注意事项</li><li>Hive中复杂数据类型的使用好处与坏处</li><li>hive分桶？</li><li>Hive元数据库是用来做什么的，存储哪些信息？</li><li>为何不使用Derby作为元数据库？</li><li>Hive什么情况下可以避免进行mapreduce？</li><li>Hive连接？</li><li>Hive MapJoin?</li><li>Hive的sort by, order by, distribute by, cluster by区别？</li><li>Hadoop计算框架特性</li><li>Hive优化常用手段</li><li>数据倾斜整理(转)</li><li>使用Hive如何进行抽样查询？</li></ol><h4 id="3-Spark"><a href="#3-Spark" class="headerlink" title="3. Spark"></a>3. Spark</h4><ol><li>Spark的运行模式</li><li>RDD是如何容错的？</li><li>Spark和MapReduce的区别</li><li>说一下Spark的RDD</li><li>自己实现一个RDD，需要实现哪些函数或者部分？</li><li>MapReduce和Spark的区别</li><li>Spark的Stage是怎么划分的？如何优化？</li><li>宽依赖与窄依赖区别</li><li>Spark性能调优</li><li>Flink、Storm与Spark Stream的区别（未）</li><li>说下spark中的transform和action</li><li>RDD、DataFrame和DataSet的区别</li><li>Spark执行任务流程（standalone、yarn）</li><li>Spark的数据容错机制</li><li>Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</li><li>Spark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？以及要注意的地方</li><li>driver的功能是什么？</li><li>spark端口</li><li>RDD有哪几种创建方式</li><li>map和flatmap的区别</li><li>Spark的基本工作流程</li></ol><h4 id="4-Flink"><a href="#4-Flink" class="headerlink" title="4. Flink"></a>4. Flink</h4><h5 id="4-1-Flink核心概念和基础"><a href="#4-1-Flink核心概念和基础" class="headerlink" title="4.1 Flink核心概念和基础"></a>4.1 Flink核心概念和基础</h5><blockquote><p>第一部分：Flink 中的核心概念和基础篇，包含了 Flink 的整体介绍、核心概念、算子等考察点。</p></blockquote><p>第二部分：Flink 进阶篇，包含了 Flink 中的数据传输、容错机制、序列化、数据热点、反压等实际生产环境中遇到的问题等考察点。</p><p>第三部分：Flink 源码篇，包含了 Flink 的核心代码实现、Job 提交流程、数据交换、分布式快照机制、Flink SQL 的原理等考察点。</p><h4 id="5-Storm："><a href="#5-Storm：" class="headerlink" title="5. Storm："></a><strong>5. Storm：</strong></h4><p>Storm的可靠性如何实现？包括spout和bolt两部分</p><p>怎么提高Storm的并发度？</p><p>Storm如何处理反压机制？</p><p>Storm中的Stream grouping有哪几种方式？</p><p>Storm的组件介绍</p><p>Storm怎么完成对单词的计数？</p><p>简述Strom的计算结构</p><h4 id="6-kafka："><a href="#6-kafka：" class="headerlink" title="6. kafka："></a><strong>6. kafka：</strong></h4><p>kafka介绍</p><p>Kafka与传统消息队列的区别？</p><p>kafka的零拷贝</p><p>kafka消息持久化和顺序读写？</p><h4 id="7-Kylin"><a href="#7-Kylin" class="headerlink" title="7. Kylin"></a>7. Kylin</h4><p>简介Kylin</p><p>Kylin的工作原理</p><p>Kylin的技术框架</p><p>Cube、Cuboid 和 Cube Segment</p><p>Kylin 对维度表的的要求</p><p>Cube的构建过程</p><p>全量构建和增量构建的区别</p><p>流式构建原理</p><h3 id="三-数据库"><a href="#三-数据库" class="headerlink" title="三. 数据库"></a>三. 数据库</h3><pre><code>     1）两大引擎Innodb引擎和MyIASM引擎，      2）mysql索引原理和底层实现BTREE、B+ TREE</code></pre><h3 id="四-数据仓库"><a href="#四-数据仓库" class="headerlink" title="四. 数据仓库"></a>四. 数据仓库</h3><p>​    1）拉链表<br>​    2）星型模型和雪花模型<br>​    3）维度建模过程</p><h3 id="五-操作系统"><a href="#五-操作系统" class="headerlink" title="五. 操作系统"></a>五. 操作系统</h3><p>   1）线程和进程，进程间的通信方式<br>   2）死锁<br>   3）内存分页<br>   4）同步异步阻塞</p><h3 id="六-计算机网络"><a href="#六-计算机网络" class="headerlink" title="六. 计算机网络"></a>六. 计算机网络</h3><ol><li>简述TCP和UDP的区别</li><li>七层协议每一层的任务及作用</li><li>简述http状态码</li><li>简述http协议与https协议</li><li>简述SSL协议</li><li>解析DNS过程</li><li>三次握手，四次挥手的过程？？为什么三握？</li></ol><h3 id="七-Linux"><a href="#七-Linux" class="headerlink" title="七. Linux"></a>七. Linux</h3><h4 id="1-比较常用Linux指令"><a href="#1-比较常用Linux指令" class="headerlink" title="1. 比较常用Linux指令"></a>1. 比较常用Linux指令</h4><p>　　1.1、ls/ll、cd、mkdir、rm-rf、cp、mv、ps -ef | grep xxx、kill、free-m、tar -xvf file.tar、（说那么十几二十来个估计差不多了）</p><h4 id="2-进程相关"><a href="#2-进程相关" class="headerlink" title="2. 进程相关"></a>2. 进程相关</h4><h5 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h5><p>　　2.1、ps -ef | grep xxx</p><p>　　2.2、ps -aux | grep xxx（-aux显示所有状态）</p><h5 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h5><p>　　3.1、kill -9[PID]  —(PID用查看进程的方式查找)</p><p>4、启动/停止服务</p><p>　　4.1、cd到bin目录cd/</p><p>　　4.2、./startup.sh  –打开（先确保有足够的权限）</p><p>　　4.3、./shutdown.sh —关闭</p><p>5、查看日志</p><p>　　5.1、cd到服务器的logs目录（里面有xx.out文件）</p><p>　　5.2、tail -f xx.out –此时屏幕上实时更新日志。ctr+c停止</p><p>　　5.3、查看最后100行日志 tail -100 xx.out </p><p>　　5.4、查看关键字附件的日志。如：cat filename | grep -C 5 ‘关键字’（关键字前后五行。B表示前，A表示后，C表示前后） —-使用不多**<br>**</p><p>　　5.5、还有vi查询啥的。用的也不多。</p><p>6、查看端口：（如查看某个端口是否被占用）</p><p>　　6.1、netstat -anp | grep 端口号（状态为LISTEN表示被占用）</p><p>7、查找文件</p><p>　　7.1、查找大小超过xx的文件： find . -type f -size +xxk —–(find . -type f -mtime -1 -size +100k -size-400k)–查区间大小的文件</p><p>　　7.2、通过文件名：find / -name xxxx  —整个硬盘查找</p><p>　　其余的基本上不常用</p><p>8、vim（vi）编辑器　　</p><p>　　有命令模式、输入模式、末行模式三种模式。<br>　　命令模式：查找内容(/abc、跳转到指定行(20gg)、跳转到尾行(G)、跳转到首行(gg)、删除行(dd)、插入行(o)、复制粘贴(yy,p)<br>　　输入模式：编辑文件内容<br>　　末行模式：保存退出(wq)、强制退出(q!)、显示文件行号(set number)<br>　　在命令模式下，输入a或i即可切换到输入模式，输入冒号(:)即可切换到末行模式；在输入模式和末行模式下，按esc键切换到命令模式</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;面试题目梳理&quot;&gt;&lt;a href=&quot;#面试题目梳理&quot; class=&quot;headerlink&quot; title=&quot;面试题目梳理&quot;&gt;&lt;/a&gt;面试题目梳理&lt;/h2&gt;&lt;h3 id=&quot;一-数据结构和算法-LeetCode&quot;&gt;&lt;a href=&quot;#一-数据结构和算法-LeetCode&quot;
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
