<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>DataQuaner</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://dataquaner.github.io/"/>
  <updated>2019-12-17T06:48:38.070Z</updated>
  <id>https://dataquaner.github.io/</id>
  
  <author>
    <name>Leon</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习系列之决策树算法（01）：决策树特征选择</title>
    <link href="https://dataquaner.github.io/2019/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8801%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>https://dataquaner.github.io/2019/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8801%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</id>
    <published>2019-12-17T06:49:02.060Z</published>
    <updated>2019-12-17T06:48:38.070Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-什么是特征选择"><a href="#1-什么是特征选择" class="headerlink" title="1.什么是特征选择"></a>1.什么是特征选择</h2><p>【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。</p><p>【那么选择的是什么样的特征呢？】这里我们选择的特征需要是<strong>对训练数据有分类能力</strong>的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征<strong>没有分类能力</strong>，舍去这个特征对学习的精度不会有特别大的影响。</p><blockquote><p>特征选择是决定用哪个特征来划分特征空间。</p></blockquote><p>比如女生找男朋友，可能这个女生<strong>首先</strong>会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了</p><p>【那怎么判断哪个特征有更好的分类能力呢？】这时候【<strong>信息增益</strong>】就要出场了。</p><h2 id="2-信息增益"><a href="#2-信息增益" class="headerlink" title="2.信息增益"></a>2.信息增益</h2><p>为了解释什么是信息增益，我们首先要讲解一下什么是【<strong>熵（entropy）</strong>】</p><h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a><strong>熵（Entropy）</strong></h3><blockquote><p>在热力学与化学中：</p><p>熵是一种测量在动力学方面【<strong>不能做功的能量的总数</strong>】，当总体熵增加，其<strong>做功能力</strong>也下降，熵的度量是<strong>能量退化</strong>的指标。</p></blockquote><p>1948 年，香农把热力学中的熵引入到信息论中，称为<strong>香农熵</strong>。根据维基百科的描述：</p><blockquote><p>在信息论中，熵是接收的<strong>每条消息</strong>中包含的<strong>信息的平均量</strong>。</p></blockquote><p>更一般的，【<strong>熵表示随机变量的不确定性</strong>】。假设一个有限取值的离散随机变量 X 的概率分布如下：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=P%28X+%3D+x_i%29+%3D+p_i%2C%5C+%5C+%5C+%5C+i+%3D+1%2C+2%2C+%5Ccdots%2C+n" class="lazyload"></p><p>那么它的熵定义为：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H%28X%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+p_i+%5Clog_%7Bb%7D+p_i" class="lazyload"></p><p>上式中的 b 通常取 2 或者自然对数 <em>e</em>，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。</p><p>从上式中，我们可以看到，<strong>熵与 X 的取值是没有关系的，它只与 X 的分布有关</strong>，所以 H 也可以写作 p 的函数：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H%28p%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+p_i%5Clog+p_i" class="lazyload"></p><p>我们现在来看两个随机变量的情况。</p><p>假设随机变量 (X, Y) 的联合概率分布如下：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=P%28X+%3D+x_i%2C+Y+%3D+y_j%29+%3D+p_%7Bij%7D%2C%5C+%5C+%5C+%5C+i+%3D+1%2C+2%2C+%5Ccdots%2C+n%3B%5C+j+%3D+1%2C+2%2C+%5Ccdots%2C+m" class="lazyload"></p><p>我们使用<strong>条件熵（conditional entropy）H(Y|X)</strong>来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。</p><blockquote><p>条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。</p></blockquote><p>是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H%28Y%7CX%29+%3D+%5Csum_%7Bi%3D1%7D%5En+p_i+H%28Y%7CX%3Dx_i%29%2C%5C+%5C+%5C+%5C+p_i+%3D+P%28X%3Dx_i%29%2C%5C+i+%3D+1%2C+2%2C+%5Ccdots%2C+n" class="lazyload"></p><p>有了上面的公式以后，条件熵的定义就非常容易理解了。</p><p>那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？</p><h3 id="信息增益的定义与信息增益算法"><a href="#信息增益的定义与信息增益算法" class="headerlink" title="信息增益的定义与信息增益算法"></a>信息增益的定义与信息增益算法</h3><p>既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？</p><p>没错，由于熵表示不确定性，严格来说，<strong>信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」</strong>。</p><p>我们给出信息增益的最终定义：</p><blockquote><p>特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。</p></blockquote><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=g%28D%2C+A%29+%3D+H%28D%29+-+H%28D%7CA%29" class="lazyload"></p><p><em>这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。</em></p><p>假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。</p><p>很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。</p><blockquote><p><strong>ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要</strong></p><p><strong>信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大</strong></p><p><strong>条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定</strong></p><p><strong>信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类</strong></p></blockquote><p>下面我们给出信息增益的算法。</p><blockquote><p>首先对数据做一些介绍：</p><ul><li>假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=C_k" class="lazyload"> ， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%7CC_k%7C" class="lazyload"> 为属于这个类的样本的个数，很显然 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5Csum_%7Bk%3D1%7D%5EK+%7CC_k%7C+%3D+%7CD%7C" class="lazyload"> 。</li><li>再假设我们有一个特征叫 A，A 有 n 个不同的取值 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5C%7Ba_1%2C+a_2%2C+%5Ccdots%2C+a_n%5C%7D" class="lazyload"> ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=D_i" class="lazyload"> ， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%7CD_i%7C" class="lazyload"> 是这个子集的样本个数，很显然 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En+%7CD_i%7C+%3D+%7CD%7C" class="lazyload"> 。</li><li>我们把 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=D_i" class="lazyload"> 中属于类别 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=C_k" class="lazyload"> 的集合称作 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=D_%7Bik%7D%2C%5C+D_%7Bik%7D+%3D+D_i+%5Cbigcap+C_k" class="lazyload"> ， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%7CD_%7Bik%7D%7C" class="lazyload"> 是其样本个数。</li></ul><p>信息增益的计算就分为如下几个步骤：</p><ol><li>计算 D 的经验熵 H(D)：</li></ol><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H%28D%29+%3D+-%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D+%5Clog_2+%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D" class="lazyload"></p><p>\2. 计算 A 对 D 的经验条件熵 H(D|A)：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H%28D%7CA%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+H%28D_i%29+%3D+-+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D+%5Clog_2+%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D" class="lazyload"></p><p>\3. 计算信息增益 g(D, A)：</p><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=g%28D%2C+A%29+%3D+H%28D%29+-+H%28D%7CA%29" class="lazyload"></p></blockquote><h2 id="3-信息增益比"><a href="#3-信息增益比" class="headerlink" title="3.信息增益比"></a>3.信息增益比</h2><p>看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？</p><p>按照经验来看，【<strong>以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差</strong>】。</p><p><strong>ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。</strong>类别越多代表特征越不确定，即熵越多，类别的信息增益越小。</p><p>为了校正这一误差，我们引入了【<strong>信息增益比（information gain ratio）</strong>】，又叫做信息增益率，它的定义如下：</p><blockquote><p>特征 A 对训练数据集 D 的信息增益比 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=g_R%28D%2C+A%29" class="lazyload"> 定义为其信息增益 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=g%28D%2C+A%29" class="lazyload"> 与训练数据集 D 关于特征 A 的值的熵 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H_A%28D%29" class="lazyload"> 之比。</p></blockquote><blockquote><p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=g_R%28D%2C+A%29+%3D+%5Cfrac%7Bg%28D%2C+A%29%7D%7BH_A%28D%29%7D" class="lazyload"></p><p>其中， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=H_A%28D%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+%5Clog_2+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D" class="lazyload"> ，n 是 A 取值的个数。</p></blockquote><p>两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。</p><h2 id="4-ID3-：-最大信息增益"><a href="#4-ID3-：-最大信息增益" class="headerlink" title="4. ID3 ： 最大信息增益"></a>4. ID3 ： 最大信息增益</h2><blockquote><p>ID3以信息增益为准则来选择最优划分属性</p></blockquote><p>信息增益的计算要基于信息熵（度量样本集合纯度的指标）</p><p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-e0ce667594401d2f8daf3ee6a7da9151_hd.jpg" class="lazyload">信息熵越小，数据集X的纯度越大</p><p>因此，假设于数据集D上建立决策树，数据有K个类别：</p><p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-f6d10699fdbe216617836c7e8732ba58_hd.jpg" class="lazyload"></p><p>公式（1）中：</p><p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-181bbc695d6de40eff56d86518d84f29_hd.jpg" class="lazyload">表示第k类样本的数据占数据集D样本总数的比例</p><p>公式（2）表示的是以特征A作为分割的属性，得到的信息熵：</p><p>Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合</p><p>因此，该公式求的是以属性A为划分，n个分支的信息熵总和</p><p>公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好</p><p>但是这种分割算法存在一定的<strong>缺陷</strong>：</p><blockquote><p>假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，<strong>ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。</strong>为减小这一影响，有学者提出C4.5的分类算法。</p></blockquote><h2 id="5-C4-5-：最大信息增益率"><a href="#5-C4-5-：最大信息增益率" class="headerlink" title="5. C4.5 ：最大信息增益率"></a>5. C4.5 ：<strong>最大信息增益率</strong></h2><blockquote><p>C4.5基于信息增益率准则选择最优分割属性的算法</p></blockquote><p>信息增益比率通过引入一个被称作【<strong>分裂信息(Split information)</strong>】的项来惩罚取值较多的属性。</p><p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-c35719627c479737cb680c3f4d8cdf6d_hd.jpg" class="lazyload"></p><p>上式，<strong>分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征</strong>。因此C4.5决策树先从候选划分属性中找出<strong>信息增益高于平均水平</strong>的属性，在从中选择<strong>增益率最高</strong>的。</p><p>对于连续值属性来说，可取值数目不再有限，因此可以采用<strong>离散化技术</strong>（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。</p><h2 id="6-CART-：最小基尼指数"><a href="#6-CART-：最小基尼指数" class="headerlink" title="6.CART ：最小基尼指数"></a>6.CART ：<strong>最小基尼指数</strong></h2><blockquote><p>CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归</p></blockquote><p>CART是一棵<strong>二叉树</strong>，采用【<strong>二元切分法</strong>】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，<strong>既可以用于分类也可以用于回归</strong>。CART分类时，使用<strong>基尼指数（Gini）</strong>来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。<strong>CART中每一次迭代都会降低GINI系数。</strong></p><p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-79214da261d75829046953ab9cb8b03a_hd.jpg" class="lazyload">Di表示以A是属性值划分成n个分支里的数目</p><p>Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后<strong>基尼指数最小的属性作为最优化分属性。</strong></p><h3 id="7-分类树和回归树"><a href="#7-分类树和回归树" class="headerlink" title="7.分类树和回归树"></a>7.<strong>分类树和回归树</strong></h3><p>提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。</p><p>作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。</p><p>回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p>]]></content>
    
    <summary type="html">
    
      决策树
    
    </summary>
    
    
      <category term="-- MachineLearning -- DecisionTree" scheme="https://dataquaner.github.io/categories/MachineLearning-DecisionTree/"/>
    
    
      <category term="-- 决策树" scheme="https://dataquaner.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>xgboost算法模型输出的解释</title>
    <link href="https://dataquaner.github.io/2019/12/16/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/"/>
    <id>https://dataquaner.github.io/2019/12/16/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/</id>
    <published>2019-12-16T06:46:13.604Z</published>
    <updated>2019-12-16T06:46:13.604Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p> 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。</p><h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2. 数据集"></a>2. 数据集</h2><p> 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"> </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">data = iris.data[:<span class="number">100</span>]</span><br><span class="line"><span class="keyword">print</span> data.shape</span><br><span class="line"><span class="comment">#(100L, 4L)</span></span><br><span class="line"><span class="comment">#一共有100个样本数据, 维度为4维</span></span><br><span class="line"> </span><br><span class="line">label = iris.target[:<span class="number">100</span>]</span><br><span class="line"><span class="keyword">print</span> label</span><br><span class="line"><span class="comment">#正好选取label为0和1的数据</span></span><br><span class="line">[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="3-训练集与测试集"><a href="#3-训练集与测试集" class="headerlink" title="3. 训练集与测试集"></a>3. 训练集与测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"> </span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="4-Xgboost建模"><a href="#4-Xgboost建模" class="headerlink" title="4. Xgboost建模"></a>4. Xgboost建模</h2><h3 id="4-1-模型初始化设置"><a href="#4-1-模型初始化设置" class="headerlink" title="4.1 模型初始化设置"></a>4.1 模型初始化设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">dtrain=xgb.DMatrix(train_x,label=train_y)</span><br><span class="line">dtest=xgb.DMatrix(test_x)</span><br><span class="line"> </span><br><span class="line">params=&#123;<span class="string">'booster'</span>:<span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary:logistic'</span>,</span><br><span class="line">    <span class="string">'eval_metric'</span>: <span class="string">'auc'</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="string">'lambda'</span>:<span class="number">10</span>,</span><br><span class="line">    <span class="string">'subsample'</span>:<span class="number">0.75</span>,</span><br><span class="line">    <span class="string">'colsample_bytree'</span>:<span class="number">0.75</span>,</span><br><span class="line">    <span class="string">'min_child_weight'</span>:<span class="number">2</span>,</span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.025</span>,</span><br><span class="line">    <span class="string">'seed'</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">'nthread'</span>:<span class="number">8</span>,</span><br><span class="line">     <span class="string">'silent'</span>:<span class="number">1</span>&#125;</span><br><span class="line"> </span><br><span class="line">watchlist = [(dtrain,<span class="string">'train'</span>)]</span><br></pre></td></tr></table></figure><h3 id="4-2-建模与预测"><a href="#4-2-建模与预测" class="headerlink" title="4.2 建模与预测"></a>4.2 建模与预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bst=xgb.train(params,dtrain,num_boost_round=<span class="number">100</span>,evals=watchlist)</span><br><span class="line"> </span><br><span class="line">ypred=bst.predict(dtest)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 设置阈值, 输出一些评价指标</span></span><br><span class="line">y_pred = (ypred &gt;= <span class="number">0.5</span>)*<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> <span class="string">'AUC: %.4f'</span> % metrics.roc_auc_score(test_y,ypred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'ACC: %.4f'</span> % metrics.accuracy_score(test_y,y_pred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Recall: %.4f'</span> % metrics.recall_score(test_y,y_pred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'F1-score: %.4f'</span> %metrics.f1_score(test_y,y_pred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Precesion: %.4f'</span> %metrics.precision_score(test_y,y_pred)</span><br><span class="line">metrics.confusion_matrix(test_y,y_pred)</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">AUC: <span class="number">1.0000</span></span><br><span class="line">ACC: <span class="number">1.0000</span></span><br><span class="line">Recall: <span class="number">1.0000</span></span><br><span class="line">F1-score: <span class="number">1.0000</span></span><br><span class="line">Precesion: <span class="number">1.0000</span></span><br><span class="line">array([[<span class="number">13</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>, <span class="number">12</span>]], dtype=int64)</span><br></pre></td></tr></table></figure><p>Yeah, 完美的模型, 完美的预测!</p><h3 id="4-3-可视化输出"><a href="#4-3-可视化输出" class="headerlink" title="4.3 可视化输出"></a>4.3 可视化输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对于预测的输出有三种方式</span></span><br><span class="line">?bst.predict</span><br><span class="line">Signature: bst.predict(data, output_margin=<span class="literal">False</span>, ntree_limit=<span class="number">0</span>, pred_leaf=<span class="literal">False</span>, pred_contribs=<span class="literal">False</span>, approx_contribs=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">pred_leaf : bool</span><br><span class="line">    When this option <span class="keyword">is</span> on, the output will be a matrix of (nsample, ntrees)</span><br><span class="line">    <span class="keyword">with</span> each record indicating the predicted leaf index of each sample <span class="keyword">in</span> each tree.</span><br><span class="line">    Note that the leaf index of a tree <span class="keyword">is</span> unique per tree, so you may find leaf <span class="number">1</span></span><br><span class="line">    <span class="keyword">in</span> both tree <span class="number">1</span> <span class="keyword">and</span> tree <span class="number">0.</span></span><br><span class="line"> </span><br><span class="line">pred_contribs : bool</span><br><span class="line">    When this option <span class="keyword">is</span> on, the output will be a matrix of (nsample, nfeats+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">with</span> each record indicating the feature contributions (SHAP values) <span class="keyword">for</span> that</span><br><span class="line">    prediction. The sum of all feature contributions <span class="keyword">is</span> equal to the prediction.</span><br><span class="line">    Note that the bias <span class="keyword">is</span> added <span class="keyword">as</span> the final column, on top of the regular features.</span><br></pre></td></tr></table></figure><h4 id="4-3-1-得分"><a href="#4-3-1-得分" class="headerlink" title="4.3.1 得分"></a>4.3.1 得分</h4><p>默认的输出就是得分, 这没什么好说的, 直接上code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ypred = bst.predict(dtest)</span><br><span class="line">ypred</span><br></pre></td></tr></table></figure><p>Out[32]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,</span><br><span class="line">        <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,</span><br><span class="line">        <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,</span><br><span class="line">        <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,</span><br><span class="line">        <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>], dtype=float32)</span><br></pre></td></tr></table></figure><p>在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出.</p><h4 id="4-3-2-所属的叶子节点"><a href="#4-3-2-所属的叶子节点" class="headerlink" title="4.3.2 所属的叶子节点"></a>4.3.2 所属的叶子节点</h4><p>当设置<code>pred_leaf=True</code>的时候, 这时就会输出每个样本在所有树中的叶子节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ypred_leaf = bst.predict(dtest, pred_leaf=<span class="literal">True</span>)</span><br><span class="line">ypred_leaf</span><br></pre></td></tr></table></figure><p>Out[33]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, ..., <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       ..., </span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, ..., <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以<code>ypred_leaf</code>的维度为<code>[100*100]</code>.</p><p>对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树).</p><p>那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xgb.to_graphviz(bst, num_trees=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#可视化第一棵树的生成情况</span></span><br></pre></td></tr></table></figure><p><img alt="img" data-src="https://images2017.cnblogs.com/blog/957413/201710/957413-20171017204407818-1932629185.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接输出模型的迭代工程</span></span><br><span class="line">bst.dump_model(<span class="string">"model.txt"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">booster[<span class="number">0</span>]:</span><br><span class="line"><span class="number">0</span>:[f3&lt;<span class="number">0.75</span>] yes=<span class="number">1</span>,no=<span class="number">2</span>,missing=<span class="number">1</span></span><br><span class="line">    <span class="number">1</span>:leaf=<span class="number">-0.019697</span></span><br><span class="line">    <span class="number">2</span>:leaf=<span class="number">0.0214286</span></span><br><span class="line">booster[<span class="number">1</span>]:</span><br><span class="line"><span class="number">0</span>:[f2&lt;<span class="number">2.35</span>] yes=<span class="number">1</span>,no=<span class="number">2</span>,missing=<span class="number">1</span></span><br><span class="line">    <span class="number">1</span>:leaf=<span class="number">-0.0212184</span></span><br><span class="line">    <span class="number">2</span>:leaf=<span class="number">0.0212</span></span><br><span class="line">booster[<span class="number">2</span>]:</span><br><span class="line"><span class="number">0</span>:[f2&lt;<span class="number">2.35</span>] yes=<span class="number">1</span>,no=<span class="number">2</span>,missing=<span class="number">1</span></span><br><span class="line">    <span class="number">1</span>:leaf=<span class="number">-0.0197404</span></span><br><span class="line">    <span class="number">2</span>:leaf=<span class="number">0.0197235</span></span><br><span class="line">booster[<span class="number">3</span>]: ……</span><br></pre></td></tr></table></figure><p>通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值.</p><p>在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值.</p><p>同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leaf1   <span class="number">-1.381214</span></span><br><span class="line">leaf2    <span class="number">1.410950</span></span><br></pre></td></tr></table></figure><p>在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了<code>&#39;objective&#39;: &#39;binary:logistic&#39;</code>, 因此使用函数将累加的值转换为实际的打分:</p><p>f(x)=1/(1+exp(−x))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>/float(<span class="number">1</span>+np.exp(<span class="number">1.38121416</span>))</span><br><span class="line">Out[<span class="number">24</span>]: <span class="number">0.20081407112186503</span></span><br><span class="line"><span class="number">1</span>/float(<span class="number">1</span>+np.exp(<span class="number">-1.410950</span>))</span><br><span class="line">Out[<span class="number">25</span>]: <span class="number">0.8039157403338895</span></span><br></pre></td></tr></table></figure><p>这就与<code>ypred = bst.predict(dtest)</code> 的分值相对应上了.</p><h4 id="4-3-2-特征重要性"><a href="#4-3-2-特征重要性" class="headerlink" title="4.3.2 特征重要性"></a>4.3.2 特征重要性</h4><p>接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ypred_contribs = bst.predict(dtest, pred_contribs=<span class="literal">True</span>)</span><br><span class="line">ypred_contribs</span><br></pre></td></tr></table></figure><p>Out[37]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>]], dtype=float32)</span><br></pre></td></tr></table></figure><p>输出的<code>ypred_contribs</code>的维度为<code>[100,5]</code>, 通过阅读前面的文档注释就可以知道, 最后一列是<code>bias</code>, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的.</p><p>通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">score_a = sum(ypred_contribs[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> score_a</span><br><span class="line"><span class="comment"># -1.38121373579</span></span><br><span class="line">score_b = sum(ypred_contribs[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> score_b</span><br><span class="line"><span class="comment"># 1.41094945744</span></span><br></pre></td></tr></table></figure><p>相同的分值, 相同的处理情况.</p><p>到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。</p>]]></content>
    
    <summary type="html">
    
      xgboost算法模型输出的解释
    
    </summary>
    
    
      <category term="MachineLearning" scheme="https://dataquaner.github.io/categories/MachineLearning/"/>
    
    
      <category term="xgboost" scheme="https://dataquaner.github.io/tags/xgboost/"/>
    
  </entry>
  
  <entry>
    <title>数据存储之MySQL系列（01）：MySQL体系结构</title>
    <link href="https://dataquaner.github.io/2019/12/04/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    <id>https://dataquaner.github.io/2019/12/04/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</id>
    <published>2019-12-04T13:26:03.118Z</published>
    <updated>2019-12-04T13:26:03.118Z</updated>
    
    <summary type="html">
    
      数据存储之MySQL系列（01）：MySQL体系结构
    
    </summary>
    
    
      <category term="数据存储" scheme="https://dataquaner.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"/>
    
      <category term="MySQL" scheme="https://dataquaner.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/MySQL/"/>
    
    
      <category term="mysql" scheme="https://dataquaner.github.io/tags/mysql/"/>
    
      <category term="体系结构" scheme="https://dataquaner.github.io/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>LightGBM算法基础系列之基础理论篇（1）</title>
    <link href="https://dataquaner.github.io/2019/11/14/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/"/>
    <id>https://dataquaner.github.io/2019/11/14/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/</id>
    <published>2019-11-14T14:21:29.609Z</published>
    <updated>2019-11-14T14:21:29.609Z</updated>
    
    <content type="html"><![CDATA[<p>这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。</p>]]></content>
    
    <summary type="html">
    
      LightGBM分类算法
    
    </summary>
    
    
      <category term="MachineLearning" scheme="https://dataquaner.github.io/categories/MachineLearning/"/>
    
      <category term="LightGBM" scheme="https://dataquaner.github.io/categories/MachineLearning/LightGBM/"/>
    
    
      <category term="LightGBM" scheme="https://dataquaner.github.io/tags/LightGBM/"/>
    
  </entry>
  
</feed>
