{"meta":{"title":"DataQuaner","subtitle":"DataQuaner","description":"Data|Algorithm|Business","author":"Leon","url":"https://dataquaner.github.io","root":"/"},"pages":[{"title":"分类","date":"2019-11-10T09:22:35.000Z","updated":"2020-04-11T11:01:07.476Z","comments":true,"path":"categories/index.html","permalink":"https://dataquaner.github.io/categories/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"contact","date":"2020-04-11T02:31:05.000Z","updated":"2020-04-11T02:31:41.680Z","comments":true,"path":"contact/index.html","permalink":"https://dataquaner.github.io/contact/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"关于","date":"2019-11-14T14:27:21.000Z","updated":"2020-04-11T13:58:33.412Z","comments":true,"path":"about/index.html","permalink":"https://dataquaner.github.io/about/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"友情链接","date":"2019-11-10T09:17:30.000Z","updated":"2019-11-10T09:20:41.842Z","comments":true,"path":"link/index.html","permalink":"https://dataquaner.github.io/link/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"friends","date":"2020-04-11T02:33:44.000Z","updated":"2020-04-11T02:34:02.597Z","comments":true,"path":"friends/index.html","permalink":"https://dataquaner.github.io/friends/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"标签","date":"2019-11-10T09:21:38.000Z","updated":"2020-04-11T11:00:23.510Z","comments":true,"path":"tags/index.html","permalink":"https://dataquaner.github.io/tags/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"medias","date":"2020-04-11T10:11:04.000Z","updated":"2020-04-11T10:11:04.927Z","comments":true,"path":"medias/index.html","permalink":"https://dataquaner.github.io/medias/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"}],"posts":[{"title":"Hive函数之lateral view 和 explode的区别和使用","slug":"Hive函数之lateral view 和 explode的区别和使用","date":"2021-06-01T13:40:00.000Z","updated":"2021-06-01T05:15:17.427Z","comments":true,"path":"2021/06/01/hive-han-shu-zhi-lateral-view-he-explode-de-qu-bie-he-shi-yong/","link":"","permalink":"https://dataquaner.github.io/2021/06/01/hive-han-shu-zhi-lateral-view-he-explode-de-qu-bie-he-shi-yong/","excerpt":"","text":"explode将一行数据转换成列数据，可以用于array和map类型的数据。就是将hive一行中复杂的array或者map结构拆分成多行。 用于array的语法如下： select explode(arraycol) as newcol from tablename; explode()：函数中的参数传入的是arrary数据类型的列名。 newcol()：是给转换成的列命名一个新的名字，用于代表转换之后的列名。 tablename()：原表名。 用于map的语法如下： select explode(mapcol) as (keyname,valuename) from tablename; explode()：函数中的参数传入的是map数据类型的列名。由于map是kay-value结构的，所以它在转换的时候会转换成两列，一列是kay转换而成的，一列是value转换而成的。 keyname：表示key转换成的列名称，用于代表key转换之后的列名。 valuename：表示value转换成的列名称，用于代表value转换之后的列名称。 注意：这两个值需要在as之后用括号括起来然后以逗号分隔。 Lateral Viewlateral view是Hive中提供给UDTF的结合，它可以解决UDTF不能添加额外的select列的问题。lateral view其实就是用来和想类似explode这种UDTF函数联用的，lateral view会将UDTF生成的结果放到一个虚拟表中，然后这个虚拟表会和输入行进行join来达到连接UDTF外的select字段的目的。 格式一 lateral view udtf(expression) tableAlias as columnAlias (,columnAlias)* lateral view在UDTF前使用，表示连接UDTF所分裂的字段。 UDTF(expression)：使用的UDTF函数，例如explode()。 tableAlias：表示UDTF函数转换的虚拟表的名称。 columnAlias：表示虚拟表的虚拟字段名称，如果分裂之后有一个列，则写一个即可；如果分裂之后有多个列，按照列的顺序在括号中声明所有虚拟列名，以逗号隔开。 格式二 from basetable (lateral view)* 在from子句中使用，一般和格式一搭配使用，这个格式只是说明了lateral view的使用位置。from子句后面也可以跟多个lateral view语句，使用空格间隔就可以了。 格式三 from basetable (lateral view outer)* 它比格式二只是多了一个outer，这个outer的作用是在UDTF转换列的时候将其中的空也给展示出来，UDTF默认是忽略输出空的，加上outer之后，会将空也输出，显示为NULL。这个功能是在Hive0.12是开始支持的。 参考： [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView](https://link.zhihu.com/?target=https%3A// cwiki.apache.org/confluence/display/Hive/LanguageManual%2BLateralView) https://msd.misuland.com/pd/307 [https://cloud.tencent.com/developer/article/1437884] document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"},{"name":"explode","slug":"explode","permalink":"https://dataquaner.github.io/tags/explode/"}]},{"title":"Hive函数之自定义函数UDF、UDAF、UDTF介绍及区别","slug":"Hive函数之自定义函数UDF、UDAF、UDTF介绍及区别","date":"2021-06-01T13:40:00.000Z","updated":"2021-06-01T05:15:17.517Z","comments":true,"path":"2021/06/01/hive-han-shu-zhi-zi-ding-yi-han-shu-udf-udaf-udtf-jie-shao-ji-qu-bie/","link":"","permalink":"https://dataquaner.github.io/2021/06/01/hive-han-shu-zhi-zi-ding-yi-han-shu-udf-udaf-udtf-jie-shao-ji-qu-bie/","excerpt":"","text":"前言 Hive中有三种UDF: 1、用户定义函数(user-defined function)UDF； 2、用户定义聚集函数（user-defined aggregate function ， UDAF ）； 3、用户定义表生成函数（user-defined table-generating function， UDTF ）; UDF操作作用于单个数据行，并且产生一个数据行作为输出。大多数函数都属于这一类（比如数学函数和字符串函数）。 UDAF 接受多个输入数据行，并产生一个输出数据行。像COUNT和MAX这样的函数就是聚集函数。 UDTF 操作作用于单个数据行，并且产生多个数据行——-一个虚拟表作为输出。lateral view explore() 简单来说： UDF:返回对应值，一对一 UDAF：返回聚类值，多对一 UDTF：返回拆分值，一对多 UDF1、UDF函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容。 2、编写UDF函数的时候需要注意一下几点： a）自定义UDF需要继承org.apache.hadoop.hive.ql.UDF。 b）需要实现evaluate函数，evaluate函数支持重载。 例：写一个返回字符串长度的Demo: import org.apache.hadoop.hive.ql.exec.UDF; public class GetLength extends UDF{ public int evaluate(String str) { try{ return str.length(); }catch(Exception e){ return -1; } } } 3、步骤 a）把程序打包放到目标机器上去； b）进入hive客户端，添加jar包： hive> add jar /root/hive_udf.jar c）创建临时函数： hive> create temporary function getLen as 'com.raphael.len.GetLength'; d）查询HQL语句： hive> select getLen(info) from apachelog; OK 60 29 87 102 69 60 67 79 66 Time taken: 0.072 seconds, Fetched: 9 row(s) e）销毁临时函数： hive> DROP TEMPORARY FUNCTION getLen; UDAF多行进一行出，如sum()、min()，用在group by时 1.必须继承 org.apache.hadoop.hive.ql.exec.UDAF(函数类继承) org.apache.hadoop.hive.ql.exec.UDAFEvaluator(内部类Evaluator实现UDAFEvaluator接口) 2.Evaluator需要实现 init、iterate、terminatePartial、merge、terminate这几个函数 init(): 类似于构造函数，用于UDAF的初始化 iterate(): 接收传入的参数，并进行内部的轮转，返回boolean terminatePartial(): 无参数，其为iterate函数轮转结束后，返回轮转数据，类似于hadoop的Combiner merge(): 接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean terminate(): 返回最终的聚集函数结果 UDTFUDTF介绍UDTF(User-Defined Table-Generating Functions) 用来解决 输入一行输出多行(On-to-many maping) 的需求。 编写自己需要的UDTF继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,实现initialize, process, close三个方法。 UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）。 初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward()调用产生一行；如果产生多列可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数。 最后close()方法调用，对需要清理的方法进行清理。 下面是我写的一个用来切分”key:value;key:value;”这种字符串，返回结果为key, value两个字段。供参考： import java.util.ArrayList; import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; public class ExplodeMap extends GenericUDTF{ @Override public void close() throws HiveException { // TODO Auto-generated method stub } @Override public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException { if (args.length != 1) { throw new UDFArgumentLengthException(\"ExplodeMap takes only one argument\"); } if (args[0].getCategory() != ObjectInspector.Category.PRIMITIVE) { throw new UDFArgumentException(\"ExplodeMap takes string as a parameter\"); } ArrayList&lt;String> fieldNames = new ArrayList&lt;String>(); ArrayList&lt;ObjectInspector> fieldOIs = new ArrayList&lt;ObjectInspector>(); fieldNames.add(\"col1\"); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); fieldNames.add(\"col2\"); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,fieldOIs); } @Override public void process(Object[] args) throws HiveException { String input = args[0].toString(); String[] test = input.split(\";\"); for(int i=0; i&lt;test.length; i++) { try { String[] result = test[i].split(\":\"); forward(result); } catch (Exception e) { continue; } } } } 3. 使用方法UDTF有两种使用方法，一种直接放到select后面，一种和lateral view一起使用。 1：直接select中使用 select explode_map(properties) as (col1,col2) from src; 不可以添加其他字段使用 select a, explode_map(properties) as (col1,col2) from src 不可以嵌套调用 select explode_map(explode_map(properties)) from src 不可以和group by/cluster by/distribute by/sort by一起使用 select explode_map(properties) as (col1,col2) from src group by col1, col2 和lateral view一起使用 select src.id, mytable.col1, mytable.col2 from src lateral view explode_map(properties) mytable as col1, col2; 此方法更为方便日常使用。执行过程相当于单独执行了两次抽取，然后union到一个表里。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"},{"name":"UDF","slug":"UDF","permalink":"https://dataquaner.github.io/tags/UDF/"},{"name":"UDAF","slug":"UDAF","permalink":"https://dataquaner.github.io/tags/UDAF/"},{"name":"UDTF","slug":"UDTF","permalink":"https://dataquaner.github.io/tags/UDTF/"}]},{"title":"大数据计算离线实时概念区别梳理","slug":"大数据计算离线计算VS实时计算VS批计算vs流计算","date":"2021-03-26T13:25:00.000Z","updated":"2021-03-28T16:11:34.096Z","comments":true,"path":"2021/03/26/da-shu-ju-ji-suan-chi-xian-ji-suan-vs-shi-shi-ji-suan-vs-pi-ji-suan-vs-liu-ji-suan/","link":"","permalink":"https://dataquaner.github.io/2021/03/26/da-shu-ju-ji-suan-chi-xian-ji-suan-vs-shi-shi-ji-suan-vs-pi-ji-suan-vs-liu-ji-suan/","excerpt":"","text":"1. 离线计算VS实时计算离线计算​ 离线计算，通常也称为“批处理”，表示那些离线批量、延时较高的静态数据处理过程。离线计算适用于实时性要求不高的场景，比如离线报表、数据分析等，延时一般在分钟级或小时级，多数场景是定时周期性执行一个Job任务，任务周期可以小到分钟级，比如每五分钟做一次统计分析，大到月级别、年级别，比如每月执行一次任务。我们最熟悉的MapReduce就是一个离线计算框架，Spark SQL也通常用于离线计算任务。 实时计算​ 实时计算，通常也称为“实时流计算”、“流式计算”，表示那些实时或者低延时的动态流数据处理过程。实时计算通常应用在实时性要求高的场景，比如实时ETL、实时监控等，延时一般都在毫秒级甚至更低。目前比较流行的实时框架有Spark Streaming与Flink。其中，Spark Streaming属于微批处理，是一种把流当作一种批的设计思想，具有非常高的吞吐量但延时也较高，这使得Streaming的场景也得到了一定的限制；Flink则是事件驱动的流处理引擎，是一种把批当作一种有限的流的设计思想，具有高吞吐，低延时，高性能的特点 离线和实时应该指的是：数据处理的延迟； 批量和流式指的是：数据处理的方式。 首先先说下批量计算和流式计算： 图中显示了一个计算的基本流程，receiver处负责从数据源接收数据，并发送给下游的task，数据由task处理后由sink端输出。 以图为例，批量和流式处理数据粒度不一样，批量每次处理一定大小的数据块（输入一般采用文件系统），一个task处理完一个数据块之后，才将处理好的中间数据发送给下游。流式计算则是以record为单位，task在处理完一条记录之后，立马发送给下游。 假如我们是对一些固定大小的数据做统计，那么采用批量和流式效果基本相同，但是流式有一个好处就是可以实时得到计算中的结果，这对某些应用很有帮助，比如每1分钟统计一下请求server的request次数。 那问题来了，既然流式系统也可以做批量系统的事情，而且还提供了更多的功能，那为什么还需要批量系统呢？因为早期的流式系统并不成熟，存在如下问题： 1.流式系统的吞吐不如批量系统 2.流式系统无法提供精准的计算 后面的介绍Storm、Spark streaming、Flink主要根据这两点来进行介绍。 批量和流式的区别： 1.数据处理单位： 批量计算按数据块来处理数据，每一个task接收一定大小的数据块，比如MR，map任务在处理完一个完整的数据块后（比如128M），然后将中间数据发送给reduce任务。 流式计算的上游算子处理完一条数据后，会立马发送给下游算子，所以一条数据从进入流式系统到输出结果的时间间隔较短（当然有的流式系统为了保证吞吐，也会对数据做buffer）。 这样的结果就是：批量计算往往得等任务全部跑完之后才能得到结果，而流式计算则可以实时获取最新的计算结果。 2.数据源： 批量计算通常处理的是有限数据（bound data），数据源一般采用文件系统，而流式计算通常处理无限数据（unbound data），一般采用消息队列作为数据源。 3.任务类型： 批量计算中的每个任务都是短任务，任务在处理完其负责的数据后关闭，而流式计算往往是长任务，每个work一直运行，持续接受数据源传过来的数据。 离线=批量？实时=流式？ 习惯上我们认为离线和批量等价；实时和流式等价，但其实这种观点并不完全正确。 假设一种情况：当我们拥有一个非常强大的硬件系统，可以毫秒级的处理Gb级别的数据，那么批量计算也可以毫秒级得到统计结果（当然这种情况非常极端，目前不可能），那我们还能说它是离线计算吗？ *所以说离线和实时应该指的是：数据处理的延迟；批量和流式指的是：数据处理的方式。两者并没有必然的关系。事实上Spark streaming就是采用小批量（batch）的方式来实现实时计算。* 可以参考下面链接：https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101。作者是Google实时计算的负责人，里面阐述了他对批量和实时的理解，并且作者认为批量计算只是流式计算的子集，一个设计良好的流式系统完全可以替代批量系统。本人也从中受到了很多启发。 2. 实时查询 Vs 即席查询实时查询实时查询，通常也称为在线查询，是对不断变化的数据进行实时的查询，要求数据修改后能够快速被查询到。通常我们见到的实时查询多是API的方式，少数以SQL方式。在线查询场景中最常见的生态组件大概就是HBase了，HBase能够提供强一致性的低延时数据访问，非常适合一般的在线业务。 即席查询即席查询，英文名称为Ad hoc query，起初是在数据仓库领域中用户根据特定需求定义的一种实时查询方式。通常情况下，即席查询的表现是借助于大数据SQL查询组件进行交互式查询，比如Hive、Impala、Presto等SQL查询组件。因此严格意义上说，即席查询和上述中的实时查询还是有一定区别的。 3. OLTP Vs OLAPOLTPOLTP（On-Line Transaction Processing），可称为在线事务处理，一般应用于在线业务交易系统，比如银行交易、订单交易等。OLTP的主要特点是能够支持频繁的在线操作（增删改），以及快速的访问查询。因为要用于在线交易，所以一般要求支持事务特性。 OLAPOLAP（On-Line Analytical Processing），可称为在线分析处理，较多的应用在数据仓库领域，支持复杂查询的数据分析，侧重于为业务提供决策支持。目前常见是的实时OLAP场景，比如Druid（Apache Druid，不同于阿里Druid）、ClickHouse等存储组件能够较好的满足需求。 4. 行式存储 Vs 列式存储行式存储行式存储（Row-based），简称“行存”，我们常见的关系型数据库比如MySQL、Oracle、DB2、SQL Server等都是采用行存的方式。总的来说，行存有利于写，但缺不利于读，因为行存是把同一条数据存放在相同位置，这样增删改比较高效，但是查询时会增加io的消耗。从上面举例我们也能看出，行存一般应用于OLTP场景。 列式存储列式存储（Column-based），简称“列存”，这里是相对于行式存储的一种数据存储方式，一般应用于分布式存储/数据库中。总的来说，列存有利于读，但不利于写，这就意味着写路径上的增删改有一定的性能损耗。常见的列存包括Parquet、Arrow等，其最大特点是能够减少不必要的io消耗，主要表现在列裁剪与列压缩方面。与行存相反，列存更适应于OLAP场景。 参考文献【1】实时计算And 离线计算，都是计算啊，区别搁哪呢？ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"BigData","slug":"BigData","permalink":"https://dataquaner.github.io/categories/BigData/"}],"tags":[{"name":"DataComputing","slug":"DataComputing","permalink":"https://dataquaner.github.io/tags/DataComputing/"}]},{"title":"大数据开发工程师面试资料汇总","slug":"数据开发工程师面试知识汇总@20210217","date":"2021-02-17T17:25:00.000Z","updated":"2021-02-21T15:21:14.485Z","comments":true,"path":"2021/02/18/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20210217/","link":"","permalink":"https://dataquaner.github.io/2021/02/18/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20210217/","excerpt":"","text":"目录[TOC] 一. Hadoop篇1. 并行计算模型MapReduce1.1 MapReduce 的工作原理 ​ MapReduce是一个基于集群的计算平台，是一个简化分布式编程的计算框架，是一个将分布式计算抽象为Map和Reduce两个阶段的编程模型。（这句话记住了是可以用来装逼的） 执行步骤：切片&gt;分词&gt;映射&gt;分区&gt;排序&gt;聚合&gt;shuffle&gt;reduce 1）Map()阶段 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数 重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出 对输出的key、value进行分区 对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中 2）Reduce阶段 多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上 对多个map的输出进行合并、排序。 重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出 把reduce的输出保存到文件中 特别说明： 切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理 分词，映射，分区，排序，聚合 都属map阶段 混洗 横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段 规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义 分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容 补充 切片：HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map任务的数量：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce任务的数量： 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误） 1.2 MapReduce中shuffle工作流程及优化 shuffle主要功能是把map task的输出结果有效地传送到reduce端。 简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。 前奏： 1. 在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。 2. 在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。前面我们知道这个job有3个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。 主要工作流程map端分区，排序，溢写，拷贝，reduce端合并 1）Map端shuffle 分区Partition MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。 写入内存缓冲区： 在我们的例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入内存缓冲区中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。 这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写，字面意思很直观。 溢写Spill： 这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。 排序Sort： 当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。 合并Map端：在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节在于，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。 CombineReduce端：在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。 2）Reduce端shuffle1. Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 2. Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。 增加combiner，压缩溢写的文件。 3. Reducer的输入文件。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。至于怎样才能让这个文件出现在内存中，之后的性能优化篇我再说。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。 3）shuffle优化 压缩：对数据进行压缩，减少写读数据量； 减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好； 内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招； 补充 1. Map端 1) io.sort.mb 用于map输出排序的内存缓冲区大小 类型：Int 默认：100mb 备注：如果能估算map输出大小，就可以合理设置该值来尽可能减少溢出写的次数，这对调优很有帮助。 2) io.sort.spill.percent map输出排序时的spill阀值（即使用比例达到该值时，将缓冲区中的内容spill 到磁盘） 类型：float 默认：0.80 3) io.sort.factor 归并因子（归并时的最多合并的流数），map、reduce阶段都要用到 类型：Int 默认：10 备注：将此值增加到100是很常见的。 4) min.num.spills.for.combine 运行combiner所需的最少溢出写文件数（如果已指定combiner） 类型：Int 默认：3 5) mapred.compress.map.output map输出是否压缩 类型：Boolean 默认：false 备注：如果map输出的数据量非常大，那么在写入磁盘时压缩数据往往是个很好的主意，因为这样会让写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量。 6) mapred.map.output.compression.codec 用于map输出的压缩编解码器 类型：Classname 默认：org.apache.hadoop.io.compress.DefaultCodec 备注：推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的 TeraSort作业，运行时间减少60%，且明显快于Zlib压缩。 7) tasktracker.http.threads 每个tasktracker的工作线程数，用于将map输出到reducer。 （注：这是集群范围的设置，不能由单个作业设置） 类型：Int 默认：40 备注：tasktracker开http服务的线程数。用于reduce拉取map输出数据，大集群可以将其设为40~50。 2. reduce端 1) mapred.reduce.slowstart.completed.maps 调用reduce之前，map必须完成的最少比例 类型：float 默认：0.05 2) mapred.reduce.parallel.copies reducer在copy阶段同时从mapper上拉取的文件数 类型：int 默认：5 3) mapred.job.shuffle.input.buffer.percent 在shuffle的复制阶段，分配给map输出的缓冲区占堆空间的百分比 类型：float 默认：0.70 4) mapred.job.shuffle.merge.percent map输出缓冲区（由mapred.job.shuffle.input.buffer.percent定义）使用比例阀值，当达到此阀值，缓冲区中的数据将会被归并然后spill 到磁盘。 类型：float 默认：0.66 5) mapred.inmem.merge.threshold map输出缓冲区中文件数 类型：int 默认：1000 备注：0或小于0的数意味着没有阀值限制，溢出写将有mapred.job.shuffle.merge.percent单独控制。 6) mapred.job.reduce.input.buffer.percent 在reduce过程中，在内存中保存map输出的空间占整个堆空间的比例。 类型：float 默认：0.0 备注：reduce阶段开始时，内存中的map输出大小不能大于该值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存。然而，如果reducer需要的内存较少，则可以增加此值来最小化访问磁盘的次数，以提高reduce性能。 4. 数据仓库工具Hive4.1 Hive 和普通关系型数据库的区别 Hive和关系型数据库存储文件的系统不同, Hive使用的是HDFS(Hadoop的分布式文件系统),关系型数据则是服务器本地的文件系统。 Hive使用的计算模型是MapReduce,而关系型数据库则是自己设计的计算模型. 关系型数据库都是为实时查询业务设计的,而Hive则是为海量数据做挖掘而设计的,实时性差;实时性的区别导致Hive的应用场景和关系型数据库有很大区别。 Hive很容易扩展自己的存储能力和计算能力,这几是继承Hadoop的,而关系型数据库在这方面要比Hive差很多。 4.2 Hive内部表和外部表的区别 创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径， 不对数据的位置做任何改变。 删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 4.3 Hive分区表和分桶表的区别分区在HDFS上的表现形式是一个目录， 分桶是一个单独的文件 分区: 细化数据管理，直接读对应目录，缩小mapreduce程序要扫描的数据量 分桶： 1、提高join查询的效率（用分桶字段做连接字段）2、提高采样的效率 4.4 Hive 支持哪些数据格式可支持Text，SequenceFile，ParquetFile，ORC，RCFILE等 补充 TextFile： TextFile文件不支持块压缩，默认格式，数据不做压缩，磁盘开销大，数据解析开销大。这边不做深入介绍。 RCFile： Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能，但是不支持模式演进。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。 ORCFile： 存储方式：数据按行分块 每块按照列存储 ，压缩快 快速列存取，效率比rcfile高,是rcfile的改良版本，相比RC能够更好的压缩，能够更快的查询，但还是不支持模式演进。 Parquet： Parquet能够很好的压缩，有很好的查询性能，支持有限的模式演进。但是写速度通常比较慢。这中文件格式主要是用在Cloudera Impala上面的。 性能对比 读操作 存储效率 4.5 Hive元数据库作用及存储内容​ 本质上只是用来存储hive中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。为数据库创建的目录一般在hive数据仓库目录下 4.6 HiveSQL 支持的几种排序区别1）Order By：全局排序，只有一个Reducer 使用 ORDER BY 子句排序 ASC（ascend）: 升序（默认） DESC（descend）: 降序 ORDER BY 子句在SELECT语句的结尾 案例实操 查询员工信息按工资升序排列 hive (default)> select * from emp order by sal; 查询员工信息按工资降序排列 hive (default)> select * from emp order by sal desc; 2）Sort By：每个MapReduce内部排序 Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。 设置reduce个数 hive (default)> set mapreduce.job.reduces=3; 查看设置reduce个数 hive (default)> set mapreduce.job.reduces; 根据部门编号降序查看员工信息 hive (default)> select * from emp sort by deptno desc; 将查询结果导入到文件中（按照部门编号降序排序） hive (default)> insert overwrite local directory '/opt/module/datas/sortby-result' select * from emp sort by deptno desc; 3）Distribute By：分区排序 Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。 案例实操： 先按照部门编号分区，再按照员工编号降序排序。 hive (default)> set mapreduce.job.reduces=3; hive (default)> insert overwrite local directory '/opt/module/datas/distribute-result' select * from emp distribute by deptno sort by empno desc; 注意： ​ distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。4）Cluster By​ 当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。 以下两种写法等价 hive (default)> select * from emp cluster by deptno; hive (default)> select * from emp distribute by deptno sort by deptno; --注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。 --cluster by ：sort by 和 distribute by的组合 4.7 Hive 的动态分区 ​ 往hive分区表中插入数据时，如果需要创建的分区很多，比如以表中某个字段进行分区存储，则需要复制粘贴修改很多sql去执行，效率低。因为hive是批处理系统，所以hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。 使用动态分区表必须配置的参数 set hive.exec.dynamic.partition =true（默认false）,表示开启动态分区功能； set hive.exec.dynamic.partition.mode = nonstrict(默认strict),表示允许所有分区都是动态的，否则必须有静态分区字段； 动态分区相关调优参数 set hive.exec.max.dynamic.partitions.pernode=100 （默认100，一般可以设置大一点，比如1000）； 表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。 set hive.exec.max.dynamic.partitions =1000(默认值) ； 表示一个动态分区语句可以创建的最大动态分区个数，超出报错； set hive.exec.max.created.files =10000(默认) 全局可以创建的最大文件个数，超出报错。 4.8 Hive MapJoin ​ MapJoin是Hive的一种优化操作，其适用于小表JOIN大表的场景，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率 使用 方法一： 在Hive0.11前，必须使用MAPJOIN来标记显示地启动该优化操作，由于其需要将小表加载进内存所以要注意小表的大小 SELECT /*+ MAPJOIN(smalltable)*/ .key,value FROM smalltable JOIN bigtable ON smalltable.key = bigtable.key 方法二： 在Hive0.11后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机 hive.auto.convert.join 默认值为true，自动开启MAPJOIN优化 hive.mapjoin.smalltable.filesize 默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中 注意：使用默认启动该优化的方式如果出现默名奇妙的BUG(比如MAPJOIN并不起作用),就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化 hive.auto.convert.join=false(关闭自动MAPJOIN转换操作) hive.ignore.mapjoin.hint=false(不忽略MAPJOIN标记) 对于以下查询是不支持使用方法二(MAPJOIN标记)来启动该优化的 select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne) ) firstjoin JOIN smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo) 但是，如果使用的是方法一即没有MAPJOIN标记则以上查询语句将会被作为两个MJ执行，进一步的，如果预先知道表大小是能够被加载进内存的，则可以通过以下属性来将两个MJ合并成一个MJ hive.auto.convert.join.noconditionaltask：Hive在基于输入文件大小的前提下将普通JOIN转换成MapJoin，并是否将多个MJ合并成一个 hive.auto.convert.join.noconditionaltask.size：多个MJ合并成一个MJ时，其表的总的大小须小于该值，同时hive.auto.convert.join.noconditionaltask必须为true 4.9 HQL 和 SQL 有哪些常见的区别 总体一致：Hive-sql与SQL基本上一样，因为当初的设计目的，就是让会SQL不会编程MapReduce的也能使用Hadoop进行处理数据。 区别：Hive没有delete和update。 Hive不支持等值连接 --SQL中对两表内联可以写成： select * from dual a,dual b where a.key = b.key; --Hive中应为 select * from dual a join dual b on a.key = b.key; --而不是传统的格式： SELECT t1.a1 as c1, t2.b1 as c2FROM t1, t2 WHERE t1.a2 = t2.b2 分号字符 --分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如： select concat(key,concat(';',key)) from dual; --但HiveQL在解析语句时提示： FAILED: Parse Error: line 0:-1 mismatched input '&lt;EOF>' expecting ) in function specification --解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成： select concat(key,concat('\\073',key)) from dual; IS [NOT] NULL --SQL中null代表空值, 值得警惕的是, --在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False. Hive不支持将数据插入现有的表或分区中 hive不支持INSERT INTO 表 Values（）, UPDATE, DELETE操作 hive支持嵌入mapreduce程序，来处理复杂的逻辑如： FROM ( 1. MAP doctext USING 'python wc_mapper.py' AS (word, cnt) 2. FROM docs 3. CLUSTER BY word 4. ) a 5. REDUCE word, cnt USING 'python wc_reduce.py'; --doctext: 是输入 --word, cnt: 是map程序的输出 --CLUSTER BY: 将wordhash后，又作为reduce程序的输入并且map程序、reduce程序可以单独使用，如： 1. FROM ( 2. FROM session_table 3. SELECT sessionid, tstamp, data 4. DISTRIBUTE BY sessionid SORT BY tstamp 5. ) a 6. REDUCE sessionid, tstamp, data USING 'session_reducer.sh'; --DISTRIBUTE BY: 用于给reduce程序分配行数据 4.10 Hive开窗函数假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。 1. SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。 SELECT *, /*前三次贷款的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv1, /*历史所有贷款 累加到下一次贷款 的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) AS pv2 FROM loan ; ​ 其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。 窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用： n PRECEDING : 前n行 n preceding n FOLLOWING：后n行 CURRENT ROW ： 当前行 如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为: ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. 窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。 2. NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。 SELECT *, #将数据按name切分成10区，并返回属于第几个分区 NTILE(10) OVER (PARTITION BY name ORDER BY orderdate) AS f1, #将数据按照name分区，并按照orderdate排序，返回排序序号 ROW_NUMBER() OVER (PARTITION BY name ORDER BY orderdate) AS f2, #将数据按照name分区，并按照orderdate排序，返回排序序号 RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f3, #将数据按照name分区，并按照orderdate排序，返回排序序号 DENSE_RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f4 FROM loan; 其中第一个函数NTILE(10)是将数据按name切分成10区，并返回属于第几个分区。 可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。语法是：ntile (num) over ([partition_clause] order_by_clause) as your_bucket_num 然后可以根据桶号，选取前或后 n分之几的数据。 后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。 ROW_NUMBER()返回的是一列连续的序号。 RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}. DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}. 3. LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点 SELECT *, #取上一笔贷款的日期,缺失默认填NULL LAG(orderdate, 1) OVER(PARTITION BY name ORDER BY orderdate) AS last_dt, #取下一笔贷款的日期,缺失指定填'1970-1-1' LEAD(orderdate, 1,'1970-1-1') OVER(PARTITION BY name ORDER BY orderdate) AS next_dt, #取最早一笔贷款的日期 FIRST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS first_dt, #取新一笔贷款的日期 LAST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS latest_dt FROM loan; LAG(n)将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录 4. GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate GROUPING SETS(substring(orderdate,1,7), orderdate) ORDER BY GROUPING__ID; GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。 比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5. 如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null. SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with CUBE ORDER BY GROUPING__ID; with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with ROLLUP ORDER BY GROUPING__ID; with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。 4.11 HiveUDF UDAF UDTF函数UDF：一进一出 实现方法： 继承UDF类 重写evaluate方法 将该java文件编译成jar 在终端输入如下命令： hive> add jar test.jar; hive> create temporary function function_name as 'com.hrj.hive.udf.UDFClass'; hive> select function_name(t.col1) from table t; hive> drop temporary function function_name; UDAF：多进一出 实现方法: 用户的UDAF必须继承了org.apache.hadoop.hive.ql.exec.UDAF； 用户的UDAF必须包含至少一个实现了org.apache.hadoop.hive.ql.exec的静态类，诸如实现了 UDAFEvaluator 一个计算函数必须实现的5个方法的具体含义如下： init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。 iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。 terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。 merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。 terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。 部分聚集结果的数据类型和最终结果的数据类型可以不同。 UDTF：一进多出 实现方法： 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF initialize()：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型） process：初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列 可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数 最后close()方法调用，对需要清理的方法进行清理 4.12 Hive数据倾斜问题0. 什么是数据倾斜 ​ 对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。 来源百度百科 ​ 对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 1. 数据倾斜的现象 ​ 多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。 2. 数据倾斜的影响 1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：Out Of Memory。 2）运行速度慢 ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。 一个经验结论是：一般情况下，OOM的原因都是数据倾斜\\ 3. 如何定位数据倾斜 ​ 数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。 原因： 查看任务-》查看Stage-》查看代码 ​ 某个task执行特别慢的情况 ​ 某个task莫名其妙内存溢出的情况 ​ 查看导致数据倾斜的key的数据分布情况 也可从以下几种情况考虑： 1、是不是有OOM情况出现，一般是少数内存溢出的问题 2、是不是应用运行时间差异很大，总体时间很长 3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题 4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断 5、看看代码里面是否有一些导致Shuffle的算子出现 4. 数据倾斜的几种典型情况（重点） 数据源中的数据分布不均匀，Spark需要频繁交互 数据集中的不同Key由于分区方式，导致数据倾斜 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要） 聚合操作中，数据集中的数据分布不均匀（主要） JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀 数据集中少数几个key数据量很大，不重要，其他数据均匀 注意： 需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题 只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以 5. 数据倾斜的处理方法 ​ 发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。 5.1 检查数据，找出异常的key ​ 如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个 df.select(\"key\").sample(false,0.1).(k=>(k,1)).reduceBykey(_+_).map(k=>(k._2,k._1)).sortByKey(false).take(10) ​ 如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。 经过分析，倾斜的数据主要有以下三种情况: null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。 无效数据，大量重复的测试数据或是对结果影响不大的有效数据。 有效数据，业务导致的正常数据分布。 解决办法 第1，2种情况，直接对数据进行过滤即可。 第3种情况则需要进行一些特殊操作，常见的有以下几种做法。 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。 对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。 使用reduceByKey 代替 groupByKey 使用map join。 举例：如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下： 将原始的 key 转化为 key + 随机值(例如Random.nextInt)对数据进行 reduceByKey(func)将 key + 随机值 转成 key再对数据进行 reduceByKey(func)tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。 tips2: 单独处理异常数据时，可以配合使用Map Join解决 5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互 解决方案1：避免数据源的数据倾斜 实现原理：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive或者Kafka中还是会发生数据倾斜。 适用情况：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 总结：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题 5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜 解决方案1：调整并行度 实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。 总结：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效 解决方案2： 缓解数据倾斜**（自定义Partitioner）** 适用场景：大量不同的Key被分配到了相同的Task造成该Task数据量过大。 解决方案： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。 优势： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。 劣势： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 5.2 检查Spark运行过程相关操作 5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要） 解决方案：Reduce side Join转变为Map side Join 方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。 5.2.2 聚合操作中，数据集中的数据分布不均匀（主要） 解决方案：两阶段聚合（局部聚合+全局聚合） 适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案 实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案 将相同key的数据分拆处理 5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀 解决方案：为倾斜key增加随机前/后缀 适用场景：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。 解决方案：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。 优势：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。 劣势：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。 注意：具有倾斜Key的RDD数据集中，key的数量比较少 5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀 解决方案：随机前缀和扩容RDD进行join 适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。 实现思路：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀） 5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀 解决方案：过滤少数倾斜Key 适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化） Hive优化目标 在有限的资源下，执行效率更高 常见问题 数据倾斜 map数设置 reduce数设置 其他 Hive执行 HQL –&gt; Job –&gt; Map/Reduce 执行计划 explain [extended] hql 样例 select col,count(1) from test2 group by col; explain select col,count(1) from test2 group by col; Hive表优化 分区 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; 静态分区 动态分区 分桶 set hive.enforce.bucketing=true; set hive.enforce.sorting=true; 数据 相同数据尽量聚集在一起 Hive Job优化 并行化执行 每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间 set hive.exec.parallel= true; set hive.exec.parallel.thread.numbe=8; 本地化执行 job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB) job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4) job的reduce数必须为0或者1 set hive.exec.mode.local.auto=true; 当一个job满足如下条件才能真正使用本地模式: job合并输入小文件 set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 合并文件数由mapred.max.split.size限制的大小决定 job合并输出小文件** set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件 set hive.merge.size.per.task=64000000;合并之后的文件大小 JVM重利用 set mapred.job.reuse.jvm.num.tasks=20; JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。 压缩数据 set hive.exec.compress.output=true; set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec; set mapred.output.compression.type=BLOCK; set hive.exec.compress.intermediate=true; set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec; set hive.intermediate.compression.type=BLOCK; 中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式 hive查询最终的输出也可以压缩 Hive Map优化 set mapred.map.tasks =10; 无效 (1)默认map个数 default_num=total_size/block_size; (2)期望大小 goal_num=mapred.map.tasks; (3)设置处理的文件大小 split_size=max(mapred.min.split.size,block_size); split_num=total_size/split_size; (4)计算的map个数 compute_map_num=min(split_num,max(default_num,goal_num)) 经过以上的分析，在设置map个数的时候，可以简答的总结为以下几点： 增大mapred.min.split.size的值 如果想增加map个数，则设置mapred.map.tasks为一个较大的值 如果想减小map个数，则设置mapred.min.split.size为一个较大的值 情况1：输入文件size巨大，但不是小文件 情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。 map端聚合 set hive.map.aggr=true; 推测执行 mapred.map.tasks.apeculative.execution Hive Shuffle优化 Map端 io.sort.mb io.sort.spill.percent min.num.spill.for.combine io.sort.factor io.sort.record.percent Reduce端 mapred.reduce.parallel.copies mapred.reduce.copy.backoff io.sort.factor mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent Hive Reduce优化 需要reduce操作的查询 group by,join,distribute by,cluster by… order by比较特殊,只需要一个reduce sum,count,distinct… 聚合函数 高级查询 推测执行 mapred.reduce.tasks.speculative.execution hive.mapred.reduce.tasks.speculative.execution Reduce优化 numRTasks = min[maxReducers,input.size/perReducer] maxReducers=hive.exec.reducers.max perReducer = hive.exec.reducers.bytes.per.reducer hive.exec.reducers.max 默认 ：999 hive.exec.reducers.bytes.per.reducer 默认:1G set mapred.reduce.tasks=10;直接设置 计算公式 Hive查询操作优化 join优化 关联操作中有一张表非常小 不等值的链接操作 set hive.auto.current.join=true; hive.mapjoin.smalltable.filesize默认值是25mb select /*+mapjoin(A)*/ f.a,f.b from A t join B f on (f.a=t.a) hive.optimize.skewjoin=true;如果是Join过程出现倾斜，应该设置为true set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化 mapjoin 简单总结下,mapjoin的使用场景: Bucket join 两个表以相同方式划分桶 两个表的桶个数是倍数关系 crete table order(cid int,price float) clustered by(cid) into 32 buckets; crete table customer(id int,first string) clustered by(id) into 32 buckets; select price from order t join customer s on t.cid=s.id join 优化前 select m.cid,u.id from order m join customer u on m.cid=u.id where m.dt='2013-12-12'; join优化后 select m.cid,u.id from (select cid from order where dt='2013-12-12')m join customer u on m.cid=u.id; group by 优化 hive.groupby.skewindata=true;如果是group by 过程出现倾斜 应该设置为true set hive.groupby.mapaggr.checkinterval=100000;--这个是group的键对应的记录条数超过这个值则会进行优化 count distinct 优化 优化前 select count(distinct id) from tablename 优化后 select count(1) from (select distinct id from tablename) tmp; select count(1) from (select id from tablename group by id) tmp; 优化前 select a,sum(b),count(distinct c),count(distinct d) from test group by a 优化后 select a,sum(b) as b,count(c) as c,count(d) as d from(select a,0 as b,c,null as d from test group by a,c union all select a,0 as b,null as c,d from test group by a,d union all select a,b,null as c,null as d from test)tmp1 group by a; #二. Spark篇 1. SparkCore1.1 Spark工作原理1. Spark是什么Spark是一种通用分布式并行计算框架。和Mapreduce最大不同就是spark是基于内存的迭代式计算。 Spark的Job处理的中间输出结果可以保存在内存中，从而不再需要读写HDFS，除此之外，一个MapReduce 在计算过程中只有map 和reduce 两个阶段，处理之后就结束了，而在Spark的计算模型中，可以分为n阶段，因为它内存迭代式的，我们在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。 因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。其不仅实现了MapReduce的算子map 函数和reduce函数及计算模型，还提供更为丰富的算子，如filter、join、groupByKey等。是一个用来实现快速而同用的集群计算的平台。 2. Spark工作原理框图 第一层级工作流程 a. 构建Spark Application的运行环境（启动SparkContext） b. SparkContext在初始化过程中分别创建DAGScheduler作业调度和TaskScheduler任务调度两级调度模块 c. SparkContext向资源管理器（可以是Standalone、Mesos、Yarn）申请运行Executor资源； d. 由资源管理器分配资源并启动StandaloneExecutorBackend，executor，之后向SparkContext申请Task； e. DAGScheduler将job 划分为多个stage,并将Stage提交给TaskScheduler; g. Task在Executor上运行，运行完毕释放所有资源。 第二层级DAGScheduler作业调度生成过程 DAGScheduler是一个面向stage 的作业调度器。 作业调度模块是基于任务阶段的高层调度模块，它为每个Spark作业计算具有依赖关系的多个调度阶段（通常根据shuffle来划分），然后为每个阶段构建出一组具体的任务（通常会考虑数据的本地性等），然后以TaskSets（任务组）的形式提交给任务调度模块来具体执行。 主要三大功能 接受用户提交的job。将job根据类型划分为不同的stage，记录哪些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset； 决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给TaskScheduler； 重新提交shuffle输出丢失的stage给taskScheduler； DAG如何将Job划分为多个stage 划分依据：**宽窄依赖**。何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。 切割规则：从后往前，遇到宽依赖就切割stage； Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是宽窄依赖，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以taskSet的形式提交给TaskScheduler运行，stage是由一组并行的task组成。 一旦driver程序中出现action，就会生成一个job，比如count等， ​ 向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。 ​ 每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是宽依赖，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。 第三层级谈谈spark中的宽窄依赖 RDD和它的父RDD的关系有两种类型：窄依赖和宽依赖 宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生 窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生 区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示： 1.2 Spark的shuffle原理和过程Shuffle过程框图 主要逻辑如下： 1）首先每一个MapTask会根据ReduceTask的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。 2）其次MapTask产生的结果会根据设置的partition算法填充到每个bucket中。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中。 当ReduceTask启动时，它会根据自己task的id和所依赖的Mapper的id从远端或本地的block manager中取得相应的bucket作为Reducer的输入进行处理。 这里的bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等。 Spark shuffle可以分为两部分：Shuffle Write 和 Shuffle Fetch Shuffle Write 由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。 shuffle write 的任务很简单，那么实现也很简单：将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。图示如下： 上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数），缓冲区被称为 bucket，其大小为spark.shuffle.file.buffer.kb ，默认是 32KB（Spark 1.1 版本以前是 100KB）。 其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。 ShuffleMapTask 的执行过程很简单：先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由partitioner.partition(record.getKey()))决定。每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 FileSegment。之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。 这样的实现很简单，但有几个问题： 产生的 FileSegment 过多。每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。 缓冲区占用内存空间大。每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了cores * R * 32 KB。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。 目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图： 可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。先执行完的 ShuffleMapTask 形成 ShuffleBlocki，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlocki’，每个 ShuffleBlock 被称为 FileSegment。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过spark.shuffle.consolidateFiles=true来开启。 Shuffle Fetch先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey： 很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了： 在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？ 边 fetch 边处理还是一次性 fetch 完再处理？ fetch 来的数据存放到哪里？ 怎么获得要 fetch 的数据的存放位置？ 在什么时候 fetch？当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。理论上讲，一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 spark.reducer.maxMbInFlight，这里用 softBuffer 表示，默认大小为 48MB。一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。 边 fetch 边处理还是一次性 fetch 完再处理？边 fetch 边处理。本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？答案是使用可以 aggregate 的数据结构，比如 HashMap。每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 \\ record，直接将其放进 HashMap 里面。如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 func(hashMap.get(Key), Value)，比如上面 WordCount 例子中的 func 就是 hashMap.get(Key) ＋ Value，并将 func 的结果重新 put(key) 到 HashMap 中去。这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。 // MapReduce reduce(K key, Iterable&lt;V> values) { result = process(key, values) return result } // Spark reduce(K key, Iterable&lt;V> values) { result = null for (V value : values) result = func(result, value) return result } MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。因此一些算法比如求平均数，在 process 里面很好实现，直接sum(values)/values.length，而在 Spark 中 func 可以实现sum(values)，但不好实现/values.length。更多的 func 将会在下面的章节细致分析。 fetch 来的数据存放到哪里？刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。如果spark.shuffle.spill = false就只用内存。内存使用的是AppendOnlyMap ，类似 Java 的HashMap，内存＋磁盘使用的是ExternalAppendOnlyMap，如果内存空间不足时，ExternalAppendOnlyMap可以将 \\ records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。 怎么获得要 fetch 的数据的存放位置？在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。 1.3 Spark的Stage划分及优化 窄依赖指父RDD的每一个分区最多被一个子RDD的分区所用，表现为 一个父RDD的分区对应于一个子RDD的分区 两个父RDD的分区对应于一个子RDD 的分区。 宽依赖指子RDD的每个分区都要依赖于父RDD的所有分区，这是shuffle类操作 Stage: 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。Stage的划分，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。 比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个； 如果是rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage. 会根据RDD之间的依赖关系将DAG图划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中 Stage划分思路 因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。 在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！ 而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。 总结 map,filter为窄依赖， groupbykey为宽依赖 遇到一个宽依赖就分一个stage 1.4 Spark和MapReduce的区别整体对比概念 Spark Shuffle 与MapReduce Shuffle的设计思想相同，但是实现细节优化方式不同。 1. 从逻辑角度来讲，Shuffle 过程就是一个 GroupByKey 的过程，两者没有本质区别。只是 MapReduce 为了方便 GroupBy 存在于不同 partition 中的 key/value records，就提前对 key 进行排序。Spark 认为很多应用不需要对 key 排序，就默认没有在 GroupBy 的过程中对 key 排序。 2. 从数据流角度讲，两者有差别。 MapReduce 只能从一个 Map Stage shuffle 数据，Spark 可以从多个 Map Stages shuffle 数据 3 .Shuffle write/read 实现上有一些区别。 以前对 shuffle write/read 的分类是 sort-based 和 hash-based。MapReduce 可以说是 sort-based，shuffle write 和 shuffle read 过程都是基于key sorting 的 (buffering records + in-memory sort + on-disk external sorting)。早期的 Spark 是 hash-based，shuffle write 和 shuffle read 都使用 HashMap-like 的数据结构进行 aggregate (without key sorting)。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based (only sort partition id, without key sorting)，shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经“你中有我，我中有你”，界限已经不那么清晰。 4. 从数据 fetch 与数据计算的重叠粒度来讲，两者有细微区别。 MapReduce 是粗粒度，reducer fetch 到的 records 先被放到 shuffle buffer 中休息，当 shuffle buffer 快满时，才对它们进行 combine()。而 Spark 是细粒度，可以即时将 fetch 到的 record 与 HashMap 中相同 key 的 record 进行 aggregate。 解说：1、MapReduce在Map阶段完成之后数据会被写入到内存中的一个环形缓冲区（后续的分区/分组/排序在这里完成）；Spark的Map阶段完成之后直接输出到磁盘。2、受第一步的影响，MapReduce输出的数据是有序的（针对单个Map数据来说）；Spark的数据是无序的（可以使用RDD算子达到排序的效果）。3、MapReduce缓冲区的数据处理完之后会spill到磁盘形成一个文件，文件数量达到阈值之后将会进行merge操作，将多个小文件合并为一个大文件；Spark没有merge过程，一个Map中如果有对应多个Reduce的数据，则直接写多个磁盘文件。4、MapReduce全部通过网络来获得数据；对于本地数据Spark可以直接读取 1.5 宽依赖与窄依赖区别RDD和它的父RDD的关系有两种类型：窄依赖和宽依赖 宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生 窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生 区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示： 1.6 Spark RDD 原理1. RDD是什么 RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合 Dataset：就是一个集合，用于存放数据的 Destributed：分布式，可以并行在集群计算 Resilient：表示弹性的，弹性表示 RDD中的数据可以存储在内存或者磁盘中； RDD中的分区是可以改变的； A list of partitions：一个分区列表，RDD中的数据都存储在一个分区列表中 A function for computing each split：作用在每一个分区中的函数 A list of dependencies on other RDDs：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的 Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向 可选项，数据本地性，数据位置最优 2. RDD操作​ RDD创建后就可以在RDD上进行数据处理。RDD支持两种操作：转换（transformation），即从现有的数据集创建一个新的数据集；动作（action），即在数据集上进行计算后，返回一个值给Driver程序。 转换（transformation） ​ RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。 ​ RDD中所有的Transformation都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的Action时，这些Transformation才会真正运行。 #### map(func)** 返回一个新的分布式数据集，该数据集由每一个输入元素经过func函数转换后组成 #### **fitler(func)** 返回一个新的数据集，该数据集由经过func函数计算后返回值为true的输入元素组成 #### **flatMap(func)** 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（因此func返回一个序列，而不是单一元素） #### **mapPartitions(func)** 类似于map，但独立地在RDD上每一个分片上运行，因此在类型为T的RDD上运行时，func函数类型必须是Iterator[T]=>Iterator[U] #### **mapPartitionsWithSplit(func)** 类似于mapPartitons，但func带有一个整数参数表示分片的索引值。因此在类型为T的RDD上运行时，func函数类型必须是(Int,Iterator[T])=>Iterator[U] #### **sample(withReplacement,fraction,seed)** 根据fraction指定的比例对数据进行采样，可以选择是否用随机数进行替换，seed用于随机数生成器种子 #### **union(otherDataSet)** 返回一个新数据集，新数据集是由原数据集和参数数据集联合而成 #### **distinct([numTasks])** 返回一个包含原数据集中所有不重复元素的新数据集 #### **groupByKey([numTasks])** 在一个(K,V)数据集上调用，返回一个(K,Seq[V])对的数据集。注意默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数来改变它 #### **reduceByKey(func,[numTasks])** 在一个(K,V)对的数据集上调用，返回一个(K,V)对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。与groupByKey类似，reduceByKey任务的个数是可以通过第二个可选参数来设置的 #### **sortByKey([[ascending],numTasks])** 在一个(K,V)对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的(K,V)对数据集。升序或降序由ascending布尔参数决定 #### **join(otherDataset0,[numTasks])** 在类型为(K,V)和(K,W)数据集上调用，返回一个相同的key对应的所有元素在一起的(K,(V,W))数据集 #### **cogroup(otherDataset,[numTasks])** 在类型为(K,V)和(K,W)数据集上调用，返回一个(K,Seq[V],Seq[W])元祖的数据集。这个操作也可以称为groupwith #### **cartesain(ohterDataset)** 笛卡尔积，在类型为T和U类型的数据集上调用，返回一个(T,U)对数据集(两两的元素对) 动作（action） #### **reduce(func)** 通过函数func(接收两个参数，返回一个参数)聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的并行运行 #### **collect()** 在驱动程序中，以数组形式返回数据集中的所有元素。通常在使用filter或者其他操作返回一个足够小的数据子集后再使用会比较有用 #### **count()** 返回数据集元素个数 #### **first()** 返回数据集第一个元素(类似于take(1)) #### **take(n)** 返回一个由数据集前n个元素组成的数组 注意 这个操作目前并非并行执行，而是由驱动程序计算所有的元素 #### **takeSample(withReplacement,num,seed)** 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否由随机数替换不足的部分，seed用户指定随机数生成器种子 #### **saveAsTextFile(path)** 将数据集的元素以textfile的形式保存到本地文件系统—HDFS或者任何其他Hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行 #### **saveAsSequenceFile(path)** 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是本地系统、HDFS或者任何其他的Hadoop支持的文件系统。这个只限于由key-value对组成，并实现了Hadoop的Writable接口，或者可以隐式的转换为Writable的RDD(Spark包括了基本类型转换，例如Int、Double、String等) #### **countByKey()** 对(K,V)类型的RDD有效，返回一个(K,Int)对的map，表示每一个key对应的元素个数 #### **foreach(func)** 在数据集的每一个元素上，运行函数func进行更新。通常用于边缘效果，例如更新一个叠加器，或者和外部存储系统进行交互，如HBase 3. RDD共享变量在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。 (1). 广播变量（Broadcast Variables） – 广播变量缓存到各个节点的内存中，而不是每个 Task – 广播变量被创建后，能在集群中运行的任何函数调用 – 广播变量是只读的，不能在被广播后修改 – 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本 val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量(2). 累加器 ​ 累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值。 4. RDD缓存Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 transformation自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。 ​ Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。 ​ Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择： 如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。 如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。 如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。 如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。 在不会使用cached RDD的时候，及时使用unpersist方法来释放它。 1.7 RDD有哪几种创建方式1) 使用程序中的集合创建rdd2) 使用本地文件系统创建rdd3) 使用hdfs创建rdd，4) 基于数据库db创建rdd5) 基于Nosql创建rdd，如hbase6) 基于s3创建rdd，7) 基于数据流，如socket创建rdd 1.8 Spark的RDD DataFrame和DataSet的区别RDD的优点： 相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。 面向对象编程，直接存储的java对象，类型转化也安全 RDD的缺点： 由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦 默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁 DataFrame的优点： 结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表 有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。 hive兼容，支持hql、udf等 DataFrame的缺点： 编译时不能类型转化安全检查，运行时才能确定是否有问题 对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象 DateSet的优点： DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据 和RDD一样，支持自定义对象存储 和DataFrame一样，支持结构化数据的sql查询 采用堆外内存存储，gc友好 类型转化安全，代码友好 如此回答有3个坑（容易引起面试官追问）： 1）Spark shuffle 与 MapReduce shuffle（或者Spark 与 MR 的区别） 2）Spark内存模型 3）对gc（垃圾回收）的了解 1.10 Spark 的通信机制分布式的通信方式 RPC RMI JMS EJB Web Serivice 通信框架Akka​ Hadoop MR中的计算框架，jobTracker和TaskTracker间是由于通过heartbeat的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色的高效的Akka和netty通信系统 1.11 Spark的数据容错机制一般而言，对于分布式系统，数据集的容错性通常有两种方式： 1） 数据检查点（在Spark中对应Checkpoint机制）。 2） 记录数据的更新（在Spark中对应Lineage血统机制）。 对于大数据分析而言，数据检查点操作成本较高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低，同时会消耗大量存储资源。 Spark选择记录更新的方式。但更新粒度过细时，记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列记录下来，以便恢复丢失的分区。 Lineage（血统）机制​ 每个RDD除了包含分区信息外，还包含它从父辈RDD变换过来的步骤，以及如何重建某一块数据的信息，因此RDD的这种容错机制又称“血统”（Lineage）容错。Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做以便恢复数据。 ​ 相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新计算和恢复丢失的数据分区。但这种数据模型粒度较粗，因此限制了Spark的应用场景。所以可以说Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能方面的提升。 ​ RDD在Lineage容错方面采用如下两种依赖来保证容错方面的性能： 窄依赖（Narrow Dependeny）：窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。其中，1个父RDD分区对应1个子RDD分区，可以分为如下两种情况： 子RDD分区与父RDD分区一一对应（如map、filter等算子）。一个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。 宽依赖（Wide Dependency，源码中称为Shuffle Dependency）： 宽依赖是指一个父RDD分区对应多个子RDD分区，可以分为如下两种情况： 一个父RDD对应所有子RDD分区（未经协同划分的Join）。 一个父RDD对应多个RDD分区（非全部分区）（如groupByKey）。 窄依赖与宽依赖关系如图3-10所示。 从图3-10可以看出对依赖类型的划分：根据父RDD分区是对应一个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，对于需要重新计算的子分区而言，只需要父分区的一部分数据，因此其余数据的重算就导致了冗余计算。 图3-10 两种依赖关系 对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，在通过重新计算恢复数据的情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。 窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点（前面调度机制中已讲过）。 依赖关系在lineage容错中的应用总结如下： 1）窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成，并且父RDD的计算结果进行hash并传到对应节点上之后，才能计算子RDD。 2）数据丢失时，对于窄依赖，只需要重新计算丢失的那一块数据来恢复；对于宽依赖，则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖时，需要在适当的时机设置数据检查点（checkpoint机制在下节讲述）。可见Spark在容错性方面要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。 在Spark容错机制中，如果一个节点宕机了，而且运算属于窄依赖，则只要重算丢失的父RDD分区即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。更深入地来说：在窄依赖关系中，当子RDD的分区丢失，重算其父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，因此不存在冗余计算。而在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区使用，其中有一部分数据对应的是其他不需要重新计算的子RDD分区中的数据，因此在宽依赖关系下，这样计算就会产生冗余开销，这也是宽依赖开销更大的原因。为了减少这种冗余开销，通常在Lineage血统链比较长，并且含有宽依赖关系的容错中使用Checkpoint机制设置检查点。 Checkpoint（检查点）机制通过上述分析可以看出Checkpoint的本质是将RDD写入Disk来作为检查点。这种做法是为了通过lineage血统做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。 1.13 Spark性能调优1) 常用参数说明--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可 --executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时处理大批量数据时容易内存不足，再多申请一点，如6G --num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等 --executor-cores 2 : 每个executor内的核数，即每个executor中的任务task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce任务的并行度是executor数目*executor中的任务数 yarn集群中一般有资源申请上限，如，executor-memory*num-executors &lt; 400G 等，所以调试参数时要注意这一点 —-spark.default.parallelism 200 ： Spark作业的默认为500~1000个比较合适,如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num-executors * executor-cores的2~3倍比较合适。 -- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor内存中能占的最大比例。默认值是0.6 —-spark.shuffle.memoryFraction 0.2 ： 设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2，如果shuffle聚合时使用的内存超出了这个20%的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能 —-spark.yarn.executor.memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存 2) Spark常用编程建议 避免创建重复的RDD，尽量复用同一份数据。 尽量避免使用shuffle类算子，因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子 用aggregateByKey和reduceByKey替代groupByKey,因为前两个是预聚合操作，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。 repartition适用于RDD[V], partitionBy适用于RDD[K, V] mapPartitions操作替代普通map，foreachPartitions替代foreach filter操作之后进行coalesce操作，可以减少RDD的partition数量 如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER 尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作 多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow spark中的Group/join/XXXByKey等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数 尽量保证每轮Stage里每个task处理的数据量&gt;128M 如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲 2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD) 若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores=1， executor-memory=m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores=size/m(向上取整) 如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以hash join的方式实现，具体原理参考下一节的shuffle过程 如果需要在repartition重分区之后还要进行排序，可直接使用repartitionAndSortWithinPartitions，比分解操作效率高，因为它可以一边shuffle一边排序 3) shuffle性能优化3.1 什么是shuffle操作 spark中的shuffle操作功能：将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join操作，类似洗牌的操作。这些分布在各个存储节点上的数据重新打乱然后汇聚到不同节点的过程就是shuffle过程。 3.2 哪些操作中包含shuffle操作 RDD的特性是不可变的带分区的记录集合，Spark提供了Transformation和Action两种操作RDD的方式。Transformation是生成新的RDD，包括map, flatMap, filter, union, sample, join, groupByKey, cogroup, ReduceByKey, cros, sortByKey, mapValues等；Action只是返回一个结果，包括collect，reduce，count，save，lookupKey等 Spark所有的算子操作中是否使用shuffle过程要看计算后对应多少分区： 若一个操作执行过程中，结果RDD的每个分区只依赖上一个RDD的同一个分区，即属于窄依赖，如map、filter、union等操作，这种情况是不需要进行shuffle的，同时还可以按照pipeline的方式，把一个分区上的多个操作放在同一个Task中进行 若结果RDD的每个分区需要依赖上一个RDD的全部分区，即属于宽依赖，如repartition相关操作（repartition，coalesce）、*ByKey操作（groupByKey，ReduceByKey，combineByKey、aggregateByKey等）、join相关操作（cogroup，join）、distinct操作，这种依赖是需要进行shuffle操作的 3.3 shuffle操作过程 shuffle过程分为shuffle write和shuffle read两部分 shuffle write： 分区数由上一阶段的RDD分区数控制，shuffle write过程主要是将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上（当前stage结束之后，每个task处理的数据按key进行分类，数据先写入内存缓冲区，缓冲区满，溢写spill到磁盘文件，最终相同key被写入同一个磁盘文件）创建的磁盘文件数量=当前stage中task数量*下一个stage的task数量 shuffle read：从上游stage的所有task节点上拉取属于自己的磁盘文件，每个read task会有自己的buffer缓冲，每次只能拉取与buffer缓冲相同大小的数据，然后聚合，聚合完一批后拉取下一批，边拉取边聚合。分区数由Spark提供的一些参数控制，如果这个参数值设置的很小，同时shuffle read的数据量很大，会导致一个task需要处理的数据非常大，容易发生JVM crash，从而导致shuffle数据失败，同时executor也丢失了，就会看到Failed to connect to host 的错误(即executor lost)。 shuffle过程中，各个节点会通过shuffle write过程将相同key都会先写入本地磁盘文件中，然后其他节点的shuffle read过程通过网络传输拉取各个节点上的磁盘文件中的相同key。这其中大量数据交换涉及到的网络传输和文件读写操作是shuffle操作十分耗时的根本原因 3.4 spark的shuffle类型 参数spark.shuffle.manager用于设置ShuffleManager的类型。Spark1.5以后，该参数有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2以前的默认值，Spark1.2之后的默认值都是SortShuffleManager。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 由于SortShuffleManager默认会对数据进行排序，因此如果业务需求中需要排序的话，使用默认的SortShuffleManager就可以；但如果不需要排序，可以通过bypass机制或设置HashShuffleManager避免排序，同时也能提供较好的磁盘读写性能。 HashShuffleManager流程： SortShuffleManager流程： 3.5 如何开启bypass机制 bypass机制通过参数spark.shuffle.sort.bypassMergeThreshold设置，默认值是200，表示当ShuffleManager是SortShuffleManager时，若shuffle read task的数量小于这个阈值（默认200）时，则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式写数据，但最后会将每个task产生的所有临时磁盘文件合并成一个文件，并创建索引文件。 这里给出的调优建议是，当使用SortShuffleManager时，如果的确不需要排序，可以将这个参数值调大一些，大于shuffle read task的数量。那么此时就会自动开启bypass机制，map-side就不会进行排序了，减少排序的性能开销，提升shuffle操作效率。但这种方式并没有减少shuffle write过程产生的磁盘文件数量，所以写的性能没有改变。 3.6 HashShuffleManager优化建议 如果使用HashShuffleManager，可以设置spark.shuffle.consolidateFiles参数。该参数默认为false，只有当使用HashShuffleManager且该参数设置为True时，才会开启consolidate机制，大幅度合并shuffle write过程产生的输出文件，对于shuffle read task 数量特别多的情况下，可以极大地减少磁盘IO开销，提升shuffle性能。参考社区同学给出的数据，consolidate性能比开启bypass机制的SortShuffleManager高出10% ~ 30%。 3.7 shuffle调优建议 除了上述的几个参数调优，shuffle过程还有一些参数可以提高性能： - spark.shuffle.file.buffer : 默认32M，shuffle Write阶段写文件时的buffer大小，若内存资源比较充足，可适当将其值调大一些（如64M），减少executor的IO读写次数，提高shuffle性能 - spark.shuffle.io.maxRetries ： 默认3次，Shuffle Read阶段取数据的重试次数，若shuffle处理的数据量很大，可适当将该参数调大。 3.8 shuffle操作过程中的常见错误 SparkSQL中的shuffle错误： org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 org.apache.spark.shuffle.FetchFailedException:Failed to connect to hostname/192.168.xx.xxx:50268 RDD中的shuffle错误： WARN TaskSetManager: Lost task 17.1 in stage 4.1 (TID 1386, spark050013): java.io.FileNotFoundException: /data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/2f/temp_shuffle_e22e013a-5392-4edb-9874-a196a1dad97c (没有那个文件或目录) FetchFailed(BlockManagerId(6083b277-119a-49e8-8a49-3539690a2a3f-S155, spark050013, 8533), shuffleId=1, mapId=143, reduceId=3, message= org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/0e/shuffle_1_143_0.data, offset=997061, length=112503} 处理shuffle类操作的注意事项： 减少shuffle数据量：在shuffle前过滤掉不必要的数据，只选取需要的字段处理 针对SparkSQL和DataFrame的join、group by等操作：可以通过 spark.sql.shuffle.partitions控制分区数，默认设置为200，可根据shuffle的量以及计算的复杂度提高这个值，如2000等 RDD的join、group by、reduceByKey等操作：通过spark.default.parallelism控制shuffle read与reduce处理的分区数，默认为运行任务的core总数，官方建议为设置成运行任务的core的2~3倍 提高executor的内存：即spark.executor.memory的值 分析数据验证是否存在数据倾斜的问题：如空值如何处理，异常数据（某个key对应的数据量特别大）时是否可以单独处理，可以考虑自定义数据分区规则，如何自定义可以参考下面的join优化环节 4) join性能优化Spark所有的操作中，join操作是最复杂、代价最大的操作，也是大部分业务场景的性能瓶颈所在。所以针对join操作的优化是使用spark必须要学会的技能。 spark的join操作也分为Spark SQL的join和Spark RDD的join。 4.1 Spark SQL 的join操作 4.1.1 Hash Join Hash Join的执行方式是先将小表映射成Hash Table的方式，再将大表使用相同方式映射到Hash Table，在同一个hash分区内做join匹配。 hash join又分为broadcast hash join和shuffle hash join两种。其中Broadcast hash join，顾名思义，就是把小表广播到每一个节点上的内存中，大表按Key保存到各个分区中，小表和每个分区的大表做join匹配。这种情况适合一个小表和一个大表做join且小表能够在内存中保存的情况。如下图所示： 当Hash Join不能适用的场景就需要Shuffle Hash Join了，Shuffle Hash Join的原理是按照join Key分区，key相同的数据必然分配到同一分区中，将大表join分而治之，变成小表的join，可以提高并行度。执行过程也分为两个阶段： shuffle阶段：分别将两个表按照join key进行分区，将相同的join key数据重分区到同一节点 hash join阶段：每个分区节点上的数据单独执行单机hash join算法 Shuffle Hash Join的过程如下图所示： 4.1.2 Sort-Merge Join SparkSQL针对两张大表join的情况提供了全新的算法——Sort-merge join，整个过程分为三个步骤： Shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式进行处理 sort阶段：对单个分区节点的两表数据，分别进行排序 merge阶段：对排好序的两张分区表数据执行join操作。分别遍历两个有序序列，遇到相同的join key就merge输出，否则继续取更小一边的key，即合并两个有序列表的方式。 sort-merge join流程如下图所示。 4.2 Spark RDD的join操作 Spark的RDD join没有上面这么多的分类，但是面临的业务需求是一样的。如果是大表join小表的情况，则可以将小表声明为broadcast变量，使用map操作快速实现join功能，但又不必执行Spark core中的join操作。 如果是两个大表join，则必须依赖Spark Core中的join操作了。Spark RDD Join的过程可以自行阅读源码了解，这里只做一个大概的讲解。 spark的join过程中最核心的函数是cogroup方法，这个方法中会判断join的两个RDD所使用的partitioner是否一样，如果分区相同，即存在OneToOneDependency依赖，不用进行hash分区，可直接join；如果要关联的RDD和当前RDD的分区不一致时，就要对RDD进行重新hash分区，分到正确的分区中，即存在ShuffleDependency，需要先进行shuffle操作再join。因此提升join效率的一个思路就是使得两个RDD具有相同的partitioners。 所以针对Spark RDD的join操作的优化建议是： 如果需要join的其中一个RDD比较小，可以直接将其存入内存，使用broadcast hash join 在对两个RDD进行join操作之前，使其使用同一个partitioners，避免join操作的shuffle过程 如果两个RDD其一存在重复的key也会导致join操作性能变低，因此最好先进行key值的去重处理 4.3 数据倾斜优化 均匀数据分布的情况下，前面所说的优化建议就足够了。但存在数据倾斜时，仍然会有性能问题。主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。 shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。 4.3.1 分析数据分布 如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。 4.3.2 数据倾斜的解决方案 这里参考美团技术博客中给出的几个方案。 1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。 2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key 3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量 4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。 5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。 6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作 5) 其他错误总结(1) 报错信息 java.lang.OutOfMemory, unable to create new native thread Caused by: java.lang.OutOfMemoryError: unable to create new native thread at java.lang.Thread.start0(Native Method) at java.lang.Thread.start(Thread.java:640) 解决方案： 上面这段错误提示的本质是Linux操作系统无法创建更多进程，导致出错，并不是系统的内存不足。因此要解决这个问题需要修改Linux允许创建更多的进程，就需要修改Linux最大进程数 （2）报错信息 由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实就是将/tmp目录挂载到内存当中, 那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误No Space Left on the device（Shuffle临时文件过多） 解决方案： 修改配置文件spark-env.sh,把临时文件引入到一个自定义的目录中去, 即: export SPARK_LOCAL_DIRS=/home/utoken/datadir/spark/tmp （3）报错信息 Worker节点中的work目录占用许多磁盘空间, 这些是Driver上传到worker的文件, 会占用许多磁盘空间 解决方案： 需要定时做手工清理work目录 （4）spark-shell提交Spark Application如何解决依赖库 解决方案： 利用–driver-class-path选项来指定所依赖的jar文件，注意的是–driver-class-path后如果需要跟着多个jar文件的话，jar文件之间使用冒号:来分割。 （5）内存不足或数据倾斜导致Executor Lost，shuffle fetch失败，Task重试失败等（spark-submit提交） TaskSetManager: Lost task 1.0 in stage 6.0 (TID 100, 192.168.10.37): java.lang.OutOfMemoryError: Java heap space INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.37:57139 (size: 42.0 KB, free: 24.2 MB) INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.38:53816 (size: 42.0 KB, free: 24.2 MB) INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 102, 192.168.10.37, ANY, 2152 bytes) 解决方案： 增加worker内存，或者相同资源下增加partition数目，这样每个task要处理的数据变少，占用内存变少 如果存在shuffle过程，设置shuffle read阶段的并行数 2. SparkSQL2.1 Spark SQL 的原理和运行机制 从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作 Parser 解析 SQL，生成 Unresolved Logical Plan 由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan Optimizer根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan CBO 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan Spark 以 DAG 的方法执行上述 Physical Plan 在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率 Parser Spark SQL 使用 Antlr 进行记法和语法解析，并生成 UnresolvedPlan。 当用户使用 SparkSession.sql(sqlText : String) 提交 SQL 时，SparkSession 最终会调用 SparkSqlParser 的 parsePlan 方法。该方法分两步 使用 Antlr 生成的 SqlBaseLexer 对 SQL 进行词法分析，生成 CommonTokenStream 使用 Antlr 生成的 SqlBaseParser 进行语法分析，得到 LogicalPlan Analyzer 从 Analyzer 的构造方法可见 Analyzer 持有一个 SessionCatalog 对象的引用 Analyzer 继承自 RuleExecutor[LogicalPlan]，因此可对 LogicalPlan 进行转换 Optimizer Spark SQL 目前的优化主要是基于规则的优化，即 RBO （Rule-based optimization） 每个优化以 Rule 的形式存在，每条 Rule 都是对 Analyzed Plan 的等价转换 RBO 设计良好，易于扩展，新的规则可以非常方便地嵌入进 Optimizer RBO 目前已经足够好，但仍然需要更多规则来 cover 更多的场景 优化思路主要是减少参与计算的数据量以及计算本身的代价 PushdownPredicatePushdownPredicate 是最常见的用于减少参与计算的数据量的方法。 SparkPlanner 得到优化后的 LogicalPlan 后，SparkPlanner 将其转化为 SparkPlan 即物理计划。 本例中由于 score 表数据量较小，Spark 使用了 BroadcastJoin。因此 score 表经过 Filter 后直接使用 BroadcastExchangeExec 将数据广播出去，然后结合广播数据对 people 表使用 BroadcastHashJoinExec 进行 Join。再经过 Project 后使用 HashAggregateExec 进行分组聚合。 至此，一条 SQL 从提交到解析、分析、优化以及执行的完整过程就介绍完毕。 2.3 Spark SQL 的优化策略1）内存列式存储与内存缓存表 Spark SQL可以通过cacheTable将数据存储转换为列式存储，同时将数据加载到内存缓存。cacheTable相当于在分布式集群的内存物化视图，将数据缓存，这样迭代的或者交互式的查询不用再从HDFS读数据，直接从内存读取数据大大减少了I/O开销。列式存储的优势在于Spark SQL只需要读出用户需要的列，而不需要像行存储那样每次都将所有列读出，从而大大减少内存缓存数据量，更高效地利用内存数据缓存，同时减少网络传输和I/O开销。数据按照列式存储，由于是数据类型相同的数据连续存储，所以能够利用序列化和压缩减少内存空间的占用。 2）列存储压缩 为了减少内存和硬盘空间占用，Spark SQL采用了一些压缩策略对内存列存储数据进行压缩。Spark SQL的压缩方式要比Shark丰富很多，如它支持PassThrough、RunLengthEncoding、DictionaryEncoding、BooleanBitSet、IntDelta、LongDelta等多种压缩方式，这样能够大幅度减少内存空间占用、网络传输和I/O开销。 3）逻辑查询优化 SparkSQL在逻辑查询优化（见图8-4）上支持列剪枝、谓词下压、属性合并等逻辑查询优化方法。列剪枝为了减少读取不必要的属性列、减少数据传输和计算开销，在查询优化器进行转换的过程中会优化列剪枝。 下面介绍一个逻辑优化的例子。 SELECT Class FROM （SELECT ID，Name，Class FROM STUDENT ） S WHERE S.ID=1 Catalyst将原有查询通过谓词下压，将选择操作ID=1优先执行，这样过滤大部分数据，通过属性合并将最后的投影只做一次，最终保留Class属性列。 4）Join优化 Spark SQL深度借鉴传统数据库的查询优化技术的精髓，同时在分布式环境下调整和创新特定的优化策略。现在Spark SQL对Join进行了优化，支持多种连接算法，现在的连接算法已经比Shark丰富，而且很多原来Shark的元素也逐步迁移过来，如BroadcastHashJoin、BroadcastNestedLoopJoin、HashJoin、LeftSemiJoin，等等。 下面介绍其中的一个Join算法。 BroadcastHashJoin将小表转化为广播变量进行广播，这样避免Shuffle开销，最后在分区内做Hash连接。这里使用的就是Hive中Map Side Join的思想，同时使用DBMS中的Hash连接算法做连接。 随着Spark SQL的发展，未来会有更多的查询优化策略加入进来，同时后续Spark SQL会支持像Shark Server一样的服务端和JDBC接口，兼容更多的持久化层，如NoSQL、传统的DBMS等。一个强有力的结构化大数据查询引擎正在崛起。 3. SparkStreaming3.1 原理剖析（源码级别）和运行机制 3.2 Spark Dstream 及其 API 操作 3.3 Spark Streaming 消费 Kafka 的两种方式 3.4 Spark 消费 Kafka 消息的 Offset 处理 3.5 窗口操作 4. SparkMlib可实现聚类、分类、推荐等算法 三. Flink Flink 集群的搭建 Flink 的架构原理 Flink 的编程模型 Flink 集群的 HA 配置 Flink DataSet 和 DataSteam API 序列化 Flink 累加器 状态 State 的管理和恢复 窗口和时间 并行度 Flink 和消息中间件 Kafka 的结合 Flink Table 和 SQL 的原理和用法 四. Kafka1. Kafka 的设计Kafka 将消息以 topic 为单位进行归纳 将向 Kafka topic 发布消息的程序成为 producers. 将预订 topics 并消费消息的程序成为 consumer. Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker. producers 通过网络将消息发送到 Kafka 集群，集群向消费者提供消息 2. 数据传输的三种事务定义数据传输的事务定义通常有以下三种级别： （1）最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输 （2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输. （3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而 且仅仅被传输一次，这是大家所期望的 3. Kafka 判断一个节点是否活着两大条件（1）节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连 接 （2）如果节点是个 follower,他必须能及时的同步 leader 的写操作，延时不能太久 4. Kafa consumer 是否可以消费指定分区消息？​ Kafa consumer 消费消息时，向 broker 发出”fetch”请求去消费特定分区的消息，consumer 指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer 拥有 了 offset 的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的 5. Kafka 消息是采用 Pull 模式or Push 模式？​ Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统 的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息 一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的 consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的 consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消 息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时， consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式 ​ Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push 模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还 是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一 次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决 定这些策略 ​ Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询， 直到新消息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达 (当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发 6. Kafka 存储在硬盘上的消息格式是什么？消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和 CRC32 校验码。 消息长度: 4 bytes (value: 1+4+n) 版本号: 1 byte CRC 校验码: 4 bytes 具体的消息: n bytes 7. Kafka 高效文件存储设计特点(1).Kafka 把 topic 中一个 parition 大文件分成多个小文件段，通过多个小文件段，就容易定 期清除或删除已经消费完文件，减少磁盘占用。 (2).通过索引信息可以快速定位 message 和确定 response 的最大大小。 (3).通过 index 元数据全部映射到 memory，可以避免 segment file 的 IO 磁盘操作。 (4).通过索引文件稀疏存储，可以大幅降低 index 文件元数据占用空间大小。 8. Kafka 与传统消息系统之间有三个关键区别(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留 (2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据 提升容错能力和高可用性 (3).Kafka 支持实时的流式处理 9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中 副本因子不能大于 Broker 的个数； 第一个分区（编号为 0）的第一个副本放置位置是随机从 brokerList 选择的； 其他分区的第一个副本放置位置相对于第 0 个分区依次往后移。也就是如果我们有 5 个Broker，5 个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个Broker 上，依次类推； 剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的 10. Kafka 新建的分区会在哪个目录下创建在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录， 这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘 上用于提高读写性能。 当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个 目录下创建文件夹用于存放数据。 但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？ 答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic 名+分区 ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就 是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁 盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。 11. partition 的数据如何保存到硬盘topic 中的多个 partition 以文件夹的形式保存到 broker，每个分区序号从 0 递增， 且消息有序 Partition 文件下有多个 segment（xxx.index，xxx.log） segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为 1g 如果大小大于 1g 时，会滚动一个新的 segment 并且以上一个 segment 最后一条消息的偏移 量命名 12. kafka 的 ack 机制request.required.acks 有三个值 0 1 -1 0:生产者不会等待 broker 的 ack，这个延迟最低但是存储的保证最弱当 server 挂掉的时候 就会丢数据 1：服务端会等待 ack 值 leader 副本确认接收到消息后发送 ack 但是如果 leader 挂掉后他 不确保是否复制完成新 leader 也会导致数据丢失 -1：同样在 1 的基础上 服务端会等所有的 follower 的副本受到数据后才会受到 leader 发出 的 ack，这样数据不会丢失 13. Kafka 的消费者如何消费数据​ 消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置 等到下次消费时，他会接着上次位置继续消费 14. 消费者负载均衡策略​ 一个消费者组中的一个分片对应一个消费者成员，他能保证每个消费者成员都能访问，如 果组中成员太多会有空闲的成员 15. 数据有序​ 一个消费者组里它的内部是有序的 ​ 消费者组与消费者组之间是无序的 16. kafaka 生产数据时数据的分组策略​ 生产者决定数据产生到集群的哪个 partition 中 ​ 每一条消息都是以（key，value）格式 ​ Key 是由生产者发送数据传入 ​ 所以生产者（key）决定了数据产生到集群的哪个 partition 五. 数据仓库5.1 数仓概念相关1. 数据仓库、数据集市、数据库之间的区别 数据仓库 ：数据仓库是一个面向主题的、集成的、随时间变化的、但信息本身相对稳定的数据集合，用于对管理决策过程的支持。是企业级的，能为整个企业各个部门的运行提供决策支持手段； 数据集市：则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。 数据库：是一种软件，用来实现数据库逻辑过程，属于物理层； 数据仓库是数据库概念的升级，从数据量来说，数据仓库要比数据库更庞大德多，主要用于数据挖掘和数据分析，辅助领导做决策 只是数据库内的数据时限要远远的长于操作型环境中的数据时限。在操作型环境中一般只保存有6090天的数据，而在数据仓库中则要需要保存较长时限的数据（例如：510年），以适应DSS进行趋势分析的要求。 2. OLAP、OLTP概念及用途 OLAP: 即On-Line Analysis Processing在线分析处理。 OLAP的特点：联机分析处理的主要特点，是直接仿照用户的多角度思考模式，预先为用户组建多维的数据模型，维指的是用户的分析角度。 OLTP: 即On-Line Transaction Processing联机事务处理过程(OLTP) OLTP的特点：结构复杂、实时性要求高。 OLAP和OLTP区别 1、基本含义不同：OLTP是传统的关系型数据库的bai主要应用，主要是基本的、日常的事务处理，记du录即时的增、删、改、查，比如在银行存取一笔款，就是一个事务交易。OLAP即联机分析处理，是数据仓库的核心部心，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。典型的应用就是复杂的动态报表系统。 2、实时性要求不同：OLTP实时性要求高，OLTP 数据库旨在使事务应用程序仅写入所需的数据，以便尽快处理单个事务。OLAP的实时性要求不是很高，很多应用顶多是每天更新一下数据。 3、数据量不同：OLTP数据量不是很大，一般只读/写数十条记录，处理简单的事务。OLAP数据量大，因为OLAP支持的是动态查询，所以用户也许要通过将很多数据的统计后才能得到想要知道的信息，例如时间序列分析等等，所以处理的数据量很大。 4、用户和系统的面向性不同：OLTP是面向顾客的,用于事务和查询处理。OLAP是面向市场的,用于数据分析。 5、数据库设计不同：OLTP采用实体-联系ER模型和面向应用的数据库设计。OLAP采用星型或雪花模型和面向主题的数据库设计。 3. 事实表、维度表、拉链表概念及区别 事实表：事实表其实质就是通过各种维度和一些指标值得组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。每行数据代表一个业务事件，（下单、支付、退款、评价等） 。“事实”这个术语表示的是业务事件的度量值（可统计次数、个数、金额等） 事务型事实表 周期快照型事实表 累积快照型事实表 维度表：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。 拉链表：拉链表，它是一种维护历史状态，以及最新状态数据的一种表。拉链表也是分区表，有些不变的数据或者是已经达到状态终点的数据就会把它放在分区里面，分区字段一般为开始时间：start_date和结束时间：end_date。一般在该天有效的数据，它的end_date是大于等于该天的日期的。获取某一天全量的数据，可以通过表中的start_date和end_date来做筛选，选出固定某一天的数据。例如我想取截止到20190813的全量数据，其where过滤条件就是where start_date&lt;=’20190813’ and end_date&gt;=20190813。 拉链表使用场景和实现方式？ 【yy总结】 拉链表使用场景：需要查看历史某一时间节点的状态，同时考虑到存储空间。 实现方式： 首先是拉链表dw_order_his的设置，有start_date和end_date两个字段； 其次在ods层创建一个ods_order_update表，储存当变化数据（包括insert和update的数据） 源表（order） ods_order_update表和dw_order_his表的交集进行封链操作，end_date=current_date ods_oder_update数据插入到his表中，对于记录的end_date=9999-12-31,start_date=current_date 【使用场景】 在数据仓库的数据模型设计过程中，经常会遇到下面这种表的设计：  有一些表的数据量很大，比如一张用户表，大约10亿条记录，50个字段，这种表，即使使用ORC压缩，单张表的存储也会超过100G，在HDFS使用双备份或者三备份的话就更大一些。  表中的部分字段会被update更新操作，如用户联系方式，产品的描述信息，订单的状态等等。  需要查看某一个时间点或者时间段的历史快照信息，比如，查看某一个订单在历史某一个时间点的状态。  表中的记录变化的比例和频率不是很大，比如，总共有10亿的用户，每天新增和发生变化的有200万左右，变化的比例占的很小。 【实现方式】 全量主要数据表加载的策略为每次加载时需要根据主键将目标表的数据与源表数据进行比对，删除目标表中在源数据表中的相关记录，然后将源表数据全部插入目标表。表现在脚本上为先delete相关记录，然后insert所有记录。主表加载策略主要用于大部分主表的加载，比如客户信息等主要数据表。 增量拉链是指每次加载时，将源表数据视为增量抽取后的结果，加载到目标表时需要考虑数据历史情况。一般数据发生变化时关闭旧数据链，然后开新数据链。增量拉链针对的是历史表情况，由于数据仓库中记录了大部分数据历史表变化情况，因此增量拉链加载策略在数据仓库中是使用比较广泛的一种加载策略。通常这种历史表都含有start_date和end_date字段，首先全字段对比源数据和目标表得出真正的增量数据，这里的全字段不包含start_date和end_date字段，然后根据主键对目标表进行关旧链操作，然后对新增数据开新链，这种拉链策略同样可以处理全量数据。 【拉链表性能优化】 拉链表当然也会遇到查询性能的问题，比如说我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了，个人认为两个思路来解决： 在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。 保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。 4. 全量表、增量表、快照表概念及区别 全量表：全量表没有分区，表中的数据是前一天的所有数据，比如说今天是24号，那么全量表里面拥有的数据是23号的所有数据，每次往全量表里面写数据都会覆盖之前的数据，所以全量表不能记录历史的数据情况，只有截止到当前最新的、全量的数据。 增量表：增量表，就是记录每天新增数据的表，比如说，从24号到25号新增了那些数据，改变了哪些数据，这些都会存储在增量表的25号分区里面。上面说的快照表的25号分区和24号分区（都是t+1，实际时间分别对应26号和25号），它两的数据相减就是实际时间25号到26号有变化的、增加的数据，也就相当于增量表里面25号分区的数据。 快照表：那么要能查到历史数据情况又该怎么办呢？这个时候快照表就派上用途了，快照表是有时间分区的，每个分区里面的数据都是分区时间对应的前一天的所有全量数据，比如说当前数据表有3个分区，24号，25号，26号。其中，24号分区里面的数据就是从历史到23号的所有数据，25号分区里面的数据就是从历史到24号的所有数据，以此类推。 5. 什么叫维度和度量值 维度：说明数据，维度是指可指定不同值的对象的描述性属性或特征。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。 度量：事实表和维度交叉汇聚的点，度量和维度构成OLAP的主要概念，这里面对于在事实表或者一个多维立方体里面存放的数值型的、连续的字段，就是度量。这符合上面的意思，有标准，一个度量字段肯定是统一单位，例如元、户数。如果一个度量字段，其中的度量值可能是欧元又有可能是美元，那这个度量可没法汇总。在统一计量单位下，对不同维度的描述。 6. 什么叫缓慢变化维（Slowly Changing Dimensions，SCD)​ 维度建模的数据仓库中，有一个概念叫Slowly Changing Dimensions，中文一般翻译成缓慢变化维，经常被简写为SCD。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随时间发生变化的维度我们一般称之为缓慢变化维，并且把处理维度表的历史变化信息的问题称为处理缓慢变化维的问题，有时也简称为处理SCD的问题。 处理缓慢变化维的方法通常分为三种方式： 第一种方式是直接覆盖原值。这样处理，最容易实现，但是没有保留历史数据，无法分析历史变化信息。第一种方式通常简称为“TYPE 1”。 第二种方式是添加维度行。这样处理，需要代理键的支持。实现方式是当有维度属性发生变化时，生成一条新的维度记录，主键是新分配的代理键，通过自然键可以和原维度记录保持关联。第二种方式通常简称为“TYPE 2”。例如将当前行的状态设置为off，并设置一个endtime时间戳，将当前时间标记上。同时新增1行，将其状态标记为on，设置begintime时间戳为上一个记录的endtime+1。 第三种方式是添加属性列。这种处理的实现方式是对于需要分析历史信息的属性添加一列，来记录该属性变化前的值，而本属性字段使用TYPE 1来直接覆盖。这种方式的优点是可以同时分析当前及前一次变化的属性值，缺点是只保留了最后一次变化信息。第三种方式通常简称为“TYPE 3”。 6.3 维度表Ø 维度表可以看作是用户来分析数据的窗口，Ø 维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息，Ø 维度表包含帮助汇总数据的特性的层次结构。 6.4 维度分类维度的类型: 缓慢变化维(Slowly Changing Dimension) 快速变化维(Rapidly Changing Dimension) 大维(Huge Dimension)和迷你维(Mini-Dimension) 退化维(Degenerate Dimension) **缓慢变化维(SCD):大多数的维度的内容都会有不同程度的改变。比如：雇员的升职客户更改了他的名称或地址我们如何去处理这些维度中的变化呢？下面提供了三个处理缓慢变化维的方式直接更新到原先记录中标记记录有效时间的开始日期和结束日期，加入版本控制在记录中添加一个字段来记录历史 *快速变化维(FCD):当某个维度的变化是非常快的时候，我们认定他为快速变化维(具体要看实际的变化频率)，比如：产品的价格，地产的价格等例如在一个分析用户如何使用搜索引擎的DW项目中，将用户搜索的关键字作为一个维度。由于用户使用的关键字会快速变化，因此关键字维度中的数据量会迅速增加。另外一个例子就是精度为秒的时间维度，每秒就会增加一个维度值 通常RCD的处理可以分为3类： Rapidly Changing Small Dimensions – 即维度表字段并不多，表的数据量也不大的情况。这种情形应用SCD中的Type2就可以了。(即：新增一行，旧行置过期)Rapidly Changing Large Dimensions – 即维度表字段较多，表的数据量较大的情况。这种情形Type2会增加过多的行并导致效率降低，因此通常采用Type3.（新增列：仅保存上一个值previous_value,current_value）Rapidly Changing Monster Dimensions – 最糟糕的情况，即维度表字段较多，表的数据量很大，且维度表中的一部分字段频繁变化的情况。此时应将相对稳定的字段和频繁变化的字段分割开，频繁变化的字段独立出来形成新的维度表与事实表相连或形成新的雪花关系。（维表分离） 大维度(HugeDimension): 数据仓库中最有意思的维度是一些非常大的维度，比如客户，产品等等。一个大的企业客户维度往往有上百万记录，每条记录又有上百个字段。而大的个人客户维度则会超过千万条记录，这些个人客户维度有时也会有十多个字段，但大多数时候比较少见的维度也只有不多的几个属性。大维度需要特殊的处理。由于数据量大，很多涉及大维度数据仓库功能可能会很慢，效率很低。你需要采用高效率的设计方法、选择正确的索引、或者采用其它优化技术来处理以下问题，包括：向大维度表填充数据非限制维度的浏览性能，尤其是那些属性较少的维度多限制的维度属性值的浏览时间涉及大维度表的对事实表查询的低效率问题为处理第二类修改所需要增加的额外的记录 迷你维(MiniDimension): 将常用的大维度中的少数字段提取出来，形成一个字段少的维度，在查询的时候便可以使用迷你维中的字段这样的设计明显提高查询效率 普通维: 普通维是基于一个维表的维度，由维表中的不同列来表示维度中的不同级别。 雪花维： 雪花维是基于多个维表的维度，各个维表间以外键关联，分别存储在同一维度中不同级别的成员列值。 父子维： 父子维是基于两个维表列的维度，由维表中的两列来共同定义各个成员的隶属关系，一列称为成员键列，标识每个成员，另一列称为父键列，标识每个成员的父代。父子维度通俗的话来讲，这个表是自反 的，即外键本身就是引用的主键；类似这样的关系，如公司组织结构，分公司是总公司的一部分，部门是分公司的一部分，当然如果定义得好的话员工是部门的一部 分；通常公司的组织架构并非处在等层次上的，例如总公司下面的部门看起来就和分公司是一样的层次。因此父子维的层次通常不固定的。因为父子维的复杂的自引用关系，如果按照缓慢维度的全历史记录方式来处理，必然导致逻辑关系混乱，处理起来比较棘手；任何一个组织的变动 (修改名称，更改引用，新增等等操作 )将会引起其下属节点相应的变动；任何一个意外都会导致整个结构的变化，同时发生意外后所带来的逻辑关系很难理顺。而 SQLServer2000中 Analysis Service对于这种急剧的变化处理并不稳定。因此建议按照缓慢变化维的覆盖方式解决，即只根据主键这个唯一标志进行判断是否是新增还是修改。索引：与在其他关系数据库中一样，索引对数据仓库的性能具有重要作用。每个维度表都必须在主键上建立索引。在其他列如标识层次结构级别的列上，索引对于某些专用查询的性能也很游泳。事实数据表必须在由维度表外键构成的组件主键上建立索引。粒度(Grain) 层次(Hierarchy)：Ø 粒度是指数据仓库的数据单位中保存数据的细化或综合程度的级别。细化程度越高，粒度级就越小；相反，细化程度越低，粒度级就越大。设计粒度是设计数据仓库中的一个重要的前提Ø 层次指描述明细数据的层次一些影响维度建模的因素:Ø 数据或展现的安全性Ø 复杂的查询和分析 5.2 数仓分层设计1. 数据仓库分为5层： ODS层 （原始数据层） BDM DWD层 （明细数据层） FDM DWS层 （服务数据层） GDM DWT层（数据主题层） ADM ADS层 （数据应用层） APP 2. 各层主要负责职责 ODS层（原始数据层）：存放原始数据，直接加载原始日志、数据，数据保存原貌不做处理。 DWD层（明细数据层）：结构与粒度原始表保持一致，对ODS层数据进行清洗（去除空值、脏数据、超过极限范围的数据） DWS层 （服务数据层）：以DWD为基础，进行轻度汇总 DWT层（数据主题层）：以DWS为基础，进行累积汇总 ADS层 （数据应用层）：为各种统计报表提供数据 3. **为什么要分层？** 减少重复开发：规范数据分层， 通过的中间层数据， 能够减少极大的重复计算， 增加一次计算结果的复用性。 把复杂问题简单化：一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复 隔离原始数据：不论是数据的异常还是数据的敏感性， 使真实数据与统计数据解耦开 4. 数仓中每层表的建模？怎么建模？（1）ODS： 特点是保持原始数据的原貌，不作修改！ 原始数据怎么建模，ODS就怎么建模！举例： 用户行为数据特征是一条记录就是一行！ ODS层表(line string) 业务数据，参考Sqoop导入的数据类型进行建模！ （2）DWD层：特点从ODS层，将数据进行ETL（清洗），轻度聚合，再展开明细！ 在展开明细时，对部分维度表进行降维操作 例如：将商品一二三级分类表，sku商品表，spu商品表，商品品牌表合并汇总为一张维度表！ 对事实表，参考星型模型的建模策略，按照选择业务过程→声明粒度→确认维度→确认事实思路进行建模 选择业务过程： 选择感兴趣的事实表声明粒度： 选择最细的粒度！可以由最细的粒度通过聚合的方式得到粗粒度！确认维度： 根据3w原则确认维度，挑选自己感兴趣的维度确认事实： 挑选感兴趣的度量字段，一般是从事实表中选取！ DWS层： 根据业务需求进行分主题建模！一般是建宽表！ DWT层： 根据业务需求进行分主题建模！一般是建宽表！ ADS层： 根据业务需求进行建模！ 5.3 数仓建模1. 维度建模概念、类型、过程维度建模：维度建模是一种将数据结构化的逻辑设计方法，它将客观世界划分为度量和上下文。度量是常常是以数值形式出现，事实周围有上下文包围着，这种上下文被直观地分成独立的逻辑块，称之为维度。它与实体-关系建模有很大的区别，实体-关系建模是面向应用，遵循第三范式，以消除数据冗余为目标的设计技术。维度建模是面向分析，为了提高查询性能可以增加数据冗余，反规范化的设计技术。 维度建模过程：确定业务流程-&gt;确定粒度-&gt;确定纬度-&gt;确定事实 建模四步走： 1.选取要建模的业务处理流程 关注业务处理流程，而不是业务部门！ 2.定义业务处理的粒度 “如何描述事实表的单个行？” 3.选定用于每个事实表行的维度 常见维度包括日期、产品等 4.确定用于形成每个事实表行的数字型事实 典型的事实包括订货量、支出额这样的可加性数据 2. 星型模型和雪花模型概念、区别​ 在维度建模的基础上又分为三种模型： 星型模型、 雪花模型、 星座模型。 星型模型：雪花模型与星型模型的区别主要在于维度的层级，标准的星型模型维度只有一层，而雪花模型可能会涉及多级。 雪花模型： 比较靠近3NF， 但是无法完全遵守， 因为遵循3NF的性能成本太高。 星座模型：星座模型与前两种情况的区别是事实表的数量， 星座模型是基于多个事实表。基本上是很多数据仓库的常态， 因为很多数据仓库都是多个事实表的。 所以星座不星座只反映是否有多个事实表， 他们之间是否共享一些维度表。所以星座模型并不和前两个模型冲突。 如何选择 ​ 模型的选择首先就是星座不星座这个只跟数据和需求有关系， 跟设计没关系， 不用选择。星型还是雪花， 取决于性能优先， 还是灵活更优先。​ 目前实际企业开发中， 不会绝对选择一种， 根据情况灵活组合， 甚至并存（一层维度和多层维度都保存） 。 但是整体来看， 更倾向于维度更少的星型模型。 尤其是Hadoop体系， 减少Join就是减少Shuffle， 性能差距很大。（关系型数据可以依靠强大的主键索引） 总结 通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。 5. 4 数据治理1）数据压缩 2）小文件合并 3）冷数据处理 5.5 总线架构 总线架构 事实一致性 维度一致性 5.6 数仓总结![image-20210220171930469](/Users/liyu/Library/Application Support/typora-user-images/image-20210220171930469.png) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Job","slug":"Job","permalink":"https://dataquaner.github.io/categories/Job/"}],"tags":[{"name":"Job","slug":"Job","permalink":"https://dataquaner.github.io/tags/Job/"}]},{"title":"数据可视化分析平台开源方案集锦","slug":"2021-02-11-数据可视化分析平台开源方案集锦","date":"2021-02-11T01:25:00.000Z","updated":"2021-02-15T03:21:39.690Z","comments":true,"path":"2021/02/11/2021-02-11-shu-ju-ke-shi-hua-fen-xi-ping-tai-kai-yuan-fang-an-ji-jin/","link":"","permalink":"https://dataquaner.github.io/2021/02/11/2021-02-11-shu-ju-ke-shi-hua-fen-xi-ping-tai-kai-yuan-fang-an-ji-jin/","excerpt":"","text":"B/S 架构的数据可视化分析平台开源方案不完全集锦，供各位参考。 排名不分先后。欢迎补充。 kibana Elasticsearch 专用的数据分析检索仪表盘。ELK Stack 中的 K。 日志系统常见的可视化开源解决方案。 使用 Nodejs+AnglarJs+React 开发,元数据存储在 ES 的一个索引中。 Elastic公司维护开源，社区非常活跃，持续迭代中。 grafana 可视化仪表盘和图形编辑器，是一款常用的指标分析和监控工具。支持Graphite、Elasticsearch、OpenTSDB、Prometheus 和 InfluxDB 作为数据源。 使用 Golang+TypeScript+AngularJS 开发，元数据支持 mysql 和 postgres。 Grafana Labs 公司维护，社区非常活跃，持续迭代中。 Superset 孵化中的准企业级 BI 应用。 很多大公司都在内部使用。 支持的数据源有 MySQL、Postgres、Vertica、Oracle、Microsoft SQL Server、SQLite、Greenplum、Firebird、MariaDB、Sybase、IBM DB2、Exasol、MonetDB、Snowflake、Redshift、Clickhouse、Apache Kylin 等！ 使用 Python+Flask+react+jQuery开发，默认使用 sqlite 存储元数据。 由Airbnb开源，现已归属于 Apache 孵化项目，社区非常活跃，持续迭代中。 Zeppelin 支持交互式数据分析的多用途 notebook 编辑器工具，可以接入不同的数据处理引擎和解释器，包括 Apache Spark，Python，JDBC，Markdown和Shell 等。内置Apache Spark集成。 Java+Angular 开发，元数据 notebook 默认使用本地文件系统存储在git仓库中。 由 Apache 开源，持续迭代中，目前版本 0.8。 Hue 开发和访问SQL、数据应用的工作台，支持智能的SQL和任务编辑器、Dashboard 、任务工作流调度、数据浏览器。 Hadoop生态系统可视化利器。 SQL支持： Hive、Impala、MySQL、Oracle、KSQL / Kafka SQL、Solr SQL、Presto、PostgreSQL、Redshift、BigQuery、AWS Athena、Spark SQL、Phoenix、Kylin等。 任务支持：MapReduce、Java、Pig、Sqoop、Shell、DistCp、Spark等。 使用Python+Django+jquery 开发，元数据默认使用 SQLite存储。 Hue 由 Cloudera Desktop 演化而来，最后 Cloudera 公司将其贡献给Apache基金会的Hadoop社区。 CBoard 国产BI 报表和dashboard平台。 支持JDBC数据源，Saiku2.x数据源，Kylin1.6，Elasticsearch 1.x, 2.x, 5.x。 使用 Java Spring+MyBatis+AngularJS+Bootstrap 开发。元数据使用MySQL5+/SQLServer。 上海楚国公司开源，最近发现官方出了收费的企业版，这个社区版显得low了很多。 Mining![img](data:image/svg+xml;utf8,) Python写的BI应用（Pandas web 界面） OpenMining 支持基于 ORM SQLAlchemy 的所有数据库。 使用 Python+Lua+AngularJs+jQuery开发，元数据存储在MongoDB。 由Avelino 和 UP! Essência开发，master分支的最新 commit 已经是2016年了 Saiku 经典的OLAP开源方案，Saiku是一个模块化分析套件，提供轻量级OLAP，易于嵌入，可扩展和可配置。 支持 Mondrian, XMLA 或者 Mongo数据源链接类型。 其提供一个Schema设计器、交互式的报表引擎、展示板和nosql连接技术。使用REST API连接OLAP系统。 使用Java+backbone+jQuery开发，使用JackRabbit管理树状元数据。 最初叫做Pentaho分析工具，起初是基于OLAP4J库用GWT（google web toolkit）包装的一个前端分析工具。后改名Saiku，Analytical Labs 提供支持。 Metabase 简单快速的方式使用BI和分析。支持Postgres、MySQL、Druid、SQL Server、Redshift、MongoDB、Google BigQuery、SQLite、H2、Oracle、Vertica、Presto、Snowflake。支持不写SQL 的方式做可视化分析。支持docker、jar包方式安装。 使用clojure和node开发，前端使用react框架。元数据默认存储在H2数据库中。 社区较为活跃，项目也在持续更新中。 redash![img](data:image/svg+xml;utf8,) SQL editor+可视化，支持35种数据源：Amazon Athena、Amazon DynamoDB、Amazon Redshift、Axibase Time Series Database、Cassandra、ClickHouse、CockroachDB、CSV、Databricks、DB2 by IBM、Druid、Elasticsearch、Google Analytics、Google BigQuery、Google Spreadsheets、Graphite、Greenplum、Hive、Impala、InfluxDB、JIRA、JSON、Apache Kylin、MapD、MemSQL、Microsoft SQL Server、MongoDB、MySQL、Oracle、PostgreSQL、Presto、Prometheus、Python、Qubole、Rockset、Salesforce、ScyllaDB、Shell Scripts、Snowflake、SQLite、TreasureData、Vertica、Yandex AppMetrrica、Yandex Metrica。 后端使用Python 前端使用Angular、React，元数据环境使用PostgreSQL &amp; Redis。 该项目目前也比较活跃，持续迭代中。 SqlPad 不知放在这里是否合适，SqlPad一款基于web 的 SQL 编辑器，支持MySQL, Postgres, SQL Server, Vertica, Crate, Presto, SAP HANA, 和 Cassandra，支持数据可视化。但不支持仪表板等功能。 使用Nodejs+React开发，元数据存储在 Nedb中。 由Rick Bergfalk开发，持续维护中。 结语这些是我收集或调研过的一些数据可视化开源方案，它们或许在成熟稳定的企业级方案面前显得支离杂乱，也或许在牛人遍地的大厂内部显得不够专业。但它们开发者给提供了优秀的参考案例和二次开发的母版，给小企业带来了几乎免费的数据分析和可视化的能力。由衷的感谢这些令人兴奋的项目，感谢为开源奉献的人们。 由于本人没有全部体验和深入调研上述项目，上述简介仅供参考，以官方为准。 作者：磊仔🙈🙈🙈🙈🙈🙈链接：https://juejin.cn/post/6844903760867622920来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/categories/Data-Analysis/"}],"tags":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/tags/Data-Analysis/"},{"name":"BI","slug":"BI","permalink":"https://dataquaner.github.io/tags/BI/"},{"name":"Superset","slug":"Superset","permalink":"https://dataquaner.github.io/tags/Superset/"}]},{"title":"如何构建用户标签体系（转载）","slug":"如何构建用户标签体系","date":"2020-12-27T06:35:00.000Z","updated":"2020-12-28T07:38:32.252Z","comments":true,"path":"2020/12/27/ru-he-gou-jian-yong-hu-biao-qian-ti-xi/","link":"","permalink":"https://dataquaner.github.io/2020/12/27/ru-he-gou-jian-yong-hu-biao-qian-ti-xi/","excerpt":"","text":"亚马逊的CEO Jeff Bezos曾说过他的梦想，「如果我有一百万的用户，我就会做一百万个不同的网站！」，做这个基础是先对用户打标签。 而目前基于标签的智能推荐系统，已经有了成熟商业应用，比如：淘宝的千人千面，美团外卖的智能推荐，腾讯的社交广告。 一、思考的背景从16年开始，互联网用户增长趋缓，同比仅增长。一方面，不论是线上还是线下，新用户的获取成本都很高。另一方面，用户时间增长也在趋缓。在用户花费时间趋向饱和情况下，不同的产品之间同样存在竞争关系。 在这个背景下，随着用户量增长，运营人员面临新的挑战，有以下核心诉求： 一般运营活动中，怎么对不同用户群体分层，提高流量的分发效率？ 对于个体用户，怎么深入到日常使用场景，提高流量的转化效率？ 落到产品设计层面，需要解决以下问题： 怎么设计一个完善的用户标签体系？怎么打标签？打哪些标签？谁来打？ 怎么使用用户标签，创造商业价值？ 二、怎样建立用户的标签体系？讲到用户运营，我觉得有两项基本工作是可以拿出来讲讲的： 一个是用户触达体系，一个是用户成长激励体系。 用户触达简单来说就是给用户推送提醒、活动、召回等各类消息，加上标签化，就可以更有针对性及个性化的为用户推送； 而用户成长激励就是我们在各类APP上常见的新手任务、日常任务、营销活动、会员体系等等；两者亦有相辅相成的作用。 其实，这两项内容的基础建设做起来并不难，但由于网上缺少这类实操案例的讲解，让很多初次上手去做的运营缺少认知和了解（比如：我早期需要独立完成这些工作的时候，就搜不到很清楚的内容能够帮助我），导致接到任务在构思时，缺少参考内容。 所以，这次就跟大家聊聊这两块的经验，希望对你所有启发。 2.1 做标签体系前先理清“用户类型”首先，运营推送分两种，一种我们叫全量推送，比如节日活动、产品重大更新等，我们需要尽可能多的覆盖用户；所以这种直接通过后台push、短信、公众号等就可以操作。 而另一种是精准推送，比如新手引导、沉默预警、流失召回等行为，我们需要更有针对性的进行推送，但在推送之前，我们肯定要事先知道“这些用户”是谁，对吧。 因此，用户标签体系工作的开展逻辑就分为三部分，我们运营先理清楚这个“人”，然后通过“条件”标签化定义这个人，最后是策略“触达”这个人。 给谁推送（运营需求）&gt;如何定义这个人（技术实现）- 何时触发推送（ 运营制定） 题外话：我们运营在上手执行任何工作之前，一定要自我事先理清楚任务的目的、逻辑和顺序，这样才不会忙一两天的作业交上去，又被领导打回来改来改去，好像领导不理解你的想法或者你做完成却发现不理解领导到底想要什么；原因就是虽然你的执行效率上去了，但却没留时间让自己思考这到底是为什么。 现在流行的精细化运营，需要我们对用户进行多维度多方面的划分；我就不详细列举了，这里概以我们常用到的用户分层为例讲解。 那不同的产品根据业务类型不同，对用户层级的划分会有不同的界定，以我们家的工具产品为例。大致可分为四层：新增用户-活跃用户-会员用户-核心用户 2.2 做标签体系前理清“推送需求”通过用户分层，我们明确了想要推送的四类用户，那下一步要思考的是，我要针对不同层级的用户哪些行为进行推送呢？例如： 新增用户：作为运营，我想在用户注册后推送一条欢迎&amp;引导上手文案，并且希望在当天内触成用户完成核心功能的体验，三天内督促完善新手任务；这期间我就需要根据用户行为和完成状态的不同，推送不同的文案； 活跃用户：作为运营，我想能够判断出当前用户的活跃次数&amp;天数，以此发放不同的优惠政策，看是否通过不同的营销切入点，最大化转化非会员用户； 再如分群：运营希望给当前的月卡且在本月将到期的会员用户，单独推送到期提醒及续费优惠活动等。 以此类推，那可能有人要问了，为什么做标签体系之前，我自己要梳理的这么清楚？直接按照人口属性、行为属性定义后，提需求给技术不就行了，需要说明两点： 标签体系化工作是一个长期过程，需要阶段性的进行，因此你不可能一上来全面覆盖全面标签化，它是随着产品&amp;业务的发展深度，而进行动态演化的；所以你也不要担心想的不全面；回归需求本身，清楚现产品阶段，你最关心什么，要给谁推送，按照节奏进行。 标签是分级的，等下看图会更详细，那为什么标签要这样划分级别要清楚，假设第一级标签，你在后台筛选的是活跃用户，那么按照用户的活跃状态，我还可以再分一级，筛选出低频活跃用户或者高频活跃用户。如果你不自己不清楚怎么算低频活跃怎么算高频活跃，那怎么对标签下定义呢？ 2.3 标签定义的四个维度当你理清楚了人并且也清楚当前最想要对他的什么行为&amp;状态进行推送，那标签体系化的第一期工作，就可以进入到落地执行环节了。 我们一般做标签体系，大致先满足四个维度：人口属性、行为属性、商业属性、消费属性，四个方面来进行标签填充。 人口属性：指的是用户运营画像。比如性别、年龄、地域、设备型号等。这一维度告诉我们他是谁。 题外话：用户画像一般分两种，一种是用户产品画像，产品经理通过海量用户抽象出产品使用者画像，类似于高度概括的那种，以便判断这个功能是否能够满足这一群体用户的需要；另一种是用户运营画像，因为运营更多是精准触达、活动等，所需的是具体的属性。 行为属性：指用户使用产品的日常行为和关键行为，比如注册、签到、活跃状态、功能使用等，这一维度告诉我们他当前做了什么。 商业属性：指用户在产品上的当前付费状态。比如免费用户、会员用户、免费用户也可根据时间划分为 3 天/ 7 天/ 30天的免费试用用户，按付费类型可为年卡/季卡/月卡用户。这一维度告诉了我们他在产品上的状态。 消费属性：这里用到的是 RFM 模型。RFM 模型大家都知道， Rencency（最近一次消费），Frequency（消费频率）、Monetary（消费金额），我们一般分为8个维度。 除了上述四个基本层面的标签建设外，还有一个可根据运营需要添加或不添加。 偏好属性：记录用户在产品上的行为偏好。比如每日都会签到的用户、参与过会员促销活动的用户、参与过拉新活动的用户等等。这一部分用户有的属于利益驱动型用户，可以作为运营上线活动，第一批种子用户或者测试用户。 2.4 标签化的推送制定在完成了标签定义之后，就是推送制定，针对不同的用户层级，我们的运营动作也所有不同，在上面略有提及，如果是固定的文案&amp;营销推送，可以做成自动化，比如会员到期提醒、沉默预警提醒、流失召回等。 如果有活动或者AB测试的需求，则可以通过后台按照以上标签筛选进行针对性推送。 至于推送的文案&amp;时间&amp;活动类型，各家的产品&amp;使用环境都不同，我就不要误导你什么21点推送&amp;新手推引导，活跃推营销等等啦，方法只有一个站在你的角度，多推多看多琢磨。 三、标签系统的结构标签系统可以分为三个部分：数据加工层，数据服务层，数据应用层。每个层面面向用户对象不一样，处理事务有所不同。层级越往下，与业务的耦合度就越小。层级越往上，业务关联性就越强。 以M电商公司为例，来说明该系统的构成。 数据加工层。数据加工层收集，清洗和提取来处理数据。M公司有多个产品线：电商交易，电子书阅读，金融支付，智能硬件等等。每个产品线的业务数据又是分属在不同位置。为了搭建完善的用户标签体系，需要尽可能汇总最大范围内的数据。同时每个产品线的也要集合所有端的数据，比如：App，web，微信，其它第三方合作渠道。 收集了所有数据之后，需要经过清洗：去重，去刷单数据，去无效数据，去异常数据等等。然后再是提取特征数据，这部分就要根据产品和运营人员提的业务数据要求来做就好。 数据业务层。数据加工层为业务层提供最基础数据能力，提供数据原材料。业务层属于公共资源层，并不归属某个产品或业务线。它主要用来维护整个标签体系，集中在一个地方来进行管理。 在这一层，运营人员和产品能够参与进来，提出业务要求：将原材料进行切割。主要完成以下核心任务： 定义业务方需要的标签。 创建标签实例。 执行业务标签实例，提供相应数据。 数据应用层。应用层的任务是赋予产品和运营人员标签的工具能力，聚合业务数据，转化为用户的枪火弹药，提供数据应用服务。 业务方能够根据自己的需求来使用，共享业务标签，但彼此业务又互不影响。实践中可应用到以下几块： 精准化营销。 个性化推送。 四、标签体系的设计3.1. 业务梳理搭建用户标签体系容易陷入用户画像陷阱，照葫芦画瓢，不利于标签体系的维护和后期的扩展。可以按下面的思路来梳理标签体系： 有哪些产品线？产品线有哪些来源渠道？一一列出。 每个产品线有哪些业务对象？比如用户，商品。 最后再根据对象聚合业务，每个对象涉及哪些业务？每个业务下哪些业务数据和用户行为？ 结果类似如下： 3.2. 标签的分类按业务对象整理了业务数据后，可以继续按照对象的属性来进行分类，主要目的： 方便管理标签，便于维护和扩展。 结构清晰，展示标签之间的关联关系。 为标签建模提供子集。方便独立计算某个标签下的属性偏好或者权重。 梳理标签分类时，尽可能按照MECE原则，相互独立，完全穷尽。每一个子集的组合都能覆盖到父集所有数据。标签深度控制在四级比较合适，方便管理，到了第四级就是具体的标签实例。 3.3 标签的模型按数据的实效性来看，标签可分为 静态属性标签。长期甚至永远都不会发生改变。比如性别，出生日期，这些数据都是既定的事实，几乎不会改变。 动态属性标签。存在有效期，需要定期地更新，保证标签的有效性。比如用户的购买力，用户的活跃情况。 从数据提取维度来看，标签数据又可以分为3种类型。 事实标签。既定事实，从原始数据中提取。比如通过用户设置获取性别，通过实名认证获取生日，星座等信息。 模型标签。没有对应数据，需要定义规则，建立模型来计算得出标签实例。比如支付偏好度。 预测标签。参考已有事实数据，来预测用户的行为或偏好。比如用户a的历史购物行为与群体A相似，使用协同过滤算法，预测用户a也会喜欢某件物品。 3.4. 标签的处理为什么要从两个维度来对标签区分？这是为了方便用户标签的进一步处理。 静态动态的划分是面向业务维度，便于运营人员理解业务。这一点能帮助他们： 理解标签体系的设计。 表达自己的需求。 事实标签，模型标签，预测标签是面向数据处理维度，便于技术人员理解标签模块功能分类，帮助他们： 设计合理数据处理单元，相互独立，协同处理。 标签的及时更新及数据响应的效率。 以上面的标签图表为例，面临以下问题： 属性信息缺失怎么办？比如，现实中总有用户未设置用户性别，那怎么才能知道用户的性别呢？ 行为属性，消费属性的标签能不能灵活设置？比如，活跃运营中需要做A/B test，不能将品牌偏好规则写死，怎么办？ 既有的属性创建不了我想要的标签？比如，用户消费能力需要综合结合多项业务的数据才合理，如何解决？ 模型标签的定义解决的就是从无到有的问题。建立模型，计算用户相应属性匹配度。现实中，事实标签也存在数据缺失情况。 比如用户性别未知，但是可以根据用户浏览商品，购买商品的历史行为来计算性别偏好度。当用户购买的女性化妆品和内衣较多，偏好值趋近于性别女，即可以推断用户性别为女。 模型计算规则的开放解决的是标签灵活配置的问题。运营人员能够根据自己的需求，灵活更改标签实例的定义规则。比如图表中支付频度实例的规则定义，可以做到： 时间的开放。支持时间任意选择：昨天，前天，近x天，自定义某段时间等等。 支付笔数的开放。大于，等于，小于某个值，或者在某两个值区间。 标签的组合解决就是标签扩展的问题。除了原有属性的规则定义，还可以使用对多个标签进行组合，创建新的复合型标签。比如定义用户的消费能力等级。 标签最终呈现的形态要满足两个需求： 标签的最小颗粒度要触达到具体业务事实数据，同时支持对应标签实例的规则自定义。 不同的标签可以相互自由组合为新的标签，同时支持标签间的关系，权重自定义。 五、实践分享数据应用层即为标签的使用场景，最典型的应用场景是：精准推送。 精准推送。该场景对标签的实效性要求并不高，可以只考虑离线的历史数据，不需要结合实时数据，是标签首选的实践场景。运营人员使用标签筛选出目标用户，定向推送活动。推送渠道根据活动的需要来进行多渠道投放，能够支持微信，App，短信。 运营主要工作基本就是不停地生产活动，向用户投食，监测活动的效果，不断优化投放策略：找到不同用户对应的最佳匹配活动。这块主要关注活动以下环节： 活动前：目标用户，活动内容，投放渠道。 活动中：效果监控和跟踪。 活动后：效果复盘和优化。 除精准推送外，用户标签还有其它的应用场景。在技术层面上，对算法建模及响应性能也有更高的要求： 推荐栏位 消费周期评估 广告投放 促销排期 另外，用户的数据信息不仅局限于应用内本身。仅通过用户昵称或手机号已经足以爬取到用户在全网内留下的所有信息，从而构建丰富的用户画像。你多大？在哪里工作？家庭人员情况？在技术面前，都是一张透明的白纸。只不过目前这样做要花费很多人力，成本太高。 前天，产品群里有人问为啥有的产品引导用户关联第三方账号？同样是为了获取用户数据，用户一般并不知晓，以为只是增加新的登录方式。 建议及想法如果你的产品微信粉丝数量接近千万级，不防试一试，用标签做精准营销。微信聚合了大量的粉丝，向商家端开放了粉丝的基本信息，提供了开放接口能力及多种消息触达方式，是最好的试验场。 微信聚合了最大和最优质的流量。从这个角度出发，基于微信提供的能力，做一款针对C端营销的CRM营销产品，存在着很大的商业机会。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"用户标签","slug":"用户标签","permalink":"https://dataquaner.github.io/categories/%E7%94%A8%E6%88%B7%E6%A0%87%E7%AD%BE/"}],"tags":[{"name":"用户","slug":"用户","permalink":"https://dataquaner.github.io/tags/%E7%94%A8%E6%88%B7/"},{"name":"标签","slug":"标签","permalink":"https://dataquaner.github.io/tags/%E6%A0%87%E7%AD%BE/"}]},{"title":"Java程序的层级结构（Controller、Service、Dao、Entity层）","slug":"Java程序的层级结构（Controller、Service、Dao、Entity层）","date":"2020-11-12T12:47:00.000Z","updated":"2020-11-12T12:49:52.226Z","comments":true,"path":"2020/11/12/java-cheng-xu-de-ceng-ji-jie-gou-controller-service-dao-entity-ceng/","link":"","permalink":"https://dataquaner.github.io/2020/11/12/java-cheng-xu-de-ceng-ji-jie-gou-controller-service-dao-entity-ceng/","excerpt":"","text":"Entity层Entity层：实体层，用于存放实体类，与数据库中的属性值基本保持一致，包含有该实体类的属性和对应属性的get、set方法。 DAO层DAO层：持久层，与数据库进行交互。DAO层首先会创建DAO接口，然后在配置文件中定义该接口的具体实现类，接着就可以在模块中调用DAO的接口并进行相应数据业务的处理，不需要去关注该接口的具体实现类是什么。DAO层的数据源和数据库连接的参数都是在配置文件中进行配置的，主要对数据进行持久化操作，对外提供对数据库的增删改查操作。 Service层service层：业务层，用来控制业务。Service层主要负责业务模块的逻辑应用设计，先创建接口，再创建相应的实现类，然后在配置文件里进行配置实现其关联，接着就可以调用service层的接口进行业务逻辑应用的处理。对Service层的业务逻辑进行封装有利于业务逻辑的独立性和重复利用性。 Controller层Controller层：控制层，控制业务逻辑流程。Controller层负责具体的业务模块流程的控制，主要调用Service层里面的接口去控制具体的业务流程，控制的配置也需要在配置文件中进行配置。与Service层不同，Controller层负责具体的业务模块流程的控制，Service层负责业务模块的逻辑应用设置。Controller层一般会与前台的js文件进行数据的交互。 总结具体项目中，主要流程为Controller层调用Service层，Service层调用Dao层，调用的参数在Entity层进行定义。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Java","slug":"Java","permalink":"https://dataquaner.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://dataquaner.github.io/tags/Java/"}]},{"title":"Flink三天光速入门","slug":"Flink三天入门","date":"2020-11-11T06:35:00.000Z","updated":"2020-11-11T08:52:10.966Z","comments":true,"path":"2020/11/11/flink-san-tian-ru-men/","link":"","permalink":"https://dataquaner.github.io/2020/11/11/flink-san-tian-ru-men/","excerpt":"","text":"1. 初识 Flink在当前数据量激增的时代，各种业务场景都有大量的业务数据产生，对于这些不断产的数据应该如何进行有效的处理，成为当下大多数公司所面临的问题。目前比较流行的大数据处理引擎 Apache Spark，基本上已经取代了 MapReduce 成为当前大数据处理的标准。但 对实时数据处理来说，Apache Spark 的 Spark-Streaming 还有性能改进的空间。对于 Spark-Streaming 的流计算本质上还是批（微批）计算，Apache Flink 就是近年来在开源社区不断发展的技术中的能够同时支持高吞吐、低延迟、高性能的纯实时的分布式处理框架(主要贡献者是阿里(官网支持汉化阅读)，QPS可达30W+)。 Flink 是什么1. Flink 的发展历史在 2010 年至 2014 年间，由柏林工业大学、柏林洪堡大学和哈索普拉特纳研究所联合发 起名为Stratosphere:Information Management on the Cloud研究项目，该项目在当时的社区逐渐具有了一定的社区知名度。2014 年 4 月，Stratosphere 代码被贡献给 Apache 软件基金会，成为 Apache 基金会孵化器项目。初期参与该项目的核心成员均是 Stratosphere 曾经的核心成员，之后团队的大部分创始成员离开学校，共同创办了一家名叫 Data Artisans 的公司，其主要业务便是将 Stratosphere，也就是之后的 Flink 实现商业化。在项目孵化 期间，项目 Stratosphere 改名为 Flink。Flink 在德语中是快速和灵敏的意思，用来体现流 式数据处理器速度快和灵活性强等特点，同时使用棕红色松鼠图案作为 Flink 项目的 Logo， 也是为了突出松鼠灵活快速的特点，由此，Flink 正式进入社区开发者的视线。 2014 年 12 月，该项目成为 Apache 软件基金会顶级项目，从 2015 年 9 月发布第一个稳 定版本 0.9，到目前为止已经发布到 1.9 的版本，更多的社区开发成员逐步加入，现在 Flink 在全球范围内拥有 350 多位开发人员，不断有新的特性发布。同时在全球范围内，越来越多 的公司开始使用 Flink，在国内比较出名的互联网公司如阿里巴巴、美团、滴滴等，都在大 规模使用 Flink 作为企业的分布式大数据处理引擎。 2. Flink 的定义Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale 3. 有界流和无界流任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或 移动应用程序上的用户交互记录，所有这些数据都形成一种流。 无界流： 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流 的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理， 因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事 件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流： 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行 计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。跟Spark-Stream类似。Apache Flink 擅长处理无界和有界数据集精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 4. 有状态的计算架构数据产生的本质，其实是一条条真实存在的事件按照时间顺序源源不断的产生，我们很难在数据产生的过程中进行计算并直接产生统计结果，因为这不仅对系统有非常高的要求， 还必须要满足高性能、高吞吐、低延时等众多目标。而有状态流计算架构（如图所示）的提 出，从一定程度上满足了企业的这种需求，企业基于实时的流式数据，维护所有计算过程的 状态，所谓状态就是计算过程中产生的中间计算结果，每次计算新的数据进入到流式系统中 都是基于中间状态结果的基础上进行运算，最终产生正确的统计结果。基于有状态计算的方式最大的优势是不需要将原始数据重新从外部存储中拿出来，从而进行全量计算，因为这种计算方式的代价可能是非常高的。从另一个角度讲，用户无须通过调度和协调各种批量计算 工具，从数据仓库中获取数据统计结果，然后再落地存储，这些操作全部都可以基于流式计 算完成，可以极大地减轻系统对其他框架的依赖，减少数据计算过程中的时间损耗以及硬件存储。 2. 为什么要使用 Flink可以看出有状态流计算将会逐步成为企业作为构建数据平台的架构模式，而目前从社区 来看，能够满足的只有 Apache Flink。Flink 通过实现 Google Dataflow 流式计算模型实现 了高吞吐、低延迟、高性能兼具实时流式计算框架。同时 Flink 支持高度容错的状态管理， 防止状态在计算过程中因为系统异常而出现丢失，Flink 周期性地通过分布式快照技术 Checkpoints实现状态的持久化维护，使得即使在系统停机或者异常的情况下都能计算出正 确的结果。 Flink用户 众多，自2019年1月起，阿里巴巴逐步将内部维护的Blink回馈给Flink开源社区，目前贡献代码已超过100万行，国内包括腾讯、百度、字节跳动等公司，国外包括Uber、Lyft、Netflix等公司都是Flink的使用者。 3. Flink 的应用场景在实际生产的过程中，大量数据在不断地产生，例如金融交易数据、互联网订单数据、 GPS 定位数据、传感器信号、移动终端产生的数据、通信信号数据等，以及我们熟悉的网络 流量监控、服务器产生的日志数据，这些数据最大的共同点就是实时从不同的数据源中产生， 然后再传输到下游的分析系统。针对这些数据类型主要包括实时智能推荐、复杂事件处理、 实时欺诈检测、实时数仓与 ETL 类型、流数据分析类型、实时报表类型等实时业务场景，而 Flink 对于这些类型的场景都有着非常好的支持 1. 实时智能推荐智能推荐会根据用户历史的购买行为，通过推荐算法训练模型，预测用户未来可能会购 买的物品。对个人来说，推荐系统起着信息过滤的作用，对 Web/App 服务端来说，推荐系统 起着满足用户个性化需求，提升用户满意度的作用。推荐系统本身也在飞速发展，除了算法 越来越完善，对时延的要求也越来越苛刻和实时化。利用 Flink 流计算帮助用户构建更加实 时的智能推荐系统，对用户行为指标进行实时计算，对模型进行实时更新，对用户指标进行 实时预测，并将预测的信息推送给 Wep/App 端，帮助用户获取想要的商品信息，另一方面也 帮助企业提升销售额，创造更大的商业价值。 2. 复杂事件处理对于复杂事件处理，比较常见的案例主要集中于工业领域，例如对车载传感器、机械设备等实时故障检测，这些业务类型通常数据量都非常大，且对数据处理的时效性要求非常高。 通过利用 Flink 提供的 CEP（复杂事件处理）进行事件模式的抽取，同时应用 Flink 的 Sql 进行事件数据的转换，在流式系统中构建实时规则引擎，一旦事件触发报警规则，便立即将 告警结果传输至下游通知系统，从而实现对设备故障快速预警监测，车辆状态监控等目的。 3. 实时欺诈检测在金融领域的业务中，常常出现各种类型的欺诈行为，例如信用卡欺诈、信贷申请欺诈等，而如何保证用户和公司的资金安全，是近年来许多金融公司及银行共同面对的挑战。 随着不法分子欺诈手段的不断升级，传统的反欺诈手段已经不足以解决目前所面临的问题。 以往可能需要几个小时才能通过交易数据计算出用户的行为指标，然后通过规则判别出具有 欺诈行为嫌疑的用户，再进行案件调查处理，在这种情况下资金可能早已被不法分子转移， 从而给企业和用户造成大量的经济损失。而运用 Flink 流式计算技术能够在毫秒内就完成对 欺诈判断行为指标的计算，然后实时对交易流水进行规则判断或者模型预测，这样一旦检测 出交易中存在欺诈嫌疑，则直接对交易进行实时拦截，避免因为处理不及时而导致的经济损 失。 4. 实时数仓与 ETL 结合离线数仓通过利用流计算诸多优势和 SQL 灵活的加工能力，对流式数据进行实时清洗、归并、结构化处理，为离线数仓进行补充和优化。另一方面结合实时数据 ETL 处理能 力，利用有状态流式计算技术，可以尽可能降低企业由于在离线数据计算过程中调度逻辑的复杂度，高效快速地处理企业需要的统计结果，帮助企业更好地应用实时数据所分析出来的结果。 5. 流数据分析实时计算各类数据指标，并利用实时结果及时调整在线系统相关策略，在各类内容投放、 无线智能推送领域有大量的应用。流式计算技术将数据分析场景实时化，帮助企业做到实时化分析 Web 应用或者 App 应用的各项指标，包括 App 版本分布情况、Crash 检测和分布等， 同时提供多维度用户行为分析，支持日志自主分析，助力开发者实现基于大数据技术的精细 化运营、提升产品质量和体验、增强用户黏性。 6. 实时报表分析实时报表分析是近年来很多公司采用的报表统计方案之一，其中最主要的应用便是实时大屏展示。利用流式计算实时得出的结果直接被推送到前端应用，实时显示出重要指标的变 换情况。最典型的案例便是淘宝的双十一活动，每年双十一购物节，除疯狂购物外，最引人 注目的就是天猫双十一大屏不停跳跃的成交总额。在整个计算链路中包括从天猫交易下单购买到数据采集、数据计算、数据校验，最终落到双十一大屏上展现的全链路时间压缩在 5 秒以内，顶峰计算性能高达数三十万笔订单/秒，通过多条链路流计算备份确保万无一失。 而在其他行业，企业也在构建自己的实时报表系统，让企业能够依托于自身的业务数据，快 速提取出更多的数据价值，从而更好地服务于企业运行过程中。 4. Flink 的特点和优势Flink 的具体优势和特点有以下几点 1. 同时支持高吞吐、低延迟、高性能Flink 是目前开源社区中唯 一 一套集高吞吐、低延迟、高性能三者于一身的分布式流式数据处理框架。像 Apache Spark 也只能兼顾高吞吐和高性能特性，主要因为在 Spark Streaming 流式计算中无法做到低延迟保障；而流式计算框架 Apache Storm 只能支持低延迟和高性能特性，但是无法满足高吞吐的要求。而满足高吞吐、低延迟、高 性能这三个目标对分布式流式计算框架来说是非常重要的。 2. 支持事件时间（Event Time）概念在流式计算领域中，窗口计算的地位举足轻重，但目前大多数框架窗口计算采用的都是系统时间(Process Time)，也是事件传输到计算框架处理时，系统主机的当前时间。Flink 能够支持基于事件时间(Event Time)语义进行窗口计算，也就是使用事件产生的时间，这种基于事件驱动的机制使得事件即使乱序到达，流系统也能够计算出 确的结果，保持了事件原本产生时的时序性，尽可能避免网络传输或硬件系统的影响。 Event Time/Processing Time/Ingestion Time，也就是事件时间、处理时间、提取时间，那么这三个时间有什么区别和联系 下图是一个信号站，分别列出了事件时间、处理时间、提取时间的先后顺序。当然上面图示需要你对Flink有一个基本的了解。我们先白话解释，然后在官方解释。 Event Time：也就是事件发生的时间，事件的发生时间。我们有些同学可能会模糊，这里举个例子，我们产生日志的时间，这个应该清楚的，日志的时间戳就是发生时间。 Processing Time：也就是处理时间，我们看到了这个已经进入Flink程序，也就是我们读取数据源时间，也就是日志到达Flink的时间，但是这个时间是本地机器的时间。 Ingestion Time：也就是提取时间，我们看到它比处理时间还晚一些，这个时候数据已经发送给窗口，也就是发送给窗口的时间，也就是程序处理计算的时间。 3. 支持有状态计算Flink 在 1.4 版本中实现了状态管理，所谓状态就是在流式计算过程中将算子的中间结果数据保存在内存或者文件系统中，等下一个事件进入算子后可以从之前的状态中 获取中间结果中计算当前的结果，从而无须每次都基于全部的原始数据来统计结果，这 种方式极大地提升了系统的性能，并降低了数据计算过程的资源消耗。对于数据量大且运算逻辑非常复杂的流式计算场景，有状态计算发挥了非常重要的作用。 4.支持高度灵活的窗口（Window）操作在流处理应用中，数据是连续不断的，需要通过窗口的方式对流数据进行一定范围 的聚合计算，例如统计在过去的 1 分钟内有多少用户点击某一网页，在这种情况下，我 们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行再计 算。Flink 将窗口划分为基于 Time、Count、Session，以及 Data-driven 等类型的窗口 操作，窗口可以用灵活的触发条件定制化来达到对复杂的流传输模式的支持，用户可以 定义不同的窗口触发机制来满足不同的需求。 5.基于轻量级分布式快照（CheckPoint）实现的容错Flink 能够分布式运行在上千个节点上，将一个大型计算任务的流程拆解成小的计算过程，然后将 tesk 分布到并行节点上进行处理。在任务执行过程中，能够自动发现事件处理过程中的错误而导致数据不一致的问题，比如：节点宕机、网路传输问题，或 是由于用户因为升级或修复问题而导致计算服务重启等。在这些情况下，通过基于分布 式快照技术的Checkpoints，将执行过程中的状态信息进行持久化存储，一旦任务出现异常停止，Flink 就能够从 Checkpoints 中进行任务的自动恢复，以确保数据在处理过 程中的精准一致性（Exactly-Once）。快照是默认自动开启实现的。 6.基于 JVM 实现独立的内存管理内存管理是所有计算框架需要重点考虑的部分，尤其对于计算量比较大的计算场 景，数据在内存中该如何进行管理显得至关重要。针对内存管理，Flink 实现了自身管理内存的机制，尽可能减少 JVM GC 对系统的影响。另外，Flink 通过序列化/反序列化 方法将所有的数据对象转换成二进制在内存中存储，降低数据存储的大小的同时，能够更加有效地对内存空间进行利用，降低 GC 带来的性能下降或任务异常的风险，因此 Flink 较其他分布式处理的框架会显得更加稳定，不会因为 JVM GC 等问题而影响整个 应用的运行。 7. Save Points（保存点）对于 7*24 小时运行的流式应用，数据源源不断地接入，在一段时间内应用的终止有可能导致数据的丢失或者计算结果的不准确，例如进行集群版本的升级、停机运维操 作等操作。值得一提的是，Flink 通过 SavePoints技术将任务执行的快照保存在存储介质上，当任务重启的时候可以直接从事先保存的 Save Points 恢复原有的计算状态， 使得任务继续按照停机之前的状态运行，Save Points 技术可以让用户更好地管理和运 维实时流式应用。不过需要手动启动跟恢复数据。 5. 常见实时计算框架对比 产品 模型 API 保证次数 容错机制 状态管理 延时 吞吐量 Storm Native(数据实时进入处理) 组合式(基础API) At-least-once(至少一次) Record ACK(ACK机制) 无 低 低 Trident Micro-Batching(划分为小批次处理) 组合式 Exactly-once(精准一致性) Record ACK 基于操作(每个操作都有一个状态) 中等 中等 Spark-Streaming Micro-Batching 声明式(提供封装后的高阶函数，比如Count) Exactly-once RDD CheckPoint(基于RDD做CheckPoint) 基于DStream 中等 高 Flink Native 声明式 Exactly-once CheckPoint(Flink的一种快照) 基于操作 低 高 模型：Storm 和 Flink 是真正的一条一条处理数据；而 Trident（Storm 的封装框架） 和 Spark Streaming 其实都是小批处理，一次处理一批数据（小批量）。 API：Storm 和 Trident 都使用基础 API 进行开发，比如实现一个简单的 sum 求和操作； 而 Spark Streaming 和 Flink 中都提供封装后的高阶函数，可以直接拿来使用，这样就 比较方便了。 保证次数：在数据处理方面，Storm 可以实现至少处理一次，但不能保证仅处理一次， 这样就会导致数据重复处理问题，所以针对计数类的需求，可能会产生一些误差； Trident 通过事务可以保证对数据实现仅一次的处理，Spark Streaming 和 Flink 也是 如此。 容错机制：Storm和Trident可以通过ACK机制实现数据的容错机制，而Spark Streaming 和 Flink 可以通过 CheckPoint 机制实现容错机制。 状态管理：Storm 中没有实现状态管理，Spark Streaming 实现了基于 DStream 的状态 管理，而 Trident 和 Flink 实现了基于操作的状态管理。 延时：表示数据处理的延时情况，因此 Storm 和 Flink 接收到一条数据就处理一条数据， 其数据处理的延时性是很低的；而 Trident 和 Spark Streaming 都是小型批处理，它们 数据处理的延时性相对会偏高。 吞吐量：Storm 的吞吐量其实也不低，只是相对于其他几个框架而言较低；Trident 属于中等；而 Spark Streaming 和 Flink 的吞吐量是比较高的。 2.Flink编程入门1. Flink 的开发环境Flink 课程选择的是 Apache Flink 1.9.1 版本，是目前较的稳定版本，并且 兼容性比较好。下载地址： https://flink.apache.org/zh/downloads.html 1. 开发工具先说明一下开发工具的问题。官方建议使用IntelliJ IDEA，因为它默认集成了 Scala和Maven环境，使用更加方便，当然使用 Eclipse 也是可以的。本文使用 IDEA。开发Flink 程序时，可以使用Java、Python或者Scala语言，本教程使用 Scala，因为 使用 Scala 实现函数式编程会比较简洁。 2. 配置依赖开发 Flink 应用程序需要最低限度的 API 依赖。最低的依赖库包括：flink-scala和 flink-streaming-scala。大多数应用需要依赖特定的连接器或其他类库，例如 Kafka 的连 接器、TableAPI、CEP 库等。这些不是 Flink 核心依赖的一部分，因此必须作为依赖项手 动添加到应用程序中。 与其他运行用户自定义应用的大多数系统一样，Flink 中有两大类依赖类库 Flink 核心依赖：Flink 本身包含运行所需的一组类和依赖，比如协调、网络通讯、checkpoint、容错处理、API、算子(如窗口操作)、 资源管理等，这些类和依赖形成了 Flink 运行时的核心。当 Flink 应用启动时，这些依赖必须可用。这些核心类和依赖被打包在 flink-dist jar 里。它们是 Flink lib 文件夹下的一部分，也是 Flink 基本容器镜像的一部分。 这些依赖类似 Java String 和 List 的核心类库(rt.jar, charsets.jar等)。Flink 核心依赖不包含连接器和类库（如 CEP、SQL、ML 等），这样做的目的是默认情况下避免在类路径中具有过多的依赖项和类。 实际上，我们希望尽可能保持核心依赖足够精简，以保证一个较小的默认类路径，并且避免依赖冲突。 用户应用依赖：是指特定的应用程序需要的类库，如连接器，formats等。用户应用代码和所需的连接器以及其他类库依赖通常被打包到 application jar 中。用户应用程序依赖项不需包括 Flink DataSet / DataStream API 以及运行时依赖项，因为它们已经是 Flink 核心依赖项的一部分。 Flink官方依赖文档说明：官方依赖入手 2.WordCount演示添加pom依赖 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;groupId>com.sowhat&lt;/groupId> &lt;artifactId>Flink-Test&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;dependencies> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-scala_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-streaming-scala_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;!-- 上述两个是核心依赖--> &lt;dependency> &lt;groupId>org.apache.hadoop&lt;/groupId> &lt;artifactId>hadoop-common&lt;/artifactId> &lt;version>2.7.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.hadoop&lt;/groupId> &lt;artifactId>hadoop-client&lt;/artifactId> &lt;version>2.7.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-connector-kafka_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.kafka&lt;/groupId> &lt;artifactId>kafka-clients&lt;/artifactId> &lt;version>0.11.0.3&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-connector-filesystem_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.bahir&lt;/groupId> &lt;artifactId>flink-connector-redis_2.11&lt;/artifactId> &lt;version>1.0&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;version>5.1.44&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-table-planner_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-table-api-scala-bridge_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-cep-scala_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;plugins> &lt;!-- 该插件用于将Scala代码编译成class文件 --> &lt;plugin> &lt;groupId>net.alchim31.maven&lt;/groupId> &lt;artifactId>scala-maven-plugin&lt;/artifactId> &lt;version>3.4.6&lt;/version> &lt;executions> &lt;execution> &lt;!-- 声明绑定到maven的compile阶段 --> &lt;goals> &lt;goal>testCompile&lt;/goal> &lt;goal>compile&lt;/goal> &lt;/goals> &lt;/execution> &lt;/executions> &lt;/plugin> &lt;!-- Java Compiler https://blog.csdn.net/liupeifeng3514/article/details/80236077 --> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-compiler-plugin&lt;/artifactId> &lt;version>3.1&lt;/version> &lt;configuration> &lt;source>1.8&lt;/source> &lt;!-- 开始代码时指定的JDK版本--> &lt;target>1.8&lt;/target> &lt;!-- 编译成.class 文件所需版本--> &lt;/configuration> &lt;/plugin> &lt;plugin> &lt;groupId>org.apache.maven.plugins&lt;/groupId> &lt;artifactId>maven-assembly-plugin&lt;/artifactId> &lt;version>3.0.0&lt;/version> &lt;configuration> &lt;descriptorRefs> &lt;descriptorRef>jar-with-dependencies&lt;/descriptorRef> &lt;/descriptorRefs> &lt;/configuration> &lt;executions> &lt;execution> &lt;id>make-assembly&lt;/id> &lt;phase>package&lt;/phase> &lt;goals> &lt;goal>single&lt;/goal> &lt;!-- 表示会出现一个无任何依赖的jar，还有一个包含所有依赖的jar -- > &lt;/goals> &lt;/execution> &lt;/executions> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136 注意事项: 所有这些 依赖项 的作用域都应该设置为 provided 。 这意味着需要这些依赖进行编译，但不应将它们打包到项目生成的应用程序jar文件中，因为这些依赖项是 Flink 的核心依赖，服务器运行环境在应用启动前已经是可用的状态了。 我们强烈建议保持这些依赖的作用域为 provided。 如果它们的作用域未设置为 provided ，则典型的情况是因为包含了 Flink 的核心依赖而导致生成的jar包变得过大。 最糟糕的情况是添加到应用程序的 Flink 核心依赖项与你自己的一些依赖项版本冲突（通常通过反向类加载来避免）。 IntelliJ 上的一些注意事项: 为了可以让 Flink 应用在 IntelliJ IDEA 中运行，这些 Flink 核心依赖的作用域需要设置为 compile 而不是provided 。 否则 IntelliJ 不会添加这些依赖到 classpath，会导致应用运行时抛出 NoClassDefFountError 异常。为了避免声明这些依赖的作用域为 compile (因为我们不推荐这样做)， 上文给出的 Java 和 Scala 项目模板使用了一个小技巧：添加了一个 profile，仅当应用程序在 IntelliJ 中运行时该 profile 才会被激活， 然后将依赖作用域设置为 compile ，从而不影响应用 jar 包。 1. 流式接受数据案例需求：采用 Netcat 数据源发送数据，使用 Flink 统计每个单词的数量。 package com.sowhat.flink import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment /** * flink的流计算的WordCount */ object FlinkStreamWordCount { def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ //3、读取数据,读取sock流中的数据 val stream: DataStream[String] = streamEnv.socketTextStream(\"IP\", 8899) //DataStream ==> spark 中Dstream //4、转换和处理数据 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")) .map((_, 1)).setParallelism(2) .keyBy(0) //分组算子 : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数 .sum(1).setParallelism(2) //聚会累加算子 //5、打印结果 result.print(\"结果\").setParallelism(1) //6、启动流计算程序 streamEnv.execute(\"wordcount\") } } 12345678910111213141516171819202122232425262728293031 找一个服务可以接受到的接口发送若干信息： $ nc -lk 8899 hadoop spark hive flink flink sowhat liu spark flink sowhat --- 结果> (hive,1) 结果> (spark,1) 结果> (hadoop,1) 结果> (flink,1) 结果> (sowhat,1) 结果> (spark,2) 结果> (liu,1) 结果> (flink,2) 结果> (sowhat,2) 结果> (flink,3) 123456789101112131415 PS：如果将代码中所有关于并行度的全部屏蔽掉，系统会自动的将全部CPU利用起来，然后利用Hash算法来将数据归类给不同的CPU核心来处理，结果可能如下： 结果:1> (hive,1) // 表示第几个核给出的结果 结果:4> (flink,1) 结果:1> (spark,1) 结果:4> (sohat,1) 结果:1> (hive,2) 结果:2> (node,1) 结果:4> (zookeeper,1) 结果:3> (manager,1) 12345678 2. 直接统计指定文件WordCount需求：读取本地数据文件，统计文件中每个单词出现的次数。 根据需求，很明显是有界流（批计算），所以采用另外一个上下文环境：ExecutionEnvironment在IDEA的resources目录下创建个wc.txt 文件内容如下： hello flink spark hello spark spark core flink stream hello fink 1234批量统计代码如下： package com.sowhat.flink import java.net.{URL, URLDecoder} import org.apache.flink.api.scala.{DataSet, ExecutionEnvironment, _} /** * Flink的批计算案例 */ object BatchWordCount { def main(args: Array[String]): Unit = { //初始化Flink批处理环境 val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val dataPath: URL = getClass.getResource(\"/wc.txt\") //使用相对路径来得到完整的文件路径 var packagePath: String = dataPath.getPath().replaceAll(\"%20\", \"\"); //解决路径中含有空格的情况 val str:String = URLDecoder.decode(packagePath, \"utf-8\"); //解决路径包含中文的情况 println(str) //读数据 val data: DataSet[String] = env.readTextFile(str) //DataSet ==> spark RDD //计算并且打印结果 data.flatMap(_.split(\" \")) .map((_, 1)) .groupBy(0) //分组算子 : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数 .sum(1) .print() } } 12345678910111213141516171819202122232425262728293031 3. Flink 的安装和部署Flink 的安装和部署主要分为本地（单机）模式和集群模式，其中本地模式只需直接解压就可以使用，不用修改任何参数，一般在做一些简单测试的时候使用。本地模式不再赘述。集群模式包含： Standalone Flink on Yarn(重点) Mesos Docker Kubernetes AWS Goole Compute Engine 目前在企业中使用最多的是 Flink on Yarn 模式。本文主讲Standalone 和Flink on Yarn这两种模式。 1. 集群基本架构Flink 整个系统主要由两个组件组成，分别为 JobManager 和 TaskManager，Flink 架构也遵循Master-Slave 架构设计原则，JobManager 为 Master 节点，TaskManager 为 Worker （Slave）节点。所有组件之间的通信都是借助于 Akka Framework，包括任务的状态以及 Checkpoint 触发等信息。 1. Client 客户端客户端负责将任务提交到集群，与 JobManager 构建 Akka连接，然后将任务提交到 JobManager，通过和 JobManager之间进行交互获取任务执行状态。客户端提交任务可以采 用 CLI 方式或者通过使用 Flink WebUI提交，也可以在应用程序中指定JobManager的RPC网络端口构建 ExecutionEnvironment 提交 Flink 应用。 2.JobManagerJobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的 应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlots 资源并命令 TaskManger 启动从客户端中获取的应用。JobManager 相当于整个集 群的 Master 节点，且整个集群中有且仅有一个活跃的 JobManager，负责整个集群的任务管 理和资源管理。JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执 行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。同时在任务执行过程 中，Flink JobManager会触发 Checkpoints 操作，每个TaskManager 节点收到 Checkpoint 触发指令后，完成 Checkpoint操作，所有的Checkpoint协调过程都是在 Flink JobManager中完成。当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。 3. TaskManagerTaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节 点上的资源申请与管理。客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager， 然后 JobManager 会根据已经注册在 JobManager 中 TaskManager 的资源情况，将任务分配给 有资源的 TaskManager 节点，然后启动并运行任务。TaskManager 从 JobManager 接收需要 部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数 据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。 可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进程的 方式有很大的区别Fink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有 效管理。 PS：可以认为JobManager类似Hadoop中ApplicationMaster，然后一个机器就是一个TaskManager，一个TaskManager可以分解成若干个Flink基本工作单元TaskSlot。 2. Standalone 集群安装和部署Standalone 是 Flink 的独立部署模式，它不依赖其他平台。在使用这种模式搭建 Flink 集群之前，需要先规划集群机器信息。在这里为了搭建一个标准的 Flink 集群，需要准备 3 台 Linux。 下载并解压文件到指定目录 修改配置文件进入到 conf 目录下，编辑 flink-conf.yaml 配置文件 jobmanager.rpc.address: hadoop101 # The RPC port where the JobManager is reachable. jobmanager.rpc.port: 6123 # The heap size for the JobManager JVM jobmanager.heap.size: 1024m # JobManager 内存大小 # The total process memory size for the TaskManager. # # Note this accounts for all memory usage within the TaskManager process, including JVM metaspace and other overhead. taskmanager.memory.process.size: 1024m # TaskManager初始化内存大小 # To exclude JVM metaspace and overhead, please, use total Flink memory size instead of 'taskmanager.memory.process.size'. # It is not recommended to set both 'taskmanager.memory.process.size' and Flink memory. # # taskmanager.memory.flink.size: 1280m # The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline. taskmanager.numberOfTaskSlots: 3 # 每一个TaskManager 有几个TaskSlots # The parallelism used for programs that did not specify and other parallelism. parallelism.default: 1 # 默认并行度 1234567891011121314151617181920212223242526272829 编辑slaves文件 vi slaves hadoop101 hadoop102 hadoop103 1234 信息分发 [root@hadoop101 home]# scp -r flink-1.9.1 root@hadoop102:`pwd` [root@hadoop101 home]# scp -r flink-1.9.1 root@hadoop103:`pwd` 12 主节点启动集群 WebUI 访问flink-conf.yaml的配置文件中rest.port是WebUI的对外端口，服务器输入hadoop101:8081即可访问(我这里随便找个别人搭建看的集群看下WebUI)。左侧栏多点点看看即可，相对来说比较简单。 将IDEA代码中的两个Flink核心依赖设置为provided然后打包(打包的时候经常性出现问题需检查)通过WebUI上传。测试结果如下： [root@hadoop101 home]# nc -lk 8899 12 21 21 12 PS：敲重点 IDEA中用到的Flink-scala核心依赖要跟服务器集群的核心依赖版本一致，否则会 报错！ java.lang.NoSuchMethodError: scala.Predef$.refArrayOps 命令行提交命令行提交 flink同样支持两种提交方式，默认不指定就是客户端方式。如果需要使用集群方式提交的话。可以在提交作业的命令行中指定-d或者–detached 进行进群模式提交。 -d,–detached If present, runs the job in detached mode（分离模式） 客户端提交方式：$FLINK_HOME/bin/flink run -c com.daxin.batch.App flinkwordcount.jar 客户端会多出来一个CliFrontend进程，就是驱动进程。 集群模式提交：$FLINK_HOME/bin/flink run -d -c com.daxin.batch.App flinkwordcount.jar 程序提交完毕退出客户端，不再打印作业进度等信息！ 1234 重要参数说明：下面针对 flink-conf.yaml 文件中的几个重要参数进行分析： jobmanager.heap.size：JobManager 节点可用的内存大小。 taskmanager.heap.size：TaskManager 节点可用的内存大小。 taskmanager.numberOfTaskSlots：每台机器可用的 Slot 数量。 parallelism.default：默认情况下 Flink 任务的并行度。 上面参数中所说的 Slot和 parallelism的区别： Slot 是静态的概念，是指 TaskManager 具有的并发执行能力。 parallelism 是动态的概念，是指程序运行时实际使用的并发能力。 设置合适的 parallelism 能提高运算效率。 比如我又4个跑道(Slot )，本次任务我占用2个(parallelism)。 一般情况下Slot &gt;= parallelism 3. Flink 提交到 YarnFlink on Yarn 模式的原理是依靠 YARN 来调度 Flink 任务，目前在企业中使用较多。 这种模式的好处是可以充分利用集群资源，提高集群机器的利用率，并且只需要 1 套 Hadoop 集群，就可以执行 MapReduce 和 Spark 任务，还可以执行 Flink 任务等，操作非常方便，不 需要维护多套集群，运维方面也很轻松。Flink on Yarn 模式需要依赖 Hadoop 集群，并且 Hadoop 的版本需要是 2.2 及以上。本文选择的 Hadoop 版本是 2.7.2。 Flink On Yarn 的内部实现原理(Snagit Editor绘制)： 当启动一个新的 Flink YARN Client 会话时，客户端首先会检查所请求的资源（容器和内存）是否可用。之后，它会上传 Flink 配置和 JAR 文件到 HDFS。 客 户 端 的 下 一 步 是 请 求 一 个 YARN 容 器 启 动 ApplicationMaster 。 JobManager 和 ApplicationMaster(AM)运行在同一个容器中，一旦它们成功地启动了，AM 就能够知道 JobManager 的地址，它会为 TaskManager 生成一个新的 Flink 配置文件（这样它才能连 上 JobManager），该文件也同样会被上传到 HDFS。另外，AM 容器还提供了 Flink 的 Web 界面服务。Flink 用来提供服务的端口是由用户和应用程序 ID 作为偏移配置的，这 使得用户能够并行执行多个 YARN 会话。 之后，AM 开始为 Flink 的 TaskManager 分配容器（Container），从 HDFS 下载 JAR 文件 和修改过的配置文件。一旦这些步骤完成了，Flink 就安装完成并准备接受任务了 Flink on Yarn 模式在使用的时候又可以分为两种： 第 1 种模式(Session-Cluster)：是在 YARN 中提前初始化一个 Flink 集群(称为 Flink yarn-session)，开辟指定的资源，以后的 Flink 任务都提交到这里。这个 Flink 集群会常驻在 YARN 集群中，除非手工停止。这种方式创建的 Flink 集群会独占资源，不管有没有 Flink 任务在执行，YARN 上面的其他任务都无法使用这些资源。一般此种方式用的较少。 第 2 种模式(Per-Job-Cluster)：每次提交 Flink 任务都会创建一个新的 Flink 集群， 每个 Flink 任务之间相互独立、互不影响，管理方便。任务执行完成之后创建的 Flink 集群也会消失，不会额外占用资源，按需使用，这使资源利用率达到最大，在工作中推荐使用这种模式。注意：Flink on Yarn 还需要两个先决条件： 配置 Hadoop 的环境变量 下载 Flink 提交到 Hadoop 的连接器(jar 包 大约40M)，并把 jar 拷贝到 Flink 的 lib 目录下 [root@hadoop101 flink-1.9.1]# cp /home/flink-shaded-hadoop-2-uber-2.7.5-7.0.jar lib/ 1 启动第一种 Session-Cluster 模式（yarn-session）1 先启动 Hadoop 集群，然后通过命令启动一个 Flink 的 yarn-session 集群： [root@hadoop101 flink-1.9.1]# bin/yarn-session.sh -n 3 -s 3 -nm sowhat -d 1 其中 yarn-session.sh 后面支持多个参数。下面针对一些常见的参数进行讲解： -n、–container 表示分配容器的数量（也就是 TaskManager 的数量）。 -D 动态属性。 -d、–detached 在后台独立运行。 -jm、–jobManagerMemory ：设置 JobManager 的内存，单位是 MB。 -nm、–name：在 YARN 上为一个自定义的应用设置一个名字。 -q、–query：显示 YARN 中可用的资源（内存、cpu 核数）。 -qu、–queue ：指定 YARN 队列。 -s、–slots ：每个 TaskManager 使用的 Slot 数量。 -tm、–taskManagerMemory ：每个 TaskManager 的内存，单位是 MB。 -z、–zookeeperNamespace ：针对 HA 模式在 ZooKeeper 上创建 NameSpace。 -id、–applicationId ：指定 YARN 集群上的任务 ID，附着到一个后台独 立运行的 yarn session 中。查看 WebUI: 由于还没有提交 Flink job，所以都是 0。这个时候注意查看本地文件系统中有一个临时文件。有了这个文件可以提交 job 到 Yarn提交 Job : 由于有了之前的配置，所以自动会提交到 Yarn 中。 bin/flink run -c com.bjsxt.flink.StreamWordCount /home/Flink-Demo-1.0-SNAPSHOT.jar 1 至此第一种模式全部完成。 启动第二种模式这种模式下不需要先启动 yarn-session。所以我们可以把前面启动的 yarn-session 集 群先停止，停止的命令是: yarn application -kill application_1576832892572_0002 //其中 application_1576832892572_0002 是ID 1 确保 Hadoop 集群是健康的情况下直接提交 Job 命令： bin/flink run -m yarn-cluster -yn 3 -ys 3 -ynm sowhat02 \\ -c com.sowhat.flink.StreamWordCount /home/Flink-Demo-1.0-SNAPSHOT.jar 12 可以看到一个全新的 yarn-session任务提交参数讲解：相对于 Yarn-Session 参数而言，只是前面加了 y。 -yn、–container 表示分配容器的数量，也就是 TaskManager 的数量。 -d、–detached：设置在后台运行。 -yjm、–jobManagerMemory:设置 JobManager 的内存，单位是 MB。 -ytm、–taskManagerMemory:设置每个 TaskManager 的内存，单位是 MB。 -ynm、–name:给当前 Flink application 在 Yarn 上指定名称。 -yq、–query：显示 yarn 中可用的资源（内存、cpu 核数） -yqu、–queue :指定 yarn 资源队列 -ys、–slots :每个 TaskManager 使用的 Slot 数量。 -yz、–zookeeperNamespace:针对 HA 模式在 Zookeeper 上创建 NameSpace -yid、–applicationID : 指定 Yarn 集群上的任务 ID,附着到一个后台独 立运行的 Yarn Session 中。 4. Flink 的HA默认情况下，每个 Flink 集群只有一个 JobManager，这将导致单点故障（SPOF），如 果这个 JobManager 挂了，则不能提交新的任务，并且运行中的程序也会失败。使用 JobManager HA，集群可以从 JobManager 故障中恢复，从而避免单点故障。用户可以在 Standalone或Flink on Yarn 集群模式下配置 Flink 集群 HA（高可用性）。 Standalone HAStandalone 模式下，JobManager 的高可用性的基本思想是，任何时候都有一个 Alive JobManager 和多个 Standby JobManager。Standby JobManager 可以在 Alive JobManager 挂掉的情况下接管集群成为 Alive JobManager，这样避免了单点故障，一旦某一个 Standby JobManager 接管集群，程序就可以继续运行。Standby JobManagers 和 Alive JobManager 实例之间没有明确区别，每个 JobManager 都可以成为 Alive 或 StandbyFlink Standalone 集群的 HA 安装和配置实现 HA 还需要依赖 ZooKeeper 和 HDFS，因此要有一个 ZooKeeper 集群和 Hadoop 集群， 首先启动 Zookeeper 集群和 HDFS 集群。本文中分配 3 台 JobManager，如下表： hadoop101 hadoop102 hadoop103 JobManager JobManager JobManager TaskManager TaskManager TaskManager 修改配置文件 conf/masters hadoop101:8081 hadoop102:8081 hadoop103:8081 123 修改配置文件 conf/flink-conf.yaml #要启用高可用，设置修改为zookeeper high-availability: zookeeper #Zookeeper的主机名和端口信息，多个参数之间用逗号隔开 high-availability.zookeeper.quorum: hadoop103:2181,hadoop101:2181,hadoop102:2181 # 建议指定HDFS的全路径。如果某个Flink节点没有配置HDFS的话，不指定HDFS的全路径 则无法识到， # storageDir存储了恢复一个JobManager所需的所有元数据。 high-availability.storageDir: hdfs://hadoop101:9000/flink/h 1234567 把修改的配置文件拷贝其他服务器中 [root@hadoop101 conf]# scp masters flink-conf.yaml root@hadoop102:`pwd` [root@hadoop101 conf]# scp masters flink-conf.yaml root@hadoop103:`pwd` 12 启动集群版本问题：目前使用 Flink1.7.1 版本测试没有问题，使用 Flink1.9 版本存在 HA 界面不能自动跳转到对应的 Alive jobManager。 Flink On Yarn HA正常基于 Yarn 提交 Flink 程序，无论是使用 yarn-session 模式还是 yarn-cluster模 式 ， 基 于 yarn 运 行 后 的 application 只 要 kill 掉 对 应 的 Flink 集 群 进 程YarnSessionClusterEntrypoint后，基于 Yarn 的 Flink 任务就失败了，不会自动进行重试，所以基于 Yarn 运行 Flink 任务，也有必要搭建 HA，这里同样还是需要借助 zookeeper 来完成，步骤如下： 修改所有 Hadoop 节点的 yarn-site.xml 将所有 Hadoop 节点的 yarn-site.xml 中的提交应用程序最大尝试次数调大 #在每台hadoop节点yarn-site.xml中设置提交应用程序的最大尝试次数，建议不低于4， # 这里重试指的ApplicationMaster &lt;property> &lt;name>yarn.resourcemanager.am.max-attempts&lt;/name> &lt;value>4&lt;/value> &lt;/property> 123456 启动 zookeeper，启动 Hadoop 集群 修改 Flink 对应 flink-conf.yaml 配置，配置内容如下： #配置依赖zookeeper模式进行HA搭建 high-availability: zookeeper #配置JobManager原数据存储路径 high-availability.storageDir: hdfs://hadoop101:9000/flink/yarnha/ #配置zookeeper集群节点 high-availability.zookeeper.quorum: hadoop101:2181,hadoop102:2181,hadoop103:2181 #yarn停止一个application重试的次数 yarn.application-attempts: 10 1234567 启动 yarn-session.sh 测试 HA： yarn-session.sh -n 2 ，也可以直接提交 Job 启动之后，可以登录 yarn 中对应的 flink WebUI，如下图示： 点击对应的 Tracking UI，进入 Flink 集群 UI查看对应的 JobManager 在哪台节点上启动：进入对应的节点，kill 掉对应的YarnSessionClusterEntrypoint进程。然后进入到 Yarn 中观察applicationxxxx_0001job 信息：点击 job ID,发现会有对应的重试信息：点击对应的Tracking U进入到 Flink 集群 UI，查看新的 JobManager 节点由原来的 hadoop103 变成了 hadoop101，说明 HA 起作用。 4. Flink 并行度和 SlotFlink中每一个worker(TaskManager)都是一个JVM进程，它可能会在独立的线程（Solt） 上执行一个或多个 subtask。Flink 的每个 TaskManager 为集群提供 Solt。Solt 的数量通常 与每个 TaskManager 节点的可用 CPU 内核数成比例，一般情况下 Slot 的数量就是每个节点 的 CPU 的核数。 Slot 的 数 量 由 集 群 中 flink-conf.yaml 配 置 文 件 中 设 置 taskmanager.numberOfTaskSlots，这个值的大小建议和节点 CPU 的数量保持一致。比如我设置=3。并行度=2的情况下：注意一点：一个TaskSlot可能执行多个job。 一个任务的并行度设置可以从 4 个层面指定: Operator Level（算子层面）。 Execution Environment Level（执行环境层面）。 Client Level（客户端层面）。 System Level（系统层面）。 这 些 并 行 度 的 优 先 级 为 ：Operator Level &gt;Execution Environment Level &gt; Client Level &gt; System Level。 1. 并行度设置之 Operator LevelOperator、Source 和 Sink 目的地的并行度可以通过调用 setParallelism()方法来指定 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")) .map((_, 1)).setParallelism(2) .keyBy(0) //分组算子 : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数 .sum(1).setParallelism(2) //聚会累加算子 //5、打印结果 result.print(\"结果\").setParallelism(1) 123456 2. 并行度设置之 Execution Environment Level任务的默认并行度可以通过调用 setParallelism()方法指定。为了以并行度 3 来执行 所有的 Operator、Source 和 Sink，可以通过如下方式设置执行环境的并行度 //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(3) //默认所有算子的并行度为3 1234 3. 并行度设置之 Client Level并行度还可以在客户端提交 Job 到 Flink 时设定。对于 CLI 客户端，可以通过-p 参数指定并行度。 bin/flink run -p 10 WordCount.jar 1 4. 并行度设置之 System Level在系统级可以通过设置flink-conf.yaml文件中的parallelism.default属性来指定所 有执行环境的默认并行度。 # The parallelism used for programs that did not specify and other parallelism. parallelism.default: 1 12 5. 并行度案例分析Flink 集群中有 3 个 TaskManager 节点，每个 TaskManager 的 Slot 数量为 3全部默认情况下：全局并行度=2End：牢记并行度设置的优先级，根据集群配置合理设置参数。 4. Flink 常用API详解1. 函数阶层Flink 根据抽象程度分层，提供了三种不同的 API 和库。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。 ProcessFunctionProcessFunction 是 Flink 所提供最底层接口。ProcessFunction 可以处理一或两条 输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的 某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件 驱动应用所需要的基于单个事件的复杂业务逻辑。 DataStream APIDataStream API 为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条 记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如map()、reduce()、aggregate() 等函数。你可以通过扩 展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。 SQL &amp; Table API：Flink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API 和 SQL 借助了 Apache Calcite 来进行查询的解析，校验以及优化。它们可以与 DataStream 和 DataSet API 无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。 另外 Flink 具有数个适用于常见数据处理应用场景的扩展库。 复杂事件处理(CEP)：模式检测是事件流处理中的一个非常常见的用例。Flink 的 CEP 库提供了 API，使用户能够以例如正则表达式或状态机的方式指定事件模式。CEP 库与 Flink 的 DataStream API 集成，以便在 DataStream 上评估模式。CEP 库的应用包括 网络入侵检测，业务流程监控和欺诈检测。 DataSet API：DataSet API 是 Flink 用于批处理应用程序的核心 API。DataSet API 所提供的基础算子包括 map、reduce、(outer) join、co-group、iterate 等。所有算子都有相应的算法和数据结构支持，对内存中的序列化数据进行操作。如果数据大小超过预留内存，则过量数据将存储到磁盘。Flink 的 DataSet API 的数据处理算法借鉴了传统数据库算法的实现，例如混合散列连接（hybrid hash-join）和外部归并排序 （external merge-sort）。 Gelly:Gelly 是一个可扩展的图形处理和分析库。Gelly 是在 DataSet API 之上实现 的，并与 DataSet API 集成。因此，它能够受益于其可扩展且健壮的操作符。Gelly 提 供了内置算法，如 label propagation、triangle enumeration 和 PageRank 算法， 也提供了一个简化自定义图算法实现的 Graph API。 2. DataStream 的编程模型DataStream 的编程模型包括四个部分：Environment，DataSource，Transformation，Sink。此乃重点，接下来主要按照这四部分讲解。 3. Flink 的 DataSource 数据源基于文件、基于集合、基于Kafka、自定义的DataSource 1. 基于文件的Source读取本地文件系统的数据，前面的案例已经讲过了。本课程主要讲基于HDFS文件系统的 Source。首先需要配置 Hadoop 的依赖 &lt;dependency> &lt;groupId>org.apache.hadoop&lt;/groupId> &lt;artifactId>hadoop-common&lt;/artifactId> &lt;version>2.7.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.hadoop&lt;/groupId> &lt;artifactId>hadoop-client&lt;/artifactId> &lt;version>2.7.2&lt;/version> &lt;/dependency> 12345678910 代码： package com.sowhat.flink.source import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object HDFSFileSource { def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ //读取HDFS文件系统上的文件 val stream: DataStream[String] = streamEnv.readTextFile(\"hdfs://hadoop101:9000/wc.txt\") //单词统计的计算 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")) .map((_, 1)) .keyBy(0) .sum(1) //定义sink result.print() streamEnv.execute(\"wordcount\") } } 123456789101112131415161718192021222324252627282930 2. 基于集合的Sourcepackage com.sowhat.flink.source import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment /** * 基站日志 * @param sid 基站的id * @param callOut 主叫号码 * @param callInt 被叫号码 * @param callType 呼叫类型 * @param callTime 呼叫时间 (毫秒) * @param duration 通话时长 （秒） */ case class StationLog(sid: String, var callOut: String, var callInt: String, callType: String, callTime: Long, duration: Long) object CollectionSource { def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ val stream: DataStream[StationLog] = streamEnv.fromCollection(Array( new StationLog(\"001\", \"1866\", \"189\", \"busy\", System.currentTimeMillis(), 0), new StationLog(\"002\", \"1866\", \"188\", \"busy\", System.currentTimeMillis(), 0), new StationLog(\"004\", \"1876\", \"183\", \"busy\", System.currentTimeMillis(), 0), new StationLog(\"005\", \"1856\", \"186\", \"success\", System.currentTimeMillis(), 20) )) stream.print() streamEnv.execute() } } 1234567891011121314151617181920212223242526272829303132333435 3. 基于Kafka的Source首 先 需 要 配 置 Kafka 连 接 器 的 依 赖 ， 另 外 更 多 的 连 接 器 可 以 查 看 官 网 ：连接器 &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-connector-kafka_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.kafka&lt;/groupId> &lt;artifactId>kafka-clients&lt;/artifactId> &lt;version>0.11.0.3&lt;/version> &lt;/dependency> 12345678910 关于Kafka的demo参考 文章 1. 消费普通StringKafka生产者： [atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic sowhat >hello world >sowhat 1234 消费者 package com.sowhat.flink.source import java.util.Properties import org.apache.flink.api.common.serialization.SimpleStringSchema import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer import org.apache.kafka.common.serialization.StringDeserializer object KafkaSource1 { def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ //连接Kafka，并且Kafka中的数据是普通字符串（String） val props = new Properties() // 链接的Kafka 集群 props.setProperty(\"bootstrap.servers\", \"hadoop101:9092,hadoop102:9092,hadoop103:9092\") // 指定组名 props.setProperty(\"group.id\", \"fink01\") // 指定KV序列化类 props.setProperty(\"key.deserializer\", classOf[StringDeserializer].getName) props.setProperty(\"value.deserializer\", classOf[StringDeserializer].getName) // 从最新数据开始读 props.setProperty(\"auto.offset.reset\", \"latest\") // 订阅主题 val stream: DataStream[String] = streamEnv.addSource(new FlinkKafkaConsumer[String](\"sowhat\", new SimpleStringSchema(), props)) stream.print() streamEnv.execute() } } 1234567891011121314151617181920212223242526272829303132333435363738394041 2. 消费KV形式Kafka模式就是输入的KV只是平常只用V而已，如果用消费者KV则我们需要代码编写生产者跟消费者。生产者： package com.sowhat.flink.source import java.util.Properties import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord} import org.apache.kafka.common.serialization.StringSerializer import scala.util.Random object MyKafkaProducer { def main(args: Array[String]): Unit = { //连接Kafka的属性 val props = new Properties() // 链接的集群 props.setProperty(\"bootstrap.servers\", \"hadoop101:9092,hadoop102:9092,hadoop103:9092\") // 序列化KV类 props.setProperty(\"key.serializer\", classOf[StringSerializer].getName) props.setProperty(\"value.serializer\", classOf[StringSerializer].getName) var producer = new KafkaProducer[String, String](props) var r = new Random() while (true) { //死循环生成键值对的数据 val data = new ProducerRecord[String, String](\"sowhat\", \"key\" + r.nextInt(10), \"value\" + r.nextInt(100)) producer.send(data) Thread.sleep(1000) } producer.close() } } 123456789101112131415161718192021222324252627282930 消费者： package com.sowhat.flink.source import java.util.Properties import org.apache.flink.api.common.typeinfo.TypeInformation import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer, KafkaDeserializationSchema} import org.apache.kafka.clients.consumer.ConsumerRecord import org.apache.kafka.common.serialization.StringDeserializer //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ object KafkaSourceByKeyValue { def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //连接Kafka的属性 val props = new Properties() props.setProperty(\"bootstrap.servers\", \"hadoop101:9092,hadoop102:9092,hadoop103:9092\") props.setProperty(\"group.id\", \"flink002\") props.setProperty(\"key.deserializer\", classOf[StringDeserializer].getName) props.setProperty(\"value.deserializer\", classOf[StringDeserializer].getName) props.setProperty(\"auto.offset.reset\", \"latest\") //设置Kafka数据源 val stream: DataStream[(String, String)] = streamEnv.addSource(new FlinkKafkaConsumer[(String, String)](\"sowhat\", new MyKafkaReader, props)) stream.print() streamEnv.execute() } //自定义一个类，从Kafka中读取键值对的数据 class MyKafkaReader extends KafkaDeserializationSchema[(String, String)] { //是否流结束 override def isEndOfStream(nextElement: (String, String)): Boolean = { false } // 反序列化 override def deserialize(record: ConsumerRecord[Array[Byte], Array[Byte]]): (String, String) = { if (record != null) { var key = \"null\" var value = \"null\" if (record.key() != null) { key = new String(record.key(), \"UTF-8\") } if (record.value() != null) { //从Kafka记录中得到Value value = new String(record.value(), \"UTF-8\") } (key, value) } else { //数据为空 (\"null\", \"null\") } } //指定类型 override def getProducedType: TypeInformation[(String, String)] = { createTuple2TypeInformation(createTypeInformation[String], createTypeInformation[String]) } } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 4. 自定义Source当然也可以自定义数据源，有两种方式实现： 通过实现 SourceFunction接口来自定义无并行度（也就是并行度只能为 1）的 Source。 通过实现 ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 来自 定义有并行度的数据源。 package com.sowhat.flink.source import org.apache.flink.streaming.api.functions.source.SourceFunction import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _} import scala.util.Random case class StationLog(sid: String, var callOut: String, var callInt: String, callType: String, callTime: Long, duration: Long) /** * 自定义的Source,需求：每隔两秒钟，生成10条随机基站通话日志数据 */ class MyCustomerSource extends SourceFunction[StationLog] { //是否终止数据流的标记 var flag = true; /** * 主要的方法，启动一个Source，并且从Source中返回数据 * 如果run方法停止，则数据流终止 */ override def run(ctx: SourceFunction.SourceContext[StationLog]): Unit = { val r = new Random() var types = Array(\"fail\", \"basy\", \"barring\", \"success\") while (flag) { 1.to(10).map(_ => { var callOut = \"1860000%04d\".format(r.nextInt(10000)) //主叫号码 var callIn = \"1890000%04d\".format(r.nextInt(10000)) //被叫号码 //生成一条数据 new StationLog(\"station_\" + r.nextInt(10), callOut, callIn, types(r.nextInt(4)), System.currentTimeMillis(), r.nextInt(20)) }).foreach(ctx.collect(_)) //发送数据到流 Thread.sleep(2000) //每隔2秒发送一次数据 } } //终止数据流 override def cancel(): Unit = { flag = false; } } object CustomerSource { def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment streamEnv.setParallelism(1) val stream: DataStream[StationLog] = streamEnv.addSource(new MyCustomerSource) stream.print() streamEnv.execute(\"SelfSource\") } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 4. Flink 的 Sink 数据目标Flink 针对 DataStream 提供了大量的已经实现的数据目标（Sink），包括文件、Kafka、Redis、HDFS、Elasticsearch 等等。 1. 基于 HDFS 的 Sink首先配置支持 Hadoop FileSystem 的连接器依赖。 &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-connector-filesystem_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> 12345 Streaming File Sink 能把数据写入 HDFS 中，还可以支持分桶写入，每一个 分桶 就对 应 HDFS 中的一个目录。默认按照小时来分桶，在一个桶内部，会进一步将输出基于滚动策 略切分成更小的文件。这有助于防止桶文件变得过大。滚动策略也是可以配置的，默认策略会根据文件大小和超时时间来滚动文件，超时时间是指没有新数据写入部分文件（part file）的时间。 需求：把自定义的Source作为数据源，把基站日志数据 写入HDFS 并且每隔10秒钟生成一个文件 package com.sowhat.flink.sink import com.sowhat.flink.source.{MyCustomerSource, StationLog} import org.apache.flink.api.common.serialization.SimpleStringEncoder import org.apache.flink.core.fs.Path import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _} object HDFSSink { //需求：把自定义的Source作为数据源，把基站日志数据写入HDFS并且每隔10钟生成一个文件 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment streamEnv.setParallelism(1) //读取数据源 val stream: DataStream[StationLog] = streamEnv.addSource(new MyCustomerSource) //默认一个小时一个目录(分桶) //设置一个滚动策略 val rolling: DefaultRollingPolicy[StationLog, String] = DefaultRollingPolicy.create() .withInactivityInterval(5000) //不活动的分桶时间 .withRolloverInterval(10000) //每隔10 生成一个文件 .build() //创建 //创建HDFS的Sink val hdfsSink: StreamingFileSink[StationLog] = StreamingFileSink.forRowFormat[StationLog]( new Path(\"hdfs://hadoop101:9000/MySink001/\"), new SimpleStringEncoder[StationLog](\"UTF-8\")) .withRollingPolicy(rolling) .withBucketCheckInterval(1000) //检查间隔时间 .build() stream.addSink(hdfsSink) streamEnv.execute() } } 12345678910111213141516171819202122232425262728293031323334353637383940 2. 基于 Redis的 SinkFlink 除了内置的 连接器 外，还有一些额外的连接器通过 Apache Bahir 发布，包括： Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) Netty (source) 这里我用 Redis 来举例，首先需要配置 Redis 连接器的依赖：需求：把netcat作为数据源，并且统计每个单词的次数，统计的结果写入Redis数据库中。导入依赖： &lt;dependency> &lt;groupId>org.apache.bahir&lt;/groupId> &lt;artifactId>flink-connector-redis_2.11&lt;/artifactId> &lt;version>1.0&lt;/version> &lt;/dependency> 12345 代码如下： package com.sowhat.flink.sink import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.connectors.redis.RedisSink import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig import org.apache.flink.streaming.connectors.redis.common.mapper.{RedisCommand, RedisCommandDescription, RedisMapper} object RedisSink { //需求：把netcat作为数据源，并且统计每个单词的次数，统计的结果写入Redis数据库中。 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment streamEnv.setParallelism(1) import org.apache.flink.streaming.api.scala._ //读取数据源 val stream: DataStream[String] = streamEnv.socketTextStream(\"hadoop101\", 8888) //计算 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")) .map((_, 1)) .keyBy(0) // 等价于groupbyKey .sum(1) //把结果写入Redis中 设置连接Redis的配置 val config: FlinkJedisPoolConfig = new FlinkJedisPoolConfig.Builder().setDatabase(3).setHost(\"hadoop101\").setPort(6379).build() //设置Redis的Sink result.addSink(new RedisSink[(String, Int)](config, new RedisMapper[(String, Int)] { //设置redis的命令 override def getCommandDescription = { new RedisCommandDescription(RedisCommand.HSET, \"sowhat\") // https://bahir.apache.org/docs/flink/current/flink-streaming-redis/ } //从数据中获取Key override def getKeyFromData(data: (String, Int)) = { data._1 } //从数据中获取Value override def getValueFromData(data: (String, Int)) = { data._2 + \"\" } })) streamEnv.execute(\"redisSink\") } } 12345678910111213141516171819202122232425262728293031323334353637383940414243 3. 基于 Kafka的 Sink由于前面有的课程已经讲过 Flink 的 Kafka 连接器，所以还是一样需要配置 Kafka 连接 器的依赖配置，接下我们还是把 WordCout 的结果写入 Kafka： 1. Kafka作为Sink的第一种（String）需求：把netcat数据源中每个单词写入Kafka package com.sowhat.flink.sink import org.apache.flink.api.common.serialization.SimpleStringSchema import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer object KafkaSinkByString { //Kafka作为Sink的第一种（String） //需求：把netcat数据源中每个单词写入Kafka def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ streamEnv.setParallelism(1) //读取数据源 val stream: DataStream[String] = streamEnv.socketTextStream(\"hadoop101\",8888) //计算 val words: DataStream[String] = stream.flatMap(_.split(\" \")) //把单词写入Kafka words.addSink(new FlinkKafkaProducer[String](\"hadoop101:9092,hadoop102:9092,hadoop103:9092\",\"sowhat\",new SimpleStringSchema())) streamEnv.execute(\"kafkaSink\") } } 12345678910111213141516171819202122 写入到Kafka后可以在终端开一个消费者。 bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic sowhat 12. Kafka作为Sink的第二种(KV)需求：把netcat作为数据源，统计每个单词的数量，并且把统计的结果写入Kafka package com.sowhat.flink.sink import java.lang import java.util.Properties import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaProducer, KafkaSerializationSchema} import org.apache.kafka.clients.producer.ProducerRecord object KafkaSinkByKeyValue { //Kafka作为Sink的第二种（KV） //把netcat作为数据源，统计每个单词的数量，并且把统计的结果写入Kafka def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._; streamEnv.setParallelism(1) //读取数据源 val stream: DataStream[String] = streamEnv.socketTextStream(\"hadoop101\", 8888) //计算 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")) .map((_, 1)) .keyBy(0) .sum(1) //创建连接Kafka的属性 var props = new Properties() props.setProperty(\"bootstrap.servers\", \"hadoop101:9092,hadoop102:9092,hadoop103:9092\") //创建一个Kafka的sink var kafkaSink = new FlinkKafkaProducer[(String, Int)]( \"sowhat\", new KafkaSerializationSchema[(String, Int)] { //自定义的匿名内部类 override def serialize(element: (String, Int), timestamp: lang.Long) = { new ProducerRecord(\"sowhat\", element._1.getBytes, (element._2 + \"\").getBytes) } }, props, //连接Kafka的数学 FlinkKafkaProducer.Semantic.EXACTLY_ONCE //精确一次 ) result.addSink(kafkaSink) streamEnv.execute(\"kafka的sink的第二种\") //--property print.key=true Kafka的命令加一个参数 } } 1234567891011121314151617181920212223242526272829303132333435363738394041 生成写入KV后可以定义消费者： bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning \\ --topic sowhat --property print.key=true Kafka的命令加一个参数 1234 4. 基于HBase的Sink引入依赖： &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-hbase --> &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-hbase_2.12&lt;/artifactId> &lt;version>1.10.0&lt;/version> &lt;/dependency> 123456 代码： packge com.sowhat.demo import java.text.SimpleDateFormat import java.util.Date import org.apache.flink.configuration.Configuration import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction} import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName} import org.apache.hadoop.hbase.client._ import org.apache.hadoop.hbase.util.Bytes class HBaseWriter extends RichSinkFunction[String] { var conn: Connection = null val scan: Scan = null var mutator: BufferedMutator = null var count:Int = 0 override def open(parameters: Configuration): Unit = { val config: org.apache.hadoop.conf.Configuration = HBaseConfiguration.create config.set(HConstants.ZOOKEEPER_QUORUM, \"IP1,IP2,IP3\") config.set(HConstants.ZOOKEEPER_CLIENT_PORT, \"2181\") config.setInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 30000) config.setInt(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 30000) conn = ConnectionFactory.createConnection(config) val tableName: TableName = TableName.valueOf(\"sowhat\") val params: BufferedMutatorParams = new BufferedMutatorParams(tableName) //设置缓存1m，当达到1m时数据会自动刷到hbase params.writeBufferSize(100) mutator = conn.getBufferedMutator(params) count = 0 } override def invoke(value: String, context: SinkFunction.Context[_]): Unit = { val cf1 = \"m\" val value1 = value.replace(\" \", \"\") val put: Put = new Put(Bytes.toBytes(\"rk\" + value1)) put.addColumn(Bytes.toBytes(cf1), Bytes.toBytes(\"time\"), Bytes.toBytes(\"v\" + value1)) mutator.mutate(put) //每满2000条刷新一下数据 if (count >= 10) { mutator.flush() count = 0 } count = count + 1 } /** * 关闭 */ override def close(): Unit = { if (conn != null) conn.close() } } --- package com.sowhat.demo import java.util.Properties import org.apache.flink.api.common.serialization.SimpleStringSchema import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.api.scala._ import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011 object HbaseRw { def main(args: Array[String]): Unit = { val properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"10.100.34.111:9092,10.100.34.133:9092\") properties.setProperty(\"group.id\", \"timer.hbase\") val env:StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val stream: DataStream[String] = env.addSource(new FlinkKafkaConsumer011[String](\"sowhat\", new SimpleStringSchema(), properties)) stream.addSink(new HBaseWriter) env.execute(\"hbase write\") } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475 5. 自定义 的 Sink当然你可以自己定义 Sink，有两种实现方式：1、实现 SinkFunction接口。2、实现 RichSinkFunction类。后者增加了生命周期的管理功能。比如需要在 Sink 初始化的时候创 建连接对象，则最好使用第二种。需求：随机生成StationLog对象，写入MySQL数据库的表t_station_log中引入依赖： &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;version>5.1.44&lt;/version> &lt;/dependency> 12345 代码如下： package com.sowhat.flink.sink import java.sql.{Connection, DriverManager, PreparedStatement} import com.sowhat.flink.source.{MyCustomerSource, StationLog} import org.apache.flink.configuration.Configuration import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction} import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment case class StationLog(sid: String, var callOut: String, var callInt: String, callType: String, callTime: Long, duration: Long) object CustomerJdbcSink { //需求：随机生成StationLog对象，写入Mysql数据库的表（t_station_log）中 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ streamEnv.setParallelism(1) val stream: DataStream[StationLog] = streamEnv.addSource(new MyCustomerSource) //数据写入Mysql，所有需要创建一个自定义的sink stream.addSink(new MyCustomerJdbcSink) streamEnv.execute(\"jdbcSink\") } /** * 自定义的Sink类 */ class MyCustomerJdbcSink extends RichSinkFunction[StationLog]{ var conn :Connection=_ var pst :PreparedStatement=_ //把StationLog对象写入Mysql表中，每写入一条执行一次 override def invoke(value: StationLog, context: SinkFunction.Context[_]): Unit = { pst.setString(1,value.sid) pst.setString(2,value.callOut) pst.setString(3,value.callInt) pst.setString(4,value.callType) pst.setLong(5,value.callTime) pst.setLong(6,value.duration) pst.executeUpdate() } //Sink初始化的时候调用一次，一个并行度初始化一次 //创建连接对象，和Statement对象 override def open(parameters: Configuration): Unit = { conn =DriverManager.getConnection(\"jdbc:mysql://localhost/test\",\"root\",\"123123\") pst =conn.prepareStatement(\"insert into t_station_log (sid,call_out,call_in,call_type,call_time,duration) values (?,?,?,?,?,?)\") } override def close(): Unit = { pst.close() conn.close() } } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 5. DataStream转换算子此时再将中间的转换算子Transformation，即通过从一个或多个 DataStream 生成新的 DataStream 的过程被称为 Transformation 操作。在转换过程中，每种操作类型被定义为不同的 Operator，Flink 程序能够将多个 Transformation 组成一个 DataFlow 的拓扑。 1. Map [DataStream-&gt;DataStream]调 用 用 户 定 义 的 MapFunction 对 DataStream[T] 数 据 进 行 处 理 ， 形 成 新 的 DataStream[T]，其中数据格式可能会发生变化，常用作对数据集内数据的清洗和转换。例如将输入数据集中的每个数值全部加 1 处理，并且将数据输出到下游数据集。 2. FlatMap [DataStream-&gt;DataStream]该算子主要应用处理输入一个元素产生一个或者多个元素的计算场景，比较常见的是在 经典例子 WordCount 中，将每一行的文本数据切割，生成单词序列如在图所示，对于输入 DataStream[String]通过 FlatMap 函数进行处理，字符串数字按逗号切割，然后形成新的整 数数据集。 val resultStream[String] = dataStream.flatMap { str => str.split(\" \") } 1 3. Filter [DataStream-&gt;DataStream]该算子将按照条件对输入数据集进行筛选操作，将符合条件(过滤表达式=true)的数据集输出，将不符合条件的数据过滤掉。如下图所示将输入数据集中偶数过滤出来，奇数从数据集中去除。 val filter:DataStream[Int] = dataStream.filter { _ % 2 == 0 } 1 4. KeyBy [DataStream-&gt;KeyedStream]该算子根据指定的 Key 将输入的 DataStream[T]数据格式转换为 KeyedStream[T]，也就是在数据集中执行 Partition 操作，将相同的 Key 值的数据放置在相同的分区中。默认是根据注定数据的hashcode来分的。 val test: DataStream[(String, Int)] = streamEnv.fromElements((\"1\", 5), (\"2\", 2), (\"2\", 4), (\"1\", 3)) val value: KeyedStream[(String, Int), String] = test.keyBy(_._1) /** * （String,Int) => 是进行keyBy的数据类型 * String => 是分流的key的数据类型 */ --- val test: DataStream[(String, Int)] = streamEnv.fromElements((\"1\", 5), (\"2\", 2), (\"2\", 4), (\"1\", 3)) val value: KeyedStream[(String, Int), Tuple] = test.keyBy(0) /** * （String,Int) => 是进行keyBy的数据类型 * Tuple => 是分流的key的数据类型 */ 12345678910111213 5. Reduce [KeyedStream-&gt;DataStream]该算子和 MapReduce 中 Reduce 原理基本一致，主要目的是将输入的KeyedStream通过 传 入 的 用 户 自 定 义 的 ReduceFunction滚 动 地 进 行 数 据 聚 合 处 理 ， 其 中 定 义 的 ReduceFunciton 必须满足运算结合律和交换律。如下代码对传入 keyedStream 数据集中相同的 key 值的数据独立进行求和运算，得到每个 key 所对应的求和值。 val test: DataStream[(String, Int)] = streamEnv.fromElements((\"a\", 3), (\"d\", 4), (\"c\", 2), (\"c\", 5), (\"a\", 5)) val value: KeyedStream[(String, Int), Tuple] = test.keyBy(0) // 滚动对第二个字段进行reduce相加求和 val reduceStream: DataStream[(String, Int)] = value.reduce { (t1, t2) => (t1._1, t1._2 + t2._2) } 1234 结果： 2&gt; (c,2) 3&gt; (a,3) 3&gt; (d,4) 2&gt; (c,7) 3&gt; (a,8) 12345PS：对于该结果需要说明下为什么key相同的出现了多次，这主要是Flink流式处理思想的体现，迭代式的输出结果。 6. Aggregations[KeyedStream-&gt;DataStream]Aggregations 是 KeyedDataStream 接口提供的聚合算子，根据指定的字段进行聚合操 作，滚动地产生一系列数据聚合结果。其实是将 Reduce 算子中的函数进行了封装，封装的 聚合操作有 sum、min、minBy、max、maxBy等，这样就不需要用户自己定义 Reduce 函数。 如下代码所示，指定数据集中第一个字段作为 key，用第二个字段作为累加字段，然后滚动地对第二个字段的数值进行累加并输出 streamEnv.setParallelism(1) val test: DataStream[(String, Int)] = streamEnv.fromElements((\"a\", 3), (\"d\", 4), (\"c\", 2), (\"c\", 5), (\"a\", 5)) val value: KeyedStream[(String, Int), Tuple] = test.keyBy(0) // 滚动对第二个字段进行reduce相加求和 val reduceStream: DataStream[(String, Int)] = value.reduce { (t1, t2) => (t1._1, t1._2 + t2._2) } // 相当于reduce更简化版的 聚合 val sumStream: DataStream[(String, Int)] = value.sum(1) 1234567 结果： (a,3) (d,4) (c,2) (c,7) (a,8) 123457. Union[DataStream -&gt;DataStream]Union 算子主要是将两个或者多个输入的数据集合并成一个数据集，需要保证两个数据 集的格式一致，输出的数据集的格式和输入的数据集格式保持一致。 import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object TestUnion { def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ streamEnv.setParallelism(1) var stream1 = streamEnv.fromElements((\"a\", 1), (\"b\", 2)) var stream2 = streamEnv.fromElements((\"b\", 5), (\"d\", 6)) var stream3 = streamEnv.fromElements((\"e\", 7), (\"f\", 8)) val result: DataStream[(String, Int)] = stream1.union(stream2, stream3) result.print() streamEnv.execute() } } 12345678910111213141516 结果： (a,1) (b,2) (e,7) (f,8) (b,5) (d,6) 1234568. Connect、CoMap、CoFlatMap[DataStream -&gt;ConnectedStream-&gt;DataStream]Connect 算子主要是为了合并两种或者多种不同数据类型的数据集，合并后会保留原来 数据集的数据类型。例如：dataStream1 数据集为(String, Int)元祖类型，dataStream2 数据集为 Int 类型，通过 connect 连接算子将两个不同数据类型的流结合在一起，形成格式 为 ConnectedStreams 的数据集，其内部数据为[(String, Int), Int]的混合数据类型，保留了两个原始数据集的数据类型。 需要注意的是，对于 ConnectedStreams 类型的数据集不能直接进行类似 Print()的操 作，需要再转换成 DataStream 类型数据集，在 Flink 中 ConnectedStreams 提供的 map()方 法和flatMap() import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object TestConnect { def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ streamEnv.setParallelism(1) val stream1: DataStream[(String, Int)] = streamEnv.fromElements((\"a\", 1), (\"b\", 2), (\"c\", 3)) val stream2: DataStream[String] = streamEnv.fromElements(\"e\", \"f\", \"g\") val stream3: ConnectedStreams[(String, Int), String] = stream1.connect(stream2) //注意得到ConnectedStreams，实际上里面的数据没有真正合并 //使用CoMap,或者CoFlatmap val result: DataStream[(String, Int)] = stream3.map( //第一个处理的函数 t => { (t._1, t._2) }, //第二个处理的函数 t => { (t, 0) } ) result.print() streamEnv.execute() } } 123456789101112131415161718192021222324252627 结果： (e,0) (f,0) (g,0) (a,1) (b,2) (c,3) 123456注意： Union 之前两个流的类型必须是一样，Connect 可以不一样，在之后的 coMap 中再去调 整成为一样的。 Connect 只能操作两个流，Union 可以操作多个。 9. Split 和 select [DataStream-&gt;SplitStream-&gt;DataStream]Split 算子是将一个 DataStream 数据集按照条件进行拆分，形成两个数据集的过程， 也是 union 算子的逆向实现。每个接入的数据都会被路由到一个或者多个输出数据集中。Side Output import com.sowhat.flink.source.{MyCustomerSource, StationLog} import org.apache.flink.streaming.api.functions.ProcessFunction import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.util.Collector object TestSplitAndSelect { //需求：从自定义的数据源中读取基站通话日志，把通话成功的和通话失败的分离出来 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ streamEnv.setParallelism(1) //读取数据源 val stream: DataStream[StationLog] = streamEnv.addSource(new MyCustomerSource) // this needs to be an anonymous inner class, so that we can analyze the type val successTag = OutputTag[StationLog](\"success\") val nosuccessTag = OutputTag[StationLog](\"nosuccess\") val sideoutputStream: DataStream[StationLog] = stream.process(new ProcessFunction[StationLog, StationLog] { override def processElement(value: StationLog, ctx: ProcessFunction[StationLog, StationLog]#Context, out: Collector[StationLog]): Unit = { if (value.callType.equals(\"success\")) { ctx.output(successTag, value) } else { ctx.output(nosuccessTag, value) } } }) sideoutputStream.getSideOutput(successTag).print(\"成功数据\") sideoutputStream.getSideOutput(nosuccessTag).print(\"未成功数据\") //切割 val splitStream: SplitStream[StationLog] = stream.split( //流并没有真正切割 log => { if (log.callType.equals(\"success\")) { Seq(\"Success\") } else { Seq(\"NOSuccess\") } } ) //选择不同的流 根据标签得到不同流 val stream1: DataStream[StationLog] = splitStream.select(\"Success\") val stream2: DataStream[StationLog] = splitStream.select(\"NOSuccess\") stream.print(\"原始数据\") stream1.print(\"通话成功\") stream2.print(\"通话不成功\") streamEnv.execute() } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 函数类和富函数类前面学过的所有算子几乎都可以自定义一个函数类、富函数类作为参数。因为 Flink 暴露者两种函数类的接口，常见的函数接口有： MapFunction FlatMapFunction ReduceFunction 。。。。。 富函数接口它其他常规函数接口的不同在于：可以获取运行环境的上下文，在上下文环境中可以管理状态，并拥有一些生命周期方法，所以可以实现更复杂的功能。富函数的接口有： RichMapFunction RichFlatMapFunction RichFilterFunction RichSinkFunction 1. 普通函数类型普通函数类举例：按照指定的时间格式输出每个通话的拨号时间和结束时间。resources目录下station.log文件内容如下： station_0,18600003612,18900004575,barring,1577080453123,0 station_9,18600003186,18900002113,success,1577080453123,32 station_3,18600003794,18900009608,success,1577080453123,4 station_1,18600000005,18900007729,fail,1577080453123,0 station_1,18600000005,18900007729,success,1577080603123,349 station_8,18600007461,18900006987,barring,1577080453123,0 station_5,18600009356,18900006066,busy,1577080455129,0 station_4,18600001941,18900003949,busy,1577080455129,0 12345678代码如下： package com.sowhat.flink.transformation import java.net.URLDecoder import java.text.SimpleDateFormat import java.util.Date import com.sowhat.flink.source.StationLog import org.apache.flink.api.common.functions.MapFunction import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object TestFunctionClass { //计算出每个通话成功的日志中呼叫起始和结束时间,并且按照指定的时间格式 //数据源来自本地文件 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 var filePath = getClass.getResource(\"/station.log\").getPath filePath = URLDecoder.decode(filePath, \"utf-8\") val stream: DataStream[StationLog] = streamEnv.readTextFile(filePath) .map(line => { var arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) //定义一个时间格式 val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\") //计算通话成功的起始和结束时间 val result: DataStream[String] = stream.filter(_.callType.equals(\"success\")) .map(new MyMapFunction(format)) //result.print() val result1: DataStream[String] = stream.filter(_.callType.equals(\"success\")).map { x => { val startTime = x.callTime val endTime = startTime + x.duration * 1000 \"主叫号码：\" + x.callOut + \",被叫号码:\" + x.callInt + \",呼叫起始时间:\" + format.format(new Date(startTime)) + \",呼叫结束时间:\" + format.format(new Date(endTime)) } } result1.print() streamEnv.execute() } //自定义一个函数类 指定输入 跟输出类型 class MyMapFunction(format: SimpleDateFormat) extends MapFunction[StationLog, String] { override def map(value: StationLog): String = { val startTime = value.callTime val endTime = startTime + value.duration * 1000 \"主叫号码：\" + value.callOut + \",被叫号码:\" + value.callInt + \",呼叫起始时间:\" + format.format(new Date(startTime)) + \",呼叫结束时间:\" + format.format(new Date(endTime)) } } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 2. 富函数类型富函数类举例：把呼叫成功的通话信息转化成真实的用户姓名，通话用户对应的用户表 （在 Mysql 数据中）由于需要从数据库中查询数据，就需要创建连接，创建连接的代码必须写在生命周期的 open 方法中。所以需要使用富函数类。Rich Function 有一个生命周期的概念。典型的生命周期方法有： open()方法是 rich function 的初始化方法，当一个算子例如 map 或者 filter 被调用 之前 open()会被调用。 close()方法是生命周期中的最后一个调用的方法，做一些清理工作。 getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函数执行的 并行度，任务的名字，以及 state 状态 package com.sowhat.flink.transformation import java.sql.{Connection, DriverManager, PreparedStatement, ResultSet} import com.sowhat.flink.source.StationLog import org.apache.flink.api.common.functions.RichMapFunction import org.apache.flink.configuration.Configuration import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object TestRichFunctionClass { /** * 把通话成功的电话号码转换成真是用户姓名，用户姓名保存在Mysql表中 * @param args */ def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 var filePath = getClass.getResource(\"/station.log\").getPath val stream: DataStream[StationLog] = streamEnv.readTextFile(filePath) .map(line => { var arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) //计算：把电话号码变成用户姓名 val result: DataStream[StationLog] = stream.filter(_.callType.equals(\"success\")) .map(new MyRichMapFunction) result.print() streamEnv.execute() } //自定义一个富函数类 class MyRichMapFunction extends RichMapFunction[StationLog, StationLog] { var conn: Connection = _ var pst: PreparedStatement = _ override def open(parameters: Configuration): Unit = { conn = DriverManager.getConnection(\"jdbc:mysql://localhost/test\", \"root\", \"123123\") pst = conn.prepareStatement(\"select name from t_phone where phone_number=?\") } override def close(): Unit = { pst.close() conn.close() } override def map(value: StationLog): StationLog = { // 获取上下文信息 比如获取子线程 println(getRuntimeContext.getTaskNameWithSubtasks) //查询主叫号码对应的姓名 pst.setString(1, value.callOut) val result: ResultSet = pst.executeQuery() if (result.next()) { value.callOut = result.getString(1) } //查询被叫号码对应的姓名 pst.setString(1, value.callInt) val result2: ResultSet = pst.executeQuery() if (result2.next()) { value.callInt = result2.getString(1) } value } } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364 3. 底层 ProcessFunctionAPIProcessFunction 是一个低层次的流处理操作，允许返回所有 Stream 的基础构建模块，可以说是Flink的杀手锏了。 访问 Event 本身数据（比如：Event 的时间，Event 的当前 Key 等） 管理状态 State（仅在 Keyed Stream 中） 管理定时器 Timer（包括：注册定时器，删除定时器等） 总而言之，ProcessFunction 是 Flink 最底层的 API，也是功能最强大的。 需求：监控每一个手机，如果在 5 秒内呼叫它的通话都是失败的，发出警告信息。注意： 本demo中会用到状态编程，只要知道状态的意思，不需要掌握。后面的文章中会详细讲解 State 编程。 package com.sowhat.flink.transformation import com.sowhat.flink.source.StationLog import org.apache.flink.api.common.state.{ValueState, ValueStateDescriptor} import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.util.Collector /** * 监控每一个手机号码，如果这个号码在5秒内，所有呼叫它的日志都是失败的，则发出告警信息 * 如果在5秒内只要有一个呼叫不是fail则不用告警 */ /** * 基站日志 * @param sid 基站的id * @param callOut 主叫号码 * @param callInt 被叫号码 * @param callType 呼叫类型 * @param callTime 呼叫时间 (毫秒) * @param duration 通话时长 （秒） */ case class StationLog(sid: String, var callOut: String, var callInt: String, callType: String, callTime: Long, duration: Long) object TestProcessFunction { def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 通过 netcat 发送 数据源 val stream: DataStream[StationLog] = streamEnv.socketTextStream(\"IP1\", 8888) .map(line => { val arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) // 按照呼入电话分组 val result: DataStream[String] = stream.keyBy(_.callInt) .process(new MonitorCallFail) result.print() streamEnv.execute() } //自定义一个底层的类 第一个是key类型，第二个是处理对象类型，第三个是返回类型 class MonitorCallFail extends KeyedProcessFunction[String, StationLog, String] { //使用一个状态对象记录时间 lazy val timeState: ValueState[Long] = getRuntimeContext.getState(new ValueStateDescriptor[Long](\"time\", classOf[Long])) override def processElement(value: StationLog, ctx: KeyedProcessFunction[String, StationLog, String]#Context, out: Collector[String]): Unit = { //从状态中取得时间 val time:Long = timeState.value() if (time == 0 &amp;&amp; value.callType.equals(\"fail\")) { //表示第一次发现呼叫失败，记录当前的时间 //获取当前系统时间，并注册定时器 val nowTime:Long = ctx.timerService().currentProcessingTime() //定时器在5秒后触发 val onTime:Long = nowTime + 5 * 1000L ctx.timerService().registerProcessingTimeTimer(onTime) //把触发时间保存到状态中 timeState.update(onTime) } if (time != 0 &amp;&amp; !value.callType.equals(\"fail\")) { //表示有一次成功的呼叫,必须要删除定时器 ctx.timerService().deleteProcessingTimeTimer(time) timeState.clear() //清空状态中的时间 } } //时间到了，定时器执行, override def onTimer(timestamp: Long, ctx: KeyedProcessFunction[String, StationLog, String]#OnTimerContext, out: Collector[String]): Unit = { val warnStr:String = \"触发的时间：\" + timestamp + \" 手机号 ：\" + ctx.getCurrentKey out.collect(warnStr) timeState.clear() } } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667 4. 侧输出流 Side Output在 Flink 处理数据流时，我们经常会遇到这样的情况：在处理一个数据源时，往往需要将该源中的不同类型的数据做分割处理，如果使用 filter 算子对数据源进行筛选分割的话，势必会造成数据流的多次复制，造成不必要的性能浪费；flink 中的侧输出就是将数据 流进行分割，而不对流进行复制的一种分流机制。flink 的侧输出的另一个作用就是对延时迟到的数据进行处理，这样就可以不必丢弃迟到的数据。在后面的文章中会讲到！案例：根据基站的日志，请把呼叫成功的 Stream（主流）和不成功的 Stream（侧流） 分别输出。 package com.sowhat.flink.transformation import com.sowhat.flink.source.StationLog import org.apache.flink.streaming.api.functions.ProcessFunction import org.apache.flink.util.Collector object TestSideOutputStream { import org.apache.flink.streaming.api.scala._ var notSuccessTag: OutputTag[StationLog] = new OutputTag[StationLog](\"not_success\") //不成功的侧流标签 //把呼叫成功的日志输出到主流，不成功的到侧流 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //读取数据源 var filePath: String = getClass.getResource(\"/station.log\").getPath val stream: DataStream[StationLog] = streamEnv.readTextFile(filePath) .map(line => { var arr: Array[String] = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) val result: DataStream[StationLog] = stream.process(new CreateSideOuputStream(notSuccessTag)) result.print(\"主流\") //一定要根据主流得到侧流 val sideStream: DataStream[StationLog] = result.getSideOutput(notSuccessTag) sideStream.print(\"侧流\") streamEnv.execute() } class CreateSideOuputStream(tag: OutputTag[StationLog]) extends ProcessFunction[StationLog, StationLog] { override def processElement(value: StationLog, ctx: ProcessFunction[StationLog, StationLog]#Context, out: Collector[StationLog]): Unit = { if (value.callType.equals(\"success\")) { //输出主流 out.collect(value) } else { //输出侧流 ctx.output(tag, value) } } } } 1234567891011121314151617181920212223242526272829303132333435363738 5. Flink State管理跟恢复Flink 是一个默认就有状态的分析引擎，前面的 WordCount 案例可以做到单词的数量的累加，其实是因为在内存中保证了每个单词的出现的次数，这些数据其实就是状态数据。但是如果一个 Task 在处理过程中挂掉了，那么它在内存中的状态都会丢失，所有的数据都需要重新计算。从容错和消息处理的语义（At -least-once 和 Exactly-once）上来说，Flink 引入了 State 和CheckPoint。 State 一般指一个具体的 Task/Operator 的状态(Task Slot/ 转换算子)，State 数据默认保存在 Java 的堆内存中。 CheckPoint（可以理解为CheckPoint是把State数据持久化存储了）则表示了一个 Flink Job 在一个特定时刻的一份全局状态快照，即包含了所有Task/Operator 的状态。 1. 常用 StateFlink 有两种常见的 State 类型，分别是: Keyed State(键控状态) Operator State(算子状态) 1. Keyed State（键控状态）Keyed State：顾名思义就是基于 KeyedStream上的状态，这个状态是跟特定的 Key 绑定的。KeyedStream 流上的每一个 Key，都对应一个 State。Flink 针对 Keyed State 提供了 以下可以保存 State 的数据结构： ValueState&lt;T&gt;:保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 update(T) 进行更新，通过 T value() 进行检索。 ListState&lt;T&gt;:保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上 进行检索。可以通过 add(T) 或者 addAll(List) 进行添加元素，通过 Iterable get()获得整个列表。还可以通过 update(List) 覆盖当前的列表。 ReducingState&lt;T&gt;:保存一个单值，表示添加到状态的所有值的聚合。接口与 ListState 类似，但使用 add(T) 增加元素，会使用提供的 ReduceFunction 进行聚合。 AggregatingState&lt;IN, OUT&gt;:保留一个单值，表示添加到状态的所有值的聚合。和 ReducingState 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 ListState 类似，但使用 add(IN) 添加的元素会用指定的 AggregateFunction 进行聚 合。 FoldingState&lt;T, ACC&gt;:保留一个单值，表示添加到状态的所有值的聚合。 与 ReducingState 相反，聚合类型可能与添加到状态的元素类型不同。接口与 ListState 类似，但使用 add（T）添加的元素会用指定的 FoldFunction 折叠成聚合值。 MapState&lt;UK, UV&gt;:维护了一个映射列表。 你可以添加键值对到状态中，也可以获得 反映当前所有映射的迭代器。使用 put(UK，UV) 或者 putAll(Map&lt;UK，UV&gt;) 添加映射。 使用 get(UK) 检索特定 key。 使用 entries()，keys() 和 values() 分别检索映射、 键和值的可迭代视图。 2. Operator State（算子状态）Operator State 与 Key 无关，而是与Operator绑定，整个 Operator 只对应一个 State。 比如：Flink 中的 Kafka Connector 就使用了 Operator State，它会在每个 Connector 实例 中，保存该实例消费 Topic 的所有(partition, offset)映射。 3. Keyed State 案例demo1：监控每一个手机号码，如果这个号码在5秒内，所有呼叫它的日志都是失败的，demo2 需求：计算每个手机的呼叫间隔时间，单位是毫秒。 package com.sowhat.flink.state import java.net.{URL, URLDecoder} import com.sowhat.flink.BatchWordCount.getClass import com.sowhat.flink.source.StationLog import org.apache.flink.api.common.functions.RichFlatMapFunction import org.apache.flink.api.common.state.{ValueState, ValueStateDescriptor} import org.apache.flink.configuration.Configuration import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.util.Collector /** * 基站日志 * @param sid 基站的id * @param callOut 主叫号码 * @param callInt 被叫号码 * @param callType 呼叫类型 * @param callTime 呼叫时间 (毫秒) * @param duration 通话时长 （秒） */ case class StationLog(sid: String, var callOut: String, var callInt: String, callType: String, callTime: Long, duration: Long) /** * 第一种方法的实现 * 统计每个手机的呼叫时间间隔，单位是毫秒 */ object TestKeyedState1 { def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 val filePath: URL = getClass.getResource(\"/station.log\") //使用相对路径来得到完整的文件路径 val packagePath: String = filePath.getPath().replaceAll(\"%20\", \"\"); //解决路径中含有空格的情况 val str:String = URLDecoder.decode(packagePath, \"utf-8\"); //解决路径包含中文的情况 val stream: DataStream[StationLog] = streamEnv.readTextFile(str) .map(line => { val arr:Array[String] = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) stream.keyBy(_.callOut) //分组 .flatMap(new CallIntervalFunction) .print() streamEnv.execute() } //输出的是一个二元组（手机号码，时间间隔） class CallIntervalFunction extends RichFlatMapFunction[StationLog, (String, Long)] { //定义一个状态，用于保存前一次呼叫的时间 private var preCallTimeState: ValueState[Long] = _ override def open(parameters: Configuration): Unit = { preCallTimeState = getRuntimeContext.getState(new ValueStateDescriptor[Long](\"pre\", classOf[Long])) } override def flatMap(value: StationLog, out: Collector[(String, Long)]): Unit = { //从状态中取得前一次呼叫的时间 val preCallTime:Long = preCallTimeState.value() if (preCallTime == null || preCallTime == 0) { //状态中没有，肯定是第一次呼叫 preCallTimeState.update(value.callTime) } else { //状态中有数据,则要计算时间间隔 val interval:Long = Math.abs(value.callTime - preCallTime) out.collect((value.callOut, interval)) } } } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768 结果： 4&gt; (18600003532,7000) 2&gt; (18600003713,0) 1&gt; (18600003502,9000) 1&gt; (18600003502,0) 1&gt; (18600003502,9000) 1&gt; (18600007699,0) 1&gt; (18600000005,150000) 1234567stationlog.txt文件信息如下： station_1,18600000005,18900007729,fail,1577080453123,0 station_1,18600000005,18900007729,success,1577080603123,349 station_8,18600007461,18900006987,barring,1577080453123,0 station_5,18600009356,18900006066,busy,1577080455129,0 station_4,18600001941,18900003949,busy,1577080455129,0 ...自己造数据即可 123456还有第二种简单的方法：调用flatMapWithState 算子 package com.sowhat.flink.state import java.net.{URL, URLDecoder} import com.sowhat.flink.source.StationLog import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment /** * 第二种方法的实现 * 统计每个手机的呼叫时间间隔，单位是毫秒 */ object TestKeyedState2 { def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 val filePath: URL = getClass.getResource(\"/station.log\") //使用相对路径来得到完整的文件路径 val packagePath: String = filePath.getPath().replaceAll(\"%20\", \"\"); //解决路径中含有空格的情况 val str: String = URLDecoder.decode(packagePath, \"utf-8\"); //解决路径包含中文的情况 val stream: DataStream[StationLog] = streamEnv.readTextFile(str) .map(line => { var arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) stream.keyBy(_.callOut) //分组 //有两种情况1、状态中有上一次的通话时间，2、没有。采用scala中的模式匹配 .mapWithState[(String, Long), StationLog] { case (in: StationLog, None) => ((in.callOut, 0), Some(in)) //状态中没有值 是第一次呼叫 case (in: StationLog, pre: Some[StationLog]) => { //状态中有值，是第二次呼叫 var interval:Long = Math.abs(in.callTime - pre.get.callTime) ((in.callOut, interval), Some(in)) } }.filter(_._2 != 0) .print() streamEnv.execute() } } 1234567891011121314151617181920212223242526272829303132333435363738394041 2. CheckPoint当程序出现问题需要恢复State 数据的时候，只有程序提供支持才可以实现State 的容错。State 的容错需要依靠 CheckPoint机制，这样才可以保证 Exactly-once 这种语义，但是注意，它只能保证 Flink 系统内的 Exactly-once，比如 Flink 内置支持的算子。针对 Source 和 Sink 组件，如果想要保证 Exactly-once 的话，则这些组件本身应支持这种语义。 1. CheckPoint 原理Flink 中基于异步轻量级的分布式快照技术提供了 Checkpoints容错机制，分布式快照可以将同一时间点 Task/Operator 的状态数据全局统一快照处理，包括前面提到的 Keyed State和 Operator State。Flink 会在输入的数据集上间隔性地生成 checkpoint barrier， 通过栅栏（barrier）将间隔时间段内的数据划分到相应的 checkpoint 中。如下图:比如序列偶数求和跟奇数求和： 2. CheckPoint 参数和设置默认情况下 Flink 不开启检查点的，用户需要在程序中通过调用方法配置和开启检查点，另外还可以调整其他相关参数： Checkpoint 开启和时间间隔指定： 开启检查点并且指定检查点时间间隔为 1000ms，根据实际情况自行选择，如果状态比较大，则建议适当增加该值。streamEnv.enableCheckpointing(1000) exactly-ance 和 at-least-once 语义选择：选择 exactly-once 语义保证整个应用内端到端的数据一致性，这种情况比较适合于数据要求比较高，不允许出现丢数据或者数据重复，与此同时，Flink 的性能也相对较弱，而 at-least-once 语义更适合于时廷和吞吐量要求非常高但对数据的一致性要求不高的场景。 如下通过setCheckpointingMode()方法来设 定语义模式， 默认情况 使用的是 exactly-once 模式。 streamEnv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACT LY_ONCE)； //或者 streamEnv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.AT_LE AST_ONCE) 123 Checkpoint 超时时间：超时时间指定了每次 Checkpoint 执行过程中的上限时间范围，一旦 Checkpoint 执行时 间超过该阈值，Flink 将会中断 Checkpoint 过程，并按照超时处理。该指标可以通过 setCheckpointTimeout 方法设定，默认为 10分钟。streamEnv.getCheckpointConfig.setCheckpointTimeout(50000) 检查点之间最小时间间隔：该参数主要目的是设定两个 Checkpoint 之间的最小时间间隔，防止出现例如状态数据过大而导致 Checkpoint 执行时间过长，从而导致 Checkpoint 积压过多，最终 Flink 应用密集地触发 Checkpoint 操作，会占用了大量计算资源而影响到整个应用的性能。 streamEnv.getCheckpointConfig.setMinPauseBetweenCheckpoints(600) 1 最大并行执行的检查点数量：通过 setMaxConcurrentCheckpoints()方法设定能够最大同时执行的 Checkpoint 数量。 在默认情况下只有一个检查点可以运行，根据用户指定的数量可以同时触发多个 Checkpoint，进而提升 Checkpoint 整体的效率。 streamEnv.getCheckpointConfig.setMaxConcurrentCheckpoints(1) 1 是否删除 Checkpoint 中保存的数据：设置为 RETAIN_ON_CANCELLATION：表示一旦 Flink 处理程序被 cancel 后，会保留 CheckPoint 数据，以便根据实际需要恢复到指定的 CheckPoint。 设置为 DELETE_ON_CANCELLATION：表示一旦 Flink 处理程序被 cancel 后，会删除 CheckPoint 数据，只有 Job 执行失败的时候才会保存 CheckPoint。 //删除 streamEnv.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckp ointCleanup.DELETE_ON_CANCELLATION) //保留 streamEnv.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckp ointCleanup.RETAIN_ON_CANCELLATION) 1234 TolerableCheckpointFailureNumber：设置可以容忍的检查的失败数，超过这个数量则系统自动关闭和停止任务。 streamEnv.getCheckpointConfig.setTolerableCheckpointFailureNumber(1) 1 3. 保存机制 StateBackend(状态后端)默认情况下，State 会保存在 TaskManager 的内存中，CheckPoint会存储在 JobManager的内存中。State和 CheckPoint的存储位置取决于StateBackend的配置。Flink 一共提供 了 3 种 StateBackend。包括基于内存的 MemoryStateBackend、基于文件系统的FsStateBackend，以及基于 RockDB 作为存储介质的 RocksDBState-Backend。 1. MemoryStateBackend基于内存的状态管理具有非常快速和高效的特点，但也具有非常多的限制，最主要的就 是内存的容量限制，一旦存储的状态数据过多就会导致系统内存溢出等问题，从而影响整个 应用的正常运行。同时如果机器出现问题，整个主机内存中的状态数据都会丢失，进而无法 恢复任务中的状态数据。因此从数据安全的角度建议用户尽可能地避免在生产环境中使用 MemoryStateBackend。 // 设定存储空间为10G streamEnv.setStateBackend(new MemoryStateBackend(10*1024*1024)) 12 2. FsStateBackend和 MemoryStateBackend有所不同，FsStateBackend 是基于文件系统的一种状态管理器， 这里的文件系统可以是本地文件系统，也可以是 HDFS 分布式文件系统。FsStateBackend 更适合任务状态非常大的情况，例如应用中含有时间范围非常长的窗口计算，或 Key/value State 状态数据量非常大的场景。TaskManager仍然使用内存保存数据，但是进行CheckPoint的时候是将数据保存到FS中。 streamEnv.setStateBackend(new FsStateBackend(\"hdfs://hadoop101:9000/checkpoint/cp1\")) 1 3. RocksDBStateBackendRocksDBStateBackend 是 Flink 中内置的第三方状态管理器，和前面的状态管理器不同，RocksDBStateBackend 需要单独引入相关的依赖包到工程中。 &lt;dependency> &lt;groupId>org.apache.flink&lt;/groupId> &lt;artifactId>flink-statebackend-rocksdb_2.11&lt;/artifactId> &lt;version>1.9.1&lt;/version> &lt;/dependency> 12345 RocksDBStateBackend 采用异步的方式进行状态数据的 Snapshot，任务中的状态数据首先被写入本地 RockDB 中，这样在 RockDB 仅会存储正在进行计算的热数据，而需要进行 CheckPoint 的时候，会把本地的数据直接复制到远端的 FileSystem 中。 与 FsStateBackend 相比，RocksDBStateBackend 在性能上要比 FsStateBackend 高一些，主要是因为借助于 RocksDB 在本地存储了最新热数据，然后通过异步的方式再同步到文件系 统中，但 RocksDBStateBackend和 MemoryStateBackend相比性能就会较弱一些。RocksDB 克服了 State 受内存限制的缺点，同时又能够持久化到远端文件系统中，推荐在生产中使用。 streamEnv.setStateBackend(new RocksDBStateBackend (\"hdfs://hadoop101:9000/checkpoint/cp2\")) 1 4. 全局配置 StateBackend以上的代码都是单 job 配置状态后端，也可以全局配置状态后端，需要修改 flink-conf.yaml 配置文件: state.backend: filesystem filesystem 表示使用 FsStateBackend, jobmanager 表示使用 MemoryStateBackend rocksdb 表示使用 RocksDBStateBackend。 --- flink-conf.yaml 配置文件中 state.checkpoints.dir: hdfs://hadoop101:9000/checkpoints 1234567 默认情况下，如果设置了 CheckPoint 选项，则 Flink 只保留最近成功生成的 1 个 CheckPoint，而当 Flink 程序失败时，可以通过最近的 CheckPoint 来进行恢复。但是，如果希望保留多个CheckPoint，并能够根据实际需要选择其中一个进行恢复，就会更加灵活。 添加如下配置，指定最多可以保存的 CheckPoint 的个数。 state.checkpoints.num-retained: 2 1 4. Checkpoint案例案例:设置 HDFS 文件系统的状态后端，取消 Job 之后再次恢复 Job。使用WordCount案例来测试一下HDFS的状态后端，先运行一段时间Job，然后cancel，在重新启动，看看状态是否是连续的 package com.sowhat.flink.state import org.apache.flink.runtime.state.filesystem.FsStateBackend import org.apache.flink.streaming.api.CheckpointingMode import org.apache.flink.streaming.api.environment.CheckpointConfig.ExternalizedCheckpointCleanup import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object TestCheckPointByHDFS { //使用WordCount案例来测试一下HDFS的状态后端，先运行一段时间Job，然后cancel，在重新启动，看看状态是否是连续的 def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //开启CheckPoint并且设置一些参数 streamEnv.enableCheckpointing(5000) //每隔5秒开启一次CheckPoint streamEnv.setStateBackend(new FsStateBackend(\"hdfs://hadoop101:9000/checkpoint/cp1\")) //存放检查点数据 streamEnv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) streamEnv.getCheckpointConfig.setCheckpointTimeout(5000) streamEnv.getCheckpointConfig.setMaxConcurrentCheckpoints(1) streamEnv.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) //终止job保留检查的数据 //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ //3、读取数据,读取sock流中的数据 val stream: DataStream[String] = streamEnv.socketTextStream(\"hadoop101\", 8888) //DataStream ==> spark 中Dstream //4、转换和处理数据 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")) .map((_, 1)).setParallelism(2) .keyBy(0) //分组算子 : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数 .sum(1).setParallelism(2) //聚会累加算子 //5、打印结果 result.print(\"结果\").setParallelism(1) //6、启动流计算程序 streamEnv.execute(\"wordcount\") } } 1234567891011121314151617181920212223242526272829303132333435363738 打包上传到WebUI: 在nc -lk 8888 输入若干单词。然后查找 WebUI 的输出。然后通过WebUI将任务取消。最后尝试将任务重启。 ./flink run -d -s hdfs://hadoop101:9000/checkpoint/cp1/精确到跟meta数据同级目录 -c com.sowhat.flink.state.CheckpointOnFsBackend /home/Flink-Demo-1.0-SNAPSHOT.jar 1 也可以通过WebUI 重启，指定 MainClass跟 CheckPoint即可。此处关键在于CheckPoint路径要写对！ 5. SavePointSavepoints 是检查点的一种特殊实现，底层实现其实也是使用 Checkpoints 的机制。 Savepoints 是用户以手工命令的方式触发 Checkpoint,并将结果持久化到指定的存储路径 中，其主要目的是帮助用户在升级和维护集群过程中保存系统中的状态数据，避免因为停机运维或者升级应用等正常终止应用的操作而导致系统无法恢复到原有的计算状态的情况，从而无法实现从端到端的 Excatly-Once 语义保证。 配置 Savepoints 的存储路径在 flink-conf.yaml 中配置 SavePoint 存储的位置，设置后，如果要创建指定 Job 的 SavePoint，可以不用在手动执行命令时指定 SavePoint 的位置。 state.savepoints.dir: hdfs:/hadoop101:9000/savepoints 1 在代码中设置算子 ID为了能够在作业的不同版本之间以及 Flink 的不同版本之间顺利升级，强烈推荐程序员 通过手动给算子赋予 ID，这些 ID 将用于确定每一个算子的状态范围。如果不手动给各算子 指定 ID，则会由 Flink 自动给每个算子生成一个 ID。而这些自动生成的 ID 依赖于程序的结 构，并且对代码的更改是很敏感的。因此，强烈建议用户手动设置 ID。 package com.sowhat.flink.state import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment object TestSavePoints { def main(args: Array[String]): Unit = { //1、初始化Flink流计算的环境 val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment //修改并行度 streamEnv.setParallelism(1) //默认所有算子的并行度为1 //2、导入隐式转换 import org.apache.flink.streaming.api.scala._ //3、读取数据,读取sock流中的数据 val stream: DataStream[String] = streamEnv.socketTextStream(\"hadoop101\",8888) //DataStream ==> spark 中Dstream .uid(\"socket001\") //4、转换和处理数据 val result: DataStream[(String, Int)] = stream.flatMap(_.split(\" \")).uid(\"flatmap001\") .map((_, 1)).setParallelism(2).uid(\"map001\") .keyBy(0)//分组算子 : 0 或者 1 代表下标。前面的DataStream[二元组] , 0代表单词 ，1代表单词出现的次数 .sum(1).uid(\"sum001\") //5、打印结果 result.print(\"结果\").setParallelism(1) //6、启动流计算程序 streamEnv.execute(\"wordcount\") } } 1234567891011121314151617181920212223242526 触发 SavePoint //先启动Job [root@hadoop101 bin]# ./flink run -c com.bjsxt.flink.state.TestSavepoints -d /home/Flink-Demo-1.0-SNAPSHOT.jar [root@hadoop101 bin]# ./flink list 获取 job 对应ID //再取消Job [root@hadoop101 bin]# ./flink savepoint 6ecb8cfda5a5200016ca6b01260b94ce // 触发SavePoint [root@hadoop101 bin]# ./flink cancel 6ecb8cfda5a5200016ca6b01260b94ce 1234567 从 SavePoint 启动 Job大致方法跟上面的CheckPoint启动Job类似。 6. 总结若干个常用的状态算子大致如何存储的要了解。CheckPoint的原理主要是图示，理解如何保证精准一致性的。CheckPoint一般有基于内存的，基于HDFS的跟基于DB的，整体来说基于DB的把数据存储早DB中跟HDFS中是最好的。SavePoint是手动触发的CheckPoint，一般方便线上迁移的功能等，并且尽量给每一个算子自定义一个UID， 6. Window 窗口无界数据变为若干个有界数据。Windows 计算是流式计算中非常常用的数据计算方式之一，通过按照固定时间或长度将数据流切分成不同的窗口，然后对数据进行相应的聚合运算，从而得到一定时间范围内的统计结果。例如统计最近 5 分钟内某基站的呼叫数，此时基站的数据在不断地产生，但是通过 5 分钟的窗口将数据限定在固定时间范围内，就可以对该范围内的有界数据执行聚合处理， 得出最近 5 分钟的基站的呼叫数量。 1. Window分类1. Global Window 和 Keyed Window在运用窗口计算时，Flink根据上游数据集是否为KeyedStream类型，对应的Windows 也 会有所不同。 Keyed Window: 上游数据集如果是 KeyedStream 类型，则调用 DataStream API 的window()方法，数据会根据 Key 在不同的 Task 实例中并行分别计算，最后得出针对每个 Key 统计的结果。 Global Window:如果是 Non-Keyed 类型，则调用 WindowsAll()方法，所有的数据都会在窗口算子中由到一个 Task 中计算，并得到全局统计结果。 //读取文件数据 val data = streamEnv.readTextFile(getClass.getResource(\"/station.log\").getPath) .map(line=>{ var arr =line.split(\",\") new StationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.to Long) }) //Global Window data.windowAll(自定义的WindowAssigner) //Keyed Window data.keyBy(_.sid).window(自定义的WindowAssigner) 12345678910 2. Time Window 和 Count Window基于业务数据的方面考虑，Flink 又支持两种类型的窗口，一种是基于时间的窗口叫Time Window。还有一种基于输入数据数量的窗口叫 Count Window 3. Time Window(时间窗口)根据不同的业务场景，Time Window 也可以分为三种类型，分别是滚动窗口(Tumbling Window)、滑动窗口(Sliding Window)和会话窗口(Session Window) 滚动窗口(Tumbling Window)滚动窗口是根据固定时间进行切分，且窗口和窗口之间的元素互不重叠。这种类型的窗 口的最大特点是比较简单。只需要指定一个窗口长度(window size)。 //每隔5秒统计每个基站的日志数量 data.map(stationLog=>((stationLog.sid,1))) .keyBy(_._1) .timeWindow(Time.seconds(5)) //.window(TumblingEventTimeWindows.of(Time.seconds(5))) 跟上面同样功能 .sum(1) //聚合 123456 其中时间间隔可以是 Time.milliseconds(x)、Time.seconds(x)或 Time.minutes(x)。 滑动窗口(Sliding Window)滑动窗口也是一种比较常见的窗口类型，其特点是在滚动窗口基础之上增加了窗口滑动时间(Slide Time)，且允许窗口数据发生重叠。当 Windows size 固定之后，窗口并不像 滚动窗口按照 Windows Size 向前移动，而是根据设定的 Slide Time 向前滑动。窗口之间的 数据重叠大小根据 Windows size 和 Slide time 决定，当 Slide time 小于 Windows size 便会发生窗口重叠，Slide size 大于 Windows size 就会出现窗口不连续，数据可能不能在 任何一个窗口内计算，Slide size 和 Windows size 相等时，Sliding Windows 其实就是 Tumbling Windows。 //每隔3秒计算最近5秒内，每个基站的日志数量 data.map(stationLog=>((stationLog.sid,1))) .keyBy(_._1) .timeWindow(Time.seconds(5),Time.seconds(3)) //.window(SlidingEventTimeWindows.of(Time.seconds(5),Time.seconds(3))) .sum(1) 12345 会话窗口(Session Window)会话窗口(Session Windows)主要是将某段时间内活跃度较高的数据聚合成一个窗口 进行计算，窗口的触发的条件是 Session Gap，是指在规定的时间内如果没有数据活跃接入， 则认为窗口结束，然后触发窗口计算结果。需要注意的是如果数据一直不间断地进入窗口， 也会导致窗口始终不触发的情况。与滑动窗口、滚动窗口不同的是，Session Windows 不需 要有固定 windows size 和 slide time，只需要定义 session gap，来规定不活跃数据的时 间上限即可。 //3秒内如果没有数据进入，则计算每个基站的日志数量 data.map(stationLog=>((stationLog.sid,1))) .keyBy(_._1).window(EventTimeSessionWindows.withGap(Time.seconds(3))).sum(1) 123 4. Count Window(数量窗口)Count Window 也有滚动窗口、滑动窗口等。由于使用比较少TODO，比如五条数据算一批次这样的统计。 2. Window的API在以后的实际案例中 Keyed Window使用最多，所以我们需要掌握 Keyed Window 的算子， 在每个窗口算子中包含了 Windows Assigner、Windows Trigger(窗口触发器)、Evictor (数据剔除器)、Lateness(时延设定)、Output Tag(输出标签)以及 Windows Funciton 等组成部分，其中 Windows Assigner 和 Windows Funciton 是所有窗口算子必须指定的属性， 其余的属性都是根据实际情况选择指定。 stream.keyBy(...) // 是Keyed类型数据集 .window(...) //指定窗口分配器类型 [.trigger(...)] //指定触发器类型(可选) [.evictor(...)] //指定evictor或者不指定(可选) [.allowedLateness(...)] //指定是否延迟处理数据(可选) [.sideOutputLateData(...)] //指定Output Lag(可选) .reduce/aggregate/fold/apply() //指定窗口计算函数 [.getSideOutput(...)] //根据Tag输出数据(可选) 12345678 Windows Assigner: 指定窗口的类型，定义如何将数据流分配到一个或多个窗口。 Windows Trigger: 指定窗口触发的时机，定义窗口满足什么样的条件触发计算。 Evictor: 用于数据剔除。 allowedLateness: 标记是否处理迟到数据，当迟到数据到达窗口中是否触发计算。 Output Tag: 标记输出标签，然后在通过 getSideOutput 将窗口中的数据根据标签输出。 Windows Funciton: 定义窗口上数据处理的逻辑，例如对数据进行 sum 操作。 3. 窗口聚合函数如果定义了 Window Assigner 之后，下一步就可以定义窗口内数据的计算逻辑，这也就是 Window Function 的定义。Flink 中提供了四种类型的 Window Function，分别为 ReduceFunction、AggregateFunction 以及 ProcessWindowFunction,（sum 和 max)等。 前三种类型的 Window Fucntion 按照计算原理的不同可以分为两大类： 一类是增量聚合函数：对应有 ReduceFunction、AggregateFunction； 另一类是全量窗口函数，对应有 ProcessWindowFunction（还有 WindowFunction）。 增量聚合函数计算性能较高，占用存储空间少，主要因为基于中间状态的计算结果，窗口中只维护中间结果状态值，不需要缓存原始数据。而全量窗口函数使用的代价相对较高， 性能比较弱，主要因为此时算子需要对所有属于该窗口的接入数据进行缓存，然后等到窗口触发的时候，对所有的原始数据进行汇总计算。 1. ReduceFunctionReduce要求输入跟输出类型要一样！这点切记。需求：每隔5秒统计每个基站的日志数量 object TestReduceFunctionByWindow { //每隔5秒统计每个基站的日志数量 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 val stream: DataStream[StationLog] = streamEnv.socketTextStream(\"hadoop101\", 8888) .map(line => { val arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) //开窗 stream.map(log => ((log.sid, 1))) .keyBy(_._1) .timeWindow(Time.seconds(5)) //开窗 .reduce((t1, t2) => (t1._1, t1._2 + t2._2)) .print() streamEnv.execute() } } 123456789101112131415161718192021222324 2. AggregateFunction和 ReduceFunction 相似，AggregateFunction 也是基于中间状态计算结果的增量计算 函数，但 AggregateFunction 在窗口计算上更加通用。AggregateFunction 接口相对 ReduceFunction 更加灵活，输入跟输出类型不要求完全一致，实现复杂度也相对较高。AggregateFunction 接口中定义了三个 需要复写的方法，其中 add()定义数据的添加逻辑，getResult 定义了根据 accumulator 计 算结果的逻辑，merge 方法定义合并 accumulator 的逻辑。初始化，分区内如何处理，分区间如何处理，最终如何输出。 需求：每隔3秒计算最近5秒内，每个基站的日志数量 object TestAggregatFunctionByWindow { //每隔3秒计算最近5秒内，每个基站的日志数量 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ //读取数据源 val stream: DataStream[StationLog] = streamEnv.socketTextStream(\"hadoop101\", 8888) .map(line => { val arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) //开窗 val value: DataStream[(String, Long)] = stream.map(log => ((log.sid, 1))) .keyBy(_._1) .window(SlidingProcessingTimeWindows.of(Time.seconds(5), Time.seconds(3))) //开窗，滑动窗口 .aggregate(new MyAggregateFunction, new MyWindowFunction) // 到底是数字对应哪个基站 // aggregate(增量函数，全量函数) value.print() streamEnv.execute() } /** * 里面的add方法，是来一条数据执行一次，getResult在窗口结束的时候执行一次 * in,累加器acc,out * https://blog.csdn.net/chilimei8516/article/details/100796930 */ class MyAggregateFunction extends AggregateFunction[(String, Int), Long, Long] { override def createAccumulator(): Long = 0 //初始化一个累加器 acc，开始的时候为0 // 分区内操作 override def add(value: (String, Int), accumulator: Long): Long = accumulator + value._2 // 结果返回 override def getResult(accumulator: Long): Long = accumulator // 分区间操作 override def merge(a: Long, b: Long): Long = a + b } // WindowFunction 输入数据来自于AggregateFunction ， // 在窗口结束的时候先执行AggregateFunction对象的getResult，然后再执行apply // in,out,key,window class MyWindowFunction extends WindowFunction[Long, (String, Long), String, TimeWindow] { override def apply(key: String, window: TimeWindow, input: Iterable[Long], out: Collector[(String, Long)]): Unit = { out.collect((key, input.iterator.next())) //next得到第一个值，迭代器中只有一个值 } } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 3. ProcessWindowFunction前面提到的ReduceFunction和 AggregateFunction 都是基于中间状态实现增量计算的 窗口函数，虽然已经满足绝大多数场景，但在某些情况下，统计更复杂的指标可能需要依赖于窗口中所有的数据元素，或需要操作窗口中的状态数据和窗口元数据，这时就需要使用到 ProcessWindowsFunction，ProcessWindowsFunction能够更加灵活地支持基于窗口全部数据元素的结果计算 ， 例如对整个窗口 数 据排序取TopN， 这样的需要就必须使用ProcessWindowFunction。 需求：每隔5秒统计每个基站的日志数量 object TestProcessWindowFunctionByWindow { //每隔5秒统计每个基站的日志数量 def main(args: Array[String]): Unit = { val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment import org.apache.flink.streaming.api.scala._ streamEnv.setParallelism(1) //读取数据源 val stream: DataStream[StationLog] = streamEnv.socketTextStream(\"hadoop101\", 8888) .map(line => { var arr = line.split(\",\") new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).trim.toLong, arr(5).trim.toLong) }) //开窗 stream.map(log => ((log.sid, 1))) .keyBy(_._1) // .timeWindow(Time.seconds(5))//开窗 .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .process(new ProcessWindowFunction[(String, Int), (String, Long), String, TimeWindow] { //一个窗口结束的时候调用一次(一个分组执行一次) in,out,key,windows override def process(key: String, context: Context, elements: Iterable[(String, Int)], out: Collector[(String, Long)]): Unit = { println(\"------------\") //注意：整个窗口的数据保存到Iterable，里面有很多行数据。Iterable的size就是日志的总条数 out.collect((key, elements.size)) } }).print() streamEnv.execute() } } 1234567891011121314151617181920212223242526272829 需求：窗口函数读数据然后将数据写入到neo4j，感觉其实应该用 自定的Sink 更合适一些。 object DealDataFromKafka { def main(args: Array[String]): Unit = { val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment environment.setParallelism(1) val properties: Properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"IP1:9092,IP2:9092\") properties.setProperty(\"group.id\", \"timer\") // 从最新数据开始读 properties.setProperty(\"auto.offset.reset\", \"latest\") //val dataStream: DataStream[String] = environment.addSource(new FlinkKafkaConsumer011[String](\"sowhat\", new SimpleStringSchema(), properties)) val dataStream: DataStream[String] = environment.socketTextStream(\"IP\", 8889) val winData: AllWindowedStream[String, TimeWindow] = dataStream.timeWindowAll(Time.seconds(4)) var pre: Int = 0 var tmp: Int = 0 val timeWithHashCode: DataStream[(Int, String)] = winData.process(new ProcessAllWindowFunction[String, (Int, String), TimeWindow]() { override def process(context: Context, elements: Iterable[String], out: Collector[(Int, String)]): Unit = { val driver: Driver = GraphDatabase.driver(\"bolt://IP:9314\", AuthTokens.basic(\"neo4j\", \"neo4j0fcredithc\")) val session: Session = driver.session() elements.foreach(value => { tmp += 1 var now: Int = value.hashCode now = tmp session.run(s\"CREATE (a:Test {id:${now}, time:'${value}'})\") if (pre != 0) { session.run(s\"MATCH (begin:Test{id:${pre}}) ,(end:Test{id:${now}}) MERGE (begin)-[like:Time_Link]->(end)\") } out.collect((tmp, s\" MATCH (begin:Test{id:${pre}}) ,(end:Test{id:${now}}) MERGE (begin)-[like:Time_Link]->(end)\")) pre = now } ) // session.close() // driver.close() } }) timeWithHashCode.print(\"HashCode With time:\") environment.execute(\"getData\") } } 1234567891011121314151617181920212223242526272829303132333435363738394041 End窗口的分类从不同的维度来说， 上游是否为KeyedStream，不同数据集调用不同方法。 根据上游数据是时间窗口(滚动窗口、滑动窗口、会话窗口)还是数据量窗口。 窗口若干API调用方法，窗口的聚合函数(reduceFunction、AggregateFunction、ProcessWindowFunction、WindowFunction)。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/tags/Flink/"}]},{"title":"【hive日常使用问题记录】Hive建表导致的ORC序列化错误","slug":"【hive日常使用问题记录】Hive建表导致的ORC序列化错误","date":"2020-09-13T08:00:00.000Z","updated":"2020-09-13T05:19:56.268Z","comments":true,"path":"2020/09/13/hive-ri-chang-shi-yong-wen-ti-ji-lu-hive-jian-biao-dao-zhi-de-orc-xu-lie-hua-cuo-wu/","link":"","permalink":"https://dataquaner.github.io/2020/09/13/hive-ri-chang-shi-yong-wen-ti-ji-lu-hive-jian-biao-dao-zhi-de-orc-xu-lie-hua-cuo-wu/","excerpt":"","text":"问题描述：hive表在创建时候指定存储格式 STORED AS ORC tblproperties ('orc.compress'='SNAPPY'); 当insert数据到表时抛出异常 Caused by: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.hive.ql.io.orc.OrcSerde$OrcSerdeRow at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:98) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:743) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:97) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:115) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:169) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:561) 此时查看表结构 desc formatted persons_orc; ​ 可以看到SerDe Library 的格式是LazySimpleSerDe，序列化格式不是orc的,所以抛出异常 解决办法：这里将表的序列化方式修改为orc即可 ALTER TABLE persons_orc SET FILEFORMAT ORC; 再看序列化格式已经是orc，使用insert(insert overwrite table persons_orc select * from persons;)插入数据可以ok 可以参考详细解释:http://www.imooc.com/article/252830 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"-- Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"-- Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"}]},{"title":"大数据开发工程师面试资料汇总","slug":"数据开发工程师面试知识汇总@20200705","date":"2020-07-05T17:25:00.000Z","updated":"2020-07-05T16:21:35.586Z","comments":true,"path":"2020/07/06/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20200705/","link":"","permalink":"https://dataquaner.github.io/2020/07/06/shu-ju-kai-fa-gong-cheng-shi-mian-shi-zhi-shi-hui-zong-20200705/","excerpt":"","text":"目录[TOC] 一. Hadoop篇1. 并行计算模型MapReduce1.1 MapReduce 的工作原理 ​ MapReduce是一个基于集群的计算平台，是一个简化分布式编程的计算框架，是一个将分布式计算抽象为Map和Reduce两个阶段的编程模型。（这句话记住了是可以用来装逼的） 执行步骤：切片&gt;分词&gt;映射&gt;分区&gt;排序&gt;聚合&gt;shuffle&gt;reduce 1）Map()阶段 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数 重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出 对输出的key、value进行分区 对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中 2）Reduce阶段 多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上 对多个map的输出进行合并、排序。 重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出 把reduce的输出保存到文件中 特别说明： 切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理 分词，映射，分区，排序，聚合 都属map阶段 混洗 横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段 规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义 分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容 补充 切片：HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map任务的数量：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce任务的数量： 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误） 1.2 MapReduce中shuffle工作流程及优化 shuffle主要功能是把map task的输出结果有效地传送到reduce端。 简单些可以这样说，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。 前奏： 1. 在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。 2. 在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。前面我们知道这个job有3个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。 主要工作流程map端分区，排序，溢写，拷贝，reduce端合并 1）Map端shuffle 分区Partition MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。 写入内存缓冲区： 在我们的例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入内存缓冲区中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。 这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写，字面意思很直观。 溢写Spill： 这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。 排序Sort： 当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。 合并Map端：在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节在于，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。 CombineReduce端：在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。 2）Reduce端shuffle1. Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 2. Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。 增加combiner，压缩溢写的文件。 3. Reducer的输入文件。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。至于怎样才能让这个文件出现在内存中，之后的性能优化篇我再说。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。 3）shuffle优化 压缩：对数据进行压缩，减少写读数据量； 减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好； 内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招； 补充 1. Map端 1) io.sort.mb 用于map输出排序的内存缓冲区大小 类型：Int 默认：100mb 备注：如果能估算map输出大小，就可以合理设置该值来尽可能减少溢出写的次数，这对调优很有帮助。 2) io.sort.spill.percent map输出排序时的spill阀值（即使用比例达到该值时，将缓冲区中的内容spill 到磁盘） 类型：float 默认：0.80 3) io.sort.factor 归并因子（归并时的最多合并的流数），map、reduce阶段都要用到 类型：Int 默认：10 备注：将此值增加到100是很常见的。 4) min.num.spills.for.combine 运行combiner所需的最少溢出写文件数（如果已指定combiner） 类型：Int 默认：3 5) mapred.compress.map.output map输出是否压缩 类型：Boolean 默认：false 备注：如果map输出的数据量非常大，那么在写入磁盘时压缩数据往往是个很好的主意，因为这样会让写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量。 6) mapred.map.output.compression.codec 用于map输出的压缩编解码器 类型：Classname 默认：org.apache.hadoop.io.compress.DefaultCodec 备注：推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的 TeraSort作业，运行时间减少60%，且明显快于Zlib压缩。 7) tasktracker.http.threads 每个tasktracker的工作线程数，用于将map输出到reducer。 （注：这是集群范围的设置，不能由单个作业设置） 类型：Int 默认：40 备注：tasktracker开http服务的线程数。用于reduce拉取map输出数据，大集群可以将其设为40~50。 2. reduce端 1) mapred.reduce.slowstart.completed.maps 调用reduce之前，map必须完成的最少比例 类型：float 默认：0.05 2) mapred.reduce.parallel.copies reducer在copy阶段同时从mapper上拉取的文件数 类型：int 默认：5 3) mapred.job.shuffle.input.buffer.percent 在shuffle的复制阶段，分配给map输出的缓冲区占堆空间的百分比 类型：float 默认：0.70 4) mapred.job.shuffle.merge.percent map输出缓冲区（由mapred.job.shuffle.input.buffer.percent定义）使用比例阀值，当达到此阀值，缓冲区中的数据将会被归并然后spill 到磁盘。 类型：float 默认：0.66 5) mapred.inmem.merge.threshold map输出缓冲区中文件数 类型：int 默认：1000 备注：0或小于0的数意味着没有阀值限制，溢出写将有mapred.job.shuffle.merge.percent单独控制。 6) mapred.job.reduce.input.buffer.percent 在reduce过程中，在内存中保存map输出的空间占整个堆空间的比例。 类型：float 默认：0.0 备注：reduce阶段开始时，内存中的map输出大小不能大于该值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存。然而，如果reducer需要的内存较少，则可以增加此值来最小化访问磁盘的次数，以提高reduce性能。 2. 分布式文件系统HDFS2.1 HDFS 的体系架构和读写流程1）体系架构 采用Master-Slaver模式： NameNode中心服务器（Master）:维护文件系统树、以及整棵树内的文件目录、负责整个数据集群的管理。 DataNode分布在不同的机架上（Slaver）：在客户端或者NameNode的调度下，存储并检索数据块，并且定期向NameNode发送所存储的数据块的列表。 客户端与NameNode获取元数据；与DataNode交互获取数据。 默认情况下，每个DataNode都保存了3个副本，其中两个保存在同一个机架的两个不同的节点上。另一个副本放在不同机架上的节点上。 补充 基本概念 机架：HDFS集群，由分布在多个机架上的大量DataNode组成，不同机架之间节点通过交换机通信，HDFS通过机架感知策略，使NameNode能够确定每个DataNode所属的机架ID，使用副本存放策略，来改进数据的可靠性、可用性和网络带宽的利用率。 数据块(block)：HDFS最基本的存储单元，默认为64M，用户可以自行设置大小。 元数据：指HDFS文件系统中，文件和目录的属性信息。HDFS实现时，采用了 镜像文件（Fsimage） + 日志文件（EditLog）的备份机制。文件的镜像文件中内容包括：修改时间、访问时间、数据块大小、组成文件的数据块的存储位置信息。目录的镜像文件内容包括：修改时间、访问控制权限等信息。日志文件记录的是：HDFS的更新操作。 NameNode启动的时候，会将镜像文件和日志文件的内容在内存中合并。把内存中的元数据更新到最新状态。 用户数据：HDFS存储的大部分都是用户数据，以数据块的形式存放在DataNode上。 在HDFS中，NameNode 和 DataNode之间使用TCP协议进行通信。DataNode每3s向NameNode发送一个心跳。每10次心跳后，向NameNode发送一个数据块报告自己的信息，通过这些信息，NameNode能够重建元数据，并确保每个数据块有足够的副本。 2）HDFS文件读流程 （1）客户端通过调用FileSystem的open方法获取需要读取的数据文件，对HDFS来说该FileSystem就是DistributeFileSystem （2）DistributeFileSystem通过RPC来调用NameNode，获取到要读的数据文件对应的bock存储在哪些NataNode之上 （3）客户端先到最佳位置（距离最近）的DataNode上调用FSDataInputStream的read方法，通过反复调用read方法，可以将数据从DataNode传递到客户端 （4）当读取完所有的数据之后，FSDataInputStream会关闭与DataNode的连接，然后寻找下一块的最佳位置，客户端只需要读取连续的流。 （5）一旦客户端完成读取操作之后，就对FSDataInputStream调用close方法来完成资源的关闭操作 3）HDFS文件写操作 （1）客户端通过调用DistributeFileSystem的create方法来创建一个文件 （2）DistributeFileSystem会对NameNode发起RPC请求，在文件系统的名称空间中创建一个新的文件，此时会进行各种检查，比如：检查要创建的文件是否已经存在，如果该文件不存在，NameNode就会为该文件创建一条元数据记录 （3）客户端调用FSDataOututStream的write方法将数据写到一个内部队列中。假设副本数为3，那么将队列中的数据写到3个副本对应的存储的DataNode上。 （4）FSDataOututStream内部维护着一个确认队列，当接收到所有DataNode确认写完的消息后，数据才会从确认队列中删除 （5）当客户端完成数据的写入后，会对数据流调用close方法来关闭相关资源 补充 写入过程客户端奔溃怎么处理？（租约恢复） 2.2 HDFS 常用操作命令1）查看文件常用命令 命令格式1.hdfs dfs -ls path 查看文件列表2.hdfs dfs -lsr path 递归查看文件列表3.hdfs dfs -du path 查看path下的磁盘情况，单位字节 使用示例1.hdfs dfs -ls / 查看当前目录2.hdfs dfs - lsr / 递归查看当前目录 2）创建文件夹 命令格式hdfs dfs -mkdir path 使用用例hdfs dfs -mkdir /user/iron注：该命令可递归创建文件夹，不可重复创建，在Linux文件系统中不可见 3）创建文件 命令格式hdfs dfs -touchz path 使用用例hdfs dfs -touchz /user/iron/iron.txt注：该命令不可递归创建文件即当该文件的上级目录不存在时无法创建该文件，可重复创建但会覆盖原有的内容 4）复制文件和目录 命令格式hdfs dfs -cp 源目录 目标目录 使用用例hdfs dfs -cp /user/iron /user/iron01注：该命令会将源目录的整个目录结构都复制到目标目录中hdfs dfs -cp /user/iron/* /user/iron01注：该命令只会将源目录中的文件及其文件夹都复制到目标目录中 5）移动文件和目录 命令格式hdfs dfs -mv 源目录 目标目录 使用用例hdfs dfs -mv /user/iron /user/iron01 6）赋予权限 命令格式hdfs dfs -chmod [权限参数][拥有者][:[组]] path 使用用例hdfs dfs -chmod 777 /user/*注：该命令是将user目录下的所用文件及其文件夹（不包含子文件夹中的文件）赋予最高权限：读，写，执行777表示该用户，该用户的同组用户，其他用户都具有最高权限 7）上传文件 命令格式hdfs dfs -put 源文件夹 目标文件夹 使用用例hdfs dfs -put /home/hadoop01/iron /user/iron01注：该命令上传Linux文件系统中iron整个文件夹hdfs dfs -put /home/hadoop01/iron/* /user/iron01注：该命令上传Linux文件系统中iron文件夹中的所有文件（不包括文件夹）类似命令：hdfs dfs -copyFromLocal 源文件夹 目标文件夹 作用同puthdfs dfs -moveFromLocal 源文件夹 目标文件夹 上传后删除本地 8）下载文件 命令格式hdfs dfs -get源文件夹 目标文件夹 使用用例hdfs dfs -get /user/iron01 /home/hadoop01/iron注：该命令下载hdfs文件系统中的iron01整个文件夹到Linux文件系统中hdfs dfs -get /user/iron01/* /home/hadoop01/iron注：该命令下载hdfs文件系统中的iron01整个文件夹到Linux文件系统中（不包含文件夹）类似命令hdfs dfs -copyToLocal 源文件夹 目标文件夹 作用同gethdfs dfs -moveToLocal 源文件夹 目标文件夹 get后删除源文件 9）查看文件内容 命令格式hadoop fs -cat path 从头查看这个文件hadoop fs -tail path 从尾部查看最后1K 使用用例hadoop fs -cat /userjzl/home/book/1.txthadoop fs -tail /userjzl/home/book/1.txt 10）删除文件 命令格式hdfs dfs -rm 目标文件hdfs dfs -rmr 目标文件 递归删除（慎用） 使用用例hdfs dfs -rm /user/test.txt 删除test.txt文件hdfs dfs -rmr /user/testdir 递归删除testdir文件夹注：rm不可以删除文件夹 3. 通用资源管理器Yarn3.1 Yarn 的产生背景和架构3.2 Yarn 中的角色划分和各自的作用3.3 Yarn 的配置和常用的资源调度策略3.4 Yarn 进行一次任务资源调度的过程4. 数据仓库工具Hive4.1 Hive 和普通关系型数据库的区别 Hive和关系型数据库存储文件的系统不同, Hive使用的是HDFS(Hadoop的分布式文件系统),关系型数据则是服务器本地的文件系统。 Hive使用的计算模型是MapReduce,而关系型数据库则是自己设计的计算模型. 关系型数据库都是为实时查询业务设计的,而Hive则是为海量数据做挖掘而设计的,实时性差;实时性的区别导致Hive的应用场景和关系型数据库有很大区别。 Hive很容易扩展自己的存储能力和计算能力,这几是继承Hadoop的,而关系型数据库在这方面要比Hive差很多。 4.2 Hive内部表和外部表的区别 创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径， 不对数据的位置做任何改变。 删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 4.3 Hive分区表和分桶表的区别分区在HDFS上的表现形式是一个目录， 分桶是一个单独的文件 分区: 细化数据管理，直接读对应目录，缩小mapreduce程序要扫描的数据量 分桶： 1、提高join查询的效率（用分桶字段做连接字段）2、提高采样的效率 4.4 Hive 支持哪些数据格式可支持Text，SequenceFile，ParquetFile，ORC，RCFILE等 补充 TextFile： TextFile文件不支持块压缩，默认格式，数据不做压缩，磁盘开销大，数据解析开销大。这边不做深入介绍。 RCFile： Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能，但是不支持模式演进。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。 ORCFile： 存储方式：数据按行分块 每块按照列存储 ，压缩快 快速列存取，效率比rcfile高,是rcfile的改良版本，相比RC能够更好的压缩，能够更快的查询，但还是不支持模式演进。 Parquet： Parquet能够很好的压缩，有很好的查询性能，支持有限的模式演进。但是写速度通常比较慢。这中文件格式主要是用在Cloudera Impala上面的。 性能对比 读操作 存储效率 4.5 Hive元数据库作用及存储内容​ 本质上只是用来存储hive中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。为数据库创建的目录一般在hive数据仓库目录下 4.6 HiveSQL 支持的几种排序区别1）Order By：全局排序，只有一个Reducer 使用 ORDER BY 子句排序 ASC（ascend）: 升序（默认） DESC（descend）: 降序 ORDER BY 子句在SELECT语句的结尾 案例实操 查询员工信息按工资升序排列 hive (default)> select * from emp order by sal; 查询员工信息按工资降序排列 hive (default)> select * from emp order by sal desc; 2）Sort By：每个MapReduce内部排序 Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。 设置reduce个数 hive (default)> set mapreduce.job.reduces=3; 查看设置reduce个数 hive (default)> set mapreduce.job.reduces; 根据部门编号降序查看员工信息 hive (default)> select * from emp sort by deptno desc; 将查询结果导入到文件中（按照部门编号降序排序） hive (default)> insert overwrite local directory '/opt/module/datas/sortby-result' select * from emp sort by deptno desc; 3）Distribute By：分区排序 Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。 案例实操： 先按照部门编号分区，再按照员工编号降序排序。 hive (default)> set mapreduce.job.reduces=3; hive (default)> insert overwrite local directory '/opt/module/datas/distribute-result' select * from emp distribute by deptno sort by empno desc; 注意： ​ distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。4）Cluster By​ 当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。 以下两种写法等价 hive (default)> select * from emp cluster by deptno; hive (default)> select * from emp distribute by deptno sort by deptno; --注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。 --cluster by ：sort by 和 distribute by的组合 4.7 Hive 的动态分区 ​ 往hive分区表中插入数据时，如果需要创建的分区很多，比如以表中某个字段进行分区存储，则需要复制粘贴修改很多sql去执行，效率低。因为hive是批处理系统，所以hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。 使用动态分区表必须配置的参数 set hive.exec.dynamic.partition =true（默认false）,表示开启动态分区功能； set hive.exec.dynamic.partition.mode = nonstrict(默认strict),表示允许所有分区都是动态的，否则必须有静态分区字段； 动态分区相关调优参数 set hive.exec.max.dynamic.partitions.pernode=100 （默认100，一般可以设置大一点，比如1000）； 表示每个maper或reducer可以允许创建的最大动态分区个数，默认是100，超出则会报错。 set hive.exec.max.dynamic.partitions =1000(默认值) ； 表示一个动态分区语句可以创建的最大动态分区个数，超出报错； set hive.exec.max.created.files =10000(默认) 全局可以创建的最大文件个数，超出报错。 4.8 Hive MapJoin ​ MapJoin是Hive的一种优化操作，其适用于小表JOIN大表的场景，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率 使用 方法一： 在Hive0.11前，必须使用MAPJOIN来标记显示地启动该优化操作，由于其需要将小表加载进内存所以要注意小表的大小 SELECT /*+ MAPJOIN(smalltable)*/ .key,value FROM smalltable JOIN bigtable ON smalltable.key = bigtable.key 方法二： 在Hive0.11后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机 hive.auto.convert.join 默认值为true，自动开启MAPJOIN优化 hive.mapjoin.smalltable.filesize 默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中 注意：使用默认启动该优化的方式如果出现默名奇妙的BUG(比如MAPJOIN并不起作用),就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化 hive.auto.convert.join=false(关闭自动MAPJOIN转换操作) hive.ignore.mapjoin.hint=false(不忽略MAPJOIN标记) 对于以下查询是不支持使用方法二(MAPJOIN标记)来启动该优化的 select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne) ) firstjoin JOIN smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo) 但是，如果使用的是方法一即没有MAPJOIN标记则以上查询语句将会被作为两个MJ执行，进一步的，如果预先知道表大小是能够被加载进内存的，则可以通过以下属性来将两个MJ合并成一个MJ hive.auto.convert.join.noconditionaltask：Hive在基于输入文件大小的前提下将普通JOIN转换成MapJoin，并是否将多个MJ合并成一个 hive.auto.convert.join.noconditionaltask.size：多个MJ合并成一个MJ时，其表的总的大小须小于该值，同时hive.auto.convert.join.noconditionaltask必须为true 4.9 HQL 和 SQL 有哪些常见的区别 总体一致：Hive-sql与SQL基本上一样，因为当初的设计目的，就是让会SQL不会编程MapReduce的也能使用Hadoop进行处理数据。 区别：Hive没有delete和update。 Hive不支持等值连接 --SQL中对两表内联可以写成： select * from dual a,dual b where a.key = b.key; --Hive中应为 select * from dual a join dual b on a.key = b.key; --而不是传统的格式： SELECT t1.a1 as c1, t2.b1 as c2FROM t1, t2 WHERE t1.a2 = t2.b2 分号字符 --分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如： select concat(key,concat(';',key)) from dual; --但HiveQL在解析语句时提示： FAILED: Parse Error: line 0:-1 mismatched input '&lt;EOF>' expecting ) in function specification --解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成： select concat(key,concat('\\073',key)) from dual; IS [NOT] NULL --SQL中null代表空值, 值得警惕的是, --在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False. Hive不支持将数据插入现有的表或分区中 hive不支持INSERT INTO 表 Values（）, UPDATE, DELETE操作 hive支持嵌入mapreduce程序，来处理复杂的逻辑如： FROM ( 1. MAP doctext USING 'python wc_mapper.py' AS (word, cnt) 2. FROM docs 3. CLUSTER BY word 4. ) a 5. REDUCE word, cnt USING 'python wc_reduce.py'; --doctext: 是输入 --word, cnt: 是map程序的输出 --CLUSTER BY: 将wordhash后，又作为reduce程序的输入并且map程序、reduce程序可以单独使用，如： 1. FROM ( 2. FROM session_table 3. SELECT sessionid, tstamp, data 4. DISTRIBUTE BY sessionid SORT BY tstamp 5. ) a 6. REDUCE sessionid, tstamp, data USING 'session_reducer.sh'; --DISTRIBUTE BY: 用于给reduce程序分配行数据 4.10 Hive开窗函数假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。 1. SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。 SELECT *, /*前三次贷款的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv1, /*历史所有贷款 累加到下一次贷款 的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) AS pv2 FROM loan ; ​ 其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。 窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用： n PRECEDING : 前n行 n preceding n FOLLOWING：后n行 CURRENT ROW ： 当前行 如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为: ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. 窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。 2. NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。 SELECT *, #将数据按name切分成10区，并返回属于第几个分区 NTILE(10) OVER (PARTITION BY name ORDER BY orderdate) AS f1, #将数据按照name分区，并按照orderdate排序，返回排序序号 ROW_NUMBER() OVER (PARTITION BY name ORDER BY orderdate) AS f2, #将数据按照name分区，并按照orderdate排序，返回排序序号 RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f3, #将数据按照name分区，并按照orderdate排序，返回排序序号 DENSE_RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f4 FROM loan; 其中第一个函数NTILE(10)是将数据按name切分成10区，并返回属于第几个分区。 可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。语法是：ntile (num) over ([partition_clause] order_by_clause) as your_bucket_num 然后可以根据桶号，选取前或后 n分之几的数据。 后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。 ROW_NUMBER()返回的是一列连续的序号。 RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}. DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}. 3. LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点 SELECT *, #取上一笔贷款的日期,缺失默认填NULL LAG(orderdate, 1) OVER(PARTITION BY name ORDER BY orderdate) AS last_dt, #取下一笔贷款的日期,缺失指定填'1970-1-1' LEAD(orderdate, 1,'1970-1-1') OVER(PARTITION BY name ORDER BY orderdate) AS next_dt, #取最早一笔贷款的日期 FIRST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS first_dt, #取新一笔贷款的日期 LAST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS latest_dt FROM loan; LAG(n)将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录 4. GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate GROUPING SETS(substring(orderdate,1,7), orderdate) ORDER BY GROUPING__ID; GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。 比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5. 如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null. SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with CUBE ORDER BY GROUPING__ID; with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with ROLLUP ORDER BY GROUPING__ID; with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。 4.11 HiveUDF UDAF UDTF函数UDF：一进一出 实现方法： 继承UDF类 重写evaluate方法 将该java文件编译成jar 在终端输入如下命令： hive> add jar test.jar; hive> create temporary function function_name as 'com.hrj.hive.udf.UDFClass'; hive> select function_name(t.col1) from table t; hive> drop temporary function function_name; UDAF：多进一出 实现方法: 用户的UDAF必须继承了org.apache.hadoop.hive.ql.exec.UDAF； 用户的UDAF必须包含至少一个实现了org.apache.hadoop.hive.ql.exec的静态类，诸如实现了 UDAFEvaluator 一个计算函数必须实现的5个方法的具体含义如下： init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。 iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则就返回true。 terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。 merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。 terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。 部分聚集结果的数据类型和最终结果的数据类型可以不同。 UDTF：一进多出 实现方法： 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF initialize()：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型） process：初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列 可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数 最后close()方法调用，对需要清理的方法进行清理 4.12 Hive数据倾斜问题0. 什么是数据倾斜 ​ 对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。 来源百度百科 ​ 对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 1. 数据倾斜的现象 ​ 多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。 2. 数据倾斜的影响 1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：Out Of Memory。 2）运行速度慢 ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。 一个经验结论是：一般情况下，OOM的原因都是数据倾斜\\ 3. 如何定位数据倾斜 ​ 数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。 原因： 查看任务-》查看Stage-》查看代码 ​ 某个task执行特别慢的情况 ​ 某个task莫名其妙内存溢出的情况 ​ 查看导致数据倾斜的key的数据分布情况 也可从以下几种情况考虑： 1、是不是有OOM情况出现，一般是少数内存溢出的问题 2、是不是应用运行时间差异很大，总体时间很长 3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题 4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断 5、看看代码里面是否有一些导致Shuffle的算子出现 4. 数据倾斜的几种典型情况（重点） 数据源中的数据分布不均匀，Spark需要频繁交互 数据集中的不同Key由于分区方式，导致数据倾斜 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要） 聚合操作中，数据集中的数据分布不均匀（主要） JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀 数据集中少数几个key数据量很大，不重要，其他数据均匀 注意： 需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题 只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以 5. 数据倾斜的处理方法 ​ 发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。 5.1 检查数据，找出异常的key ​ 如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个 df.select(\"key\").sample(false,0.1).(k=>(k,1)).reduceBykey(_+_).map(k=>(k._2,k._1)).sortByKey(false).take(10) ​ 如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。 经过分析，倾斜的数据主要有以下三种情况: null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。 无效数据，大量重复的测试数据或是对结果影响不大的有效数据。 有效数据，业务导致的正常数据分布。 解决办法 第1，2种情况，直接对数据进行过滤即可。 第3种情况则需要进行一些特殊操作，常见的有以下几种做法。 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。 对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。 使用reduceByKey 代替 groupByKey 使用map join。 举例：如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下： 将原始的 key 转化为 key + 随机值(例如Random.nextInt)对数据进行 reduceByKey(func)将 key + 随机值 转成 key再对数据进行 reduceByKey(func)tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。 tips2: 单独处理异常数据时，可以配合使用Map Join解决 5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互 解决方案1：避免数据源的数据倾斜 实现原理：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive或者Kafka中还是会发生数据倾斜。 适用情况：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 总结：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题 5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜 解决方案1：调整并行度 实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。 总结：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效 解决方案2： 缓解数据倾斜**（自定义Partitioner）** 适用场景：大量不同的Key被分配到了相同的Task造成该Task数据量过大。 解决方案： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。 优势： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。 劣势： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 5.2 检查Spark运行过程相关操作 5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要） 解决方案：Reduce side Join转变为Map side Join 方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。 5.2.2 聚合操作中，数据集中的数据分布不均匀（主要） 解决方案：两阶段聚合（局部聚合+全局聚合） 适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案 实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案 将相同key的数据分拆处理 5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀 解决方案：为倾斜key增加随机前/后缀 适用场景：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。 解决方案：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。 优势：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。 劣势：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。 注意：具有倾斜Key的RDD数据集中，key的数量比较少 5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀 解决方案：随机前缀和扩容RDD进行join 适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。 实现思路：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀） 5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀 解决方案：过滤少数倾斜Key 适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 4.13 HiveSQL 的优化（系统参数调整、SQL 语句优化） Hive优化目标 在有限的资源下，执行效率更高 常见问题 数据倾斜 map数设置 reduce数设置 其他 Hive执行 HQL –&gt; Job –&gt; Map/Reduce 执行计划 explain [extended] hql 样例 select col,count(1) from test2 group by col; explain select col,count(1) from test2 group by col; Hive表优化 分区 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; 静态分区 动态分区 分桶 set hive.enforce.bucketing=true; set hive.enforce.sorting=true; 数据 相同数据尽量聚集在一起 Hive Job优化 并行化执行 每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间 set hive.exec.parallel= true; set hive.exec.parallel.thread.numbe=8; 本地化执行 job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB) job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4) job的reduce数必须为0或者1 set hive.exec.mode.local.auto=true; 当一个job满足如下条件才能真正使用本地模式: job合并输入小文件 set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 合并文件数由mapred.max.split.size限制的大小决定 job合并输出小文件** set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件 set hive.merge.size.per.task=64000000;合并之后的文件大小 JVM重利用 set mapred.job.reuse.jvm.num.tasks=20; JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。 压缩数据 set hive.exec.compress.output=true; set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec; set mapred.output.compression.type=BLOCK; set hive.exec.compress.intermediate=true; set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec; set hive.intermediate.compression.type=BLOCK; 中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式 hive查询最终的输出也可以压缩 Hive Map优化 set mapred.map.tasks =10; 无效 (1)默认map个数 default_num=total_size/block_size; (2)期望大小 goal_num=mapred.map.tasks; (3)设置处理的文件大小 split_size=max(mapred.min.split.size,block_size); split_num=total_size/split_size; (4)计算的map个数 compute_map_num=min(split_num,max(default_num,goal_num)) 经过以上的分析，在设置map个数的时候，可以简答的总结为以下几点： 增大mapred.min.split.size的值 如果想增加map个数，则设置mapred.map.tasks为一个较大的值 如果想减小map个数，则设置mapred.min.split.size为一个较大的值 情况1：输入文件size巨大，但不是小文件 情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。 map端聚合 set hive.map.aggr=true; 推测执行 mapred.map.tasks.apeculative.execution Hive Shuffle优化 Map端 io.sort.mb io.sort.spill.percent min.num.spill.for.combine io.sort.factor io.sort.record.percent Reduce端 mapred.reduce.parallel.copies mapred.reduce.copy.backoff io.sort.factor mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent Hive Reduce优化 需要reduce操作的查询 group by,join,distribute by,cluster by… order by比较特殊,只需要一个reduce sum,count,distinct… 聚合函数 高级查询 推测执行 mapred.reduce.tasks.speculative.execution hive.mapred.reduce.tasks.speculative.execution Reduce优化 numRTasks = min[maxReducers,input.size/perReducer] maxReducers=hive.exec.reducers.max perReducer = hive.exec.reducers.bytes.per.reducer hive.exec.reducers.max 默认 ：999 hive.exec.reducers.bytes.per.reducer 默认:1G set mapred.reduce.tasks=10;直接设置 计算公式 Hive查询操作优化 join优化 关联操作中有一张表非常小 不等值的链接操作 set hive.auto.current.join=true; hive.mapjoin.smalltable.filesize默认值是25mb select /*+mapjoin(A)*/ f.a,f.b from A t join B f on (f.a=t.a) hive.optimize.skewjoin=true;如果是Join过程出现倾斜，应该设置为true set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化 mapjoin 简单总结下,mapjoin的使用场景: Bucket join 两个表以相同方式划分桶 两个表的桶个数是倍数关系 crete table order(cid int,price float) clustered by(cid) into 32 buckets; crete table customer(id int,first string) clustered by(id) into 32 buckets; select price from order t join customer s on t.cid=s.id join 优化前 select m.cid,u.id from order m join customer u on m.cid=u.id where m.dt='2013-12-12'; join优化后 select m.cid,u.id from (select cid from order where dt='2013-12-12')m join customer u on m.cid=u.id; group by 优化 hive.groupby.skewindata=true;如果是group by 过程出现倾斜 应该设置为true set hive.groupby.mapaggr.checkinterval=100000;--这个是group的键对应的记录条数超过这个值则会进行优化 count distinct 优化 优化前 select count(distinct id) from tablename 优化后 select count(1) from (select distinct id from tablename) tmp; select count(1) from (select id from tablename group by id) tmp; 优化前 select a,sum(b),count(distinct c),count(distinct d) from test group by a 优化后 select a,sum(b) as b,count(c) as c,count(d) as d from(select a,0 as b,c,null as d from test group by a,c union all select a,0 as b,null as c,d from test group by a,d union all select a,b,null as c,null as d from test)tmp1 group by a; #二. Spark篇 1. SparkCore1.1 Spark工作原理1. Spark是什么Spark是一种通用分布式并行计算框架。和Mapreduce最大不同就是spark是基于内存的迭代式计算。 Spark的Job处理的中间输出结果可以保存在内存中，从而不再需要读写HDFS，除此之外，一个MapReduce 在计算过程中只有map 和reduce 两个阶段，处理之后就结束了，而在Spark的计算模型中，可以分为n阶段，因为它内存迭代式的，我们在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。 因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。其不仅实现了MapReduce的算子map 函数和reduce函数及计算模型，还提供更为丰富的算子，如filter、join、groupByKey等。是一个用来实现快速而同用的集群计算的平台。 2. Spark工作原理框图 第一层级工作流程 a. 构建Spark Application的运行环境（启动SparkContext） b. SparkContext在初始化过程中分别创建DAGScheduler作业调度和TaskScheduler任务调度两级调度模块 c. SparkContext向资源管理器（可以是Standalone、Mesos、Yarn）申请运行Executor资源； d. 由资源管理器分配资源并启动StandaloneExecutorBackend，executor，之后向SparkContext申请Task； e. DAGScheduler将job 划分为多个stage,并将Stage提交给TaskScheduler; g. Task在Executor上运行，运行完毕释放所有资源。 第二层级DAGScheduler作业调度生成过程 DAGScheduler是一个面向stage 的作业调度器。 作业调度模块是基于任务阶段的高层调度模块，它为每个Spark作业计算具有依赖关系的多个调度阶段（通常根据shuffle来划分），然后为每个阶段构建出一组具体的任务（通常会考虑数据的本地性等），然后以TaskSets（任务组）的形式提交给任务调度模块来具体执行。 主要三大功能 接受用户提交的job。将job根据类型划分为不同的stage，记录哪些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset； 决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给TaskScheduler； 重新提交shuffle输出丢失的stage给taskScheduler； DAG如何将Job划分为多个stage 划分依据：**宽窄依赖**。何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。 切割规则：从后往前，遇到宽依赖就切割stage； Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是宽窄依赖，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以taskSet的形式提交给TaskScheduler运行，stage是由一组并行的task组成。 一旦driver程序中出现action，就会生成一个job，比如count等， ​ 向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。 ​ 每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是宽依赖，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。 第三层级谈谈spark中的宽窄依赖 RDD和它的父RDD的关系有两种类型：窄依赖和宽依赖 宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生 窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生 区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示： 1.2 Spark的shuffle原理和过程Shuffle过程框图 主要逻辑如下： 1）首先每一个MapTask会根据ReduceTask的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。 2）其次MapTask产生的结果会根据设置的partition算法填充到每个bucket中。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中。 当ReduceTask启动时，它会根据自己task的id和所依赖的Mapper的id从远端或本地的block manager中取得相应的bucket作为Reducer的输入进行处理。 这里的bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等。 Spark shuffle可以分为两部分：Shuffle Write 和 Shuffle Fetch Shuffle Write 由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。 shuffle write 的任务很简单，那么实现也很简单：将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。图示如下： 上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数），缓冲区被称为 bucket，其大小为spark.shuffle.file.buffer.kb ，默认是 32KB（Spark 1.1 版本以前是 100KB）。 其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。 ShuffleMapTask 的执行过程很简单：先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由partitioner.partition(record.getKey()))决定。每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 FileSegment。之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。 这样的实现很简单，但有几个问题： 产生的 FileSegment 过多。每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。 缓冲区占用内存空间大。每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了cores * R * 32 KB。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。 目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图： 可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。先执行完的 ShuffleMapTask 形成 ShuffleBlocki，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlocki’，每个 ShuffleBlock 被称为 FileSegment。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过spark.shuffle.consolidateFiles=true来开启。 Shuffle Fetch先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey： 很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了： 在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？ 边 fetch 边处理还是一次性 fetch 完再处理？ fetch 来的数据存放到哪里？ 怎么获得要 fetch 的数据的存放位置？ 在什么时候 fetch？当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。理论上讲，一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 spark.reducer.maxMbInFlight，这里用 softBuffer 表示，默认大小为 48MB。一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。 边 fetch 边处理还是一次性 fetch 完再处理？边 fetch 边处理。本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？答案是使用可以 aggregate 的数据结构，比如 HashMap。每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 \\ record，直接将其放进 HashMap 里面。如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 func(hashMap.get(Key), Value)，比如上面 WordCount 例子中的 func 就是 hashMap.get(Key) ＋ Value，并将 func 的结果重新 put(key) 到 HashMap 中去。这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。 // MapReduce reduce(K key, Iterable&lt;V> values) { result = process(key, values) return result } // Spark reduce(K key, Iterable&lt;V> values) { result = null for (V value : values) result = func(result, value) return result } MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。因此一些算法比如求平均数，在 process 里面很好实现，直接sum(values)/values.length，而在 Spark 中 func 可以实现sum(values)，但不好实现/values.length。更多的 func 将会在下面的章节细致分析。 fetch 来的数据存放到哪里？刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。如果spark.shuffle.spill = false就只用内存。内存使用的是AppendOnlyMap ，类似 Java 的HashMap，内存＋磁盘使用的是ExternalAppendOnlyMap，如果内存空间不足时，ExternalAppendOnlyMap可以将 \\ records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。 怎么获得要 fetch 的数据的存放位置？在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。 1.3 Spark的Stage划分及优化 窄依赖指父RDD的每一个分区最多被一个子RDD的分区所用，表现为 一个父RDD的分区对应于一个子RDD的分区 两个父RDD的分区对应于一个子RDD 的分区。 宽依赖指子RDD的每个分区都要依赖于父RDD的所有分区，这是shuffle类操作 Stage: 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。Stage的划分，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。 比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个； 如果是rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage. 会根据RDD之间的依赖关系将DAG图划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中 Stage划分思路 因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。 在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！ 而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中。 总结 map,filter为窄依赖， groupbykey为宽依赖 遇到一个宽依赖就分一个stage 1.4 Spark和MapReduce的区别整体对比概念 Spark Shuffle 与MapReduce Shuffle的设计思想相同，但是实现细节优化方式不同。 1. 从逻辑角度来讲，Shuffle 过程就是一个 GroupByKey 的过程，两者没有本质区别。只是 MapReduce 为了方便 GroupBy 存在于不同 partition 中的 key/value records，就提前对 key 进行排序。Spark 认为很多应用不需要对 key 排序，就默认没有在 GroupBy 的过程中对 key 排序。 2. 从数据流角度讲，两者有差别。 MapReduce 只能从一个 Map Stage shuffle 数据，Spark 可以从多个 Map Stages shuffle 数据 3 .Shuffle write/read 实现上有一些区别。 以前对 shuffle write/read 的分类是 sort-based 和 hash-based。MapReduce 可以说是 sort-based，shuffle write 和 shuffle read 过程都是基于key sorting 的 (buffering records + in-memory sort + on-disk external sorting)。早期的 Spark 是 hash-based，shuffle write 和 shuffle read 都使用 HashMap-like 的数据结构进行 aggregate (without key sorting)。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based (only sort partition id, without key sorting)，shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经“你中有我，我中有你”，界限已经不那么清晰。 4. 从数据 fetch 与数据计算的重叠粒度来讲，两者有细微区别。 MapReduce 是粗粒度，reducer fetch 到的 records 先被放到 shuffle buffer 中休息，当 shuffle buffer 快满时，才对它们进行 combine()。而 Spark 是细粒度，可以即时将 fetch 到的 record 与 HashMap 中相同 key 的 record 进行 aggregate。 解说：1、MapReduce在Map阶段完成之后数据会被写入到内存中的一个环形缓冲区（后续的分区/分组/排序在这里完成）；Spark的Map阶段完成之后直接输出到磁盘。2、受第一步的影响，MapReduce输出的数据是有序的（针对单个Map数据来说）；Spark的数据是无序的（可以使用RDD算子达到排序的效果）。3、MapReduce缓冲区的数据处理完之后会spill到磁盘形成一个文件，文件数量达到阈值之后将会进行merge操作，将多个小文件合并为一个大文件；Spark没有merge过程，一个Map中如果有对应多个Reduce的数据，则直接写多个磁盘文件。4、MapReduce全部通过网络来获得数据；对于本地数据Spark可以直接读取 1.5 宽依赖与窄依赖区别RDD和它的父RDD的关系有两种类型：窄依赖和宽依赖 宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生 窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生 区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示： 1.6 Spark RDD 原理1. RDD是什么 RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合 Dataset：就是一个集合，用于存放数据的 Destributed：分布式，可以并行在集群计算 Resilient：表示弹性的，弹性表示 RDD中的数据可以存储在内存或者磁盘中； RDD中的分区是可以改变的； A list of partitions：一个分区列表，RDD中的数据都存储在一个分区列表中 A function for computing each split：作用在每一个分区中的函数 A list of dependencies on other RDDs：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的 Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向 可选项，数据本地性，数据位置最优 2. RDD操作​ RDD创建后就可以在RDD上进行数据处理。RDD支持两种操作：转换（transformation），即从现有的数据集创建一个新的数据集；动作（action），即在数据集上进行计算后，返回一个值给Driver程序。 转换（transformation） ​ RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。 ​ RDD中所有的Transformation都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的Action时，这些Transformation才会真正运行。 #### map(func)** 返回一个新的分布式数据集，该数据集由每一个输入元素经过func函数转换后组成 #### **fitler(func)** 返回一个新的数据集，该数据集由经过func函数计算后返回值为true的输入元素组成 #### **flatMap(func)** 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（因此func返回一个序列，而不是单一元素） #### **mapPartitions(func)** 类似于map，但独立地在RDD上每一个分片上运行，因此在类型为T的RDD上运行时，func函数类型必须是Iterator[T]=>Iterator[U] #### **mapPartitionsWithSplit(func)** 类似于mapPartitons，但func带有一个整数参数表示分片的索引值。因此在类型为T的RDD上运行时，func函数类型必须是(Int,Iterator[T])=>Iterator[U] #### **sample(withReplacement,fraction,seed)** 根据fraction指定的比例对数据进行采样，可以选择是否用随机数进行替换，seed用于随机数生成器种子 #### **union(otherDataSet)** 返回一个新数据集，新数据集是由原数据集和参数数据集联合而成 #### **distinct([numTasks])** 返回一个包含原数据集中所有不重复元素的新数据集 #### **groupByKey([numTasks])** 在一个(K,V)数据集上调用，返回一个(K,Seq[V])对的数据集。注意默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数来改变它 #### **reduceByKey(func,[numTasks])** 在一个(K,V)对的数据集上调用，返回一个(K,V)对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。与groupByKey类似，reduceByKey任务的个数是可以通过第二个可选参数来设置的 #### **sortByKey([[ascending],numTasks])** 在一个(K,V)对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的(K,V)对数据集。升序或降序由ascending布尔参数决定 #### **join(otherDataset0,[numTasks])** 在类型为(K,V)和(K,W)数据集上调用，返回一个相同的key对应的所有元素在一起的(K,(V,W))数据集 #### **cogroup(otherDataset,[numTasks])** 在类型为(K,V)和(K,W)数据集上调用，返回一个(K,Seq[V],Seq[W])元祖的数据集。这个操作也可以称为groupwith #### **cartesain(ohterDataset)** 笛卡尔积，在类型为T和U类型的数据集上调用，返回一个(T,U)对数据集(两两的元素对) 动作（action） #### **reduce(func)** 通过函数func(接收两个参数，返回一个参数)聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的并行运行 #### **collect()** 在驱动程序中，以数组形式返回数据集中的所有元素。通常在使用filter或者其他操作返回一个足够小的数据子集后再使用会比较有用 #### **count()** 返回数据集元素个数 #### **first()** 返回数据集第一个元素(类似于take(1)) #### **take(n)** 返回一个由数据集前n个元素组成的数组 注意 这个操作目前并非并行执行，而是由驱动程序计算所有的元素 #### **takeSample(withReplacement,num,seed)** 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否由随机数替换不足的部分，seed用户指定随机数生成器种子 #### **saveAsTextFile(path)** 将数据集的元素以textfile的形式保存到本地文件系统—HDFS或者任何其他Hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行 #### **saveAsSequenceFile(path)** 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是本地系统、HDFS或者任何其他的Hadoop支持的文件系统。这个只限于由key-value对组成，并实现了Hadoop的Writable接口，或者可以隐式的转换为Writable的RDD(Spark包括了基本类型转换，例如Int、Double、String等) #### **countByKey()** 对(K,V)类型的RDD有效，返回一个(K,Int)对的map，表示每一个key对应的元素个数 #### **foreach(func)** 在数据集的每一个元素上，运行函数func进行更新。通常用于边缘效果，例如更新一个叠加器，或者和外部存储系统进行交互，如HBase 3. RDD共享变量在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。 (1). 广播变量（Broadcast Variables） – 广播变量缓存到各个节点的内存中，而不是每个 Task – 广播变量被创建后，能在集群中运行的任何函数调用 – 广播变量是只读的，不能在被广播后修改 – 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本 val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量(2). 累加器 ​ 累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值。 4. RDD缓存Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 transformation自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。 ​ Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。 ​ Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择： 如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。 如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。 如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。 如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。 在不会使用cached RDD的时候，及时使用unpersist方法来释放它。 1.7 RDD有哪几种创建方式1) 使用程序中的集合创建rdd2) 使用本地文件系统创建rdd3) 使用hdfs创建rdd，4) 基于数据库db创建rdd5) 基于Nosql创建rdd，如hbase6) 基于s3创建rdd，7) 基于数据流，如socket创建rdd 1.8 Spark的RDD DataFrame和DataSet的区别RDD的优点： 相比于传统的MapReduce框架，Spark在RDD中内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据。 面向对象编程，直接存储的java对象，类型转化也安全 RDD的缺点： 由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦 默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁 DataFrame的优点： 结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表 有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。 hive兼容，支持hql、udf等 DataFrame的缺点： 编译时不能类型转化安全检查，运行时才能确定是否有问题 对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象 DateSet的优点： DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据 和RDD一样，支持自定义对象存储 和DataFrame一样，支持结构化数据的sql查询 采用堆外内存存储，gc友好 类型转化安全，代码友好 如此回答有3个坑（容易引起面试官追问）： 1）Spark shuffle 与 MapReduce shuffle（或者Spark 与 MR 的区别） 2）Spark内存模型 3）对gc（垃圾回收）的了解 1.10 Spark 的通信机制分布式的通信方式 RPC RMI JMS EJB Web Serivice 通信框架Akka​ Hadoop MR中的计算框架，jobTracker和TaskTracker间是由于通过heartbeat的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色的高效的Akka和netty通信系统 1.11 Spark的数据容错机制一般而言，对于分布式系统，数据集的容错性通常有两种方式： 1） 数据检查点（在Spark中对应Checkpoint机制）。 2） 记录数据的更新（在Spark中对应Lineage血统机制）。 对于大数据分析而言，数据检查点操作成本较高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低，同时会消耗大量存储资源。 Spark选择记录更新的方式。但更新粒度过细时，记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列记录下来，以便恢复丢失的分区。 Lineage（血统）机制​ 每个RDD除了包含分区信息外，还包含它从父辈RDD变换过来的步骤，以及如何重建某一块数据的信息，因此RDD的这种容错机制又称“血统”（Lineage）容错。Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做以便恢复数据。 ​ 相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新计算和恢复丢失的数据分区。但这种数据模型粒度较粗，因此限制了Spark的应用场景。所以可以说Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能方面的提升。 ​ RDD在Lineage容错方面采用如下两种依赖来保证容错方面的性能： 窄依赖（Narrow Dependeny）：窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。其中，1个父RDD分区对应1个子RDD分区，可以分为如下两种情况： 子RDD分区与父RDD分区一一对应（如map、filter等算子）。一个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。 宽依赖（Wide Dependency，源码中称为Shuffle Dependency）： 宽依赖是指一个父RDD分区对应多个子RDD分区，可以分为如下两种情况： 一个父RDD对应所有子RDD分区（未经协同划分的Join）。 一个父RDD对应多个RDD分区（非全部分区）（如groupByKey）。 窄依赖与宽依赖关系如图3-10所示。 从图3-10可以看出对依赖类型的划分：根据父RDD分区是对应一个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分区）。如果对应多个，则当容错重算分区时，对于需要重新计算的子分区而言，只需要父分区的一部分数据，因此其余数据的重算就导致了冗余计算。 图3-10 两种依赖关系 对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，在通过重新计算恢复数据的情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。 窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点（前面调度机制中已讲过）。 依赖关系在lineage容错中的应用总结如下： 1）窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成，并且父RDD的计算结果进行hash并传到对应节点上之后，才能计算子RDD。 2）数据丢失时，对于窄依赖，只需要重新计算丢失的那一块数据来恢复；对于宽依赖，则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖时，需要在适当的时机设置数据检查点（checkpoint机制在下节讲述）。可见Spark在容错性方面要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。 在Spark容错机制中，如果一个节点宕机了，而且运算属于窄依赖，则只要重算丢失的父RDD分区即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。更深入地来说：在窄依赖关系中，当子RDD的分区丢失，重算其父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，因此不存在冗余计算。而在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区使用，其中有一部分数据对应的是其他不需要重新计算的子RDD分区中的数据，因此在宽依赖关系下，这样计算就会产生冗余开销，这也是宽依赖开销更大的原因。为了减少这种冗余开销，通常在Lineage血统链比较长，并且含有宽依赖关系的容错中使用Checkpoint机制设置检查点。 Checkpoint（检查点）机制通过上述分析可以看出Checkpoint的本质是将RDD写入Disk来作为检查点。这种做法是为了通过lineage血统做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。 1.13 Spark性能调优1) 常用参数说明--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可 --executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时处理大批量数据时容易内存不足，再多申请一点，如6G --num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等 --executor-cores 2 : 每个executor内的核数，即每个executor中的任务task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce任务的并行度是executor数目*executor中的任务数 yarn集群中一般有资源申请上限，如，executor-memory*num-executors &lt; 400G 等，所以调试参数时要注意这一点 —-spark.default.parallelism 200 ： Spark作业的默认为500~1000个比较合适,如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num-executors * executor-cores的2~3倍比较合适。 -- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor内存中能占的最大比例。默认值是0.6 —-spark.shuffle.memoryFraction 0.2 ： 设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2，如果shuffle聚合时使用的内存超出了这个20%的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能 —-spark.yarn.executor.memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存 2) Spark常用编程建议 避免创建重复的RDD，尽量复用同一份数据。 尽量避免使用shuffle类算子，因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子 用aggregateByKey和reduceByKey替代groupByKey,因为前两个是预聚合操作，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。 repartition适用于RDD[V], partitionBy适用于RDD[K, V] mapPartitions操作替代普通map，foreachPartitions替代foreach filter操作之后进行coalesce操作，可以减少RDD的partition数量 如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER 尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作 多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow spark中的Group/join/XXXByKey等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数 尽量保证每轮Stage里每个task处理的数据量&gt;128M 如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲 2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD) 若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores=1， executor-memory=m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores=size/m(向上取整) 如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以hash join的方式实现，具体原理参考下一节的shuffle过程 如果需要在repartition重分区之后还要进行排序，可直接使用repartitionAndSortWithinPartitions，比分解操作效率高，因为它可以一边shuffle一边排序 3) shuffle性能优化3.1 什么是shuffle操作 spark中的shuffle操作功能：将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join操作，类似洗牌的操作。这些分布在各个存储节点上的数据重新打乱然后汇聚到不同节点的过程就是shuffle过程。 3.2 哪些操作中包含shuffle操作 RDD的特性是不可变的带分区的记录集合，Spark提供了Transformation和Action两种操作RDD的方式。Transformation是生成新的RDD，包括map, flatMap, filter, union, sample, join, groupByKey, cogroup, ReduceByKey, cros, sortByKey, mapValues等；Action只是返回一个结果，包括collect，reduce，count，save，lookupKey等 Spark所有的算子操作中是否使用shuffle过程要看计算后对应多少分区： 若一个操作执行过程中，结果RDD的每个分区只依赖上一个RDD的同一个分区，即属于窄依赖，如map、filter、union等操作，这种情况是不需要进行shuffle的，同时还可以按照pipeline的方式，把一个分区上的多个操作放在同一个Task中进行 若结果RDD的每个分区需要依赖上一个RDD的全部分区，即属于宽依赖，如repartition相关操作（repartition，coalesce）、*ByKey操作（groupByKey，ReduceByKey，combineByKey、aggregateByKey等）、join相关操作（cogroup，join）、distinct操作，这种依赖是需要进行shuffle操作的 3.3 shuffle操作过程 shuffle过程分为shuffle write和shuffle read两部分 shuffle write： 分区数由上一阶段的RDD分区数控制，shuffle write过程主要是将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上（当前stage结束之后，每个task处理的数据按key进行分类，数据先写入内存缓冲区，缓冲区满，溢写spill到磁盘文件，最终相同key被写入同一个磁盘文件）创建的磁盘文件数量=当前stage中task数量*下一个stage的task数量 shuffle read：从上游stage的所有task节点上拉取属于自己的磁盘文件，每个read task会有自己的buffer缓冲，每次只能拉取与buffer缓冲相同大小的数据，然后聚合，聚合完一批后拉取下一批，边拉取边聚合。分区数由Spark提供的一些参数控制，如果这个参数值设置的很小，同时shuffle read的数据量很大，会导致一个task需要处理的数据非常大，容易发生JVM crash，从而导致shuffle数据失败，同时executor也丢失了，就会看到Failed to connect to host 的错误(即executor lost)。 shuffle过程中，各个节点会通过shuffle write过程将相同key都会先写入本地磁盘文件中，然后其他节点的shuffle read过程通过网络传输拉取各个节点上的磁盘文件中的相同key。这其中大量数据交换涉及到的网络传输和文件读写操作是shuffle操作十分耗时的根本原因 3.4 spark的shuffle类型 参数spark.shuffle.manager用于设置ShuffleManager的类型。Spark1.5以后，该参数有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2以前的默认值，Spark1.2之后的默认值都是SortShuffleManager。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 由于SortShuffleManager默认会对数据进行排序，因此如果业务需求中需要排序的话，使用默认的SortShuffleManager就可以；但如果不需要排序，可以通过bypass机制或设置HashShuffleManager避免排序，同时也能提供较好的磁盘读写性能。 HashShuffleManager流程： SortShuffleManager流程： 3.5 如何开启bypass机制 bypass机制通过参数spark.shuffle.sort.bypassMergeThreshold设置，默认值是200，表示当ShuffleManager是SortShuffleManager时，若shuffle read task的数量小于这个阈值（默认200）时，则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式写数据，但最后会将每个task产生的所有临时磁盘文件合并成一个文件，并创建索引文件。 这里给出的调优建议是，当使用SortShuffleManager时，如果的确不需要排序，可以将这个参数值调大一些，大于shuffle read task的数量。那么此时就会自动开启bypass机制，map-side就不会进行排序了，减少排序的性能开销，提升shuffle操作效率。但这种方式并没有减少shuffle write过程产生的磁盘文件数量，所以写的性能没有改变。 3.6 HashShuffleManager优化建议 如果使用HashShuffleManager，可以设置spark.shuffle.consolidateFiles参数。该参数默认为false，只有当使用HashShuffleManager且该参数设置为True时，才会开启consolidate机制，大幅度合并shuffle write过程产生的输出文件，对于shuffle read task 数量特别多的情况下，可以极大地减少磁盘IO开销，提升shuffle性能。参考社区同学给出的数据，consolidate性能比开启bypass机制的SortShuffleManager高出10% ~ 30%。 3.7 shuffle调优建议 除了上述的几个参数调优，shuffle过程还有一些参数可以提高性能： - spark.shuffle.file.buffer : 默认32M，shuffle Write阶段写文件时的buffer大小，若内存资源比较充足，可适当将其值调大一些（如64M），减少executor的IO读写次数，提高shuffle性能 - spark.shuffle.io.maxRetries ： 默认3次，Shuffle Read阶段取数据的重试次数，若shuffle处理的数据量很大，可适当将该参数调大。 3.8 shuffle操作过程中的常见错误 SparkSQL中的shuffle错误： org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 org.apache.spark.shuffle.FetchFailedException:Failed to connect to hostname/192.168.xx.xxx:50268 RDD中的shuffle错误： WARN TaskSetManager: Lost task 17.1 in stage 4.1 (TID 1386, spark050013): java.io.FileNotFoundException: /data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/2f/temp_shuffle_e22e013a-5392-4edb-9874-a196a1dad97c (没有那个文件或目录) FetchFailed(BlockManagerId(6083b277-119a-49e8-8a49-3539690a2a3f-S155, spark050013, 8533), shuffleId=1, mapId=143, reduceId=3, message= org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer{file=/data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/0e/shuffle_1_143_0.data, offset=997061, length=112503} 处理shuffle类操作的注意事项： 减少shuffle数据量：在shuffle前过滤掉不必要的数据，只选取需要的字段处理 针对SparkSQL和DataFrame的join、group by等操作：可以通过 spark.sql.shuffle.partitions控制分区数，默认设置为200，可根据shuffle的量以及计算的复杂度提高这个值，如2000等 RDD的join、group by、reduceByKey等操作：通过spark.default.parallelism控制shuffle read与reduce处理的分区数，默认为运行任务的core总数，官方建议为设置成运行任务的core的2~3倍 提高executor的内存：即spark.executor.memory的值 分析数据验证是否存在数据倾斜的问题：如空值如何处理，异常数据（某个key对应的数据量特别大）时是否可以单独处理，可以考虑自定义数据分区规则，如何自定义可以参考下面的join优化环节 4) join性能优化Spark所有的操作中，join操作是最复杂、代价最大的操作，也是大部分业务场景的性能瓶颈所在。所以针对join操作的优化是使用spark必须要学会的技能。 spark的join操作也分为Spark SQL的join和Spark RDD的join。 4.1 Spark SQL 的join操作 4.1.1 Hash Join Hash Join的执行方式是先将小表映射成Hash Table的方式，再将大表使用相同方式映射到Hash Table，在同一个hash分区内做join匹配。 hash join又分为broadcast hash join和shuffle hash join两种。其中Broadcast hash join，顾名思义，就是把小表广播到每一个节点上的内存中，大表按Key保存到各个分区中，小表和每个分区的大表做join匹配。这种情况适合一个小表和一个大表做join且小表能够在内存中保存的情况。如下图所示： 当Hash Join不能适用的场景就需要Shuffle Hash Join了，Shuffle Hash Join的原理是按照join Key分区，key相同的数据必然分配到同一分区中，将大表join分而治之，变成小表的join，可以提高并行度。执行过程也分为两个阶段： shuffle阶段：分别将两个表按照join key进行分区，将相同的join key数据重分区到同一节点 hash join阶段：每个分区节点上的数据单独执行单机hash join算法 Shuffle Hash Join的过程如下图所示： 4.1.2 Sort-Merge Join SparkSQL针对两张大表join的情况提供了全新的算法——Sort-merge join，整个过程分为三个步骤： Shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式进行处理 sort阶段：对单个分区节点的两表数据，分别进行排序 merge阶段：对排好序的两张分区表数据执行join操作。分别遍历两个有序序列，遇到相同的join key就merge输出，否则继续取更小一边的key，即合并两个有序列表的方式。 sort-merge join流程如下图所示。 4.2 Spark RDD的join操作 Spark的RDD join没有上面这么多的分类，但是面临的业务需求是一样的。如果是大表join小表的情况，则可以将小表声明为broadcast变量，使用map操作快速实现join功能，但又不必执行Spark core中的join操作。 如果是两个大表join，则必须依赖Spark Core中的join操作了。Spark RDD Join的过程可以自行阅读源码了解，这里只做一个大概的讲解。 spark的join过程中最核心的函数是cogroup方法，这个方法中会判断join的两个RDD所使用的partitioner是否一样，如果分区相同，即存在OneToOneDependency依赖，不用进行hash分区，可直接join；如果要关联的RDD和当前RDD的分区不一致时，就要对RDD进行重新hash分区，分到正确的分区中，即存在ShuffleDependency，需要先进行shuffle操作再join。因此提升join效率的一个思路就是使得两个RDD具有相同的partitioners。 所以针对Spark RDD的join操作的优化建议是： 如果需要join的其中一个RDD比较小，可以直接将其存入内存，使用broadcast hash join 在对两个RDD进行join操作之前，使其使用同一个partitioners，避免join操作的shuffle过程 如果两个RDD其一存在重复的key也会导致join操作性能变低，因此最好先进行key值的去重处理 4.3 数据倾斜优化 均匀数据分布的情况下，前面所说的优化建议就足够了。但存在数据倾斜时，仍然会有性能问题。主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。 shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。 4.3.1 分析数据分布 如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。 4.3.2 数据倾斜的解决方案 这里参考美团技术博客中给出的几个方案。 1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。 2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key 3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量 4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。 5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。 6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作 5) 其他错误总结(1) 报错信息 java.lang.OutOfMemory, unable to create new native thread Caused by: java.lang.OutOfMemoryError: unable to create new native thread at java.lang.Thread.start0(Native Method) at java.lang.Thread.start(Thread.java:640) 解决方案： 上面这段错误提示的本质是Linux操作系统无法创建更多进程，导致出错，并不是系统的内存不足。因此要解决这个问题需要修改Linux允许创建更多的进程，就需要修改Linux最大进程数 （2）报错信息 由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实就是将/tmp目录挂载到内存当中, 那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误No Space Left on the device（Shuffle临时文件过多） 解决方案： 修改配置文件spark-env.sh,把临时文件引入到一个自定义的目录中去, 即: export SPARK_LOCAL_DIRS=/home/utoken/datadir/spark/tmp （3）报错信息 Worker节点中的work目录占用许多磁盘空间, 这些是Driver上传到worker的文件, 会占用许多磁盘空间 解决方案： 需要定时做手工清理work目录 （4）spark-shell提交Spark Application如何解决依赖库 解决方案： 利用–driver-class-path选项来指定所依赖的jar文件，注意的是–driver-class-path后如果需要跟着多个jar文件的话，jar文件之间使用冒号:来分割。 （5）内存不足或数据倾斜导致Executor Lost，shuffle fetch失败，Task重试失败等（spark-submit提交） TaskSetManager: Lost task 1.0 in stage 6.0 (TID 100, 192.168.10.37): java.lang.OutOfMemoryError: Java heap space INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.37:57139 (size: 42.0 KB, free: 24.2 MB) INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.38:53816 (size: 42.0 KB, free: 24.2 MB) INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 102, 192.168.10.37, ANY, 2152 bytes) 解决方案： 增加worker内存，或者相同资源下增加partition数目，这样每个task要处理的数据变少，占用内存变少 如果存在shuffle过程，设置shuffle read阶段的并行数 2. SparkSQL2.1 Spark SQL 的原理和运行机制 从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作 Parser 解析 SQL，生成 Unresolved Logical Plan 由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan Optimizer根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan CBO 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan Spark 以 DAG 的方法执行上述 Physical Plan 在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率 Parser Spark SQL 使用 Antlr 进行记法和语法解析，并生成 UnresolvedPlan。 当用户使用 SparkSession.sql(sqlText : String) 提交 SQL 时，SparkSession 最终会调用 SparkSqlParser 的 parsePlan 方法。该方法分两步 使用 Antlr 生成的 SqlBaseLexer 对 SQL 进行词法分析，生成 CommonTokenStream 使用 Antlr 生成的 SqlBaseParser 进行语法分析，得到 LogicalPlan Analyzer 从 Analyzer 的构造方法可见 Analyzer 持有一个 SessionCatalog 对象的引用 Analyzer 继承自 RuleExecutor[LogicalPlan]，因此可对 LogicalPlan 进行转换 Optimizer Spark SQL 目前的优化主要是基于规则的优化，即 RBO （Rule-based optimization） 每个优化以 Rule 的形式存在，每条 Rule 都是对 Analyzed Plan 的等价转换 RBO 设计良好，易于扩展，新的规则可以非常方便地嵌入进 Optimizer RBO 目前已经足够好，但仍然需要更多规则来 cover 更多的场景 优化思路主要是减少参与计算的数据量以及计算本身的代价 PushdownPredicatePushdownPredicate 是最常见的用于减少参与计算的数据量的方法。 SparkPlanner 得到优化后的 LogicalPlan 后，SparkPlanner 将其转化为 SparkPlan 即物理计划。 本例中由于 score 表数据量较小，Spark 使用了 BroadcastJoin。因此 score 表经过 Filter 后直接使用 BroadcastExchangeExec 将数据广播出去，然后结合广播数据对 people 表使用 BroadcastHashJoinExec 进行 Join。再经过 Project 后使用 HashAggregateExec 进行分组聚合。 至此，一条 SQL 从提交到解析、分析、优化以及执行的完整过程就介绍完毕。 2.3 Spark SQL 的优化策略1）内存列式存储与内存缓存表 Spark SQL可以通过cacheTable将数据存储转换为列式存储，同时将数据加载到内存缓存。cacheTable相当于在分布式集群的内存物化视图，将数据缓存，这样迭代的或者交互式的查询不用再从HDFS读数据，直接从内存读取数据大大减少了I/O开销。列式存储的优势在于Spark SQL只需要读出用户需要的列，而不需要像行存储那样每次都将所有列读出，从而大大减少内存缓存数据量，更高效地利用内存数据缓存，同时减少网络传输和I/O开销。数据按照列式存储，由于是数据类型相同的数据连续存储，所以能够利用序列化和压缩减少内存空间的占用。 2）列存储压缩 为了减少内存和硬盘空间占用，Spark SQL采用了一些压缩策略对内存列存储数据进行压缩。Spark SQL的压缩方式要比Shark丰富很多，如它支持PassThrough、RunLengthEncoding、DictionaryEncoding、BooleanBitSet、IntDelta、LongDelta等多种压缩方式，这样能够大幅度减少内存空间占用、网络传输和I/O开销。 3）逻辑查询优化 SparkSQL在逻辑查询优化（见图8-4）上支持列剪枝、谓词下压、属性合并等逻辑查询优化方法。列剪枝为了减少读取不必要的属性列、减少数据传输和计算开销，在查询优化器进行转换的过程中会优化列剪枝。 下面介绍一个逻辑优化的例子。 SELECT Class FROM （SELECT ID，Name，Class FROM STUDENT ） S WHERE S.ID=1 Catalyst将原有查询通过谓词下压，将选择操作ID=1优先执行，这样过滤大部分数据，通过属性合并将最后的投影只做一次，最终保留Class属性列。 4）Join优化 Spark SQL深度借鉴传统数据库的查询优化技术的精髓，同时在分布式环境下调整和创新特定的优化策略。现在Spark SQL对Join进行了优化，支持多种连接算法，现在的连接算法已经比Shark丰富，而且很多原来Shark的元素也逐步迁移过来，如BroadcastHashJoin、BroadcastNestedLoopJoin、HashJoin、LeftSemiJoin，等等。 下面介绍其中的一个Join算法。 BroadcastHashJoin将小表转化为广播变量进行广播，这样避免Shuffle开销，最后在分区内做Hash连接。这里使用的就是Hive中Map Side Join的思想，同时使用DBMS中的Hash连接算法做连接。 随着Spark SQL的发展，未来会有更多的查询优化策略加入进来，同时后续Spark SQL会支持像Shark Server一样的服务端和JDBC接口，兼容更多的持久化层，如NoSQL、传统的DBMS等。一个强有力的结构化大数据查询引擎正在崛起。 3. SparkStreaming3.1 原理剖析（源码级别）和运行机制 3.2 Spark Dstream 及其 API 操作 3.3 Spark Streaming 消费 Kafka 的两种方式 3.4 Spark 消费 Kafka 消息的 Offset 处理 3.5 窗口操作 4. SparkMlib可实现聚类、分类、推荐等算法 三. Flink Flink 集群的搭建 Flink 的架构原理 Flink 的编程模型 Flink 集群的 HA 配置 Flink DataSet 和 DataSteam API 序列化 Flink 累加器 状态 State 的管理和恢复 窗口和时间 并行度 Flink 和消息中间件 Kafka 的结合 Flink Table 和 SQL 的原理和用法 四. Kafka1. Kafka 的设计Kafka 将消息以 topic 为单位进行归纳 将向 Kafka topic 发布消息的程序成为 producers. 将预订 topics 并消费消息的程序成为 consumer. Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker. producers 通过网络将消息发送到 Kafka 集群，集群向消费者提供消息 2. 数据传输的三种事务定义数据传输的事务定义通常有以下三种级别： （1）最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输 （2）最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输. （3）精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而 且仅仅被传输一次，这是大家所期望的 3. Kafka 判断一个节点是否活着两大条件（1）节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连 接 （2）如果节点是个 follower,他必须能及时的同步 leader 的写操作，延时不能太久 4. Kafa consumer 是否可以消费指定分区消息？​ Kafa consumer 消费消息时，向 broker 发出”fetch”请求去消费特定分区的消息，consumer 指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer 拥有 了 offset 的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的 5. Kafka 消息是采用 Pull 模式or Push 模式？​ Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统 的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息 一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的 consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的 consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消 息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时， consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式 ​ Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push 模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还 是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一 次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决 定这些策略 ​ Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询， 直到新消息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达 (当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发 6. Kafka 存储在硬盘上的消息格式是什么？消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和 CRC32 校验码。 消息长度: 4 bytes (value: 1+4+n) 版本号: 1 byte CRC 校验码: 4 bytes 具体的消息: n bytes 7. Kafka 高效文件存储设计特点(1).Kafka 把 topic 中一个 parition 大文件分成多个小文件段，通过多个小文件段，就容易定 期清除或删除已经消费完文件，减少磁盘占用。 (2).通过索引信息可以快速定位 message 和确定 response 的最大大小。 (3).通过 index 元数据全部映射到 memory，可以避免 segment file 的 IO 磁盘操作。 (4).通过索引文件稀疏存储，可以大幅降低 index 文件元数据占用空间大小。 8. Kafka 与传统消息系统之间有三个关键区别(1).Kafka 持久化日志，这些日志可以被重复读取和无限期保留 (2).Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据 提升容错能力和高可用性 (3).Kafka 支持实时的流式处理 9. Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中 副本因子不能大于 Broker 的个数； 第一个分区（编号为 0）的第一个副本放置位置是随机从 brokerList 选择的； 其他分区的第一个副本放置位置相对于第 0 个分区依次往后移。也就是如果我们有 5 个Broker，5 个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个Broker 上，依次类推； 剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的 10. Kafka 新建的分区会在哪个目录下创建在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录， 这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘 上用于提高读写性能。 当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个 目录下创建文件夹用于存放数据。 但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？ 答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic 名+分区 ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就 是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁 盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。 11. partition 的数据如何保存到硬盘topic 中的多个 partition 以文件夹的形式保存到 broker，每个分区序号从 0 递增， 且消息有序 Partition 文件下有多个 segment（xxx.index，xxx.log） segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为 1g 如果大小大于 1g 时，会滚动一个新的 segment 并且以上一个 segment 最后一条消息的偏移 量命名 12. kafka 的 ack 机制request.required.acks 有三个值 0 1 -1 0:生产者不会等待 broker 的 ack，这个延迟最低但是存储的保证最弱当 server 挂掉的时候 就会丢数据 1：服务端会等待 ack 值 leader 副本确认接收到消息后发送 ack 但是如果 leader 挂掉后他 不确保是否复制完成新 leader 也会导致数据丢失 -1：同样在 1 的基础上 服务端会等所有的 follower 的副本受到数据后才会受到 leader 发出 的 ack，这样数据不会丢失 13. Kafka 的消费者如何消费数据​ 消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置 等到下次消费时，他会接着上次位置继续消费 14. 消费者负载均衡策略​ 一个消费者组中的一个分片对应一个消费者成员，他能保证每个消费者成员都能访问，如 果组中成员太多会有空闲的成员 15. 数据有序​ 一个消费者组里它的内部是有序的 ​ 消费者组与消费者组之间是无序的 16. kafaka 生产数据时数据的分组策略​ 生产者决定数据产生到集群的哪个 partition 中 ​ 每一条消息都是以（key，value）格式 ​ Key 是由生产者发送数据传入 ​ 所以生产者（key）决定了数据产生到集群的哪个 partition 五. 数据仓库5.1 数仓概念相关1. 数据仓库、数据集市、数据库之间的区别 数据仓库 ：数据仓库是一个面向主题的、集成的、随时间变化的、但信息本身相对稳定的数据集合，用于对管理决策过程的支持。是企业级的，能为整个企业各个部门的运行提供决策支持手段； 数据集市：则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。 数据库：是一种软件，用来实现数据库逻辑过程，属于物理层； 数据仓库是数据库概念的升级，从数据量来说，数据仓库要比数据库更庞大德多，主要用于数据挖掘和数据分析，辅助领导做决策 只是数据库内的数据时限要远远的长于操作型环境中的数据时限。在操作型环境中一般只保存有6090天的数据，而在数据仓库中则要需要保存较长时限的数据（例如：510年），以适应DSS进行趋势分析的要求。 2. OLAP、OLTP概念及用途 OLAP: 即On-Line Analysis Processing在线分析处理。 OLAP的特点：联机分析处理的主要特点，是直接仿照用户的多角度思考模式，预先为用户组建多维的数据模型，维指的是用户的分析角度。 OLTP: 即On-Line Transaction Processing联机事务处理过程(OLTP) OLTP的特点：结构复杂、实时性要求高。 OLAP和OLTP区别 1、基本含义不同：OLTP是传统的关系型数据库的bai主要应用，主要是基本的、日常的事务处理，记du录即时的增、删、改、查，比如在银行存取一笔款，就是一个事务交易。OLAP即联机分析处理，是数据仓库的核心部心，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。典型的应用就是复杂的动态报表系统。 2、实时性要求不同：OLTP实时性要求高，OLTP 数据库旨在使事务应用程序仅写入所需的数据，以便尽快处理单个事务。OLAP的实时性要求不是很高，很多应用顶多是每天更新一下数据。 3、数据量不同：OLTP数据量不是很大，一般只读/写数十条记录，处理简单的事务。OLAP数据量大，因为OLAP支持的是动态查询，所以用户也许要通过将很多数据的统计后才能得到想要知道的信息，例如时间序列分析等等，所以处理的数据量很大。 4、用户和系统的面向性不同：OLTP是面向顾客的,用于事务和查询处理。OLAP是面向市场的,用于数据分析。 5、数据库设计不同：OLTP采用实体-联系ER模型和面向应用的数据库设计。OLAP采用星型或雪花模型和面向主题的数据库设计。 3. 事实表、维度表、拉链表概念及区别 事实表：事实表其实质就是通过各种维度和一些指标值得组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。 维度表：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。 拉链表：拉链表，它是一种维护历史状态，以及最新状态数据的一种表。拉链表也是分区表，有些不变的数据或者是已经达到状态终点的数据就会把它放在分区里面，分区字段一般为开始时间：start_date和结束时间：end_date。一般在该天有效的数据，它的end_date是大于等于该天的日期的。获取某一天全量的数据，可以通过表中的start_date和end_date来做筛选，选出固定某一天的数据。例如我想取截止到20190813的全量数据，其where过滤条件就是where start_date&lt;=’20190813’ and end_date&gt;=20190813。 4. 全量表、增量表、快照表概念及区别 全量表：全量表没有分区，表中的数据是前一天的所有数据，比如说今天是24号，那么全量表里面拥有的数据是23号的所有数据，每次往全量表里面写数据都会覆盖之前的数据，所以全量表不能记录历史的数据情况，只有截止到当前最新的、全量的数据。 增量表：增量表，就是记录每天新增数据的表，比如说，从24号到25号新增了那些数据，改变了哪些数据，这些都会存储在增量表的25号分区里面。上面说的快照表的25号分区和24号分区（都是t+1，实际时间分别对应26号和25号），它两的数据相减就是实际时间25号到26号有变化的、增加的数据，也就相当于增量表里面25号分区的数据。 快照表：那么要能查到历史数据情况又该怎么办呢？这个时候快照表就派上用途了，快照表是有时间分区的，每个分区里面的数据都是分区时间对应的前一天的所有全量数据，比如说当前数据表有3个分区，24号，25号，26号。其中，24号分区里面的数据就是从历史到23号的所有数据，25号分区里面的数据就是从历史到24号的所有数据，以此类推。 4. 什么叫维度和度量值 维度：说明数据，维度是指可指定不同值的对象的描述性属性或特征。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。 度量：事实表和维度交叉汇聚的点，度量和维度构成OLAP的主要概念，这里面对于在事实表或者一个多维立方体里面存放的数值型的、连续的字段，就是度量。这符合上面的意思，有标准，一个度量字段肯定是统一单位，例如元、户数。如果一个度量字段，其中的度量值可能是欧元又有可能是美元，那这个度量可没法汇总。在统一计量单位下，对不同维度的描述。 5. 什么叫缓慢维度变化（Slowly Changing Dimensions，SCD)​ 维度建模的数据仓库中，有一个概念叫Slowly Changing Dimensions，中文一般翻译成缓慢变化维，经常被简写为SCD。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随时间发生变化的维度我们一般称之为缓慢变化维，并且把处理维度表的历史变化信息的问题称为处理缓慢变化维的问题，有时也简称为处理SCD的问题。 处理缓慢变化维的方法通常分为三种方式： 第一种方式是直接覆盖原值。这样处理，最容易实现，但是没有保留历史数据，无法分析历史变化信息。第一种方式通常简称为“TYPE 1”。 第二种方式是添加维度行。这样处理，需要代理键的支持。实现方式是当有维度属性发生变化时，生成一条新的维度记录，主键是新分配的代理键，通过自然键可以和原维度记录保持关联。第二种方式通常简称为“TYPE 2”。 第三种方式是添加属性列。这种处理的实现方式是对于需要分析历史信息的属性添加一列，来记录该属性变化前的值，而本属性字段使用TYPE 1来直接覆盖。这种方式的优点是可以同时分析当前及前一次变化的属性值，缺点是只保留了最后一次变化信息。第三种方式通常简称为“TYPE 3”。 5.2 数仓分层设计1. 数据仓库分为4层： ODS层 （原始数据层） BDM DWD层 （明细数据层） FDM DWS层 （服务数据层） GDM ADM ADS层 （数据应用层） APP 2. 各层主要负责职责ODS层（原始数据层）：存放原始数据，直接加载原始日志、数据，数据保存原貌不做处理。 DWD层（明细数据层）：结构与粒度原始表保持一致，对ODS层数据进行清洗（去除空值、脏数据、超过极限范围的数据） DWS层 （服务数据层）：以DWD为基础，进行轻度汇总 ADS层 （数据应用层）：为各种统计报表提供数据 3. **为什么要分层？** 空间换时间：通过建设多层次的数据模型供用户使用，避免用户直接使用操作型数据，可以更高效的访问数据 把复杂问题简单化：一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复 便于处理业务的变化：随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知 4. 数仓中每层表的建模？怎么建模？（1）ODS： 特点是保持原始数据的原貌，不作修改！ 原始数据怎么建模，ODS就怎么建模！举例： 用户行为数据特征是一条记录就是一行！ ODS层表(line string) 业务数据，参考Sqoop导入的数据类型进行建模！ （2）DWD层：特点从ODS层，将数据进行ETL（清洗），轻度聚合，再展开明细！ 在展开明细时，对部分维度表进行降维操作 例如：将商品一二三级分类表，sku商品表，spu商品表，商品品牌表合并汇总为一张维度表！ 对事实表，参考星型模型的建模策略，按照选择业务过程→声明粒度→确认维度→确认事实思路进行建模 选择业务过程： 选择感兴趣的事实表声明粒度： 选择最细的粒度！可以由最细的粒度通过聚合的方式得到粗粒度！确认维度： 根据3w原则确认维度，挑选自己感兴趣的维度确认事实： 挑选感兴趣的度量字段，一般是从事实表中选取！ DWS层： 根据业务需求进行分主题建模！一般是建宽表！ DWT层： 根据业务需求进行分主题建模！一般是建宽表！ ADS层： 根据业务需求进行建模！ 5.3 数仓建模1. 维度建模概念、类型、过程维度建模：维度建模是一种将数据结构化的逻辑设计方法，它将客观世界划分为度量和上下文。度量是常常是以数值形式出现，事实周围有上下文包围着，这种上下文被直观地分成独立的逻辑块，称之为维度。它与实体-关系建模有很大的区别，实体-关系建模是面向应用，遵循第三范式，以消除数据冗余为目标的设计技术。维度建模是面向分析，为了提高查询性能可以增加数据冗余，反规范化的设计技术。 维度建模过程：确定业务流程-&gt;确定粒度-&gt;确定纬度-&gt;确定事实 建模四步走： 1.选取要建模的业务处理流程 关注业务处理流程，而不是业务部门！ 2.定义业务处理的粒度 “如何描述事实表的单个行？” 3.选定用于每个事实表行的维度 常见维度包括日期、产品等 4.确定用于形成每个事实表行的数字型事实 典型的事实包括订货量、支出额这样的可加性数据 2. 星型模型和雪花模型概念、区别​ 在多维分析的商业智能解决方案中，根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。 当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型， 星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余， 如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。 当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。 雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。 它的优点是 : 通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。 此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率（空间换易用与效率）。 1.数据优化 雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。 相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。 2.业务模型 主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。 在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。 3.性能 第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。 而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。 4.ETL 雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。 星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。 总结 通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。 5.4 数仓使用经验3. 数据仓库系统的数据质量如何保证？方案？数据质量评估 完整性 准确性 及时性 一致性 4. 如何实现增量抽取？(主要采用时间戳方式，提供数据抽取和处理的性能) 5. 常见的数据治理方案1）数据压缩 2）小文件合并 3）冷数据处理 六. 数据库6.1 基本概念1. 主键、外键、超键、候选键 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 2. 为什么用自增列作为主键 如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引、 如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引、 如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的)。 数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点） 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 3. 触发器的作用？ 触发器是一种特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。 4. 什么是存储过程？用什么来调用？ 存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。 调用： 1）可以用一个命令对象来调用存储过程。 2）可以供外部程序调用，比如：java程序。 5. 存储过程的优缺点？ 优点： 1）存储过程是预编译过的，执行效率高。 2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。 3）安全性高，执行存储过程需要有一定权限的用户。 4）存储过程可以重复使用，可减少数据库开发人员的工作量。 缺点：移植性差 6. 存储过程与函数的区别 7. 什么叫视图？游标是什么？ 视图： 是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改会影响基本表。它使得我们获取数据更容易，相比多表查询。 游标： 是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。 8. 视图的优缺点 优点： 1对数据库的访问，因为视图可以有选择性的选取数据库里的一部分。 2)用户通过简单的查询可以从复杂查询中得到结果。 3)维护数据的独立性，试图可从多个表检索数据。 4)对于相同的数据可产生不同的视图。 缺点： 性能：查询视图时，必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，那么就无法更改数据 9. drop、truncate、 delete区别 最基本： drop直接删掉表。 truncate删除表中数据，再插入时自增长id又从1开始。 delete删除表中数据，可以加where字句。 （1） DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 （2） 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。 （3） 一般而言，drop &gt; truncate &gt; delete （4） 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view （5） TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。 （6） truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。 （7） delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。 （8） truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚。 （9） 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老师想触发trigger,还是用delete。 （10） Truncate table 表名 速度快,而且效率高,因为:?truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 （11） TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。 （12） 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。 10. 什么是临时表，临时表什么时候删除? 临时表可以手动删除：DROP TEMPORARY TABLE IF EXISTS temp_tb; 临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。因此在不同的连接中可以创建同名的临时表，并且操作属于本连接的临时表。创建临时表的语法与创建表语法类似，不同之处是增加关键字TEMPORARY， 如： CREATE TEMPORARY TABLE tmp_table ( NAME VARCHAR (10) NOT NULL, time date NOT NULL); select * from tmp_table; 11. 非关系型数据库和关系型数据库区别，优势比较? 非关系型数据库的优势： 性能：NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性：同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 关系型数据库的优势： 复杂查询：可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持：使得对于安全性能很高的数据访问要求得以实现。 其他： 1.对于这两类数据库，对方的优势就是自己的弱势，反之亦然。 2.NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能，比如MongoDB。 3.对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国，比如Redis set nx。 12. 数据库范式，根据某个场景设计数据表? 第一范式:(确保每列保持原子性)所有字段值都是不可分解的原子值。 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式:(确保表中的每列都和主键相关)在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键。 第三范式:(确保每列都和主键列直接相关,而不是间接相关) 数据表中的每一列数据都和主键直接相关，而不能间接相关。 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。 BCNF:符合3NF，并且，主属性不依赖于主属性。 若关系模式属于第二范式，且每个属性都不传递依赖于键码，则R属于BC范式。通常BC范式的条件有多种等价的表述：每个非平凡依赖的左边必须包含键码；每个决定因素必须包含键码。BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。一般，一个数据库设计符合3NF或BCNF就可以了。 第四范式:要求把同一表内的多对多关系删除。 第五范式:从最终结构重新建立原始结构。 13. 什么是 内连接、外连接、交叉连接、笛卡尔积等? 内连接: 只连接匹配的行 左外连接: 包含左边表的全部行（不管右边的表中是否存在与它们匹配的行），以及右边表中全部匹配的行 右外连接: 包含右边表的全部行（不管左边的表中是否存在与它们匹配的行），以及左边表中全部匹配的行 例如1：SELECT a.,b. FROM luntan LEFT JOIN usertable as b ON a.username=b.username 例如2：SELECT a.,b. FROM city as a FULL OUTER JOIN user as b ON a.username=b.username 全外连接: 包含左、右两个表的全部行，不管另外一边的表中是否存在与它们匹配的行。 交叉连接: 生成笛卡尔积－它不使用任何匹配或者选取条件，而是直接将一个数据源中的每个行与另一个数据源的每个行都一一匹配 例如：SELECT type,pub_name FROM titles CROSS JOIN publishers ORDER BY type 注意： 很多公司都只是考察是否知道其概念，但是也有很多公司需要不仅仅知道概念，还需要动手写sql,一般都是简单的连接查询，具体关于连接查询的sql练习，参见以下链接： 牛客网数据库SQL实战 leetcode中文网站数据库练习 我的另一篇文章，常用sql练习50题 14. varchar和char的使用场景? 1.char的长度是不可变的，而varchar的长度是可变的。 定义一个char[10]和varchar[10]。如果存进去的是‘csdn’,那么char所占的长度依然为10，除了字符‘csdn’外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。 2.char的存取数度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。varchar是以空间效率为首位。 3.char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。 4.两者的存储数据都非unicode的字符数据。 15. SQL语言分类 SQL语言共分为四大类： 数据查询语言DQL 数据操纵语言DML 数据定义语言DDL 数据控制语言DCL。 1. 数据查询语言DQL 数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块： SELECTFROMWHERE 2 .数据操纵语言DML 数据操纵语言DML主要有三种形式： 插入：INSERT 更新：UPDATE 删除：DELETE 3. 数据定义语言DDL 数据定义语言DDL用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等如：CREATE TABLE/VIEW/INDEX/SYN/CLUSTER 表 视图 索引 同义词 簇 DDL操作是隐性提交的！不能rollback 4. 数据控制语言DCL 数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如： GRANT：授权。 ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚—ROLLBACK；回滚命令使数据库状态回到上次最后提交的状态。其格式为：SQL&gt;ROLLBACK; COMMIT [WORK]：提交。 在数据库的插入、删除和修改操作时，只有当事务在提交到数据库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看到所做的事情，别人只有在最后提交完成后才可以看到。提交数据有三种类型：显式提交、隐式提交及自动提交。下面分别说明这三种类型。 (1) 显式提交用COMMIT命令直接完成的提交为显式提交。其格式为：SQL&gt;COMMIT； (2) 隐式提交用SQL命令间接完成的提交为隐式提交。这些命令是：ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP，EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。 (3) 自动提交若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后，系统将自动进行提交，这就是自动提交。其格式为：SQL&gt;SET AUTOCOMMIT ON； 参考文章：https://www.cnblogs.com/study-s/p/5287529.html 16. like %和-的区别 通配符的分类: %百分号通配符:表示任何字符出现任意次数(可以是0次). _下划线通配符:表示只能匹配单个字符,不能多也不能少,就是一个字符. like操作符: LIKE作用是指示mysql后面的搜索模式是利用通配符而不是直接相等匹配进行比较. 注意: 如果在使用like操作符时,后面的没有使用通用匹配符效果是和=一致的,SELECT * FROM products WHERE products.prod_name like ‘1000’;只能匹配的结果为1000,而不能匹配像JetPack 1000这样的结果. %通配符使用: 匹配以”yves”开头的记录:(包括记录”yves”) SELECT FROM products WHERE products.prod_name like ‘yves%’;匹配包含”yves”的记录(包括记录”yves”) SELECT FROM products WHERE products.prod_name like ‘%yves%’;匹配以”yves”结尾的记录(包括记录”yves”,不包括记录”yves “,也就是yves后面有空格的记录,这里需要注意) SELECT * FROM products WHERE products.prod_name like ‘%yves’; 通配符使用: SELECT *FROM products WHERE products.prod_name like ‘_yves’; 匹配结果为: 像”yyves”这样记录.SELECT\\ FROM products WHERE products.prod*name like ‘yves**’; 匹配结果为: 像”yvesHe”这样的记录.(一个下划线只能匹配一个字符,不能多也不能少) 注意事项: 注意大小写,在使用模糊匹配时,也就是匹配文本时,mysql是可能区分大小的,也可能是不区分大小写的,这个结果是取决于用户对MySQL的配置方式.如果是区分大小写,那么像YvesHe这样记录是不能被”yves__”这样的匹配条件匹配的. 注意尾部空格,”%yves”是不能匹配”heyves “这样的记录的. 注意NULL,%通配符可以匹配任意字符,但是不能匹配NULL,也就是说SELECT * FROM products WHERE products.prod_name like ‘%;是匹配不到products.prod_name为NULL的的记录. 技巧与建议: 正如所见， MySQL的通配符很有用。但这种功能是有代价的：通配符搜索的处理一般要比前面讨论的其他搜索所花时间更长。这里给出一些使用通配符要记住的技巧。 不要过度使用通配符。如果其他操作符能达到相同的目的，应该 使用其他操作符。 在确实需要使用通配符时，除非绝对有必要，否则不要把它们用 在搜索模式的开始处。把通配符置于搜索模式的开始处，搜索起 来是最慢的。 仔细注意通配符的位置。如果放错地方，可能不会返回想要的数. 参考博文：https://blog.csdn.net/u011479200/article/details/78513632 17. count(*)、count(1)、count(column)的区别 count(*)对行的数目进行计算,包含NULL count(column)对特定的列的值具有的行数进行计算,不包含NULL值。 count()还有一种使用方式,count(1)这个用法和count(*)的结果是一样的。 性能问题: 1.任何情况下SELECT COUNT(*) FROM tablename是最优选择; 2.尽量减少SELECT COUNT(*) FROM tablename WHERE COL = ‘value’ 这种查询; 3.杜绝SELECT COUNT(COL) FROM tablename WHERE COL2 = ‘value’ 的出现。 如果表没有主键,那么count(1)比count(*)快。 如果有主键,那么count(主键,联合主键)比count(*)快。 如果表只有一个字段,count(*)最快。 count(1)跟count(主键)一样,只扫描主键。count(*)跟count(非主键)一样,扫描整个表。明显前者更快一些。 18. 最左前缀原则 多列索引： ALTER TABLE people ADD INDEX lname_fname_age (lame,fname,age); 为了提高搜索效率，我们需要考虑运用多列索引,由于索引文件以B－Tree格式保存，所以我们不用扫描任何记录，即可得到最终结果。 注：在mysql中执行查询时，只能使用一个索引，如果我们在lname,fname,age上分别建索引,执行查询时，只能使用一个索引，mysql会选择一个最严格(获得结果集记录数最少)的索引。 最左前缀原则：顾名思义，就是最左优先，上例中我们创建了lname_fname_age多列索引,相当于创建了(lname)单列索引，(lname,fname)组合索引以及(lname,fname,age)组合索引。 6.2 索引1. 什么是索引？ 何为索引： 数据库索引，是数据库管理系统中一个排序的数据结构，索引的实现通常使用B树及其变种B+树。 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2. 索引的作用？它的优点缺点是什么？ 索引作用： 协助快速查询、更新数据库表中数据。 为表设置索引要付出代价的： 一是增加了数据库的存储空间 二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动)。 3.索引的优缺点？ 创建索引可以大大提高系统的性能（优点）： 1.通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 2.可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 3.可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 4.在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 5.通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 增加索引也有许多不利的方面(缺点)： 1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 2.索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 3.当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4. 哪些列适合建立索引、哪些不适合建索引？ 索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。 一般来说，应该在这些列上创建索引： （1）在经常需要搜索的列上，可以加快搜索的速度； （2）在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； （3）在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； （4）在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； （5）在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； （6）在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 对于有些列不应该创建索引： （1）对于那些在查询中很少使用或者参考的列不应该创建索引。 这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 （2）对于那些只有很少数据值的列也不应该增加索引。 这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 （3）对于那些定义为text, image和bit数据类型的列不应该增加索引。 这是因为，这些列的数据量要么相当大，要么取值很少。 (4)当修改性能远远大于检索性能时，不应该创建索引。 这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 5. 什么样的字段适合建索引 唯一、不为空、经常被查询的字段 6.MySQL B+Tree索引和Hash索引的区别?Hash索引和B+树索引的特点： Hash索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位; B+树索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问; 为什么不都用Hash索引而使用B+树索引？ Hash索引仅仅能满足”=”,”IN”和””查询，不能使用范围查询,因为经过相应的Hash算法处理之后的Hash值的大小关系，并不能保证和Hash运算前完全一样； Hash索引无法被用来避免数据的排序操作，因为Hash值的大小关系并不一定和Hash运算前的键值完全一样； Hash索引不能利用部分索引键查询，对于组合索引，Hash索引在计算Hash值的时候是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用； Hash索引在任何时候都不能避免表扫描，由于不同索引键存在相同Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要回表查询数据； Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。 补充： 1.MySQL中，只有HEAP/MEMORY引擎才显示支持Hash索引。 2.常用的InnoDB引擎中默认使用的是B+树索引，它会实时监控表上索引的使用情况，如果认为建立哈希索引可以提高查询效率，则自动在内存中的“自适应哈希索引缓冲区”建立哈希索引（在InnoDB中默认开启自适应哈希索引），通过观察搜索模式，MySQL会利用index key的前缀建立哈希索引，如果一个表几乎大部分都在缓冲池中，那么建立一个哈希索引能够加快等值查询。B+树索引和哈希索引的明显区别是： 3.如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据； 4.如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索；同理，哈希索引没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）； 5.哈希索引也不支持多列联合索引的最左匹配规则； 6.B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。 7.在大多数场景下，都会有范围查询、排序、分组等查询特征，用B+树索引就可以了。 7. B树和B+树的区别 B树，每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为nul，叶子结点不包含任何关键字信息。 B+树，所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大的顺序链接，所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (而B 树的非终节点也包含需要查找的有效信息) 8. 为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？ 1.B+的磁盘读写代价更低 B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 2.B+tree的查询效率更加稳定 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 9. 聚集索引和非聚集索引区别? 聚合索引(clustered index): 聚集索引表记录的排列顺序和索引的排列顺序一致，所以查询效率快，只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。聚集索引类似于新华字典中用拼音去查找汉字，拼音检索表于书记顺序都是按照a~z排列的，就像相同的逻辑顺序于物理顺序一样，当你需要查找a,ai两个读音的字，或是想一次寻找多个傻(sha)的同音字时，也许向后翻几页，或紧接着下一行就得到结果了。 非聚合索引(nonclustered index): 非聚集索引指定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。非聚集索引类似在新华字典上通过偏旁部首来查询汉字，检索表也许是按照横、竖、撇来排列的，但是由于正文中是a~z的拼音顺序，所以就类似于逻辑地址于物理地址的不对应。同时适用的情况就在于分组，大数目的不同值，频繁更新的列中，这些情况即不适合聚集索引。 根本区别： 聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。 6.3 事务1. 什么是事务？ 事务是对数据库中一系列操作进行统一的回滚或者提交的操作，主要用来保证数据的完整性和一致性。 2. 事务四大特性（ACID）原子性、一致性、隔离性、持久性? 原子性（Atomicity）:原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency）:事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）:隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）:持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 3. 事务的并发?事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别? 从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题，然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行， 在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行， 事务的隔离级别可以通过隔离事务属性指定。事务的并发问题 1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。 3、幻读：幻读解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。 例如：事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作 这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。 而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有跟没有修改一样，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 事务的隔离级别 读未提交：另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据脏读 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。 可重复读：在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。但是，会有幻读现象 串行化：最高的隔离级别，在这个隔离级别下，不会产生任何异常。并发的事务，就像事务是在一个个按照顺序执行一样 特别注意： MySQL默认的事务隔离级别为repeatable-read MySQL 支持 4 中事务隔离级别. 事务的隔离级别要得到底层数据库引擎的支持, 而不是应用程序或者框架的支持. Oracle 支持的 2 种事务隔离级别：READ_COMMITED , SERIALIZABLE SQL规范所规定的标准，不同的数据库具体的实现可能会有些差异 MySQL中默认事务隔离级别是“可重复读”时并不会锁住读取到的行 事务隔离级别：未提交读时，写数据只会锁住相应的行。 事务隔离级别为：可重复读时，写数据会锁住整张表。 事务隔离级别为：串行化时，读写数据都会锁住整张表。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，鱼和熊掌不可兼得啊。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed，它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 4. 事务传播行为 1.PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。 2.PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。 3.PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 4.PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 5.PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 6.PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 7.PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 5. 嵌套事务 什么是嵌套事务？ 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点就在于那个save point。看几个问题就明了了： 如果子事务回滚，会发生什么？ 父事务会回滚到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。 如果父事务回滚，会发生什么？ 父事务回滚，子事务也会跟着回滚！为什么呢，因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理。那么： 事务的提交，是什么情况？ 是父事务先提交，然后子事务提交，还是子事务先提交，父事务再提交？答案是第二种情况，还是那句话，子事务是父事务的一部分，由父事务统一提交。 参考文章：https://blog.csdn.net/liangxw1/article/details/51197560 6.4 存储引擎1. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别? 两种存储引擎的大致区别表现在： 1.InnoDB支持事务，MyISAM不支持， 这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而MyISAM就不可以了。 2.MyISAM适合查询以及插入为主的应用。 3.InnoDB适合频繁修改以及涉及到安全性较高的应用。 4.InnoDB支持外键，MyISAM不支持。 5.从MySQL5.5.5以后，InnoDB是默认引擎。 6.InnoDB不支持FULLTEXT类型的索引。 7.InnoDB中不保存表的行数，如select count() from table时，InnoDB需要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count()语句包含where条件时MyISAM也需要扫描整个表。 8.对于自增长的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中可以和其他字段一起建立联合索引。 9.DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除，效率非常慢。MyISAM则会重建表。 10.InnoDB支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’。 2. MySQL存储引擎MyISAM与InnoDB如何选择 MySQL有多种存储引擎，每种存储引擎有各自的优缺点，可以择优选择使用：MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE。 虽然MySQL里的存储引擎不只是MyISAM与InnoDB这两个，但常用的就是两个。关于MySQL数据库提供的两种存储引擎，MyISAM与InnoDB选择使用： 1.INNODB会支持一些关系数据库的高级功能，如事务功能和行级锁，MyISAM不支持。 2.MyISAM的性能更优，占用的存储空间少，所以，选择何种存储引擎，视具体应用而定。 如果你的应用程序一定要使用事务，毫无疑问你要选择INNODB引擎。但要注意，INNODB的行级锁是有条件的。在where条件没有使用主键时，照样会锁全表。比如DELETE FROM mytable这样的删除语句。 如果你的应用程序对查询性能要求较高，就要使用MyISAM了。MyISAM索引和数据是分开的，而且其索引是压缩的，可以更好地利用内存。所以它的查询性能明显优于INNODB。压缩后的索引也能节约一些磁盘空间。MyISAM拥有全文索引的功能，这可以极大地优化LIKE查询的效率。 有人说MyISAM只能用于小型应用，其实这只是一种偏见。如果数据量比较大，这是需要通过升级架构来解决，比如分表分库，而不是单纯地依赖存储引擎。 现在一般都是选用innodb了，主要是MyISAM的全表锁，读写串行问题，并发效率锁表，效率低，MyISAM对于读写密集型应用一般是不会去选用的。MEMORY存储引擎 MEMORY是MySQL中一类特殊的存储引擎。它使用存储在内存中的内容来创建表，而且数据全部放在内存中。这些特性与前面的两个很不同。每个基于MEMORY存储引擎的表实际对应一个磁盘文件。该文件的文件名与表名相同，类型为frm类型。该文件中只存储表的结构。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。值得注意的是，服务器需要有足够的内存来维持MEMORY存储引擎的表的使用。如果不需要了，可以释放内存，甚至删除不需要的表。 MEMORY默认使用哈希索引。速度比使用B型树索引快。当然如果你想用B型树索引，可以在创建索引时指定。 注意，MEMORY用到的很少，因为它是把数据存到内存中，如果内存出现异常就会影响数据。如果重启或者关机，所有数据都会消失。因此，基于MEMORY的表的生命周期很短，一般是一次性的。 3. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景? 事务处理上方面 MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 锁级别 MyISAM：只支持表级锁，用户在操作MyISAM表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 关于存储引擎MyISAM和InnoDB的其他参考资料如下： MySQL存储引擎中的MyISAM和InnoDB区别详解 MySQL存储引擎之MyISAM和Innodb总结性梳理 6.5 优化1. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序? 1.查询中用到的关键词主要包含六个，并且他们的顺序依次为 select–from–where–group by–having–order by 其中select和from是必须的，其他关键词是可选的，这六个关键词的执行顺序 与sql语句的书写顺序并不是一样的，而是按照下面的顺序来执行 from:需要从哪个数据表检索数据 where:过滤表中数据的条件 group by:如何将上面过滤出的数据分组 having:对上面已经分组的数据进行过滤的条件 select:查看结果集中的哪个列，或列的计算结果 order by :按照什么样的顺序来查看返回的数据 2.from后面的表关联，是自右向左解析 而where条件的解析顺序是自下而上的。 也就是说，在写SQL语句的时候，尽量把数据量小的表放在最右边来进行关联（用小表去匹配大表），而把能筛选出小量数据的条件放在where语句的最左边 （用小表去匹配大表） 其他参考资源：http://www.cnblogs.com/huminxxl/p/3149097.html 2. 使用explain优化sql和索引? 对于复杂、效率低的sql语句，我们通常是使用explain sql 来分析sql语句，这个语句可以打印出，语句的执行。这样方便我们分析，进行优化 table：显示这一行的数据是关于哪张表的 type：这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、index和ALL all:full table scan ;MySQL将遍历全表以找到匹配的行； index: index scan; index 和 all的区别在于index类型只遍历索引； range：索引范围扫描，对索引的扫描开始于某一点，返回匹配值的行，常见与between ，等查询； ref：非唯一性索引扫描，返回匹配某个单独值的所有行，常见于使用非唯一索引即唯一索引的非唯一前缀进行查找； eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常用于主键或者唯一索引扫描； const，system：当MySQL对某查询某部分进行优化，并转为一个常量时，使用这些访问类型。如果将主键置于where列表中，MySQL就能将该查询转化为一个常量。 possible_keys：显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句 key： 实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MySQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MySQL忽略索引 key_len：使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref：显示索引的哪一列被使用了，如果可能的话，是一个常数 rows：MySQL认为必须检查的用来返回请求数据的行数 Extra：关于MySQL如何解析查询的额外信息。将在表4.3中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思MySQL根本不能使用索引，结果是检索会很慢。 3. MySQL慢查询怎么解决? slow_query_log 慢查询开启状态。 slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）。 long_query_time 查询超过多少秒才记录。 6.6 数据库锁1. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决? MySQL有三种锁的级别：页级、表级、行级。 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般什么情况下会造成死锁? 什么是死锁？ 死锁: 是指两个或两个以上的进程在执行过程中。因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程。 表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。 死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。 那么对应的解决死锁问题的关键就是：让不同的session加锁有次序。 死锁的解决办法? 1.查出的线程杀死 killSELECT trx_MySQL_thread_id FROM information_schema.INNODB_TRX; 2.设置锁的超时时间Innodb 行锁的等待时间，单位秒。可在会话级别设置，RDS 实例该参数的默认值为 50（秒）。 生产环境不推荐使用过大的 innodb_lock_wait_timeout参数值该参数支持在会话级别修改，方便应用在会话级别单独设置某些特殊操作的行锁等待超时时间，如下：set innodb_lock_wait_timeout=1000; —设置当前会话 Innodb 行锁等待超时时间，单位秒。 3.指定获取锁的顺序 2. 有哪些锁（乐观锁悲观锁），select 时怎么加排它锁? 悲观锁（Pessimistic Lock）: 悲观锁特点:先获取锁，再进行业务操作。 即“悲观”的认为获取锁是非常有可能失败的，因此要先确保获取锁成功再进行业务操作。通常所说的“一锁二查三更新”即指的是使用悲观锁。通常来讲在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select for update时会获取被select中的数据行的行锁，因此其他并发执行的select for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。 补充：不同的数据库对select for update的实现和支持都是有所区别的， oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，MySQL就没有no wait这个选项。 MySQL还有个问题是select for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此如果在MySQL中用悲观锁务必要确定走了索引，而不是全表扫描。 乐观锁（Optimistic Lock）: 1.乐观锁，也叫乐观并发控制，它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，那么当前正在提交的事务会进行回滚。 2.\\*乐观锁的特点先进行业务操作，不到万不得已不去拿锁。*即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。 3.一般的做法是在需要锁的数据上增加一个版本号，或者时间戳， 实现方式举例如下： 乐观锁（给表加一个版本号字段） 这个并不是乐观锁的定义，给表加版本号，是数据库实现乐观锁的一种方式。 SELECT data AS old_data, version AS old_version FROM …; 根据获取的数据进行业务操作，得到new_data和new_version UPDATE SET data = new_data, version = new_version WHERE version = old_version if (updated row &gt; 0) { // 乐观锁获取成功，操作完成 } else { // 乐观锁获取失败，回滚并重试 } 注意： 乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能 乐观锁还适用于一些比较特殊的场景，例如在业务操作过程中无法和数据库保持连接等悲观锁无法适用的地方。 总结：悲观锁和乐观锁是数据库用来保证数据并发安全防止更新丢失的两种方法，例子在select … for update前加个事务就可以防止更新丢失。悲观锁和乐观锁大部分场景下差异不大，一些独特场景下有一些差别，一般我们可以从如下几个方面来判断。 响应速度： 如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁。’ 冲突频率： 如果冲突频率非常高，建议采用悲观锁，保证成功率，如果冲突频率大，乐观锁会需要多次重试才能成功，代价比较大。 重试代价： 如果重试代价大，建议采用悲观锁。 七. 操作系统7.1 Linux1. 文件管理1. cat 命令：连接文件 描述：cat 命令用于连接文件并打印到标准输出设备上。 功能： # 1.一次显示整个文件: cat filename # 2.从键盘创建一个文件: cat > filename # 3.将几个文件合并为一个文件: cat file1 file2 > file 示例： （1）把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里 cat -n log2012.log log2013.log （2）把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里 cat -b log2012.log log2013.log log.log （3）使用 here doc 生成新文件 cat >log.txt World >PWD=$(pwd) >EOF ls -l log.txt cat log.txt Hello World PWD=/opt/soft/test （4）反向列示 tac log.txt PWD=/opt/soft/test World Hello 2. chmod 命令: 权限控制Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。 用于改变 linux 系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。 以文件 log2012.log 为例： -rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。 常用参数： -c 当发生改变时，报告处理信息 -R 处理指定目录以及其子目录下所有文件 权限范围： u ：目录或者文件的当前的用户 g ：目录或者文件的当前的群组 o ：除了目录或者文件的当前用户或群组之外的用户或者群组 a ：所有的用户及群组 权限代号： r ：读权限，用数字4表示 w ：写权限，用数字2表示 x ：执行权限，用数字1表示 - ：删除权限，用数字0表示 s ：特殊权限 实例： （1）增加文件 t.log 所有用户可执行权限 chmod a+x t.log （2）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息 chmod u=r t.log -c （3）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限 chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c) （4）将 test 目录及其子目录所有文件添加可读权限 chmod u+r,g+r,o+r -R text/ -c 3. chown 命令：更改拥有者权限chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。 -c 显示更改的部分的信息 -R 处理指定目录及子目录 实例： （1）改变拥有者和群组 并显示改变信息 chown -c mail:mail log2012.log （2）改变文件群组 chown -c :mail t.log （3）改变文件夹及子文件目录属主及属组为 mail chown -cR mail: test/ 4. cp 命令：复制文件将源文件复制至目标文件，或将多个源文件复制至目标目录。 注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！ -i 提示 -r 复制目录及目录内所有项目 -a 复制的文件与原文件时间一样 实例： （1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。 cp -ai a.txt test （2）为 a.txt 建议一个链接（快捷方式） cp -s a.txt link_a.txt 5. find 命令：查找文件用于在文件树中查找文件，并作出相应的处理。 命令格式： find pathname -options [-print -exec -ok ...] 命令参数： pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print： find命令将匹配的文件输出到标准输出。 -exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为’command’ { } ;，注意{ }和\\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项： -name 按照文件名查找文件 -perm 按文件权限查找文件 -user 按文件属主查找文件 -group 按照文件所属的组来查找文件。 -type 查找某一类型的文件，诸如： b - 块设备文件 d - 目录 c - 字符设备文件 l - 符号链接文件 p - 管道文件 f - 普通文件 实例： （1）查找 48 小时内修改过的文件 find -atime -2 （2）在当前目录查找 以 .log 结尾的文件。 . 代表当前目录 find ./ -name '*.log' （3）查找 /opt 目录下 权限为 777 的文件 find /opt -perm 777 （4）查找大于 1K 的文件 find -size +1000c 查找等于 1000 字符的文件 find -size 1000c -exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。 6. head 命令：显示文件开头head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。 常用参数： -n 显示的行数（行数为复数表示从最后向前数） 实例： （1）显示 1.log 文件中前 20 行 head 1.log -n 20 （2）显示 1.log 文件前 20 字节 head -c 20 log2014.log （3）显示 t.log最后 10 行 head -n -10 t.log 7. less 命令：文件浏览less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 常用命令参数： -i 忽略搜索时的大小写 -N 显示每行的行号 -o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来 -s 显示连续空行为一行 /字符串：向下搜索“字符串”的功能 ?字符串：向上搜索“字符串”的功能 n：重复前一个搜索（与 / 或 ? 有关） N：反向重复前一个搜索（与 / 或 ? 有关） -x &lt;数字&gt; 将“tab”键显示为规定的数字空格 b 向后翻一页 d 向后翻半页 h 显示帮助界面 Q 退出less 命令 u 向前滚动半页 y 向前滚动一行 空格键 滚动一行 回车键 滚动一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 实例： （1）ps 查看进程信息并通过 less 分页显示 ps -aux | less -N （2）查看多个文件 less 1.log 2.log 可以使用 n 查看下一个，使用 p 查看前一个。 8. ln 命令：文件同步链接功能是为文件在另外一个位置建立一个同步的链接，当在不同目录需要该问题时，就不需要为每一个目录创建同样的文件，通过 ln 创建的链接（link）减少磁盘占用量。 链接分类：软件链接及硬链接 软链接： 1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 2.软链接可以 跨文件系统 ，硬链接不可以 3.软链接可以对一个不存在的文件名进行链接 4.软链接可以对目录进行链接 硬链接: 1.硬链接，以文件副本的形式存在。但不占用实际空间。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 需要注意： 第一：ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二：ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 第三：ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 常用参数： -b 删除，覆盖以前建立的链接 -s 软链接（符号链接） -v 显示详细处理过程 实例： （1）给文件创建软链接，并显示操作信息 ln -sv source.log link.log （2）给文件创建硬链接，并显示操作信息 ln -v source.log link1.log （3）给目录创建软链接 ln -sv /opt/soft/test/test3 /opt/soft/test/test5 9. locate 命令：快速查找档案​ locate 通过搜寻系统内建文档数据库达到快速找到档案，数据库由 updatedb 程序来更新，updatedb 是由 cron daemon 周期性调用的。默认情况下 locate 命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是 locate 所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb 每天会跑一次，可以由修改 crontab 来更新设定值 (etc/crontab)。 locate 与 find 命令相似，可以使用如 *、? 等进行正则匹配查找 常用参数： -l num（要显示的行数） -f 将特定的档案系统排除在外，如将proc排除在外 -r 使用正则运算式做为寻找条件 实例： （1）查找和 pwd 相关的所有文件(文件名中包含 pwd） locate pwd （2）搜索 etc 目录下所有以 sh 开头的文件 locate /etc/sh （3）查找 /var 目录下，以 reason 结尾的文件 locate -r '^/var.*reason$'（其中.表示一个字符，*表示任务多个；.*表示任意多个字符） 10. more 命令：文件分页浏览​ 功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。 命令参数： +n 从笫 n 行开始显示 -n 定义屏幕大小为n行 +/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示 -d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能 -l 忽略Ctrl+l（换页）字符 -p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似 -s 把连续的多个空行显示为一行 -u 把文件内容中的下画线去掉 常用操作命令： Enter 向下 n 行，需要定义。默认为 1 行 Ctrl+F 向下滚动一屏 空格键 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 V 调用vi编辑器 !命令 调用Shell，并执行命令 q 退出more 实例： （1）显示文件中从第3行起的内容 more +3 text.txt （2）在所列出文件目录详细信息，借助管道使每次显示 5 行 ls -l | more -5 按空格显示下 5 行。 11. mv 命令：文件移动移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。 实例： （1）将文件 test.log 重命名为 test1.txt mv test.log test1.txt （2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中 mv llog1.txt log2.txt log3.txt /test3 （3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖 mv -i log1.txt log2.txt （4）移动当前文件夹下的所有文件到上一级目录 mv * ../ 12. rm 命令：文件删除​ 删除一个目录中的一个或多个文件或目录，如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。 rm [选项] 文件… 实例： （1）删除任何 .log 文件，删除前逐一询问确认： rm -i *.log （2）删除 test 子目录及子目录中所有档案删除，并且不用一一确认： rm -rf test （3）删除以 -f 开头的文件 rm -- -f* 13. tail 命令：显示文件末尾用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 常用参数： -f 循环读取（常用于查看递增的日志文件） -n&lt;行数&gt; 显示行数（从后向前） （1）循环读取逐渐增加的文件内容 ping 127.0.0.1 > ping.log & 后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。 tail -f ping.log （查看日志） 14. touch 命令：修改时间属性Linux touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。 ls -l 可以显示档案的时间记录。 语法 touch [-acfm][-d][-r] [-t][--help][--version][文件或目录…] 参数说明： a 改变档案的读取时间记录。 m 改变档案的修改时间记录。 c 假如目的档案不存在，不会建立新的档案。与 –no-create 的效果一样。 f 不使用，是为了与其他 unix 系统的相容性而保留。 r 使用参考档的时间记录，与 –file 的效果一样。 d 设定时间与日期，可以使用各种不同的格式。 t 设定档案的时间记录，格式与 date 指令相同。 –no-create 不会建立新档案。 –help 列出指令格式。 –version 列出版本讯息。 实例 使用指令”touch”修改文件”testfile”的时间属性为当前系统时间，输入如下命令： $ touch testfile #修改文件的时间属性 首先，使用ls命令查看testfile文件的属性，如下所示： $ ls -l testfile #查看文件的时间属性 #原来文件的修改时间为16:09 -rw-r--r-- 1 hdd hdd 55 2011-08-22 16:09 testfile 执行指令”touch”修改文件属性以后，并再次查看该文件的时间属性，如下所示： $ touch testfile #修改文件时间属性为当前系统时间 $ ls -l testfile #查看文件的时间属性 #修改后文件的时间属性为当前系统时间 -rw-r--r-- 1 hdd hdd 55 2011-08-22 19:53 testfile 使用指令”touch”时，如果指定的文件不存在，则将创建一个新的空白文件。例如，在当前目录下，使用该指令创建一个空白文件”file”，输入如下命令： $ touch file #创建一个名为“file”的新的空白文件 15. vim 命令：文本编辑器Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 打开文件并跳到第 10 行：vim +10 filename.txt 。 打开文件跳到第一个匹配的行：vim +/search-term filename.txt 。 以只读模式打开文件：vim -R /etc/passwd 。 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 简单的说，我们可以将这三个模式想成底下的图标来表示： 16. whereis 命令：搜索程序名whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis 及 locate 都是基于系统内建的数据库进行搜索，因此效率很高，而find则是遍历硬盘查找文件。 常用参数： -b 定位可执行文件。 -m 定位帮助文件。 -s 定位源代码文件。 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 实例： （1）查找 locate 程序相关文件 whereis locate （2）查找 locate 的源码文件 whereis -s locate （3）查找 lcoate 的帮助文件 whereis -m locate 17. which 命令：查找文件在 linux 要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索： which 查看可执行文件的位置。 whereis 查看文件的位置。 locate 配合数据库查看文件位置。 find 实际搜寻硬盘查询文件名称。 which 是在 PATH 就是指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果。使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 常用参数： -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 实例： （1）查看 ls 命令是否存在，执行哪个 which ls （2）查看 which which which （3）查看 cd which cd（显示不存在，因为 cd 是内建命令，而 which 查找显示是 PATH 中的命令） 查看当前 PATH 配置： echo $PATH 或使用 env 查看所有环境变量及对应值 2. 文档编辑1. grep 命令：文本搜索强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。 grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。 命令格式： grep [option] pattern file|dir 常用参数： -A n –after-context显示匹配字符后n行 -B n –before-context显示匹配字符前n行 -C n –context 显示匹配字符前后n行 -c –count 计算符合样式的列数 -i 忽略大小写 -l 只列出文件内容符合指定的样式的文件名称 -f 从文件中读取关键词 -n 显示匹配内容的所在文件中行数 -R 递归查找文件夹 grep 的规则表达式: ^ #锚定行的开始 如：’^grep’匹配所有以grep开头的行。 $ #锚定行的结束 如：’grep’匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。 * #匹配零个或多个先前字符 如：’*grep’匹配所有一个或多个空格后紧跟grep的行。 .* #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。 [^] #匹配一个不在指定范围内的字符，如：’[^A-FH-Z]rep’匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 (..) #标记匹配字符，如’(love)‘，love被标记为1。 &lt; #锚定单词的开始，如:’&lt;grep’匹配包含以grep开头的单词的行。 &gt; #锚定单词的结束，如’grep&gt;‘匹配包含以grep结尾的单词的行。 x{m} #重复字符x，m次，如：’0{5}‘匹配包含5个o的行。 x{m,} #重复字符x,至少m次，如：’o{5,}‘匹配至少有5个o的行。 x{m,n} #重复字符x，至少m次，不多于n次，如：’o{5,10}‘匹配5–10个o的行。 \\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：’G\\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p。 \\W #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \\b #单词锁定符，如: ‘\\bgrep\\b’只匹配grep。 实例： （1）查找指定进程 ps -ef | grep svn （2）查找指定进程个数 ps -ef | grep svn -c （3）从文件中读取关键词 cat test1.txt | grep -f key.log （4）从文件夹中递归查找以grep开头的行，并只列出文件 grep -lR '^grep' /tmp （5）查找非x开关的行内容 grep '^[^x]' test.txt （6）显示包含 ed 或者 at 字符的内容行 grep -E 'ed|at' test.txt 2. wc 命令：文件统计wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出 命令格式： wc [option] file.. 命令参数： -c 统计字节数 -l 统计行数 -m 统计字符数 -w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串 实例： （1）查找文件的 行数 单词数 字节数 文件名 wc text.txt 结果： 7 8 70 test.txt （2）统计输出结果的行数 cat test.txt | wc -l 3. 磁盘管理1. cd 命令：切换目录cd(changeDirectory) 命令语法： cd [目录名] 说明：切换当前目录至 dirName。 实例： （1）进入要目录 cd / （2）进入 “home” 目录 cd ~ （3）进入上一次工作路径 cd - （4）把上个命令的参数作为cd参数使用。 cd !$ 2. df 命令：显示磁盘空间显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示： -a 全部文件系统列表 -h 以方便阅读的方式显示信息 -i 显示inode信息 -k 区块为1024字节 -l 只显示本地磁盘 -T 列出文件系统类型 实例： （1）显示磁盘使用情况 df -l （2）以易读方式列出所有文件系统及其类型 df -haT 3. du 命令：查看文件使用空间du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看： 命令格式： du [选项] [文件] 常用参数： -a 显示目录中所有文件大小 -k 以KB为单位显示文件大小 -m 以MB为单位显示文件大小 -g 以GB为单位显示文件大小 -h 以易读方式显示文件大小 -s 仅显示总计 -c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和 实例： （1）以易读方式显示文件夹内及子文件夹大小 du -h scf/ （2）以易读方式显示文件夹内所有文件大小 du -ah scf/ （3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和 du -hc test/ scf/ （4）输出当前目录下各个子目录所使用的空间 du -hc --max-depth=1 scf/ 4. ls命令：查看文件夹文件就是 list 的缩写，通过 ls 命令不仅可以查看 linux 文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)查看目录信息等等。 常用参数搭配： ls -a 列出目录所有文件，包含以.开始的隐藏文件 ls -A 列出除.及..的其它文件 ls -r 反序排列 ls -t 以文件修改时间排序 ls -S 以文件大小排序 ls -h 以易读大小显示 ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来 实例： (1) 按易读方式按时间反序排序，并显示文件详细信息 ls -lhrt (2) 按大小反序显示文件详细信息 ls -lrS (3)列出当前目录中所有以”t”开头的目录的详细内容 ls -l t* (4) 列出文件绝对路径（不包含隐藏文件） ls | sed \"s:^:`pwd`/:\" (5) 列出文件绝对路径（包含隐藏文件） find $pwd -maxdepth 1 | xargs ls -ld 5. mkdir 命令：创建文件夹mkdir 命令用于创建文件夹。 可用选项： -m: 对新建目录设置存取权限，也可以用 chmod 命令设置; -p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。 实例： （1）当前工作目录下创建名为 t的文件夹 mkdir t （2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建： mkdir -p /tmp/test/t1/t 6. pwd 命令：查看当前目录pwd 命令用于查看当前工作目录路径。 实例： （1）查看当前路径 pw （2）查看软链接的实际路径 pwd -P 7. rmdir 命令：删除空目录从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。 注意：不能删除非空目录 实例： （1）当 parent 子目录被删除后使它也成为空目录的话，则顺便一并删除： rmdir -p parent/child/child11 4. 网络通讯1. ifconfig 命令：查看网络端口ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 2. iptables 命令：开放端口iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如 把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp –dport 80 -j REJECT 。 开启 80 端口，因为web对外都是这个端口 iptables -A INPUT -p tcp --dport 80 -j ACCEP 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 3. netstat 命令：查看网络连接状态Linux netstat命令用于显示网络状态。 利用netstat指令可让你得知整个Linux系统的网络情况。 语法 netstat [-acCeFghilMnNoprstuvVwx][-A][--ip] 参数说明： -a或–all 显示所有连线中的Socket。 -A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定”-A unix”参数相同。 –ip或–inet 此参数的效果和指定”-A inet”参数相同。 实例 1）如何查看系统都开启了哪些端口？ 2）如何查看网络连接状况？ 3）如何统计系统当前进程连接数？ 输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 4）用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？ 严格来说，这个题目考验的是对 awk 的使用。 首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下： 4. ping 命令：检测主机网络连接Linux ping命令用于检测主机。 执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 指定接收包的次数 ping -c 2 百度一下，你就知道 5.telnet 命令：远端登陆Linux telnet命令用于远端登入。 执行telnet指令开启终端机阶段作业，并登入远端主机。 语法 telnet [-8acdEfFKLrx][-b][-e][-k][-l][-n][-S][-X][主机名称或IP地址] 参数说明： -8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定\"-F\"参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X 关闭指定的认证形态。 实例 登录远程主机 # 登录IP为 192.168.0.5 的远程主机 telnet 192.168.0.5 5. 系统管理1. date 命令：显示系统日期显示或设定系统的日期与时间。 命令参数： -d&lt;字符串&gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。 -s&lt;字符串&gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。 -u 显示GMT。 %H 小时(00-23) %I 小时(00-12) %M 分钟(以00-59来表示) %s 总秒数。起算时间为1970-01-01 00:00:00 UTC。 %S 秒(以本地的惯用法来表示) %a 星期的缩写。 %A 星期的完整名称。 %d 日期(以01-31来表示)。 %D 日期(含年月日)。 %m 月份(以01-12来表示)。 %y 年份(以00-99来表示)。 %Y 年份(以四位数来表示)。 实例： （1）显示下一天 date +%Y%m%d --date=\"+1 day\" //显示下一天的日期 （2）-d参数使用 date -d \"nov 22\" 今年的 11 月 22 日是星期三 date -d '2 weeks' 2周后的日期 date -d 'next monday' (下周一的日期) date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%d date -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%d date -d last-month +%Y%m(上个月是几月) date -d next-month +%Y%m(下个月是几月) 2. free 命令：显示内存使用情况显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。 命令参数： -b 以Byte显示内存使用情况 -k 以kb为单位显示内存使用情况 -m 以mb为单位显示内存使用情况 -g 以gb为单位显示内存使用情况 -s&lt;间隔秒数&gt; 持续显示内存 -t 显示内存使用总合 实例： （1）显示内存使用情况 free free -k free -m （2）以总和的形式显示内存的使用信息 free -t （3）周期性查询内存使用情况 free -s 10 3. kill 命令：杀掉指定进程发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用”-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 常用参数： -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 实例： （1）先使用ps查找进程pro1，然后用kill杀掉 kill -9 $(ps -ef | grep pro1) 4. ps 命令：查看运行进程（快照）ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps 工具标识进程的5种状态码: D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数： -A 显示所有进程 a 显示所有进程 -a 显示同一终端下所有进程 c 显示进程真实名称 e 显示环境变量 f 显示进程间的关系 r 显示当前终端运行的进程 -aux 显示所有包含其它使用的进程 实例： （1）显示当前所有进程环境变量及进程间关系 ps -ef （2）显示当前所有进程 ps -A （3）与grep联用查找某进程 ps -aux | grep apache （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps aux | grep '(cron|syslog)' 5. rpm 命令：管理套件Linux rpm 命令用于管理套件。 rpm(redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。 # 查看系统自带jdk rpm -qa | grep jdk # 删除系统自带jdk rpm -e --nodeps 查看jdk显示的数据 # 安装jdk rpm -ivh jdk-7u80-linux-x64.rpm 6. top 命令：显示当前进程信息（动态）显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等 常用参数： -c 显示完整的进程命令 -s 保密模式 -p &lt;进程号&gt; 指定进程显示 -n &lt;次数&gt;循环显示次数 实例： 前五行是当前系统情况整体的统计信息区。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下： 14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！） 2 users — 当前有2个用户登录系统 load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下： 5.9%us — 用户空间占用CPU的百分比。 3.4% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 90.4% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.2% si — 软中断（Software Interrupts）占用CPU的百分比 备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！ 第四行，内存状态，具体信息如下： 32949016k total — 物理内存总量（32GB） 14411180k used — 使用中的内存总量（14GB） 18537836k free — 空闲内存总量（18GB） 169884k buffers — 缓存的内存量 （169M） 第五行，swap交换分区信息，具体信息说明如下： 32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 7. top 交互命令： h 显示top交互命令帮助信息 c 切换显示命令名称和完整命令行 m 以内存使用率排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 o或者O 改变显示项目的顺序 8. yum 命令：软件包管理器yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。 基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install 4.仅更新指定的软件命令：yum update 5.列出所有可安裝的软件清单命令：yum list 6.删除软件包命令：yum remove 7.查找软件包 命令：yum search 8.清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 实例 安装 pam-devel [root@www ~]# yum install pam-devel 6. 备份压缩1. bzip2 命令：bz2文件压缩解压创建 *.bz2 压缩文件：bzip2 test.txt 。 解压 *.bz2 文件：bzip2 -d test.txt.bz2 。 2. gzip 命令：gz文件压缩解压创建一个 *.gz 的压缩文件：gzip test.txt 。 解压 *.gz 文件：gzip -d test.txt.gz 。 显示压缩的比率：gzip -l *.gz 。 3. tar 命令：文件打包用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。 弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件 常用参数： -c 建立新的压缩文件 -f 指定压缩文件 -r 添加文件到已经压缩文件包中 -u 添加改了和现有的文件到压缩包中 -x 从压缩包中抽取文件 -t 显示压缩文件中的内容 -z 支持gzip压缩 -j 支持bzip2压缩 -Z 支持compress解压文件 -v 显示操作过程 有关 gzip 及 bzip2 压缩: gzip 实例：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz 对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz bz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2 对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2 实例： （1）将文件全部打包成 tar 包 tar -cvf log.tar 1.log,2.log 或tar -cvf log.* （2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩 tar -zcvf /tmp/etc.tar.gz /etc （3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的） tar -ztvf /tmp/etc.tar.gz （4）要压缩打包 /home, /etc ，但不要 /home/dmtsai tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc 4. unzip 命令解压 *.zip 文件：unzip test.zip 。 查看 *.zip 文件的内容：unzip -l jasper.zip 7.2 操作系统理论1. 概述基本特征1. 并发并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。 并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。 操作系统通过引入进程和线程，使得程序能够并发运行。 2. 共享共享是指系统中的资源可以被多个并发进程共同使用。 有两种共享方式：互斥共享和同时共享。 互斥共享的资源称为临界资源，例如打印机等，在同一时间只允许一个进程访问，需要用同步机制来实现对临界资源的访问。 3. 虚拟虚拟技术把一个物理实体转换为多个逻辑实体。 主要有两种虚拟技术：时分复用技术和空分复用技术。 多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片并快速切换。 虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页置换到内存中。 4. 异步异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。 基本功能1. 进程管理进程控制、进程同步、进程通信、死锁处理、处理机调度等。 2. 内存管理内存分配、地址映射、内存保护与共享、虚拟内存等。 3. 文件管理文件存储空间的管理、目录管理、文件读写管理和保护等。 4. 设备管理完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。 主要包括缓冲管理、设备分配、设备处理、虛拟设备等。 系统调用如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。 Linux 的系统调用主要有以下这些： Task Commands 进程控制 fork(); exit(); wait(); 进程通信 pipe(); shmget(); mmap(); 文件操作 open(); read(); write(); 设备操作 ioctl(); read(); write(); 信息维护 getpid(); alarm(); sleep(); 安全 chmod(); umask(); chown(); 大内核和微内核1. 大内核大内核是将操作系统功能作为一个紧密结合的整体放到内核。 由于各模块共享信息，因此有很高的性能。 2. 微内核由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。 在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。 因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。 中断分类1. 外中断由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。 2. 异常由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。 3. 陷入在用户程序中使用系统调用。 2. 进程管理进程与线程 1. 进程进程是资源分配的基本单位。 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。 下图显示了 4 个程序创建了 4 个进程，这 4 个进程可以并发地执行。 *2. *线程线程是独立调度的基本单位。 一个进程中可以有多个线程，它们共享进程资源。 QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 3. 区别Ⅰ 拥有资源 进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。 Ⅱ 调度 线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。 Ⅲ 系统开销 由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。 Ⅳ 通信方面 线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。 进程状态的切换 就绪状态（ready）：等待被调度 运行状态（running） 阻塞状态（waiting）：等待资源 应该注意以下内容： 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。 进程调度算法不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。 1. 批处理系统批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。 1.1 先来先服务 first-come first-serverd（FCFS） 按照请求的顺序进行调度。 有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。 1.2 短作业优先 shortest job first（SJF） 按估计运行时间最短的顺序进行调度。 长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。 1.3 最短剩余时间优先 shortest remaining time next（SRTN） 按估计剩余时间最短的顺序进行调度。 2. 交互式系统交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。 2.1 时间片轮转 将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。 时间片轮转算法的效率和时间片的大小有很大关系： 因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。 而如果时间片过长，那么实时性就不能得到保证。 2.2 优先级调度 为每个进程分配一个优先级，按优先级进行调度。 为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 2.3 多级反馈队列 一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。 多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。 每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。 可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。 3. 实时系统实时系统要求一个请求在一个确定时间内得到响应。 分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。 进程同步1. 临界区对临界资源进行访问的那段代码称为临界区。 为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。 // entry section // critical section; // exit section 2. 同步与互斥 同步：多个进程按一定顺序执行； 互斥：多个进程在同一时刻只有一个进程能进入临界区。 3. 信号量信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。 down : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0； up ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。 down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。 如果信号量的取值只能为 0 或者 1，那么就成为了 互斥量（Mutex） ，0 表示临界区已经加锁，1 表示临界区解锁。 typedef int semaphore; semaphore mutex = 1; void P1() { down(&amp;mutex); // 临界区 up(&amp;mutex); } void P2() { down(&amp;mutex); // 临界区 up(&amp;mutex); } 使用信号量实现生产者-消费者问题 问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。 因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。 为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。 注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。 #define N 100 typedef int semaphore; semaphore mutex = 1; semaphore empty = N; semaphore full = 0; void producer() { while(TRUE) { int item = produce_item(); down(&amp;empty); down(&amp;mutex); insert_item(item); up(&amp;mutex); up(&amp;full); } } void consumer() { while(TRUE) { down(&amp;full); down(&amp;mutex); int item = remove_item(); consume_item(item); up(&amp;mutex); up(&amp;empty); } } 4. 管程使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易。 c 语言不支持管程，下面的示例代码使用了类 Pascal 语言来描述管程。示例代码的管程提供了 insert() 和 remove() 方法，客户端代码通过调用这两个方法来解决生产者-消费者问题。 monitor ProducerConsumer integer i; condition c; procedure insert(); begin // ... end; procedure remove(); begin // ... end; end monitor; 管程有一个重要特性：在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否者其它进程永远不能使用管程。 管程引入了 条件变量 以及相关的操作：wait() 和 signal() 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。 使用管程实现生产者-消费者问题 // 管程 monitor ProducerConsumer condition full, empty; integer count := 0; condition c; procedure insert(item: integer); begin if count = N then wait(full); insert_item(item); count := count + 1; if count = 1 then signal(empty); end; function remove: integer; begin if count = 0 then wait(empty); remove = remove_item; count := count - 1; if count = N -1 then signal(full); end; end monitor; // 生产者客户端 procedure producer begin while true do begin item = produce_item; ProducerConsumer.insert(item); end end; // 消费者客户端 procedure consumer begin while true do begin item = ProducerConsumer.remove; consume_item(item); end end; 进程通信进程同步与进程通信很容易混淆，它们的区别在于： 进程同步：控制多个进程按一定顺序执行； 进程通信：进程间传输信息。 进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传输一些进程同步所需要的信息。 1. 管道管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。 #include &lt;unistd.h> int pipe(int fd[2]); 它具有以下限制： 只支持半双工通信（单向交替传输）； 只能在父子进程中使用。 2. FIFO也称为命名管道，去除了管道只能在父子进程中使用的限制。 #include &lt;sys/stat.h> int mkfifo(const char *path, mode_t mode); int mkfifoat(int fd, const char *path, mode_t mode); FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。 3. 消息队列相比于 FIFO，消息队列具有以下优点： 消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难； 避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法； 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。 4. 信号量它是一个计数器，用于为多个进程提供对共享数据对象的访问。 5. 共享存储允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。 需要使用信号量用来同步对共享存储的访问。 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用使用内存的匿名段。 6. 套接字与其它通信机制不同的是，它可用于不同机器间的进程通信。 3. 死锁必要条件 互斥：每个资源要么已经分配给了一个进程，要么就是可用的。 占有和等待：已经得到了某个资源的进程可以再请求新的资源。 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。 处理方法主要有以下四种方法： 鸵鸟策略 死锁检测与死锁恢复 死锁预防 死锁避免 鸵鸟策略把头埋在沙子里，假装根本没发生问题。 因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。 当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。 大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 死锁检测与死锁恢复不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。 1. 每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。 图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。 每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。 2. 每种类型多个资源的死锁检测 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。 算法总结如下： 每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 3. 死锁恢复 利用抢占恢复 利用回滚恢复 通过杀死进程恢复 死锁预防在程序运行之前预防发生死锁。 1. 破坏互斥条件 例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 2. 破坏占有和等待条件 一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 3. 破坏不可抢占条件 4. 破坏环路等待 给资源统一编号，进程只能按编号顺序来请求资源。 死锁避免在程序运行时避免发生死锁。 1. 安全状态 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。 2. 单个资源的银行家算法 一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 3. 多个资源的银行家算法 上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。 重复以上两步，直到所有进程都标记为终止，则状态时安全的。 如果一个状态不是安全的，需要拒绝进入这个状态。 4. 内存管理虚拟内存虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。 分页系统地址映射内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。 一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。 下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。 页面置换算法在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。 页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，当有新的缓存到达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据。 页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。 1. 最佳 OPT, Optimal replacement algorithm 所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。 是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。 举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列： 开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。 2. 最近最久未使用 LRU, Least Recently Used 虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。 为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。 因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。 3. 最近未使用 NRU, Not Recently Used 每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类： R=0，M=0 R=0，M=1 R=1，M=0 R=1，M=1 当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。 NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。 4. 先进先出 FIFO, First In First Out 选择换出的页面是最先进入的页面。 该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。 5. 第二次机会算法 FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改： 当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。 6. 时钟 Clock 第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面连接起来，再使用一个指针指向最老的页面。 分段虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。 下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现。 分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。 段页式程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统的共享和保护，又拥有分页系统的虚拟内存功能。 分页与分段的比较 对程序员的透明性：分页透明，但是分段需要程序员显示划分每个段。 地址空间的维度：分页是一维地址空间，分段是二维的。 大小是否可以改变：页的大小不可变，段的大小可以动态改变。 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。 八. 计算机网络8.1 TCP和UDP的区别TCP面向连接（三次握手机制），通信前需要先建立连接；UDP面向无连接，通信前不需要建立连接； TCP保障可靠传输（按序、无差错、不丢失、不重复）；UDP不保障可靠传输，使用最大努力交付； TCP面向字节流的传输，UDP面向数据报的传输。 8.2 TCP协议的三次握手(连接)和四次挥手(关闭)1. 三次握手过程 形象理解： 客户机：【how are you ?】 服务器：【fine.And you?】 客户机：【Fine.】 具体过程：（SYN, (SYN+ACK), ACK) （1）第一次握手：建立连接时，客户端A发送SYN包（SYN=j）到服务器B，并进入SYN_SEND状态，等待服务器B确认。 （2）第二次握手：服务器B收到SYN包，必须确认客户A的SYN（ACK=j+1），同时自己也发送一个SYN包（SYN=k），即SYN+ACK包，此时服务器B进入SYN_RECV状态。 （3）第三次握手：客户端A收到服务器B的SYN＋ACK包，向服务器B发送确认包ACK（ACK=k+1），此包发送完毕，客户端A和服务器B进入ESTABLISHED状态，完成三次握手。 完成三次握手，客户端与服务器开始传送数据。 确认号：其数值等于发送方的发送序号 +1(即接收方期望接收的下一个序列号)。 2.**四次挥手过程** 由于TCP连接是全双工的，一个TCP连接存在双向的读写通道，因此每个方向都必须单独进行关闭。TCP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。 简单说来是 “先关读，后关写”，一共需要四个阶段。以客户机发起关闭连接为例： 1.服务器读通道关闭 2.客户机写通道关闭 3.客户机读通道关闭 4.服务器写通道关闭 关闭行为是在发起方数据发送完毕之后，给对方发出一个FIN（finish）数据段。直到接收到对方发送的FIN，且对方收到了接收确认ACK之后，双方的数据通信完全结束，过程中每次接收都需要返回确认数据段ACK。 详细过程： 第一阶段 客户机发送完数据之后，向服务器发送一个FIN数据段，序列号为i； 1.服务器收到FIN(i)后，返回确认段ACK，序列号为i+1，关闭服务器读通道； 2.客户机收到ACK(i+1)后，关闭客户机写通道； （此时，客户机仍能通过读通道读取服务器的数据，服务器仍能通过写通道写数据） 第二阶段 服务器发送完数据之后，向客户机发送一个FIN数据段，序列号为j； 3.客户机收到FIN(j)后，返回确认段ACK，序列号为j+1，关闭客户机读通道； 4.服务器收到ACK(j+1)后，关闭服务器写通道。 这是标准的TCP关闭两个阶段，服务器和客户机都可以发起关闭，完全对称。 FIN标识是通过发送最后一块数据时设置的，标准的例子中，服务器还在发送数据，所以要等到发送完的时候，设置FIN（此时可称为TCP连接处于半关闭状态，因为数据仍可从被动关闭一方向主动关闭方传送）。如果在服务器收到FIN(i)时，已经没有数据需要发送，可以在返回ACK(i+1)的时候就设置FIN(j)标识，这样就相当于可以合并第二步和第三步。 8.3 TCP协议的通信状态 整个通信状态如图 说明：客户端和服务器均有6个状态。其中经常问到的两个状态是TIME_WAIT和CLOSE_WAIT. 从图上可以发现， TIME_WAIT状态是客户端【发起主动关闭的一方】在四次挥手第二阶段完成时，进入的状态。 CLOSE_WAIT状态是服务端【收到被动关闭的一方】在四次挥手的第一阶段完成时，进入的状态。 TIME_WAIT状态将持续2个MSL(Max Segment Lifetime),在Windows下默认为4分钟，即240秒。TIME_WAIT状态下的socket不能被回收使用. 具体现象是对于一个处理大量短连接的服务器,如果是由服务器主动关闭客户端的连接，将导致服务器端存在大量的处于TIME_WAIT状态的socket， 甚至比处于Established状态下的socket多的多,严重影响服务器的处理能力，甚至耗尽可用的socket，停止服务。 为什么需要TIME_WAIT？【保证Server最后收到了ACK】 原因有二： 一、保证TCP协议的全双工连接能够可靠关闭 二、保证这次连接的重复数据段从网络中消失 先说第一点，如果Client直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。 再说第二点，如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。 8.4 网络编程时的同步、异步、阻塞、非阻塞同步/异步主要针对C端: 同步： 所谓同步，就是在c端发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。 例如普通B/S模式（同步）：提交请求-&gt;等待服务器处理-&gt;处理完毕返回 这个期间客户端浏览器不能干任何事 异步： 异步的概念和同步相对。当c端一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。 例如 ajax请求（异步）: 请求通过事件触发-&gt;服务器处理（这是浏览器仍然可以作其他事情）-&gt;处理完毕 阻塞/非阻塞主要针对S端: 阻塞 阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。 有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。 例如，我们在socket中调用recv函数，如果缓冲区中没有数据，这个函数就会一直等待，直到有数据才返回。而此时，当前线程还会继续处理各种各样的消息。 快递的例子：比如到你某个时候到A楼一层（假如是内核缓冲区）取快递，但是你不知道快递什么时候过来，你又不能干别的事，只能死等着。但你可以睡觉（进程处于休眠状态），因为你知道快递把货送来时一定会给你打个电话（假定一定能叫醒你）。 非阻塞 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 还是等快递的例子：如果用忙轮询的方法，每隔5分钟到A楼一层(内核缓冲区）去看快递来了没有。如果没来，立即返回。而快递来了，就放在A楼一层，等你去取。 对象的阻塞模式和阻塞函数调用 对象是否处于阻塞模式和函数是不是阻塞调用有很强的相关性，但是并不是一一对应的。阻塞对象上可以有非阻塞的调用方式，我们可以通过一定的API去轮询状 态，在适当的时候调用阻塞函数，就可以避免阻塞。而对于非阻塞对象，调用特殊的函数也可以进入阻塞调用。函数select就是这样的一个例子。 总结几句话就是： 同步/异步主要针对C端 \\1. 同步，就是我客户端（c端调用者）调用一个功能，该功能没有结束前，我（c端调用者）死等结果。 \\2. 异步，就是我（c端调用者）调用一个功能，不需要知道该功能结果，该功能有结果后通知我（c端调用者）即回调通知。 阻塞/非阻塞主要针对S端 \\3. 阻塞， 就是调用我（s端被调用者，函数），我（s端被调用者，函数）没有接收完数据或者没有得到结果之前，我不会返回。 \\4. 非阻塞， 就是调用我（s端被调用者，函数），我（s端被调用者，函数）立即返回，通过select通知调用者 8.5 进程间通信方式1.**什么是进程间通信？** 由于不同的进程运行在各自不同的内存空间中,其中一个进程对于变量的修改另一方是无法感知的,因此,进程之间的消息传递不能通过变量或其他数据结构直接进行,只能通过进程间通信来完成. 进程间通信是指不同进程间进行数据共享和数据交换. 2.**进程通信的分类** 根据进程通信时信息量大小的不同,可以将进程通信划分为两大类型: 控制信息的通信(低级通信)和大批数据信息的通信(高级通信). 低级通信主要用于进程之间的同步,互斥,终止和挂起等等控制信息的传递. 高级通信主要用于进程间数据块数据的交换和共享,常见的高级通信有管道,消息队列,共享内存等. 进程通信的方式 1)管道【管道分为有名管道和无名管道】 管道是一种半双工的通信方式,数据只能单向流动,而且只能在具有亲缘关系的进程间使用.进程的亲缘关系一般指的是父子关系.管道一般用于两个不同进程之间的通信.当一个进程创建了一个管道,并调用fork创建自己的一个子进程后,父进程关闭读管道端,子进程关闭写管道端,这样提供了两个进程之间数据流动的一种方式. 2)信号量 信号量是一个计数器,可以用来控制多个线程对共享资源的访问.,它不是用于交换大批数据,而用于多线程之间的同步.它常作为一种锁机制,防止某进程在访问资源时其它进程也访问该资源.因此,主要作为进程间以及同一个进程内不同线程之间的同步手段. 3)信号 信号是一种比较复杂的通信方式,用于通知接收进程某个事件已经发生. 4)消息队列 消息队列是消息的链表,存放在内核中并由消息队列标识符标识.消息队列克服了信号传递信息少,管道只能承载无格式字节流以及缓冲区大小受限等特点.消息队列是UNIX下不同进程之间可实现共享资源的一种机制,UNIX允许不同进程将格式化的数据流以消息队列形式发送给任意进程.对消息队列具有操作权限的进程都可以使用msget完成对消息队列的操作控制.通过使用消息类型,进程可以按任何顺序读信息,或为消息安排优先级顺序. 5)共享内存 共享内存就是映射一段能被其他进程所访问的内存,这段共享内存由一个进程创建,但多个进程都可以访问.共享内存是最快的IPC(进程间通信)方式,它是针对其它进程间通信方式运行效率低而专门设计的.它往往与其他通信机制,如信号量,配合使用,来实现进程间的同步与通信. 6)套接字(socket) 套接口也是一种进程间通信机制,与其他通信机制不同的是,它可用于不同进程及其间进程的通信. 3.**各种通信方式的优缺点** 无名管道简单方便．但局限于单向通信的工作方式．并且只能在创建它的进程及其子孙进程之间实现管道的共享：有名管道虽然可以提供给任意关系的进程使用．但是由于其长期存在于系统之中，使用不当容易出错．所以普通用户一般不建议使用。 消息队列可以不再局限于父子进程．而允许任意进程通过共享消息队列来实现进程间通信．并由系统调用函数来实现消息发送和接收之间的同步．从而使得用户在使用消息缓冲进行通信时不再需要考虑同步问题．使用方便，但是消息队列中信息的复制需要额外消耗CPU的时间．不适宜于信息量大或操作频繁的场合。 因此．对于不同的应用问题，要根据问题本身的情况来选择进程间的通信方式。 8.6 TCP的流量控制和拥塞控制流量控制【点对点】 所谓的流量控制就是让发送方的发送速率不要太快，让接收方来得及接受。利用滑动窗口机制可以很方便的在TCP连接上实现对发送方的流量控制。 拥塞控制【网络资源】 在某段时间，若对网络中的某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变化，这种情况叫做拥塞。 所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能承受现有的网络负荷。 流量控制往往指的是点对点通信量的控制，是个端到端的问题。流量控制所要做的就是控制发送端发送数据的速率，以便使接收端来得及接受。 拥塞控制的四种算法，即慢开始（Slow-start)，拥塞避免（Congestion Avoidance)快重传（Fast Restrangsmit)和快回复（Fast Recovery） 九. 大数据算法9.1 两个超大文件找共同出现的单词1.题目描述 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url? 2.思考过程 （1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。 （2）但上述方法有一个明显问题，加载一个文件的数据需要50亿*64bytes = 320G远远大于4G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。 （3）针对上述问题，我们分治算法的思想。 step1：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,…,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计） step2：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,…,b999)（为什么要这样做? 文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中） 所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url） step3：因为每个hash大约300M，所以我们再可以采用（1）中的想法 最后对两个新的url文件做hadoop计数，reduce的结果中count为2的即是重复项。 也可用其他方法。 9.2 海量数据求 TopN问题描述 有1亿个浮点数，如果找出期中最大的10000个？ 解题思路 最容易想到的方法是将数据全部排序，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。 第二种方法为局部淘汰法，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即10000。 第三种方法是分治法，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100*10000个数据里面找出最大的10000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^64=4MB，一共需要101次这样的比较。 第四种方法是Hash法。如果这1亿个数里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。 第五种方法采用最小堆。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。 9.3 海量数据找出不重复的（整数）数据(分治+位图法)题目描述 在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。 解答思路 方法一：分治法 ​ 与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。 方法二：位图法（bit-map） ​ 位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。 ​ 位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。 ​ 假设我们要对 [0,7] 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0： 0 0 0 0 0 0 0 0然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1： 0 0 0 0 1 0 1 0依次遍历，结束后，位数组是这样的： 0 1 1 0 1 1 1 0每个为 1 的位，它的下标都表示了一个数： for i in range(8): if bits[i] == 1: print(i)123这样我们其实就已经实现了排序。 对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。 那么对于这道题，我们用 2 个 bit 来表示各个数字的状态： 00 表示这个数字没出现过；01 表示这个数字出现过一次（即为题目所找的不重复整数）；10 表示这个数字出现了多次。 那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作： 遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。 方法总结判断数字是否重复的问题，位图法是一种非常高效的方法。 9.4 布隆过滤器什么是布隆过滤器 本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。 相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。 实现原理 HashMap 的问题讲述布隆过滤器的原理之前，我们先思考一下，通常你判断某个元素是否存在用的是什么？应该蛮多人回答 HashMap 吧，确实可以将值映射到 HashMap 的 Key，然后可以在 O(1) 的时间复杂度内返回结果，效率奇高。但是 HashMap 的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了。 还比如说你的数据集存储在远程服务器上，本地服务接受输入，而数据集非常大不可能一次性读进内存构建 HashMap 的时候，也会存在问题。 布隆过滤器数据结构 布隆过滤器是一个 bit 向量或者说 bit 数组，长这样： ​ 如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并对每个生成的哈希值指向的 bit 位置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成了哈希值 1、4、7，则上图转变为： Ok，我们现在再存一个值 “tencent”，如果哈希函数返回 3、4、8 的话，图继续变为： ​ 值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit 位，因此它被覆盖了。现在我们如果想查询 “dianping” 这个值是否存在，哈希函数返回了 1、5、8三个值，结果我们发现 5 这个 bit 位上的值为 0，说明没有任何一个值映射到这个 bit 位上，因此我们可以很确定地说 “dianping” 这个值不存在。而当我们需要查询 “baidu” 这个值是否存在的话，那么哈希函数必然会返回 1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。 这是为什么呢？答案跟简单，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值 “taobao” 即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断 “taobao” 这个值存在。 支持删除么​ 传统的布隆过滤器并不支持删除操作。但是名为 Counting Bloom filter 的变种可以用来测试元素计数个数是否绝对小于某个阈值，它支持元素删除。可以参考文章 Counting Bloom Filter 的原理和实现 如何选择哈希函数个数和布隆过滤器长度​ 很显然，过小的布隆过滤器很快所有的 bit 位均为 1，那么查询任何值都会返回“可能存在”，起不到过滤的目的了。布隆过滤器的长度会直接影响误报率，布隆过滤器越长其误报率越小。 ​ 另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置位 1 的速度越快，且布隆过滤器的效率越低；但是如果太少的话，那我们的误报率会变高。 如何选择适合业务的 k 和 m 值呢，这里直接贴一个公式： ​ k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。 至于如何推导这个公式，我在知乎发布的文章有涉及，感兴趣可以看看，不感兴趣的话记住上面这个公式就行了。 最佳实践​ 常见的适用常见有，利用布隆过滤器减少磁盘 IO 或者网络请求，因为一旦一个值必定不存在的话，我们可以不用进行后续昂贵的查询请求。 ​ 另外，既然你使用布隆过滤器来加速查找和判断是否存在，那么性能很低的哈希函数不是个好选择，推荐 MurmurHash、Fnv 这些。 大Value拆分​ Redis 因其支持 setbit 和 getbit 操作，且纯内存性能高等特点，因此天然就可以作为布隆过滤器来使用。但是布隆过滤器的不当使用极易产生大 Value，增加 Redis 阻塞风险，因此生成环境中建议对体积庞大的布隆过滤器进行拆分。 ​ 拆分的形式方法多种多样，但是本质是不要将 Hash(Key) 之后的请求分散在多个节点的多个小 bitmap 上，而是应该拆分成多个小 bitmap 之后，对一个 Key 的所有哈希函数都落在这一个小 bitmap 上 9.5 bit-mapBit-map的基本思想 32位机器上，对于一个整型数，比如int a=1 在内存中占32bit位，这是为了方便计算机的运算。但是对于某些应用场景而言，这属于一种巨大的浪费，因为我们可以用对应的32bit位对应存储十进制的0-31个数，而这就是Bit-map的基本思想。Bit-map算法利用这种思想处理大量数据的排序、查询以及去重。Bitmap在用户群做交集和并集运算的时候也有极大的便利。 在此我用一个简单的例子来详细介绍BitMap算法的原理。 假设我们要对0-7内的5个元素(4,7,2,5,3)进行排序(这里假设元素没有重复)。我们可以使用BitMap算法达到排序目的。要表示8个数，我们需要8个byte。 1.首先我们开辟一个字节(8byte)的空间，将这些空间的所有的byte位都设置为0 2.然后便利这5个元素，第一个元素是4，因为下边从0开始，因此我们把第五个字节的值设置为1 3.然后再处理剩下的四个元素，最终8个字节的状态如下图 4.现在我们遍历一次bytes区域，把值为1的byte的位置输出(2,3,4,5,7)，这样便达到了排序的目的 从上面的例子我们可以看出，BitMap算法的思想还是比较简单的，关键的问题是如何确定10进制的数到2进制的映射图 MAP映射： 假设需要排序或则查找的数的总数N=100000000，BitMap中1bit代表一个数字，1个int = 4Bytes = 4*8bit = 32 bit,那么N个数需要N/32 int空间。所以我们需要申请内存空间的大小为int a[1 + N/32]，其中：a[0]在内存中占32为可以对应十进制数0-31，依次类推： a[0]—————————–&gt; 0-31 a[1]——————————&gt; 32-63 a[2]——————————-&gt; 64-95 a[3]——————————–&gt; 96-127 ……………………………………………… 那么十进制数如何转换为对应的bit位，下面介绍用位移将十进制数转换为对应的bit位: 1.求十进制数在对应数组a中的下标 十进制数0-31，对应在数组a[0]中，32-63对应在数组a[1]中，64-95对应在数组a[2]中………，使用数学归纳分析得出结论：对于一个十进制数n，其在数组a中的下标为：a[n/32] 2.求出十进制数在对应数a[i]中的下标 例如十进制数1在a[0]的下标为1，十进制数31在a[0]中下标为31，十进制数32在a[1]中下标为0。 在十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32求得在对应数组a[i]中的下标。 3.位移 对于一个十进制数n,对应在数组a[n/32][n%32]中，但数组a毕竟不是一个二维数组，我们通过移位操作实现置1 a[n/32] |= 1 &lt;&lt; n % 32 移位操作： a[n&gt;&gt;5] |= 1 &lt;&lt; (n &amp; 0x1F) n &amp; 0x1F 保留n的后五位 相当于 n % 32 求十进制数在数组a[i]中的下标。 9.6 字典树Trie树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。字典树（Trie）可以保存一些 字符串 -&gt; 值 的对应关系。基本上，它跟 Java 的 HashMap 功能相同，都是 key-value 映射，只不过 Trie 的 key 只能是字符串。 Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 Trie 的强大之处就在于它的时间复杂度。它的插入和查询时间复杂度都为 O(k) ，其中 k 为 key 的长度，与 Trie 中保存了多少个元素无关。Hash 表号称是 O(1) 的，但在计算 hash 的时候就肯定会是 O(k) ，而且还有碰撞之类的问题； Trie 的缺点是空间消耗很高。 典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较，查询效率比哈希表高。 Trie树的基本性质： （1）根节点不包含字符，除根节点意外每个节点只包含一个字符。（2）从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。（3）每个节点的所有子节点包含的字符串不相同。（4）如果字符的种数为n，则每个结点的出度为n，这也是空间换时间的体现，浪费了很多的空间。（5）插入查找的复杂度为O(n)，n为字符串长度。 基本思想（以字母树为例）： 1、插入过程 对于一个单词，从根开始，沿着单词的各个字母所对应的树中的节点分支向下走，直到单词遍历完，将最后的节点标记为红色，表示该单词已插入Trie树。 2、查询过程 同样的，从根开始按照单词的字母顺序向下遍历trie树，一旦发现某个节点标记不存在或者单词遍历完成而最后的节点未标记为红色，则表示该单词不存在，若最后的节点标记为红色，表示该单词存在。 字典树的数据结构 一般可以按下面步骤构建： 利用串构建一个字典树，这个字典树保存了串的公共前缀信息，因此可以降低查询操作的复杂度。 下面以英文单词构建的字典树为例，这棵Trie树中每个结点包括26个孩子结点，因为总共有26个英文字母(假设单词都是小写字母组成)。 则可声明包含Trie树的结点信息的结构体: typedef struct Trie_node { int count; // 统计单词前缀出现的次数 struct Trie_node* next[26]; // 指向各个子树的指针 bool exist; // 标记该结点处是否构成单词 }TrieNode , *Trie; 其中next是一个指针数组，存放着指向各个孩子结点的指针。 其中next是一个指针数组，存放着指向各个孩子结点的指针。 如给出字符串”abc”,”ab”,”bd”,”dda”，根据该字符串序列构建一棵Trie树。则构建的树如下: 9.7 倒排索引倒排索引是目前搜索引擎公司对搜索引擎最常用的存储方式，也是搜索引擎的核心内容，在搜索引擎的实际应用中，有时需要按照关键字的某些值查找记录，所以是按照关键字建立索引，这个索引就被称为倒排索引。 首先你要明确，索引这东西，一般是用于提高查询效率的。举个最简单的例子，已知有5个文本文件，需要我们去查某个单词位于哪个文本文件中，最直观的做法就是挨个加载每个文本文件中的单词到内存中，然后用for循环遍历一遍数组，直到找到这个单词。这种做法就是正向索引的思路。 正向索引的这种查询效率也不需要我多吐槽了。倒排索引的思路其实也并不难。再举一个例子，有两段文本 D1：Hello, conan! D2：Hello, hattori! 第一步，找到所有的单词 Hello、conan、hattori 第二步，找到包含这些单词的文本位置 Hello（D1，D2） conan（D1） hattori（D2） 我们将单词作为Hash表的Key，将所在的文本位置作为Hash表的Value保存起来。 当我们要查询某个单词的所在位置时，只需要根据这张Hash表就可以迅速的找到目标文档。 结合之前的说的正向索引，不难发现。正向索引是通过文档去查找单词，反向索引则是通过单词去查找文档。 倒排索引的优点还包括在处理复杂的多关键字查询时，可在倒排表中先完成查询的并、交等逻辑运算，得到结果后再对记录进行存取，这样把对文档的查询转换为地址集合的运算，从而提高查找速度。 十. 数据结构和算法10.1 数组连续子数组的最大和 调整数组顺序使奇数位于偶数前面 10.2 链表1. 链表删除删除链表中的节点核心代码： class Solution: def deleteNode(self, node): node.val = node.next.val node.next = node.next.next [python 垃圾回收机制 引用计数 Python语言默认采用的垃圾收集机制是『引用计数法 Reference Counting』，该算法最早George E. Collins在1960的时候首次提出，50年后的今天，该算法依然被很多编程语言使用，『引用计数法』的原理是：每个对象维护一个ob_ref字段，用来记录该对象当前被引用的次数，每当新的引用指向该对象时，它的引用计数ob_ref加1，每当该对象的引用失效时计数ob_ref减1，一旦对象的引用计数为0，该对象立即被回收，对象占用的内存空间将被释放。它的缺点是需要额外的空间维护引用计数，这个问题是其次的，不过最主要的问题是它不能解决对象的“循环引用”，因此，也有很多语言比如Java并没有采用该算法做来垃圾的收集机制。 标记清除 『标记清除（Mark—Sweep）』算法是一种基于追踪回收（tracing GC）技术实现的垃圾回收算法。它分为两个阶段：第一阶段是标记阶段，GC会把所有的『活动对象』打上标记，第二阶段是把那些没有标记的对象『非活动对象』进行回收。那么GC又是如何判断哪些是活动对象哪些是非活动对象的呢？ 对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。 在上图中，我们把小黑圈视为全局变量，也就是把它作为root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而4和5不可达，那么1、2、3就是活动对象，4和5是非活动对象会被GC回收。 标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些容器对象，比如list、dict、tuple，instance等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象。 分代回收分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象 奇偶链表核心代码 class Solution: def oddEvenList(self, head: ListNode) -> ListNode: if not head:return head odd = head even_head = even = head.next while odd.next and even.next: odd.next = odd.next.next even.next = even.next.next odd,even = odd.next,even.next odd.next = even_head return head 知识点： 2. 链表合并1.2.1 合并两个有序链表核心代码 3. 链表反转1.3.1 K 个一组翻转链表解题答案： ​ Python版 #官方题解： class Solution: # 翻转一个子链表，并且返回新的头与尾 def reverse(self, head: ListNode, tail: ListNode): prev = tail.next p = head while prev != tail: nex = p.next p.next = prev prev = p p = nex return tail, head def reverseKGroup(self, head: ListNode, k: int) -> ListNode: hair = ListNode(0) hair.next = head pre = hair while head: tail = pre # 查看剩余部分长度是否大于等于 k for i in range(k): tail = tail.next if not tail: return hair.next nex = tail.next head, tail = self.reverse(head, tail) # 把子链表重新接回原链表 pre.next = head tail.next = nex pre = tail head = tail.next return hair.next #复杂度分析 时间复杂度：O(n)，其中 n为链表的长度。head 指针会在O(N/K) 个结点上停留，每次停留需要进行一次 O(k)的翻转操作。 空间复杂度：O(1)，我们只需要建立常数个变量。 #网友解法： 复杂度分析 时间复杂度：O(N)。 空间复杂度：O(1)。 #时间最优解法 class Solution: def reverseKGroup(self, head: ListNode, k: int) -> ListNode: # 判断链表长度是否大于K cur = head for _ in range(k): if not cur: return head cur = cur.next # 反转头部的长度为k的子链表 pre = head itera = head.next for _ in range(k-1): next = itera.next itera.next = pre pre = itera itera = next # 反转链表的剩余部分 head.next = self.reverseKGroup(cur, k) return pre 知识点 反转链表操作 4. 链表相交1.4.1 相交链表核心代码 -- 5. 链表旋转1.5.1 旋转链表核心代码 10.3 字符串1. 字符串比较比较字符串最小字母出现频次比较版本号核心代码： 2. 字符串连接3. 字符串匹配模式匹配数组中的字符串匹配通配符匹配驼峰式匹配正则表达式匹配重复叠加字符串匹配4. 字符串反转反转字符串中的单词 III仅仅反转字母反转字符串中的元音字母反转字符串反转字符串 II5. 字符串旋转旋转数字II. 左旋转字符串6. 字符串子串子串能表示从 1 到 N 数字的二进制串最长回文子串定长子串中元音的最大数目 串联所有单词的子串单字符重复子串的最大长度子串的最大出现次数10.4 二叉树1. 二叉树遍历二叉树的层序遍历2. 二叉树路径二叉树中的最大路径和3. 二叉树翻转 翻转等价二叉树4. 二叉树搜索二叉树的最大深度二叉搜索树中的搜索5. 二叉树子树出现次数最多的子树元素和10.5 堆10.6 动态规划1. 连续子数组最值连续子数组的最大和最长重复子数组K个逆序对数组最长递增子序列的个数[最长的斐波那契子序列的长度]10.7 二分查找1. 数组二分查找分割数组的最大值两个数组的交集寻找两个正序数组的中位数最长重复子数组长度最小的子数组2. 堆二分查找有序矩阵中第K小的元素第 K 个最小的素数分数找出第 k 小的距离对3. 二叉树二分查找二叉搜索树中第K小的元素[完全二叉树的节点个数](10.8 排序 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Job","slug":"Job","permalink":"https://dataquaner.github.io/categories/Job/"}],"tags":[{"name":"Job","slug":"Job","permalink":"https://dataquaner.github.io/tags/Job/"}]},{"title":"Spark面试问题梳理：选择题","slug":"Spark面试问题：选择题","date":"2020-06-21T06:35:00.000Z","updated":"2020-06-21T12:27:36.753Z","comments":true,"path":"2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/","link":"","permalink":"https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/","excerpt":"","text":"1. Spark 的四大组件下面哪个不是 (D )A.Spark Streaming B. Mlib C Graphx D.Spark R 2. 下面哪个端口不是 spark 自带服务的端口 (C )A.8080 B.4040 C.8090 D.18080 备注：8080：spark集群web ui端口，4040：sparkjob监控端口，18080：jobhistory端口 3. spark 1.4 版本的最大变化 (B )A spark sql Release 版本 B .引入 Spark R C DataFrame D.支持动态资源分配 4. Spark Job 默认的调度模式 (A )A FIFO B FAIR C 无 D 运行时指定 备注：Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。 5.哪个不是本地模式运行的条件 ( D)A spark.localExecution.enabled=true B 显式指定本地运行 C finalStage 无父 Stage D partition默认值 备注：【问题】Spark在windows能跑集群模式吗？ 我认为是可以的，但是需要详细了解cmd命令行的写法。目前win下跑spark的单机模式是没有问题的。 【关键点】spark启动机制容易被windows的命令行cmd坑 1、带空格、奇怪字符的安装路径，cmd不能识别。最典型的坑就是安装在Program Files文件夹下的程序，因为Program和Files之间有个空格，所以cmd竟不能识别。之前就把JDK安装在了Program Files下面，然后启动spark的时候，总是提示我找不到JDK。我明明配置了环境变量了啊？这就是所谓了《已经配置环境变量，spark 仍然找不到Java》的错误问题。至于奇怪的字符，如感叹号!，我经常喜欢用来将重要的文件夹排在最前面，但cmd命令提示符不能识别。 2、是否需要配置hadoop的路径的问题——答案是需要用HDFS或者yarn就配，不需要用则不需配置。目前大多数的应用场景里面，Spark大规模集群基本安装在Linux服务器上，而自己用windows跑spark的情景，则大多基于学习或者实验性质，如果我们所要读取的数据文件从本地windows系统的硬盘读取（比如说d:\\data\\ml.txt），基本上不需要配置hadoop路径。我们都知道，在编spark程序的时候，可以指定spark的启动模式，而启动模式有这么三中（以python代码举例）： （2.1）本地情况，conf = SparkConf().setMaster(“local[*]”) ——&gt;也就是拿本机的spark来跑程序 （2.2）远程情况，conf = SparkConf().setMaster(“spark://remotehost:7077”) ——&gt;远程spark主机 （2.3）yarn情况，conf = SparkConf().setMaster(“yarn-client”) ——&gt;远程或本地 yarn集群代理spark 针对这3种情况，配置hadoop安装路径都有什么作用呢？（2.1）本地的情况，直接拿本机安装的spark来运行spark程序（比如d:\\spark-1.6.2），则配不配制hadoop路径取决于是否需要使用hdfs。java程序的情况就更为简单，只需要导入相应的hadoop的jar包即可，是否配置hadoop路径并不重要。（2.2）的情况大体跟（2.1）的情况相同，虽然使用的远程spark，但如果使用本地数据，则运算的元数据也是从本地上传到远程spark集群的，无需配置hdfs。而（2.3）的情况就大不相同，经过我搜遍baidu、google、bing引擎，均没找到SparkConf直接配置远程yarn地址的方法，唯一的一个帖子介绍可以使用yarn://remote:8032的形式，则会报错“无法解析 地址”。查看Spark的官方说明，Spark其实是通过hadoop路径下的etc\\hadoop文件夹中的配置文件来寻找yarn集群的。因此，需要使用yarn来运行spark的情况，在spark那配置好hadoop的目录就尤为重要。后期经过虚拟机的验证，表明，只要windows本地配置的host地址等信息与linux服务器端相同（注意应更改hadoop-2/etc/hadoop 下各种文件夹的配置路径，使其与windows本地一致），是可以直接在win下用yarn-client提交spark任务到远程集群的。 3、是否需要配置环境变量的问题，若初次配置，可以考虑在IDE里面配置，或者在程序本身用setProperty函数进行配置。因为配置windows下的hadoop、spark环境是个非常头疼的问题，有可能路径不对而导致无法找到相应要调用的程序。待实验多次成功率提高以后，再直接配置windows的全局环境变量不迟。 4、使用Netbeans这个IDE的时候，有遇到Netbeans不能清理构建的问题。原因，极有可能是导入了重复的库，spark里面含有hadoop包，记得检查冲突。同时，在清理构建之前，记得重新编译一遍程序，再进行清理并构建。 ５、经常遇到WARN YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources资源不足无法运行的问题，添加conf.set(“spark.executor.memory”, “512m”);语句进行资源限制。先前在虚拟机跑spark，由于本身机子性能不高，给虚拟机设置的内存仅仅2G，导致hadoop和spark双开之后系统资源严重不足。因此可以缩小每个executor的运算规模。其他资源缺乏问题的解决方法参考http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html 6.下面哪个不是 RDD 的特点 (C )A. 可分区 B 可序列化 C 可修改 D 可持久化 7. 关于广播变量，下面哪个是错误的 (D )A 任何函数调用 B 是只读的 C 存储在各个节点 D 存储在磁盘或 HDFS 8. 关于累加器，下面哪个是错误的 (D )A 支持加法 B 支持数值类型 C 可并行 D 不支持自定义类型 9.Spark 支持的分布式部署方式中哪个是错误的 (D )A standalone B spark on mesos C spark on YARN D Spark on local 10.Stage 的 Task 的数量由什么决定 (A )A Partition B Job C Stage D TaskScheduler 11.下面哪个操作是窄依赖 (B )A join B filter C group D sort 12.下面哪个操作肯定是宽依赖 (C )A map B flatMap C reduceByKey D sample 13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )A http B nio C netty D Akka 备注：从spark1.3.1之后，netty完全代替 了akka 一直以来，基于Akka实现的RPC通信框架是Spark引以为豪的主要特性，也是与Hadoop等分布式计算框架对比过程中一大亮点，但是时代和技术都在演化，从Spark1.3.1版本开始，为了解决大数据块（如shuffle）的传输问题，Spark引入了Netty通信框架，到了1.6.0版本，Netty居然完全取代了Akka，承担Spark内部所有的RPC通信以及数据流传输。 那么Akka又是什么东西？从Akka出现背景来说，它是基于Actor的RPC通信系统，它的核心概念也是Message，它是基于协程的，性能不容置疑；基于scala的偏函数，易用性也没有话说，但是它毕竟只是RPC通信，无法适用大的package/stream的数据传输，这也是Spark早期引入Netty的原因。 那么Netty为什么可以取代Akka？首先不容置疑的是Akka可以做到的，Netty也可以做到，但是Netty可以做到，Akka却无法做到，原因是啥？在软件栈中，Akka相比Netty要Higher一点，它专门针对RPC做了很多事情，而Netty相比更加基础一点，可以为不同的应用层通信协议（RPC，FTP，HTTP等）提供支持，在早期的Akka版本，底层的NIO通信就是用的Netty；其次一个优雅的工程师是不会允许一个系统中容纳两套通信框架，恶心！最后，虽然Netty没有Akka协程级的性能优势，但是Netty内部高效的Reactor线程模型，无锁化的串行设计，高效的序列化，零拷贝，内存池等特性也保证了Netty不会存在性能问题。 那么Spark是怎么用Netty来取代Akka呢？一句话，利用偏函数的特性，基于Netty“仿造”出一个简约版本的Actor模型！！ 14. 默认的存储级别 (A )A MEMORY_ONLY B MEMORY_ONLY_SER C MEMORY_AND_DISK D MEMORY_AND_DISK_SER 备注： //不会保存任务数据 val NONE = new StorageLevel(false, false, false, false) //直接将RDD的partition保存在该节点的Disk上 val DISK_ONLY = new StorageLevel(true, false, false, false) //直接将RDD的partition保存在该节点的Disk上,在其他节点上保存一个相同的备份 val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) //将RDD的partition对应的原生的Java Object保存在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //那么这些partition将不会缓存,并且需要的时候被重新计算,默认缓存的级别 val MEMORY_ONLY = new StorageLevel(false, true, false, true) //将RDD的partition对应的原生的Java Object保存在JVM中,在其他节点上保存一个相同的备份 val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) //将RDD的partition反序列化后的对象存储在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //超出的partition将被保存在Disk上,并且在需要时读取 val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) //在其他节点上保存一个相同的备份 val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) //将RDD的partition序列化后存储在Tachyon中 val OFF_HEAP = new StorageLevel(false, false, true, false) 15 spark.deploy.recoveryMode 不支持那种 (D )A.ZooKeeper B. FileSystem D NONE D Hadoop 16.下列哪个不是 RDD 的缓存方法 (C )A persist() B Cache() C Memory() 17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )A Driver program B. spark master C.worker node D Cluster manager 18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )A.没区别 B.多会话 C.支持网络环境 D数据库的区别 备注： Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。 内置的derby主要问题是并发性能很差，可以理解为单线程操作。 Derby还有一个特性。更换目录执行操作，会找不到相关表等 19.DataFrame 和 RDD 最大的区别 (B )A.科学统计支持 B.多了 schema C.存储方式不一样 D.外部数据源支持 备注： 上图直观体现了RDD与DataFrame的区别：左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。 提升执行效率： RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。 减少数据读取：分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。 上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。 对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过（例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200）。 此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。 RDD和Dataset ​ DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 ​ DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为Spark SQl类型，然而RDD依赖于运行时反射机制。 DataFrame和Dataset ​ Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。因此具有如下三个特点： ​ DataSet可以在编译时检查类型 并且是面向对象的编程接口。 20.Master 的 ElectedLeader 事件后做了哪些操作 (D )A. 通知 driver B.通知 worker C.注册 application D.直接 ALIVE 34.cache后面能不能接其他算子,它是不是action操作？答：cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。 cache不是action操作 35.reduceByKey是不是action？答：不是，很多人都会以为是action，reduce rdd是action 36.数据本地性是在哪个环节确定的？具体的task运行在那他机器上，dag划分stage的时候确定的 37.RDD的弹性表现在哪几点？1）自动的进行内存和磁盘的存储切换； 2）基于Lingage的高效容错； 3）task如果失败会自动进行特定次数的重试； 4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片； 5）checkpoint和persist，数据计算之后持久化缓存 6）数据调度弹性，DAG TASK调度和资源无关 7）数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par 38.常规的容错方式有哪几种类型？1）.数据检查点,会发生拷贝，浪费资源 2）.记录数据的更新，每次更新都会记录下来，比较复杂且比较消耗性能 39.RDD通过Linage（记录数据更新）的方式为何很高效？1）lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且rDD 之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就 产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条 将复杂计算链条存储下来，计算的时候从后往前回溯 900步是上一个stage的结束，要么就checkpoint 2）记录原数据，是每次修改都记录，代价很大 如果修改一个集合，代价就很小，官方说rdd是 粗粒度的操作，是为了效率，为了简化，每次都是 操作数据集合，写或者修改操作，都是基于集合的 rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的 也可以是细粒度，读可以读其中的一条条的记录。 3）简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景 如网络爬虫，现实世界中，大多数写是粗粒度的场景 40.RDD有哪些缺陷？1）不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的 所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是 说可以一条条的读 2）不支持增量迭代计算，Flink支持 41.说一说Spark程序编写的一般步骤？答：初始化，资源，数据源，并行化，rdd转化，action算子打印输出结果或者也可以存至相应的数据存储介质，具体的可看下图： file:///E:/%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6/qq19B99AF2399E52F466CC3CF7E3B24ED5/069fa7b471f54e038440faf63233acce/640.webp 42. Spark有哪两种算子？答：Transformation（转化）算子和Action（执行）算子。 43. Spark提交你的jar包时所用的命令是什么？答：spark-submit。 44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？答：在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 45. 你所理解的Spark的shuffle过程？答：从下面三点去展开 1）shuffle过程的划分 2）shuffle的中间结果如何存储 3）shuffle的数据如何拉取过来 可以参考这篇博文：http://www.cnblogs.com/jxhd1/p/6528540.html Shuffle后续优化方向：通过上面的介绍，我们了解到，Shuffle过程的主要存储介质是磁盘，尽量的减少IO是Shuffle的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，Shuffle过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有“土豪”内存节点，减少Shuffle IO的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW计算引擎运行地更好。 MapReduce Shuffle后续优化方向：压缩：对数据进行压缩，减少写读数据量； 减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；网络框架：netty的性能据说要占优了；本节点上的数据不走网络框架：对于本节点上的Map输出，Reduce直接去读吧，不需要绕道网络框架。Spark Shuffle后续优化方向：Spark作为MapReduce的进阶架构，对于Shuffle过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是Spark的Shuffle对于我们来说在一些方面还是需要优化的。 压缩：对数据进行压缩，减少写读数据量；内存化：Spark历史版本中是有这样设计的：Map写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。 46. 你如何从Kafka中获取数据？1) 基于Receiver的方式 这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。 2) 基于Direct的方式 这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据 47. 对于Spark中的数据倾斜问题你有什么好的方案？1）前提是定位数据倾斜，是OOM了，还是任务执行缓慢，看日志，看WebUI 2)解决方法，有多个方面 · 避免不必要的shuffle，如使用广播小表的方式，将reduce-side-join提升为map-side-join ·分拆发生数据倾斜的记录，分成几个部分进行，然后合并join后的结果 ·改变并行度，可能并行度太少了，导致个别task数据压力大 ·两阶段聚合，先局部聚合，再全局聚合 ·自定义paritioner，分散key的分布，使其更加均匀 详细解决方案参考博文《Spark数据倾斜优化方法》 48.RDD创建有哪几种方式？1).使用程序中的集合创建rdd 2).使用本地文件系统创建rdd 3).使用hdfs创建rdd， 4).基于数据库db创建rdd 5).基于Nosql创建rdd，如hbase 6).基于s3创建rdd， 7).基于数据流，如socket创建rdd 如果只回答了前面三种，是不够的，只能说明你的水平还是入门级的，实践过程中有很多种创建方式。 49.Spark并行度怎么设置比较合适答：spark并行度，每个core承载24个partition,如，32个core，那么64128之间的并行度，也就是 设置64~128个partion，并行读和数据规模无关，只和内存使用量和cpu使用 时间有关 50.Spark中数据的位置是被谁管理的？答：每个数据分片都对应具体物理位置，数据的位置是被blockManager，无论 数据是在磁盘，内存还是tacyan，都是由blockManager管理 答：Spark中的数据本地性有三种： a.PROCESS_LOCAL是指读取缓存在本地节点的数据 b.NODE_LOCAL是指读取本地节点硬盘数据 c.ANY是指读取非本地节点数据 通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。 52.rdd有几种操作类型？1）transformation，rdd由一种转为另一种rdd 2）action， 3）cronroller，crontroller是控制算子,cache,persist，对性能和效率的有很好的支持 三种类型，不要回答只有2中操作 53.Spark如何处理不能被序列化的对象？将不能序列化的内容封装成object 54.collect功能是什么，其底层是怎么实现的？答：driver通过collect把集群中各个节点的内容收集过来汇总成结果，collect返回结果是Array类型的，collect把各个节点上的数据抓过来，抓过来数据是Array型，collect对Array抓过来的结果进行合并，合并后Array中只有一个元素，是tuple类型（KV类型的）的。 55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？答： 1）因为输入数据有很多task，尤其是有很多小文件的时候，有多少个输入block就会有多少个task启动； 2）spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。 3）参数可以通过spark_home/conf/spark-default.conf配置文件设置: spark.sql.shuffle.partitions 50 spark.default.parallelism 10 第一个是针对spark sql的task数量 第二个是非spark sql程序设置生效 56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?答：会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑 否则很容易出现长时间分配不到资源，job一直不能运行的情况。 57.map与flatMap的区别 map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象 flatMap：对RDD每个元素转换，然后再扁平化 将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组 对象，会抛弃值为null的值 58.列举你常用的action？collect，reduce,take,count,saveAsTextFile等 59.Spark为什么要持久化，一般什么场景下要进行persist操作？ 为什么要进行持久化？ spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤 只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。 以下场景会使用persist 1）某个步骤计算非常耗时，需要进行persist持久化 2）计算链条非常长，重新恢复要算很多步骤，很好使，persist 3）checkpoint所在的rdd要持久化persist， lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前 要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。 4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大 5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。 60.为什么要进行序列化 序列化可以减少数据的体积，减少存储空间，高效存储和传输数据，不好的是使用的时候要反序列化，非常消耗CPU 61.介绍一下join操作优化经验？ 答：join其实常见的就分为两类： map-side join 和 reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。 备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。 62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？ 答：cogroup的函数实现:这个实现根据两个要进行合并的两个RDD操作,生成一个CoGroupedRDD的实例,这个RDD的返回结果是把相同的key中两个RDD分别进行合并操作,最后返回的RDD的value是一个Pair的实例,这个实例包含两个Iterable的值,第一个值表示的是RDD1中相同KEY的值,第二个值表示的是RDD2中相同key的值.由于做cogroup的操作,需要通过partitioner进行重新分区的操作,因此,执行这个流程时,需要执行一次shuffle的操作(如果要进行合并的两个RDD的都已经是shuffle后的rdd,同时他们对应的partitioner相同时,就不需要执行shuffle,)， 场景：表关联查询 63下面这段代码输出结果是什么？ def joinRdd(sc:SparkContext) { val name= Array( Tuple2(1,\"spark\"), Tuple2(2,\"tachyon\"), Tuple2(3,\"hadoop\") ) val score= Array( Tuple2(1,100), Tuple2(2,90), Tuple2(3,80) ) val namerdd=sc.parallelize(name); val scorerdd=sc.parallelize(score); val result = namerdd.join(scorerdd); result .collect.foreach(println); } -------------------------- 答案: (1,(Spark,100)) (2,(tachyon,90)) (3,(hadoop,80)) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/tags/Spark/"}]},{"title":"Spark面试问题梳理","slug":"史上最全的spark面试题","date":"2020-06-21T06:35:00.000Z","updated":"2020-06-21T12:25:46.358Z","comments":true,"path":"2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/","link":"","permalink":"https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/","excerpt":"","text":"问题一：Spark中的RDD是什么，有哪些特性？1.RDD是什么？ RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合 Dataset：就是一个集合，用于存放数据的 Destributed：分布式，可以并行在集群计算 Resilient：表示弹性的，弹性表示 RDD中的数据可以存储在内存或者磁盘中； RDD中的分区是可以改变的； 2. 五大特性： A list of partitions：一个分区列表，RDD中的数据都存储在一个分区列表中 A function for computing each split：作用在每一个分区中的函数 A list of dependencies on other RDDs：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的 Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向 可选项，数据本地性，数据位置最优 问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）常用算子： map：用于遍历RDD，将函数应用于每一个元素，返回新的RDD（transformation算子） foreach：用于遍历RDD，将函数应用于每一个元素，无返回值（action算子） mapPatitions：用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子） foreachPatition：用于遍历操作RDD中的每一个分区，无返回值（action算子） 总结：一般使用mapPatitions和foreachPatition算子比map和foreach更加高效，推荐使用 问题三：.谈谈spark中的宽窄依赖：答：RDD和它的父RDD的关系有两种类型：窄依赖和宽依赖 宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生 窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生 区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示： 问题四：spark中如何划分stage： 概念： ​ Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是宽窄依赖，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以taskSet的形式提交给TaskScheduler运行，stage是由一组并行的task组成。 ​ Spark程序中可以因为不同的action触发众多的job，一个程序中可以有很多的job，每一个job是由一个或者多个stage构成的，后面的stage依赖于前面的stage，也就是说只有前面依赖的stage计算完毕后，后面的stage才会运行； ​ stage 的划分标准就是宽依赖：何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生； ​ 切割规则：从后往前，遇到宽依赖就切割stage； 图解： 计算格式：pipeline管道计算模式，piepeline只是一种计算思想，一种模式。 ​ spark的pipeline管道计算模式相当于执行了一个高阶函数，也就是说来一条数据然后计算一条数据，会把所有的逻辑走完，然后落地，而MapReduce是1+1=2，2+1=3这样的计算模式，也就是计算完落地，然后再计算，然后再落地到磁盘或者内存，最后数据是落在计算节点上，按reduce的hash分区落地。管道计算模式完全基于内存计算，所以比MapReduce快的原因。 管道中的RDD何时落地：shuffle write的时候，对RDD进行持久化的时候。 ​ stage的task的并行度是由stage的最后一个RDD的分区数来决定的，一般来说，一个partition对应一个task，但最后reduce的时候可以手动改变reduce的个数，也就是改变最后一个RDD的分区数，也就改变了并行度。例如：reduceByKey(+,3) 优化：提高stage的并行度：reduceByKey(+,patition的个数) ，join(+,patition的个数) 问题五：DAGScheduler分析：答： 概述：DAGScheduler是一个面向stage 的调度器； 主要入参： dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,resultHandler, localProperties.get) rdd： final RDD； cleanedFunc： 计算每个分区的函数； resultHander： 结果侦听器； 主要功能： 接受用户提交的job； 将job根据类型划分为不同的stage，记录那些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset； 决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给TaskScheduler； 重新提交shuffle输出丢失的stage给taskScheduler； 注：一个stage内部的错误不是由shuffle输出丢失造成的，DAGScheduler是不管的，由TaskScheduler负责尝试重新提交task执行。 问题六：Job的生成：​ 一旦driver程序中出现action，就会生成一个job，比如count等，向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。 ​ 每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是宽依赖，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。 问题七：有向无环图：​ DAG，有向无环图，简单的来说，就是一个由顶点和有方向性的边构成的图中，从任意一个顶点出发，没有任意一条路径会将其带回到出发点的顶点位置，为每个spark job计算具有依赖关系的多个stage任务阶段，通常根据shuffle来划分stage，如reduceByKey,groupByKey等涉及到shuffle的transformation就会产生新的stage ，然后将每个stage划分为具体的一组任务，以TaskSets的形式提交给底层的任务调度模块来执行，其中不同stage之前的RDD为宽依赖关系，TaskScheduler任务调度模块负责具体启动任务，监控和汇报任务运行情况。 问题八：RDD是什么以及它的分类： 问题九：RDD的操作 问题十: RDD缓存：​ Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 transformation自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。 ​ Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。 ​ Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择： 如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。 如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。 如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。 如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。 在不会使用cached RDD的时候，及时使用unpersist方法来释放它。 问题十一：RDD共享变量：​ 在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。 (1). 广播变量（Broadcast Variables） – 广播变量缓存到各个节点的内存中，而不是每个 Task – 广播变量被创建后，能在集群中运行的任何函数调用 – 广播变量是只读的，不能在被广播后修改 – 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本 val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量(2). 累加器 ​ 累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值 问题十二：spark-submit的时候如何引入外部jar包：在通过spark-submit提交任务时，可以通过添加配置参数来指定 –driver-class-path 外部jar包–jars 外部jar包 问题十三：spark如何防止内存溢出： driver端的内存溢出 可以增大driver的内存参数：spark.driver.memory (default 1g)这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。 map过程产生大量对象导致内存溢出这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区， 不会有shuffle的过程。 数据不平衡导致内存溢出 数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。 shuffle后内存溢出 shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。 standalone模式下资源分配不均匀导致内存溢出 在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。​ 这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。 问题十四：spark中cache和persist的区别：cache：缓存数据，默认是缓存在内存中，其本质还是调用persistpersist: 缓存数据，有丰富的数据缓存策略。数据可以保存在内存也可以保存在磁盘中，使用的时候指定对应的缓存级别就可以了。 问题十五：spark中的数据倾斜的现象，原因，后果：(1)、数据倾斜的现象 多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。(2)、数据倾斜的原因 数据问题1、key本身分布不均衡（包括大量的key为空）2、key的设置不合理 spark使用问题1、shuffle时的并发度不够2、计算方式有误 (3)、数据倾斜的后果 spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的） 过多的数据在同一个task中运行，将会把executor撑爆。 问题十六：spark数据倾斜的处理：发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。 数据问题造成的数据倾斜找出异常的key如果任务长时间卡在最后最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个。比如: df.select(“key”).sample(false,0.1).(k=>(k,1)).reduceBykey(+).map(k=>(k._2,k._1)).sortByKey(false).take(10) 如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。 经过分析，倾斜的数据主要有以下三种情况: 1、null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。2、无效数据，大量重复的测试数据或是对结果影响不大的有效数据。3、有效数据，业务导致的正常数据分布。 解决办法 第1，2种情况，直接对数据进行过滤即可（因为该数据对当前业务不会产生影响）。第3种情况则需要进行一些特殊操作，常见的有以下几种做法(1) 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。(2) 对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。(3) 使用reduceByKey 代替 groupByKey(reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义.)(4) 使用map join。 案例 如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:(1) 将原始的 key 转化为 key + 随机值(例如Random.nextInt)(2) 对数据进行 reduceByKey(func)(3) 将 key + 随机值 转成 key(4) 再对数据进行 reduceByKey(func) 案例操作流程分析： 假设说有倾斜的Key，我们给所有的Key加上一个随机数，然后进行reduceByKey操作；此时同一个Key会有不同的随机数前缀，在进行reduceByKey操作的时候原来的一个非常大的倾斜的Key就分而治之变成若干个更小的Key，不过此时结果和原来不一样，怎么破？进行map操作，目的是把随机数前缀去掉，然后再次进行reduceByKey操作。（当然，如果你很无聊，可以再次做随机数前缀），这样我们就可以把原本倾斜的Key通过分而治之方案分散开来，最后又进行了全局聚合注意1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。注意2: 单独处理异常数据时，可以配合使用Map Join解决。 2.spark使用不当造成的数据倾斜 提高shuffle并行度dataFrame和sparkSql可以设置spark.sql.shuffle.partitions参数控制shuffle的并发度，默认为200。rdd操作可以设置spark.default.parallelism控制并发度，默认参数由不同的Cluster Manager控制。 局限性: 只是让每个task执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些key的大 小非常大，即使一个task单独执行它，也会受到数据倾斜的困扰。 使用map join 代替reduce join 在小表不是特别大(取决于你的executor大小)的情况下使用，可以使程序避免shuffle的过程，自然也就没有数据倾斜的困扰了.（详细见http://blog.csdn.net/lsshlsw/article/details/50834858、http://blog.csdn.net/lsshlsw/article/details/48694893） 局限性: 因为是先将小数据发送到每个executor上，所以数据量不能太大。 ​ 问题十七：spark中map-side-join关联优化： ​ 将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。 如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。 何时使用：在海量数据中匹配少量特定数据 原理：reduce-side-join 的缺陷在于会将key相同的数据发送到同一个partition中进行运算，大数据集的传输需要长时间的IO，同时任务并发度收到限制，还可能造成数据倾斜。 reduce-side-join 运行图如下 map-side-join 运行图如下： 将少量的数据转化为Map进行广播，广播会将此 Map 发送到每个节点中，如果不进行广播，每个task执行时都会去获取该Map数据，造成了性能浪费。对大数据进行遍历，使用mapPartition而不是map，因为mapPartition是在每个partition中进行操作，因此可以减少遍历时新建broadCastMap.value对象的空间消耗，同时匹配不到的数据也不会返回。 问题十八：kafka整合sparkStreaming问题：(1)、如何实现sparkStreaming读取kafka中的数据可以这样说：在kafka0.10版本之前有二种方式与sparkStreaming整合，一种是基于receiver，一种是direct,然后分别阐述这2种方式分别是什么 receiver：是采用了kafka高级api,利用receiver接收器来接受kafka topic中的数据，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据，kafka中topic的偏移量是保存在zk中的。基本使用：还有几个需要注意的点： ​ 在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度.​ 对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。在默认配置下，这种方式可能会因为底层的失败而丢失数据. 因为receiver一直在接收数据,在其已经通知zookeeper数据接收完成但是还没有处理的时候,executor突然挂掉(或是driver挂掉通知executor关闭),缓存在其中的数据就会丢失. 如果希望做到高可靠, 让数据零丢失,如果我们启用了Write Ahead Logs(spark.streaming.receiver.writeAheadLog.enable=true）该机制会同步地将接收到的Kafka数据写入分布式文件系统(比如HDFS)上的预写日志中. 所以, 即使底层节点出现了失败, 也可以使用预写日志中的数据进行恢复. 复制到文件系统如HDFS，那么storage level需要设置成 StorageLevel.MEMORY_AND_DISK_SER，也就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER) direct: 在spark1.3之后，引入了Direct方式。不同于Receiver的方式，Direct方式没有receiver这一层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，之后根据设定的maxRatePerPartition来处理每个batch。（设置spark.streaming.kafka.maxRatePerPartition=10000。限制每秒钟从topic的每个partition最多消费的消息条数） (2) 对比这2中方式的优缺点： 采用receiver方式：这种方式可以保证数据不丢失，但是无法保证数据只被处理一次，WAL实现的是At-least-once语义（至少被处理一次），如果在写入到外部存储的数据还没有将offset更新到zookeeper就挂掉,这些数据将会被反复消费. 同时,降低了程序的吞吐量。 采用direct方式: 相比Receiver模式而言能够确保机制更加健壮. 区别于使用Receiver来被动接收数据, Direct模式会周期性地主动查询Kafka, 来获得每个topic+partition的最新的offset, 从而定义每个batch的offset的范围. 当处理数据的job启动时, 就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 **优点**： **1、简化并行读取** 如果要读取多个partition, 不需要创建多个输入DStream然后对它们进行union操作. Spark会创建跟Kafka partition一样多的RDD partition, 并且会并行从Kafka中读取数据. 所以在Kafka partition和RDD partition之间, 有一个一对一的映射关系. **2、高性能** 如果要保证零数据丢失, 在基于receiver的方式中, 需要开启WAL机制. 这种方式其实效率低下, 因为数据实际上被复制了两份, Kafka自己本身就有高可靠的机制, 会对数据复制一份, 而这里又会复制一份到WAL中. 而基于direct的方式, 不依赖Receiver, 不需要开启WAL机制, 只要Kafka中作了数据的复制, 那么就可以通过Kafka的副本进行恢复. **3、一次且仅一次的事务机制** 基于receiver的方式, 是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的. 这是消费Kafka数据的传统方式. 这种方式配合着WAL机制可以保证数据零丢失的高可靠性, 但是却无法保证数据被处理一次且仅一次, 可能会处理两次. 因为Spark和ZooKeeper之间可能是不同步的. 基于direct的方式, 使用kafka的简单api, Spark Streaming自己就负责追踪消费的offset, 并保存在checkpoint中. Spark自己一定是同步的, 因此可以保证数据是消费一次且仅消费一次。不过需要自己完成将offset写入zk的过程,在官方文档中都有相应介绍. -*简单代码实例： messages.foreachRDD(rdd=>{ val message = rdd.map(_._2)//对数据进行一些操作 message.map(method)//更新zk上的offset (自己实现) updateZKOffsets(rdd) }) sparkStreaming程序自己消费完成后，自己主动去更新zk上面的偏移量。也可以将zk中的偏移量保存在mysql或者redis数据库中，下次重启的时候，直接读取mysql或者redis中的偏移量，获取到上次消费的偏移量，接着读取数据。 问题十九：利用scala语言进行排序1.冒泡： 2.快读排序： 问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？​ Spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括worker,master,application,executors.standby节点要从zk中获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的，另外，master切换需要注意两点： ​ 1. 在master切换的过程中，所有的已经在运行的程序皆正常运行，因为spark application在运行前就已经通过cluster manager获得了计算资源，所以在运行时job本身的调度和处理master是没有任何关系的； ​ 2. 在master的切换过程中唯一的影响是不能提交新的job，一方面不能提交新的应用程序给集群，因为只有Active master才能接受新的程序的提交请求，另外一方面，已经运行的程序也不能action操作触发新的job提交请求。 问题二十一：spark master HA主从切换过程不会影响集群已有的作业运行，为什么？ 答：因为程序在运行之前，已经向集群申请过资源，这些资源已经提交给driver了，也就是说已经分配好资源了，这是粗粒度分配，一次性分配好资源后不需要再关心资源分配，在运行时让driver和executor自动交互，弊端是如果资源分配太多，任务运行完不会很快释放，造成资源浪费，这里不适用细粒度分配的原因是因为任务提交太慢。 问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？1.粗粒度：启动时就分配好资源，程序启动，后续具体使用就使用分配好的资源，不需要再分配资源。好处：作业特别多时，资源复用率较高，使用粗粒度。缺点：容易资源浪费，如果一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度。如果有999个资源闲置在那里，会造成资源大量浪费。 2.细粒度：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。 问题二十三：driver的功能是什么：1.一个spark作业运行时包括一个driver进程，也就是作业的主进程，具有main函数，并且有sparkContext的实例，是程序的入口； 2.功能：负责向集群申请资源，向master注册信息，负责了作业的调度，负责了作业的解析，生成stage并调度task到executor上，包括DAGScheduler，TaskScheduler。 问题二十四：spark的有几种部署模式，每种模式特点？1）本地模式 Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类 · local：只启动一个executor · local[k]:启动k个executor · local：启动跟cpu数目相同的 executor 2)standalone模式 分布式部署集群， 自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础， 3)Spark on yarn模式 分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端 4）Spark On Mesos模式。官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序： 1) 粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。 2) 细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。 问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？1）Spark core：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。 2）SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。 3）Spark sql：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析 4）BlinkDB ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。 5）MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib，MLI、ML Optimizer和MLRuntime。 6）GraphX是Spark中用于图和图并行计算 问题二十六：spark中worker 的主要工作是什么？主要功能：管理当前节点内存，CPU的使用情况，接受master发送过来的资源指令，通过executorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务，需要注意的是： 1.worker会不会汇报当前信息给master？worker心跳给master主要只有workid，不会以心跳的方式发送资源信息给master，这样master就知道worker是否存活，只有故障的时候才会发送资源信息； 2.worker不会运行代码，具体运行的是executor，可以运行具体application斜的业务逻辑代码，操作代码的节点，不会去运行代码。 问题二十七：简单说一下hadoop和spark的shuffle相同和差异？1）从 high-level 的角度来看，两者并没有大的差别。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。 2）从 low-level 的角度来看，两者差别不小。 Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。 3）从实现角度来看，两者也有不少差别。 Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。 如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？ Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。 问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别两者都是用mr模型来进行并行计算: 1) hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。 2) spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 3) hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。 问题二十九：RDD机制？rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。 rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。 问题三十：spark有哪些组件？答：主要有如下组件： 1）master：管理集群和节点，不参与计算。 2）worker：计算节点，进程本身不参与计算，和master汇报。 3）Driver：运行程序的main方法，创建spark context对象。 4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 5）client：用户提交程序的入口。 问题三十一：spark工作机制？答：用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。 task scheduler会将stage划分为task set分发到各个节点的executor中执行。 问题三十二：spark的优化怎么做？答： spark调优比较复杂，但是大体可以分为三个方面来进行， 1）平台层面的调优：防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet， 2）应用程序层面的调优：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等， 3）JVM层面的调优：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等 ​ 序列化在分布式系统中扮演着重要的角色，优化Spark程序时，首当其冲的就是对序列化方式的优化。Spark为使用者提供两种序列化方式： Java serialization: 默认的序列化方式。 Kryo serialization: 相较于 Java serialization 的方式，速度更快，空间占用更小，但并不支持所有的序列化格式，同时使用的时候需要注册class。spark-sql中默认使用的是kyro的序列化方式。可以在spark-default.conf设置全局参数，也可以代码中初始化时对SparkConf设置 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) ，该参数会同时作用于机器之间数据的shuffle操作以及序列化rdd到磁盘，内存。Spark不将Kyro设置成默认的序列化方式是因为它需要对类进行注册，官方强烈建议在一些网络数据传输很大的应用中使用kyro序列化。 如果你要序列化的对象比较大，可以增加参数spark.kryoserializer.buffer所设置的值。 如果你没有注册需要序列化的class，Kyro依然可以照常工作，但会存储每个对象的全类名(full class name)，这样的使用方式往往比默认的 Java serialization 还要浪费更多的空间。 可以设置 spark.kryo.registrationRequired 参数为 true，使用kyro时如果在应用中有类没有进行注册则会报错： 如上这个错误需要添加 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/tags/Spark/"}]},{"title":"HiveSQL优化 hive参数版总结","slug":"hive参数优化","date":"2020-06-11T06:50:00.000Z","updated":"2020-06-11T10:22:08.086Z","comments":true,"path":"2020/06/11/hive-can-shu-you-hua/","link":"","permalink":"https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/","excerpt":"","text":"Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。 1. Hive优化目标 在有限的资源下，执行效率更高 常见问题 数据倾斜 map数设置 reduce数设置 其他 2. Hive执行优化 HQL --&gt; Job --&gt; Map/Reduce 执行计划explain [extended] hql 样例 select col,count(1) from test2 group by col; explain select col,count(1) from test2 group by col; 3. Hive表优化 分区 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; ​ 静态分区 ​ 动态分区 分桶 set hive.enforce.bucketing=true; set hive.enforce.sorting=true; 数据 相同数据尽量聚集在一起 4. Hive Job优化并行化执行每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间 set hive.exec.parallel= true; set hive.exec.parallel.thread.numbe=8; 本地化执行 ​ job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB) ​ job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4) ​ job的reduce数必须为0或者1 ​ set hive.exec.mode.local.auto=true; ​ 当一个job满足如上条件才能真正使用本地模式: job合并输入小文件 set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 合并文件数由mapred.max.split.size限制的大小决定 job合并输出小文件 set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件 set hive.merge.size.per.task=64000000;合并之后的文件大小 JVM重利用 set mapred.job.reuse.jvm.num.tasks=20; JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。 压缩数据set hive.exec.compress.output=true; set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec; set mapred.output.compression.type=BLOCK; set hive.exec.compress.intermediate=true; set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec; set hive.intermediate.compression.type=BLOCK; 中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式 hive查询最终的输出也可以压缩 5. Hive Map优化set mapred.map.tasks =10; 无效 (1) 默认map个数 default_num = total_size / block_size; 如果想增加map个数，则设置mapred.map.tasks为一个较大的值 如果想减小map个数，则设置mapred.min.split.size为一个较大的值 情况1：输入文件size巨大，但不是小文件 情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。 6. Hive Shuffle优化 Map端 ​ io.sort.mb ​ io.sort.spill.percent ​ min.num.spill.for.combine ​ io.sort.factor ​ io.sort.record.percent Reduce端 mapred.reduce.parallel.copies mapred.reduce.copy.backoff io.sort.factor mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent 7. Hive Reduce优化 需要reduce操作的查询 group by, join, distribute by, cluster by… order by 比较特殊,只需要一个reduce sum,count,distinct… 聚合函数 高级查询 推测执行 mapred.reduce.tasks.speculative.execution hive.mapred.reduce.tasks.speculative.execution Reduce优化 numRTasks = min[maxReducers,input.size/perReducer] maxReducers=hive.exec.reducers.max perReducer = hive.exec.reducers.bytes.per.reducer hive.exec.reducers.max 默认 ：999 hive.exec.reducers.bytes.per.reducer 默认:1G set mapred.reduce.tasks=10;直接设置 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"}]},{"title":"6.Hadoop面试系列之UDF","slug":"6.Hadoop面试系列之UDF","date":"2020-06-10T14:57:19.767Z","updated":"2020-06-10T14:57:19.766Z","comments":true,"path":"2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/","link":"","permalink":"https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/","excerpt":"","text":"1. 开发步骤​ UDF简称自定义函数，它是Hive函数库的扩展，自定义函数UDF在MapReduce执行阶段发挥作用。开发步骤如下： 1） 给hive.ql.exec.UDF包开发一个自定义函数类，从UDF继承。自定义函数类实现evaluate方法。 2） 在FunctionRegistry类中注册开发的自定义函数类。 3） 打包发布至Hive客户端。 1.1 开发工具​ Eclipse是一款开源的、基于Java的可扩展开发平台。Hadoop开发人员可通过在Eclipse上面开发UDF。 1.2 UDF函数案例1）开发UDF函数类文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHelloWorld.java package org.apache.hadoop.hive.ql.udf; import org.apache.hadoop.hive.ql.exec.UDF; import org.apache.hadoop.io.Text; public class UDFHelloWorld extends UDF { public String evaluate(String str) { if (str == null) { return null; } return \"HelloWorld \" + str; } public static void main(String[] args) { helloUDF uf = new helloUDF();//Text t = new Text(\"gfsg\"); System.out.println(uf.evaluate(\"nihao\").toString()); } } 2）UDF类注册，注册方法文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java package org.apache.hadoop.hive.ql.exec; import org.apache.hadoop.hive.ql.udf.UDFHelloWorld;/*** FunctionRegistry.*/ public final class FunctionRegistry { static { registerGenericUDF(\"concat\", GenericUDFConcat.class); registerUDF(\"substr\", UDFSubstr.class, false); registerUDF(\"substring\", UDFSubstr.class, false); registerUDF(\"space\", UDFSpace.class, false); registerUDF(\"repeat\", UDFRepeat.class, false); registerUDF(\"ascii\", UDFAscii.class, false); registerUDF(\"lpad\", UDFLpad.class, false); registerUDF(\"rpad\", UDFRpad.class, false); registerUDF(\"Hello\", UDFHelloWorld.class, false); registerGenericUDF(\"size\", GenericUDFSize.class); 3）Jar包发布路径发布路径：/opt/boh/hive/lib/hive-exec-0.12.0-cdh5.0.0.jar 上传至hadoop集群执行脚本的hive客户端。 1.3Hive UDF函数1.3.1UDF函数列表 函数清单及其功能 TO_DATE(string date,'format') 格式化所需要的日期 ADD_MONTHS(Timestamp date,int n) 增加月数 date_tostring(Timestamp date,'format') 转换Date类型为指定格式字符串 MONTHS_BETWEEN（Timestamp date1，Timestamp date2） 返回两个日期之间的月数 f_age(string identityId) 验证身份证合法性并返回性别年龄 f_checkidcard(string identityId) 验证身份证合法性 1.3.2 UDF函数说明 TO_DATE函数 Select to_date('20140909111111','YYYYMMDDHH24miss') from test; 返回结果：2014-09-09 11:11:11 ADD_MONTHS函数 select add_months(to_date('20140909111111','YYYYMMDDHH24miss'),1) from test; 返回结果：2014-10-09 11:11:11 date_tostring函数 select date_tostring(to_date('20140909111111','YYYYMMDDHH24miss'),'YYYY-MM-DD') from test; 返回结果：2014-09-09 MONTHS_BETWEEN函数 select MONTHS_BETWEEN(to_date('20140909111111','YYYYMMDDHH24miss'),to_date('20140706111111','YYYYMMDDHH24miss')) from test; 返回结果：2.096774193548387 f_age函数 select f_age('511024198710148199') from test; 返回结果：127 f_checkidcard函数 select f_checkidcard('511024198710148199') from test; 返回结果：1 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[]},{"title":"Hive开窗函数梳理","slug":"Hive开窗函数总结","date":"2020-06-10T06:35:00.000Z","updated":"2020-06-10T11:15:10.879Z","comments":true,"path":"2020/06/10/hive-kai-chuang-han-shu-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/","excerpt":"","text":"本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。 假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。 1.SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。 SELECT *, /*前三次贷款的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv1, /*历史所有贷款 累加到下一次贷款 的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) AS pv2 FROM loan ; 其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。 窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用： n PRECEDING : 前n行 n preceding n FOLLOWING：后n行 CURRENT ROW ： 当前行 如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为: ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. 窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。 2.NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。 SELECT *, #将数据按name切分成10区，并返回属于第几个分区 NTILE(10) OVER (PARTITION BY name ORDER BY orderdate) AS f1, #将数据按照name分区，并按照orderdate排序，返回排序序号 ROW_NUMBER() OVER (PARTITION BY name ORDER BY orderdate) AS f2, #将数据按照name分区，并按照orderdate排序，返回排序序号 RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f3, #将数据按照name分区，并按照orderdate排序，返回排序序号 DENSE_RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f4 FROM loan; 其中第一个函数NTILE(10)是将数据按name切分成10区，并返回属于第几个分区。 可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。语法是： ntile (num) over ([partition_clause] order_by_clause) as your_bucket_num 然后可以根据桶号，选取前或后 n分之几的数据。 后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。 ROW_NUMBER()返回的是一列连续的序号。 RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}. DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}. 3.LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点 SELECT *, #取上一笔贷款的日期,缺失默认填NULL LAG(orderdate, 1) OVER(PARTITION BY name ORDER BY orderdate) AS last_dt, #取下一笔贷款的日期,缺失指定填'1970-1-1' LEAD(orderdate, 1,'1970-1-1') OVER(PARTITION BY name ORDER BY orderdate) AS next_dt, #取最早一笔贷款的日期 FIRST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS first_dt, #取新一笔贷款的日期 LAST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS latest_dt FROM loan; LAG(n)将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录 4.GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate GROUPING SETS(substring(orderdate,1,7), orderdate) ORDER BY GROUPING__ID; GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。 比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5. 如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null. SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with CUBE ORDER BY GROUPING__ID; with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with ROLLUP ORDER BY GROUPING__ID; with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"},{"name":"开窗函数","slug":"开窗函数","permalink":"https://dataquaner.github.io/tags/%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/"}]},{"title":"Hadoop核心知识之MapReduce原理","slug":"1.Hadoop面试系列之Mapreduce原理 ","date":"2020-06-08T13:14:00.000Z","updated":"2020-06-08T14:31:06.843Z","comments":true,"path":"2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/","link":"","permalink":"https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/","excerpt":"","text":"MapReduce是一个基于集群的计算平台，是一个简化分布式编程的计算框架，是一个将分布式计算抽象为Map和Reduce两个阶段的编程模型。（这句话记住了是可以用来装逼的） 1.MapReduce工作流程0) 用户提交任务 （含数据） 1) 集群首先对输入数据源进行切片 2) master 调度 worker 执行 map 任务 3) worker 读取输入源片段 4) worker 执行 map 任务，将任务输出保存在本地5) master 调度 worker 执行 reduce 任务，reduce worker 读取 map 任务的输出文件 6） 执行 reduce 任务，将任务输出保存到 HDFS 由上至下依次执行 过程 过程描述 用户提交任务job 给集群 切片 集群查找源数据 对源数据做基本处理 分词(每行执行一次map函数) 集群(yarn的appliction)分配map任务节点worker 映射 其中间数据(存在本地) 分区(partition) 中间数据 排序 (或二次排序) 中间数据 聚合(combine有无key聚合后key无序) 中间数据 分组(group) 发生在排序后混洗前(个人理解) 混洗(shuffle后key有序) 混洗横跨mapper和reducer，其发生在mapper的输出和reducer的输入阶段 规约(reduce) 集群(yarn的appliction)分配reduce任务节点worker 下面针对具体过程详细介绍： 2.切片split​ HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map任务的数量：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce任务的数量： 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误） 3.Map()阶段 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数 重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出 对输出的key、value进行分区 对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中 4. Reduce阶段 多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上 对多个map的输出进行合并、排序。 重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出 把reduce的输出保存到文件中 特别说明： 切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理 分词，映射，分区，排序，聚合 都属map阶段 混洗 横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段 规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义 分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容 5.总结执行步骤： 1）map任务处理——&gt;切片 读取输入文件内容，解析成key、value对，输入文件的每一行，就是一个key、value对，对应调用一次map函数。 写自己的逻辑，对输入的key、value（k1,v1）处理，转换成新的key、value(k2,v2)输出。 2）reduce任务处理——&gt;计算 在reduce之前，有一个shuffle的过程对多个map任务的输出进行合并、排序、分组等操作。 写reduce函数自己的逻辑，对输入的key、value（k2,{v2,…}）处理，转换成新的key、value(k3,v3)输出。 把reduce的输出保存到文件中。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"https://dataquaner.github.io/tags/MapReduce/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/tags/Hadoop/"}]},{"title":"hadoop shell命令","slug":"hadoop fs、hadoop dfs与hdfs dfs命令的区别及hadoop fs命令说明","date":"2020-06-08T10:40:00.000Z","updated":"2020-06-08T11:13:27.302Z","comments":true,"path":"2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/","link":"","permalink":"https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/","excerpt":"","text":"0.前言FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。 对HDFS文件系统，scheme是hdfs， 对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。 一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。 大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。 1. hadoop fs 命令列表FS Shell cat chgrp chmod chown copyFromLocal copyToLocal cp du dus expunge get getmerge ls lsr mkdir movefromLocal mv put rm rmr setrep stat tail test text touchz 特别说明： hadoop fs：通用的文件系统命令，针对任何系统，比如本地文件、HDFS文件、HFTP文件、S3文件系统等。 hadoop dfs：特定针对HDFS的文件系统的相关操作，但是已经不推荐使用。 hdfs dfs：与hadoop dfs类似，同样是针对HDFS文件系统的操作，替代hadoop dfs。 2. 各命令使用说明 cat使用方法：hadoop fs -cat URI [URI …] ​ 将路径指定文件的内容输出到stdout。 示例： hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2 hadoop fs -cat file:///file3 /user/hadoop/file4 返回值： 成功返回0，失败返回-1。 chgrp使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] ​ Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. ​ 改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chmod使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …] ​ 改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chown使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] ​ 改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。 copyFromLocal使用方法：hadoop fs -copyFromLocal URI ​ 除了限定源路径是一个本地文件外，和put命令相似。 copyToLocal使用方法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI ​ 除了限定目标路径是一个本地文件外，和get命令类似。 cp使用方法：hadoop fs -cp URI [URI …] ​ 将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。示例： hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir 返回值： ​ 成功返回0，失败返回-1。 du使用方法：hadoop fs -du URI [URI …] ​ 显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。示例： hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1 返回值： 成功返回0，失败返回-1。 dus使用方法：hadoop fs -dus ​ 显示文件的大小。 expunge使用方法：hadoop fs -expunge 清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。 get使用方法：hadoop fs -get [-ignorecrc] [-crc] ​ 复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。 示例： hadoop fs -get /user/hadoop/file localfile hadoop fs -get hdfs://host:port/user/hadoop/file localfile 返回值： ​ 成功返回0，失败返回-1。 getmerge使用方法：hadoop fs -getmerge [addnl] 接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。 ls使用方法：hadoop fs -ls ​ 如果是文件，则按照如下格式返回文件信息：​ 文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID​ 如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：​ 目录名 ​ 修改日期 修改时间 权限 用户ID 组ID示例： hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile 返回值： 成功返回0，失败返回-1。 mkdir使用方法：hadoop fs -mkdir ​ 接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。 示例： hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir 返回值： ​ 成功返回0，失败返回-1。 movefromLocal使用方法：dfs -moveFromLocal 输出一个”not implemented“信息。 mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1 返回值： ​ 成功返回0，失败返回-1。 put使用方法：hadoop fs -put … ​ 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile 从标准输入中读取输入。返回值： ​ 成功返回0，失败返回-1。 rm使用方法：hadoop fs -rm URI [URI …] ​ 删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir 返回值： 成功返回0，失败返回-1。 rmr使用方法：hadoop fs -rmr URI [URI …] delete的递归版本。示例： hadoop fs -rmr /user/hadoop/dir hadoop fs -rmr hdfs://host:port/user/hadoop/dir 返回值： 成功返回0，失败返回-1。 setrep使用方法：hadoop fs -setrep [-R] 改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。 示例： hadoop fs -setrep -w 3 -R /user/hadoop/dir1返回值： 成功返回0，失败返回-1。 stat使用方法：hadoop fs -stat URI [URI …] 返回指定路径的统计信息。 示例： hadoop fs -stat path返回值： 成功返回0，失败返回-1。 tail使用方法：hadoop fs -tail [-f] URI 将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。 示例： hadoop fs -tail pathname返回值： 成功返回0，失败返回-1。 test使用方法：hadoop fs -test -[ezd] URI 选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。-d 如果路径是个目录，则返回1，否则返回0。 示例： hadoop fs -test -e filenametext使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。 touchz使用方法：hadoop fs -touchz URI [URI …] 创建一个0字节的空文件。 示例： hadoop -touchz pathname返回值：成功返回0，失败返回-1。 更多详细信息访问：http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/tags/Hadoop/"},{"name":"Shell","slug":"Shell","permalink":"https://dataquaner.github.io/tags/Shell/"}]},{"title":"【Hive日常问题】导入数据成功，查询显示NULL","slug":"【Hive日常问题】导入数据成功，查询显示NULL","date":"2020-05-06T13:40:00.000Z","updated":"2020-05-06T11:17:26.029Z","comments":true,"path":"2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/","link":"","permalink":"https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/","excerpt":"","text":"问题描述hive导入数据成功，但是查询结果为NULL： load data local inpath '/user/hive/student.txt' into table hive_test.students; Loading data to table hive_test.students OK select * from hive_test.students; OK NULL NULL NULL NULL 问题原因查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格： OKNULL NULLNULL NULL查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格： CREATE TABLE students(id int, name string); CREATE TABLE students(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LINES TERMINATED BY '\\n' STORED AS TEXTFILE; OK 1 sun 2 lin document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Question","slug":"Data-Question","permalink":"https://dataquaner.github.io/categories/Data-Question/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"}]},{"title":"Python高级特性之切片","slug":"Python高级特性之切片","date":"2020-04-25T14:14:00.000Z","updated":"2020-04-25T10:10:47.302Z","comments":true,"path":"2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/","link":"","permalink":"https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/","excerpt":"","text":"1. Python可切片对象的索引方式​ 包括：正索引和负索引两部分，如下图所示，以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例： 2. Python切片操作的一般方式​ 一个完整的切片表达式包含两个“:”，用于分隔三个参数(start_index、end_index、step)。 当只有一个“:”时，默认第三个参数step=1； 当一个“:”也没有时，start_index=end_index，表示切取start_index指定的那个元素。 ​ 切片操作基本表达式：object[start_index:end_index:step] step：正负数均可，其绝对值大小决定了切取数据时的‘‘步长”，而正负号决定了“切取方向”，正表示“从左往右”取值，负表示“从右往左”取值。当step省略时，默认为1，即从左往右以步长1取值。“切取方向非常重要！”“切取方向非常重要！”“切取方向非常重要！”，重要的事情说三遍！ start_index：表示起始索引（包含该索引对应值）；该参数省略时，表示从对象“端点”开始取值，至于是从“起点”还是从“终点”开始，则由step参数的正负决定，step为正从“起点”开始，为负从“终点”开始。 end_index：表示终止索引（不包含该索引对应值）；该参数省略时，表示一直取到数据“端点”，至于是到“起点”还是到“终点”，同样由step参数的正负决定，step为正时直到“终点”，为负时直到“起点”。 3. Python切片操作详细例子​ 以下示例均以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例： >>>a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1. 切取单个元素>>>a[0] >>>0 >>>a[-4] >>>6 当索引只有一个数时，表示切取某一个元素。 2. 切取完整对象>>>a[:] #从左往右 >>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>a[::]#从左往右 >>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>a[::-1]#从右往左 >>> [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] 3. start_index和end_index全为正（+）索引的情况>>>a[1:6] >>> [1, 2, 3, 4, 5] step=1，从左往右取值，start_index=1到end_index=6同样表示从左往右取值。 >>>a[1:6:-1] >>> [] 输出为空列表，说明没取到数据。 step=-1，决定了从右往左取值，而start_index=1到end_index=6决定了从左往右取值，两者矛盾，所以为空。 >>>a[6:2] >>> [] 同样输出为空列表。 step=1，决定了从左往右取值，而start_index=6到end_index=2决定了从右往左取值，两者矛盾，所以为空。 >>>a[:6] >>> [0, 1, 2, 3, 4, 5] step=1，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值0开始一直取到end_index=6（该点不包括）。 >>>a[:6:-1] >>> [9, 8, 7] step=-1，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值9开始一直取到end_index=6（该点不包括）。 >>>a[6:] >>> [6, 7, 8, 9] step=1，从左往右取值，从start_index=6开始，一直取到“终点”值9。 >>>a[6::-1] >>> [6, 5, 4, 3, 2, 1, 0] step=-1，从右往左取值，从start_index=6开始，一直取到“起点”0。 4. start_index和end_index全为负（-）索引的情况>>>a[-1:-6] >>> [] step=1，从左往右取值，而start_index=-1到end_index=-6决定了从右往左取值，两者矛盾，所以为空。 索引-1在-6的右边（如上图） >>>a[-1:-6:-1] >>> [9, 8, 7, 6, 5] step=-1，从右往左取值，start_index=-1到end_index=-6同样是从右往左取值。 索引-1在6的右边（如上图） >>>a[-6:-1] >>> [4, 5, 6, 7, 8] step=1，从左往右取值，而start_index=-6到end_index=-1同样是从左往右取值。 索引-6在-1的左边（如上图） >>>a[:-6] >>> [0, 1, 2, 3] step=1，从左往右取值，从“起点”开始一直取到end_index=-6（该点不包括）。 >>>a[:-6:-1] >>> [9, 8, 7, 6, 5] step=-1，从右往左取值，从“终点”开始一直取到end_index=-6（该点不包括）。 >>>a[-6:] >>> [4, 5, 6, 7, 8, 9] step=1，从左往右取值，从start_index=-6开始，一直取到“终点”。 >>>a[-6::-1] >>> [4, 3, 2, 1, 0] step=-1，从右往左取值，从start_index=-6开始，一直取到“起点”。 5. start_index和end_index正（+）负（-）混合索引的情况>>>a[1:-6] >>> [1, 2, 3] start_index=1在end_index=-6的左边，因此从左往右取值，而step=1同样决定了从左往右取值，因此结果正确 >>>a[1:-6:-1] >>> [] start_index=1在end_index=-6的左边，因此从左往右取值，但step=-则决定了从右往左取值，两者矛盾，因此为空。 >>>a[-1:6] >>> [] start_index=-1在end_index=6的右边，因此从右往左取值，但step=1则决定了从左往右取值，两者矛盾，因此为空。 >>>a[-1:6:-1] >>> [9, 8, 7] start_index=-1在end_index=6的右边，因此从右往左取值，而step=-1同样决定了从右往左取值，因此结果正确。 6. 多层切片操作>>>a[:8][2:5][-1:] >>> [4] 相当于： a[:8]=[0, 1, 2, 3, 4, 5, 6, 7] a[:8][2:5]= [2, 3, 4] a[:8][2:5][-1:] = [4] 理论上可无限次多层切片操作，只要上一次返回的是非空可切片对象即可。 7. 切片操作的三个参数可以用表达式>>>a[2+1:3*2:7%3] >>> [3, 4, 5] 即：a[2+1:3*2:7%3] = a[3:6:1] 8. 其他对象的切片操作​ 前面的切片操作以list对象为例进行说明，但实际上可进行切片操作的数据类型还有很多，包括元组、字符串等等。 >>> (0, 1, 2, 3, 4, 5)[:3] >>> (0, 1, 2) 元组的切片操作 >>>'ABCDEFG'[::2] >>>'ACEG' 字符串的切片操作 >>>for i in range(1,100)[2::3][-5:]: print(i) >>>87 90 93 96 99 就是利用range()函数生成1-99的整数，然后从start_index=2（即3）开始以step=3取值，直到终点，再在新序列中取最后五个数。 4. 常用切片操作1.取偶数位置>>>b = a[::2] [0, 2, 4, 6, 8] 2.取奇数位置>>>b = a[1::2] [1, 3, 5, 7, 9] 3.拷贝整个对象>>>b = a[:] # >>>print(b) #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>print(id(a)) #41946376 >>>print(id(b)) #41921864 或 >>>b = a.copy() >>>print(b) #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>print(id(a)) #39783752 >>>print(id(b)) #39759176 需要注意的是：[:]和.copy()都属于“浅拷贝”，只拷贝最外层元素，内层嵌套元素则通过引用方式共享，而非独立分配内存，如果需要彻底拷贝则需采用“深拷贝”方式，如下例所示： >>>a = [1,2,['A','B']] >>>print('a={}'.format(a)) >>>b = a[:] >>>b[0] = 9 #修改b的最外层元素，将1变成9 >>>b[2][0] = 'D' #修改b的内嵌层元素 >>>print('a={}'.format(a)) >>>print('b={}'.format(b)) >>>print('id(a)={}'.format(id(a))) >>>print('id(b)={}'.format(id(b))) a=[1, 2, ['A', 'B']] #原始a a=[1, 2, ['D', 'B']] #b修改内部元素A为D后，a中的A也变成了D，说明共享内部嵌套元素，但外部元素1没变。 b=[9, 2, ['D', 'B']] #修改后的b id(a)=38669128 id(b)=38669192 4.修改单个元素>>>a[3] = ['A','B'] [0, 1, 2, ['A', 'B'], 4, 5, 6, 7, 8, 9] 5.在某个位置插入元素>>>a[3:3] = ['A','B','C'] [0, 1, 2, 'A', 'B', 'C', 3, 4, 5, 6, 7, 8, 9] >>>a[0:0] = ['A','B'] ['A', 'B', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 6.替换一部分元素>>>a[3:6] = ['A','B'] [0, 1, 2, 'A', 'B', 6, 7, 8, 9] 5. 总结 start_index、end_index、step三者可同为正、同为负，或正负混合。但必须遵循一个原则，即：当start_index表示的实际位置在end_index的左边时，从左往右取值，此时step必须是正数（同样表示从左往右）；当start_index表示的实际位置在end_index的右边时，表示从右往左取值，此时step必须是负数（同样表示从右往左），即两者的取值顺序必须相同。 当start_index或end_index省略时，取值的起始索引和终止索引由step的正负来决定，这种情况不会有取值方向矛盾（即不会返回空列表[]），但正和负取到的结果顺序是相反的，因为一个向左一个向右。 step的正负是必须要考虑的，尤其是当step省略时。比如a[-1:]，很容易就误认为是从“终点”开始一直取到“起点”，即a[-1:]= [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]，但实际上a[-1:]=[9]（注意不是9），原因在于step省略时step=1表示从左往右取值，而起始索引start_index=-1本身就是对象的最右边元素了，再往右已经没数据了，因此结果只含有9一个元素。 需要注意：“取单个元素（不带“:”）”时，返回的是对象的某个元素，其类型由元素本身的类型决定，而与母对象无关，如上面的a[0]=0、a[-4]=6，元素0和6都是“数值型”，而母对象a却是“list”型；“取连续切片（带“:”）”时，返回结果的类型与母对象相同，哪怕切取的连续切片只包含一个元素，如上面的a[-1:]=[9]，返回的是一个只包含元素“9”的list，而非数值型“9”。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Python","slug":"Python","permalink":"https://dataquaner.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dataquaner.github.io/tags/Python/"}]},{"title":"LeetCode数组系列之#88：合并两个有序数组","slug":"1.LeetCode刷题数组系列之#88：合并数组","date":"2020-04-25T08:16:16.000Z","updated":"2020-04-25T11:02:08.816Z","comments":true,"path":"2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/","link":"","permalink":"https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/","excerpt":"","text":"题目：合并两个有序数组难度：Easy题目描述： 题目​ 给你两个有序整数数组 nums1 和 nums2，请你将 nums2 合并到 nums1 中，使 nums1 成为一个有序数组。 说明: 初始化 nums1 和 nums2 的元素数量分别为 m 和 n 。 你可以假设 nums1 有足够的空间（空间大小大于或等于 m + n）来保存 nums2 中的元素。 示例:​ 输入:​ nums1 = [1,2,3,0,0,0], m = 3​ nums2 = [2,5,6], n = 3 ​ 输出: ​ [1,2,2,3,5,6] 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/merge-sorted-array著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 解题思路方法一：合并后排序直觉​ 最朴素的解法就是将两个数组合并之后再排序。 ​ 该算法只需要一行(Java是2行)，时间复杂度较差，为O((n + m)log(n + m))。这是由于这种方法没有利用两个数组本身已经有序这一点。 实现Python版class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -> None: \"\"\" Do not return anything, modify nums1 in-place instead. \"\"\" nums1[:] = sorted(nums1[:m] + nums2) Java版class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { System.arraycopy(nums2, 0, nums1, m, n); Arrays.sort(nums1); } } Scala版object Solution { def merge(nums1: Array[Int], m: Int, nums2: Array[Int], n: Int): Unit = { } } 复杂度分析 时间复杂度 : O((n+m)log(n+m)) 空间复杂度 : O(1) 方法二：双指针 / 从前往后直觉​ 一般而言，对于有序数组可以通过双指针法达到O(n+m)的时间复杂度。 最直接的算法实现是将指针p1 置为 nums1的开头， p2为 nums2的开头，在每一步将最小值放入输出数组中。 ​ 由于 nums1 是用于输出的数组，需要将nums1中的前m个元素放在其他地方，也就需要O(m) 的空间复杂度。 实现Python版class Solution(object): def merge(self, nums1, m, nums2, n): \"\"\" :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: void Do not return anything, modify nums1 in-place instead. \"\"\" # Make a copy of nums1. nums1_copy = nums1[:m] nums1[:] = [] # Two get pointers for nums1_copy and nums2. p1 = 0 p2 = 0 # Compare elements from nums1_copy and nums2 # and add the smallest one into nums1. while p1 &lt; m and p2 &lt; n: if nums1_copy[p1] &lt; nums2[p2]: nums1.append(nums1_copy[p1]) p1 += 1 else: nums1.append(nums2[p2]) p2 += 1 # if there are still elements to add if p1 &lt; m: nums1[p1 + p2:] = nums1_copy[p1:] if p2 &lt; n: nums1[p1 + p2:] = nums2[p2:] 复杂度分析 时间复杂度 : O(n + m) 空间复杂度 : O(m) 方法三 : 双指针 / 从后往前直觉​ 方法二已经取得了最优的时间复杂度O(n + m)，但需要使用额外空间。这是由于在从头改变nums1的值时，需要把nums1中的元素存放在其他位置。 ​ 如果我们从结尾开始改写 nums1 的值又会如何呢？这里没有信息，因此不需要额外空间。 这里的指针 p 用于追踪添加元素的位置。 实现Python版 class Solution(object): def merge(self, nums1, m, nums2, n): \"\"\" :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: void Do not return anything, modify nums1 in-place instead. \"\"\" # two get pointers for nums1 and nums2 p1 = m - 1 p2 = n - 1 # set pointer for nums1 p = m + n - 1 # while there are still elements to compare while p1 >= 0 and p2 >= 0: if nums1[p1] &lt; nums2[p2]: nums1[p] = nums2[p2] p2 -= 1 else: nums1[p] = nums1[p1] p1 -= 1 p -= 1 # add missing elements from nums2 nums1[:p2 + 1] = nums2[:p2 + 1] Java版 class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { // two get pointers for nums1 and nums2 int p1 = m - 1; int p2 = n - 1; // set pointer for nums1 int p = m + n - 1; // while there are still elements to compare while ((p1 >= 0) &amp;&amp; (p2 >= 0)) // compare two elements from nums1 and nums2 // and add the largest one in nums1 nums1[p--] = (nums1[p1] &lt; nums2[p2]) ? nums2[p2--] : nums1[p1--]; // add missing elements from nums2 System.arraycopy(nums2, 0, nums1, 0, p2 + 1); } } 复杂度 时间复杂度 : O(n + m) 空间复杂度 : O(1) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://dataquaner.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://dataquaner.github.io/tags/LeetCode/"}]},{"title":"数据倾斜问题总结","slug":"数据倾斜问题总结","date":"2020-04-22T14:14:00.000Z","updated":"2020-04-22T14:35:55.073Z","comments":true,"path":"2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/","excerpt":"","text":"0. 什么是数据倾斜 ​ 对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。 来源百度百科 ​ 对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 1. 数据倾斜的现象​ 多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。 2. 数据倾斜的影响1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：Out Of Memory。 2）运行速度慢 ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。 一个经验结论是：一般情况下，OOM的原因都是数据倾斜\\ 3. 如何定位数据倾斜​ 数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。 原因： 查看任务-》查看Stage-》查看代码 ​ 某个task执行特别慢的情况 ​ 某个task莫名其妙内存溢出的情况 ​ 查看导致数据倾斜的key的数据分布情况 也可从以下几种情况考虑： 1、是不是有OOM情况出现，一般是少数内存溢出的问题 2、是不是应用运行时间差异很大，总体时间很长 3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题 4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断 5、看看代码里面是否有一些导致Shuffle的算子出现 4. 数据倾斜的几种典型情况（重点） 数据源中的数据分布不均匀，Spark需要频繁交互 数据集中的不同Key由于分区方式，导致数据倾斜 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要） 聚合操作中，数据集中的数据分布不均匀（主要） JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀 数据集中少数几个key数据量很大，不重要，其他数据均匀 注意： 需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题 只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以 5. 数据倾斜的处理方法​ 发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。 5.1 检查数据，找出异常的key​ 如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个 df.select(\"key\").sample(false,0.1).(k=>(k,1)).reduceBykey(_+_).map(k=>(k._2,k._1)).sortByKey(false).take(10) ​ 如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。 经过分析，倾斜的数据主要有以下三种情况: null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。 无效数据，大量重复的测试数据或是对结果影响不大的有效数据。 有效数据，业务导致的正常数据分布。 解决办法 第1，2种情况，直接对数据进行过滤即可。 第3种情况则需要进行一些特殊操作，常见的有以下几种做法。 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。 对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。 使用reduceByKey 代替 groupByKey 使用map join。 举例：如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下： 将原始的 key 转化为 key + 随机值(例如Random.nextInt)对数据进行 reduceByKey(func)将 key + 随机值 转成 key再对数据进行 reduceByKey(func)tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。 tips2: 单独处理异常数据时，可以配合使用Map Join解决 5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互解决方案1：避免数据源的数据倾斜 实现原理：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive或者Kafka中还是会发生数据倾斜。 适用情况：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 总结：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题 5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜解决方案1：调整并行度 实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。 总结：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效 解决方案2： 缓解数据倾斜**（自定义Partitioner）** 适用场景：大量不同的Key被分配到了相同的Task造成该Task数据量过大。 解决方案： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。 优势： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。 劣势： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 5.2 检查Spark运行过程相关操作5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）解决方案：Reduce side Join转变为Map side Join 方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。 5.2.2 聚合操作中，数据集中的数据分布不均匀（主要）解决方案：两阶段聚合（局部聚合+全局聚合） 适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案 实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案 将相同key的数据分拆处理 5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀解决方案：为倾斜key增加随机前/后缀 适用场景：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。 解决方案：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。 优势：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。 劣势：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。 注意：具有倾斜Key的RDD数据集中，key的数量比较少 5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀解决方案：随机前缀和扩容RDD进行join 适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。 实现思路：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀） 5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀解决方案：过滤少数倾斜Key 适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Question","slug":"Data-Question","permalink":"https://dataquaner.github.io/categories/Data-Question/"}],"tags":[{"name":"数据倾斜","slug":"数据倾斜","permalink":"https://dataquaner.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"}]},{"title":"1.数据开发工程师面试题目必知必会","slug":"1.数据开发工程师面试题目必知必会","date":"2020-04-21T10:59:38.680Z","updated":"2020-04-21T10:59:38.680Z","comments":true,"path":"2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/","link":"","permalink":"https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/","excerpt":"","text":"面试题目梳理一. 数据结构和算法 LeetCode 合并数组 二元查找树转双向链表 二叉树层次遍历 堆 最小堆 排序算法 动态规划 青蛙跳台阶 贪心算法 字符串转换成整数 链表中倒数第K个结点 二维数组中的查找 替换空格 从尾到头打印链表 重建二叉树 用两个栈实现队列 斐波那契数列及变形题 二进制中1的个数 在O(1)时间删除链表结点 调整数组顺序使奇数位于偶数前面 反转链表 合并两个排序的链表 树的子结构 二叉树的镜像 顺时针打印矩阵 栈的压入、弹出序列 二叉搜索树的后序遍历序列 二叉树中和为某一值的路径 数组中出现次数超过一半的数字 最小的k个数 连续子数组的最大和 第一个只出现一次的字符 两个链表的第一个公共结点 链表中环的入口结点 二叉树的镜像 跳台阶 变态跳台阶 矩形覆盖 从上往下打印二叉树 二叉搜索树的第K个结点 二. 计算平台1. Hadoop 数据倾斜问题 hive开窗函数 hive UDF UDAFMapreduce原理 MR的Shuffle过程 Yarn的工作机制，以及MR Job提交运行过程 MapReduce1的工作机制和过程 HDFS写入过程 Fsimage 与 EditLog定义及合并过程 HDFS读过程 HDFS简介 在向HDFS中写数据的时候，当写某一副本时出错怎么处理？ namenode的HA实现 简述联邦HDFS HDFS源码解读–create() NameNode高可用中editlog同步的过程 HDFS写入过程客户端奔溃怎么处理？（租约恢复） 2. Hive Hive内部表与外部表的区别 Hive与传统数据库的区别 Hiverc文件 Hive分区 Hive分区过多有何坏处以及分区时的注意事项 Hive中复杂数据类型的使用好处与坏处 hive分桶？ Hive元数据库是用来做什么的，存储哪些信息？ 为何不使用Derby作为元数据库？ Hive什么情况下可以避免进行mapreduce？ Hive连接？ Hive MapJoin? Hive的sort by, order by, distribute by, cluster by区别？ Hadoop计算框架特性 Hive优化常用手段 数据倾斜整理(转) 使用Hive如何进行抽样查询？ 3. Spark Spark的运行模式 RDD是如何容错的？ Spark和MapReduce的区别 说一下Spark的RDD 自己实现一个RDD，需要实现哪些函数或者部分？ MapReduce和Spark的区别 Spark的Stage是怎么划分的？如何优化？ 宽依赖与窄依赖区别 Spark性能调优 Flink、Storm与Spark Stream的区别（未） 说下spark中的transform和action RDD、DataFrame和DataSet的区别 Spark执行任务流程（standalone、yarn） Spark的数据容错机制 Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？ Spark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？以及要注意的地方 driver的功能是什么？ spark端口 RDD有哪几种创建方式 map和flatmap的区别 Spark的基本工作流程 4. Flink4.1 Flink核心概念和基础 第一部分：Flink 中的核心概念和基础篇，包含了 Flink 的整体介绍、核心概念、算子等考察点。 第二部分：Flink 进阶篇，包含了 Flink 中的数据传输、容错机制、序列化、数据热点、反压等实际生产环境中遇到的问题等考察点。 第三部分：Flink 源码篇，包含了 Flink 的核心代码实现、Job 提交流程、数据交换、分布式快照机制、Flink SQL 的原理等考察点。 5. Storm：Storm的可靠性如何实现？包括spout和bolt两部分 怎么提高Storm的并发度？ Storm如何处理反压机制？ Storm中的Stream grouping有哪几种方式？ Storm的组件介绍 Storm怎么完成对单词的计数？ 简述Strom的计算结构 6. kafka：kafka介绍 Kafka与传统消息队列的区别？ kafka的零拷贝 kafka消息持久化和顺序读写？ 7. Kylin简介Kylin Kylin的工作原理 Kylin的技术框架 Cube、Cuboid 和 Cube Segment Kylin 对维度表的的要求 Cube的构建过程 全量构建和增量构建的区别 流式构建原理 三. 数据库 1）两大引擎Innodb引擎和MyIASM引擎， 2）mysql索引原理和底层实现BTREE、B+ TREE四. 数据仓库​ 1）拉链表​ 2）星型模型和雪花模型​ 3）维度建模过程 五. 操作系统 1）线程和进程，进程间的通信方式 2）死锁 3）内存分页 4）同步异步阻塞 六. 计算机网络 简述TCP和UDP的区别 七层协议每一层的任务及作用 简述http状态码 简述http协议与https协议 简述SSL协议 解析DNS过程 三次握手，四次挥手的过程？？为什么三握？ 七. Linux1. 比较常用Linux指令 1.1、ls/ll、cd、mkdir、rm-rf、cp、mv、ps -ef | grep xxx、kill、free-m、tar -xvf file.tar、（说那么十几二十来个估计差不多了） 2. 进程相关查看进程 2.1、ps -ef | grep xxx 2.2、ps -aux | grep xxx（-aux显示所有状态） 杀掉进程 3.1、kill -9[PID] —(PID用查看进程的方式查找) 4、启动/停止服务 4.1、cd到bin目录cd/ 4.2、./startup.sh –打开（先确保有足够的权限） 4.3、./shutdown.sh —关闭 5、查看日志 5.1、cd到服务器的logs目录（里面有xx.out文件） 5.2、tail -f xx.out –此时屏幕上实时更新日志。ctr+c停止 5.3、查看最后100行日志 tail -100 xx.out 5.4、查看关键字附件的日志。如：cat filename | grep -C 5 ‘关键字’（关键字前后五行。B表示前，A表示后，C表示前后） —-使用不多**** 5.5、还有vi查询啥的。用的也不多。 6、查看端口：（如查看某个端口是否被占用） 6.1、netstat -anp | grep 端口号（状态为LISTEN表示被占用） 7、查找文件 7.1、查找大小超过xx的文件： find . -type f -size +xxk —–(find . -type f -mtime -1 -size +100k -size-400k)–查区间大小的文件 7.2、通过文件名：find / -name xxxx —整个硬盘查找 其余的基本上不常用 8、vim（vi）编辑器 有命令模式、输入模式、末行模式三种模式。 命令模式：查找内容(/abc、跳转到指定行(20gg)、跳转到尾行(G)、跳转到首行(gg)、删除行(dd)、插入行(o)、复制粘贴(yy,p) 输入模式：编辑文件内容 末行模式：保存退出(wq)、强制退出(q!)、显示文件行号(set number) 在命令模式下，输入a或i即可切换到输入模式，输入冒号(:)即可切换到末行模式；在输入模式和末行模式下，按esc键切换到命令模式 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[]},{"title":"IDEA环境下Git使用总结","slug":"IDEA下使用git操作","date":"2020-04-20T13:40:00.000Z","updated":"2020-04-20T04:13:12.538Z","comments":true,"path":"2020/04/20/idea-xia-shi-yong-git-cao-zuo/","link":"","permalink":"https://dataquaner.github.io/2020/04/20/idea-xia-shi-yong-git-cao-zuo/","excerpt":"","text":"目录工作中多人使用版本控制软件协作开发，常见的应用场景归纳如下： 假设小组中有两个人，组长小张，组员小袁 [TOC] 场景一：小张创建项目并提交到远程Git仓库创建好项目，选择VCS - &gt; Import into Version Control -&gt; Create Git Repository 接下来指定本地仓库的位置，按个人习惯指定即可，例如这里选择了项目源代码同目录 点击OK后创建完成本地仓库，注意，这里仅仅是本地的。下面把项目源码添加到本地仓库。 下图是Git与提交有关的三个命令对应的操作，Add命令是把文件从IDE的工作目录添加到本地仓库的stage区，Commit命令把stage区的暂存文件提交到当前分支的仓库，并清空stage区。Push命令把本地仓库的提交同步到远程仓库。 IDEA中对操作做了一定的简化，Commit和Push可以在一步中完成。 具体操作，在项目上点击右键，选择Git菜单 因为是第一次提交，Push前需要指定远程仓库的地址。如下图，点击Define remote后，在弹出的窗口中输入远程仓库地址。 场景二：小袁从远程Git仓库上获取项目源码即克隆项目，操作如下： 输入小张Push时填写的远程仓库地址 接下来按向导操作，即可把项目从远程仓库克隆到本地仓库和IDE工作区。 场景三：小袁修改了部分源码，提交到远程仓库这个操作和首次提交的流程基本一致，分别是 Add -&gt; Commit -&gt; Push。请参考场景一 场景四：小张从远程仓库获取小袁的提交获取更新有两个命令：Fetch和Pull，Fetch是从远程仓库下载文件到本地的origin/master，然后可以手动对比修改决定是否合并到本地的master库。Push则是直接下载并合并。如果各成员在工作中都执行修改前先更新的规范，则可以直接使用Pull方式以简化操作。 场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发建分支也是一个常用的操作，例如临时修改bug、开发不确定是否加入的功能等，都可以创建一个分支，再等待合适的时机合并到主干。 创建流程如下： 选择New Branch并输入一个分支的名称 创建完成后注意IDEA的右下角，如下图，Git: wangpangzi_branch表示已经自动切换到wangpangzi_branch分支，当前工作在这个分支上。 点击后弹出一个小窗口，在Local Branches中有其他可用的本地分支选项，点击后选择Checkout即可切换当前工作的分支。 如下图，点击Checkout 注意，这里创建的分支仅仅在本地仓库，如果想让组长小张获取到这个分支，还需要提交到远程仓库。 场景六：小袁把分支提交到远程Git仓库切换到新建的分支，使用Push功能 场景七：小张获取小袁提交的分支使用Pull功能打开更新窗口，点击Remote栏后面的刷新按钮，会在Branches to merge栏中刷新出新的分支。这里并不想做合并，所以不要选中任何分支，直接点击Pull按钮完成操作。 更新后，再点击右下角，可以看到在Remote Branches区已经有了新的分支，点击后在弹出的子菜单中选择Checkout as new local branch，在本地仓库中创建该分支。完成后在Local Branches区也会出现该分支的选项，可以按上面的方法，点击后选择Checkout切换。 场景八：小张把分支合并到主干新功能开发完成，体验很好，项目组决定把该功能合并到主干上。 切换到master分支，选择Merge Changes 选择要合并的分支，点击Merge完成 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Git","slug":"Git","permalink":"https://dataquaner.github.io/categories/Git/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"https://dataquaner.github.io/tags/IDEA/"},{"name":"Git","slug":"Git","permalink":"https://dataquaner.github.io/tags/Git/"}]},{"title":"Flink面试问题梳理(基础+进阶+源码)","slug":"Flink面试问题梳理基础进阶源码","date":"2020-04-18T13:40:00.000Z","updated":"2020-04-18T14:39:18.053Z","comments":true,"path":"2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/","link":"","permalink":"https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/","excerpt":"","text":"第一部分：Flink 面试基础篇1. 简单介绍一下 Flink​ Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。 ​ Flink提供了诸多高抽象层的API以便用户编写分布式任务： DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。 DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。 Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。 ​ 此外，Flink 还针对特定的应用领域提供了领域库，例如：Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。 根据官网的介绍，Flink 的特性包含： 支持高吞吐、低延迟、高性能的流处理支持带有事件时间的窗口 （Window） 操作支持有状态计算的 Exactly-once 语义支持高度灵活的窗口 （Window） 操作，支持基于 time、count、session 以及 data-driven 的窗口操作支持具有 Backpressure 功能的持续流模型支持基于轻量级分布式快照（Snapshot）实现的容错一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理Flink 在 JVM 内部实现了自己的内存管理支持迭代计算支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存 2. Flink 相比传统的 Spark Streaming 有什么区别?​ 这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来： Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。 下面我们就分几个方面介绍两个框架的主要区别： [1] 架构模型​ Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。 [2] 任务调度​ Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。 ​ Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。 [3] 时间机制​ Spark Streaming 支持的时间机制有限，只支持处理时间。Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。 [4] 容错机制​ 对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。 ​ Flink 则使用两阶段提交协议来解决这个问题。 3. Flink 的组件栈有哪些？​ 根据 Flink 官网描述，Flink 是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。 图片来源于：https://flink.apache.org ​ 自下而上，每一层分别代表： Deploy 层：该层主要涉及了Flink的部署模式，在上图中我们可以看出，Flink 支持包括local、Standalone、Cluster、Cloud等多种部署模式。 Runtime 层：Runtime层提供了支持 Flink 计算的核心实现，比如：支持分布式 Stream 处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。 API层：API 层主要实现了面向流（Stream）处理和批（Batch）处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API，后续版本，Flink有计划将DataStream和DataSet API进行统一。 Libraries层：该层称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。 4. Flink 的运行必须依赖 Hadoop组件吗？​ Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadoop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点。 5. 你们的Flink集群规模多大？​ 大家注意，这个问题看起来是问你实际应用中的Flink集群规模，其实还隐藏着另一个问题：Flink可以支持多少节点的集群规模？ ​ 在回答这个问题时候，可以将自己生产环节中的集群规模、节点、内存情况说明，同时说明部署模式（一般是Flink on Yarn），除此之外，用户也可以同时在小集群（少于5个节点）和拥有 TB 级别状态的上千个节点上运行 Flink 任务。 6. Flink的基础编程模型了解吗？ ​ 上图是来自Flink官网的运行流程图。通过上图我们可以得知，Flink 程序的基本构建是数据输入来自一个 Source，Source 代表数据的输入端，经过 Transformation 进行转换，然后在一个或者多个Sink接收器中结束。数据流（stream）就是一组永远不会停止的数据记录流，而转换（transformation）是将一个或多个流作为输入，并生成一个或多个输出流的操作。执行时，Flink程序映射到 streaming dataflows，由流（streams）和转换操作（transformation operators）组成。 7. Flink集群有哪些角色？各自有什么作用？ ​ Flink 程序在运行时主要有 TaskManager，JobManager，Client三种角色。其中JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。 ​ TaskManager是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。 ​ Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。 8. 说说 Flink 资源管理中 Task Slot 的概念 ​ 在Flink架构角色中我们提到，TaskManager是实际负责执行计算的Worker，TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 Task Slot 的概念。 ​ 简单的说，TaskManager会将自己节点上管理的资源分为不同的Slot：固定大小的资源子集。这样就避免了不同Job的Task互相竞争内存资源，但是需要主要的是，Slot只会做内存的隔离。没有做CPU的隔离。 9. 说说 Flink 的常用算子？​ Flink 最常用的常用算子包括：Map：DataStream → DataStream，输入一个参数产生一个参数，map的功能是对输入的参数进行转换操作。Filter：过滤掉指定条件的数据。KeyBy：按照指定的key进行分组。Reduce：用来进行结果汇总合并。Window：窗口函数，根据某些特性将每个key的数据进行分组（例如：在5s内到达的数据） 10. 说说你知道的Flink分区策略？​ 什么要搞懂什么是分区策略。分区策略是用来决定数据如何发送至下游。目前 Flink 支持了8中分区策略的实现。 上图是整个Flink实现的分区策略继承图： GlobalPartitioner数据会被分发到下游算子的第一个实例中进行处理。 ShufflePartitioner数据会被随机分发到下游算子的每一个实例中进行处理。 RebalancePartitioner数据会被循环发送到下游的每一个实例中进行处理。 RescalePartitioner这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为2，编号为A和B。下游并行度为4，编号为1，2，3，4。那么A则把数据循环发送给1和2，B则把数据循环发送给3和4。假设上游并行度为4，编号为A，B，C，D。下游并行度为2，编号为1，2。那么A和B则把数据发送给1，C和D则把数据发送给2。 BroadcastPartitioner广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做join的场景。 ForwardPartitioner 用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。 KeyGroupStreamPartitioner Hash分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。 CustomPartitionerWrapper用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑。 11. Flink的并行度了解吗？Flink的并行度设置是怎样的？​ Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。这些并行实例的数量被称为并行度。 ​ 我们在实际生产环境中可以从四个不同层面设置并行度： 操作算子层面(Operator Level) 执行环境层面(Execution Environment Level) 客户端层面(Client Level) 系统层面(System Level) 需要注意的优先级：算子层面&gt;环境层面&gt;客户端层面&gt;系统层面。 12. Flink的Slot和parallelism有什么区别？官网上十分经典的图： slot是指taskmanager的并发执行能力，假设我们将 taskmanager.numberOfTaskSlots 配置为3那么每一个 taskmanager 中分配3个 TaskSlot, 3个 taskmanager 一共有9个TaskSlot。 parallelism是指taskmanager实际使用的并发能力。假设我们把 parallelism.default 设置为1，那么9个 TaskSlot 只能用1个，有8个空闲。 13. Flink有没有重启策略？说说有哪几种？Flink 实现了多种重启策略。 固定延迟重启策略（Fixed Delay Restart Strategy） 故障率重启策略（Failure Rate Restart Strategy） 没有重启策略（No Restart Strategy） Fallback重启策略（Fallback Restart Strategy） 14. 用过Flink中的分布式缓存吗？如何使用？​ Flink实现的分布式缓存和Hadoop有异曲同工之妙。目的是在本地读取文件，并把他放在 taskmanager 节点中，防止task重复拉取。 val env = ExecutionEnvironment.getExecutionEnvironment // register a file from HDFS env.registerCachedFile(\"hdfs:///path/to/your/file\", \"hdfsFile\") // register a local executable file (script, executable, ...) env.registerCachedFile(\"file:///path/to/exec/file\", \"localExecFile\", true) // define your program and execute ... val input: DataSet[String] = ... val result: DataSet[Integer] = input.map(new MyMapper()) ... env.execute() 15. 说说Flink中的广播变量，使用时需要注意什么？​ 我们知道Flink是并行的，计算过程可能不在一个 Slot 中进行，那么有一种情况即：当我们需要访问同一份数据。那么Flink中的广播变量就是为了解决这种情况。 ​ 我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。 16. 说说Flink中的窗口？​ 来一张官网经典的图： ​ Flink 支持两种划分窗口的方式，按照time和count。如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window。 ​ flink支持窗口的两个重要属性（size和interval） ​ 如果size=interval,那么就会形成tumbling-window(无重叠数据)如果size&gt;interval,那么就会形成sliding-window(有重叠数据)如果size&lt; interval, 那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。 ​ 通过组合可以得出四种基本窗口： time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5)) time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3)) count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5) count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3) 17. 说说Flink中的状态存储？​ Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和 checkpoint 交互。 ​ Flink提供了三种状态存储方式：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。 18. Flink 中的时间有哪几类​ Flink 中的时间和其他流式计算系统的时间一样分为三类：事件时间，摄入时间，处理时间三种。 如果以 EventTime 为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime。 如果以 IngesingtTime 为基准来定义时间窗口将形成 IngestingTimeWindow,以 source 的systemTime为准。 如果以 ProcessingTime 基准来定义时间窗口将形成 ProcessingTimeWindow，以 operator 的systemTime 为准。 19. Flink 中水印是什么概念，起到什么作用？​ Watermark 是 Apache Flink 为了处理 EventTime 窗口计算提出的一种机制, 本质上是一种时间戳。一般来讲Watermark经常和Window一起被用来处理乱序事件。 20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用​ TableEnvironment是Table API和SQL集成的核心概念。 这个类主要用来： 在内部catalog中注册表 注册外部catalog 执行SQL查询 注册用户定义（标量，表或聚合）函数 将DataStream或DataSet转换为表 持有对ExecutionEnvironment或StreamExecutionEnvironment的引用 21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？​ 首先大家要知道 Flink 的SQL解析是基于Apache Calcite这个开源框架。 基于此，一次完整的SQL解析过程如下： 用户使用对外提供Stream SQL的语法开发业务应用 用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划 采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划 对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行 第二部分：Flink 面试进阶篇1. Flink是如何支持批流一体的？ ​ 本道面试题考察的其实就是一句话：Flink的开发者认为批处理是流处理的一种特殊情况。批处理是有限的流处理。Flink 使用一个引擎支持了DataSet API 和 DataStream API。 2. Flink是如何做到高效的数据交换的？​ 在一个Flink Job中，数据需要在不同的task中进行交换，整个数据交换是有 TaskManager 负责的，TaskManager 的网络组件首先从缓冲buffer中收集records，然后再发送。Records 并不是一个一个被发送的，二是积累一个批次再发送，batch 技术可以更加高效的利用网络资源。 3. Flink是如何做容错的？​ Flink 实现容错主要靠强大的CheckPoint机制和State机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态。 4. Flink 分布式快照的原理是什么？​ Flink的分布式快照是根据Chandy-Lamport算法量身定做的。简单来说就是持续创建分布式数据流及其状态的一致快照。 ​ 核心思想是在 input source 端插入 barrier，控制 barrier 的同步来实现 snapshot 的备份和 exactly-once 语义。 5. Flink 是如何保证Exactly-once语义的？​ Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。分为以下几个步骤： 开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面 预提交（preCommit）将内存中缓存的数据写入文件并关闭 正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟 丢弃（abort）丢弃临时文件 ​ 若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。 6. Flink 的 kafka 连接器有什么特别的地方？​ Flink源码中有一个独立的connector模块，所有的其他connector都依赖于此模块，Flink 在1.9版本发布的全新kafka连接器，摒弃了之前连接不同版本的kafka集群需要依赖不同版本的connector这种做法，只需要依赖一个connector即可。 7. 说说 Flink的内存管理是如何做的?​ Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。 理论上Flink的内存管理分为三部分： Network Buffers：这个是在TaskManager启动的时候分配的，这是一组用于缓存网络数据的内存，每个块是32K，默认分配2048个，可以通过“taskmanager.network.numberOfBuffers”修改 Memory Manage pool：大量的Memory Segment块，用于运行时的算法（Sort/Join/Shuffle等），这部分启动的时候就会分配。下面这段代码，根据配置文件中的各种参数来计算内存的分配方法。（heap or off-heap，这个放到下节谈），内存的分配支持预分配和lazy load，默认懒加载的方式。 User Code，这部分是除了Memory Manager之外的内存用于User code和TaskManager本身的数据结构。 8. 说说 Flink的序列化如何做的?​ Java本身自带的序列化和反序列化的功能，但是辅助信息占用空间比较大，在序列化对象时记录了过多的类信息。 ​ Apache Flink摒弃了Java原生的序列化方法，以独特的方式处理数据类型和序列化，包含自己的类型描述符，泛型类型提取和类型序列化框架。 ​ TypeInformation 是所有类型描述符的基类。它揭示了该类型的一些基本属性，并且可以生成序列化器。TypeInformation 支持以下几种类型： BasicTypeInfo: 任意Java 基本类型或 String 类型 BasicArrayTypeInfo: 任意Java基本类型数组或 String 数组 WritableTypeInfo: 任意 Hadoop Writable 接口的实现类 TupleTypeInfo: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现 CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples) PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法 GenericTypeInfo: 任意无法匹配之前几种类型的类 ​ 针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。 9. Flink中的Window出现了数据倾斜，你有什么解决办法？​ window产生数据倾斜指的是数据在不同的窗口内堆积的数据量相差过多。本质上产生这种情况的原因是数据源头发送的数据量速度不同导致的。出现这种情况一般通过两种方式来解决： 在数据进入窗口前做预聚合 重新设计窗口聚合的key 10. Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？​ 数据倾斜和数据热点是所有大数据框架绕不过去的问题。处理这类问题主要从3个方面入手： 在业务上规避这类问题 ​ 例如一个假设订单场景，北京和上海两个城市订单量增长几十倍，其余城市的数据量不变。这时候我们在进行聚合的时候，北京和上海就会出现数据堆积，我们可以单独数据北京和上海的数据。 Key的设计上 ​ 把热key进行拆分，比如上个例子中的北京和上海，可以把北京和上海按照地区进行拆分聚合。 参数设置 ​ Flink 1.9.0 SQL(Blink Planner) 性能优化中一项重要的改进就是升级了微批模型，即 MiniBatch。原理是缓存一定的数据后再触发处理，以减少对State的访问，从而提升吞吐和减少数据的输出量。 11. Flink任务延迟高，想解决这个问题，你会如何入手？​ 在Flink的后台任务管理中，我们可以看到Flink的哪个算子和task出现了反压。最主要的手段是资源调优和算子调优。资源调优即是对作业中的Operator的并发数（parallelism）、CPU（core）、堆内存（heap_memory）等参数进行调优。作业参数调优包括：并行度的设置，State的设置，checkpoint的设置。 12. Flink是如何处理反压的？​ Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。下游消费者消费变慢，上游就会受到阻塞。 13. Flink的反压和Storm有哪些不同？​ Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。 ​ Flink中的反压使用了高效有界的分布式阻塞队列，下游消费变慢会导致发送端阻塞。 ​ 二者最大的区别是Flink是逐级反压，而Storm是直接从源头降速。 14. Operator Chains（算子链）这个概念你了解吗？​ 为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。这就是我们所说的算子链。 15. Flink什么情况下才会把Operator chain在一起形成算子链？​ 两个operator chain在一起的的条件： 上下游的并行度一致 下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入） 上下游节点都在同一个 slot group 中（下面会解释 slot group） 下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS） 上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD） 两个节点间数据分区方式是 forward（参考理解数据流的分区） 用户没有禁用 chain 16. 说说Flink1.9的新特性？ 支持hive读写，支持UDF Flink SQL TopN和GroupBy等优化 Checkpoint跟savepoint针对实际业务场景做了优化 Flink state查询 17. 消费kafka数据的时候，如何处理脏数据？ 可以在处理前加一个fliter算子，将不符合规则的数据过滤出去。 第三部分：Flink 面试源码篇1. Flink Job的提交流程​ 用户提交的Flink Job会被转化成一个DAG任务运行，分别是：StreamGraph、JobGraph、ExecutionGraph，Flink中JobManager与TaskManager，JobManager与Client的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。 2. Flink所谓”三层图”结构是哪几个”图”？ 一个Flink任务的DAG生成计算图大致经历以下三个过程： StreamGraph最接近代码所表达的逻辑层面的计算拓扑结构，按照用户代码的执行顺序向StreamExecutionEnvironment添加StreamTransformation构成流式图。 JobGraph从StreamGraph生成，将可以串联合并的节点进行合并，设置节点之间的边，安排资源共享slot槽位和放置相关联的节点，上传任务所需的文件，设置检查点配置等。相当于经过部分初始化和优化处理的任务图。 ExecutionGraph由JobGraph转换而来，包含了任务具体执行所需的内容，是最贴近底层实现的执行图。 3. JobManger在集群中扮演了什么角色？​ JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。 ​ JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务管理和资源管理。 ​ JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。 ​ 同时在任务执行的过程中，Flink JobManager 会触发 Checkpoint 操作，每个 TaskManager 节点 收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Fink JobManager 中完成。 ​ 当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。 4. JobManger在集群启动过程中起到什么作用？​ JobManager的职责主要是接收Flink作业，调度Task，收集作业状态和管理TaskManager。它包含一个Actor，并且做如下操作： RegisterTaskManager: 它由想要注册到JobManager的TaskManager发送。注册成功会通过AcknowledgeRegistration消息进行Ack。 SubmitJob: 由提交作业到系统的Client发送。提交的信息是JobGraph形式的作业描述信息。 CancelJob: 请求取消指定id的作业。成功会返回CancellationSuccess，否则返回CancellationFailure。 UpdateTaskExecutionState: 由TaskManager发送，用来更新执行节点(ExecutionVertex)的状态。成功则返回true，否则返回false。 RequestNextInputSplit: TaskManager上的Task请求下一个输入split，成功则返回NextInputSplit，否则返回null。 JobStatusChanged： 它意味着作业的状态(RUNNING, CANCELING, FINISHED,等)发生变化。这个消息由ExecutionGraph发送。 5. TaskManager在集群中扮演了什么角色？​ TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。 ​ 客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。 ​ TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。 ​ 可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进行的方式有很大的区别，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。 6. TaskManager在集群启动过程中起到什么作用？​ TaskManager的启动流程较为简单：启动类：org.apache.flink.runtime.taskmanager.TaskManager核心启动方法 ： selectNetworkInterfaceAndRunTaskManager启动后直接向JobManager注册自己，注册完成后，进行部分模块的初始化。 7. Flink 计算资源的调度是如何实现的？​ TaskManager中最细粒度的资源是Task slot，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。 ​ 通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。 ​ 而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot： 8. 简述Flink的数据抽象及数据交换过程？​ Flink 为了避免JVM的固有缺陷例如java对象存储密度低，FGC影响吞吐和响应等，实现了自主管理内存。MemorySegment就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。 ​ 在MemorySegment这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是Buffer。 ​ 对接从Java对象转为Buffer的中间对象是另一个抽象StreamRecord。 9. Flink 中的分布式快照机制是如何实现的？​ Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。 ​ barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。 ​ 然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。 一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。在所有sink确认快照后，意味快照着已完成。 ​ 一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。 10. 简单说说FlinkSQL的是如何实现的？​ Flink 将 SQL 校验、SQL 解析以及 SQL 优化交给了Apache Calcite。Calcite 在其他很多开源项目里也都应用到了，譬如 Apache Hive, Apache Drill, Apache Kylin, Cascading。Calcite 在新的架构中处于核心的地位，如下图所示。 构建抽象语法树的事情交给了 Calcite 去做。SQL query 会经过 Calcite 解析器转变成 SQL 节点树，通过验证后构建成 Calcite 的抽象语法树（也就是图中的 Logical Plan）。另一边，Table API 上的调用会构建成 Table API 的抽象语法树，并通过 Calcite 提供的 RelBuilder 转变成 Calcite 的抽象语法树。然后依次被转换成逻辑执行计划和物理执行计划。 在提交任务后会分发到各个 TaskManager 中运行，在运行时会使用 Janino 编译器编译代码后运行。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/tags/Flink/"},{"name":"面试必备","slug":"面试必备","permalink":"https://dataquaner.github.io/tags/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87/"}]},{"title":"机器学习系列之决策树算法（03）：决策树的剪枝","slug":"机器学习系列之决策树算法（03）：决策树的剪枝","date":"2020-04-11T11:32:09.079Z","updated":"2020-04-11T11:32:09.079Z","comments":true,"path":"2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/","excerpt":"","text":"1. 前言上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现过拟合现象。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为剪枝(pruning)。 具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss funtion)或代价函数(cost funtion)来实现。 2.剪枝算法2.1 为什么要剪枝现象 接上一次讲的生成决策树，下面给出一张图。 横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。 实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。 可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。 原因 原因1：噪声、样本冲突，即错误的样本数据。 原因2：特征即属性不能完全作为分类标准。 原因3：巧合的规律性，数据量不够大。 这个时候，就需要对生成树进行修剪，也就是剪枝。 2.2 如何进行剪枝预剪枝预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法: (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。 (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。 (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。 (4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。 优点&amp;缺点 由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【怎样精确地估计何时停止树的增长是相当困难的】。 预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。 后剪枝后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。 这里主要介绍四种： REP-错误率降低剪枝 PEP-悲观剪枝 CCP-代价复杂度剪枝 MEP-最小错误剪枝 REP(Reduced Error Pruning)方法 对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S 优点： REP 是当前最简单的事后剪枝方法之一。 它的计算复杂性是线性的。 和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。 缺点： 但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意. PEP(Pessimistic Error Pruning)方法 为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, PEP 方法对误差估计增加了连续性校正(continuity correction)。关于PEP方法的数据解释待后续开专题梳理。 优点： PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一 PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。 TIPS： 个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。 MEP(Minimum Error Pruning)方法 MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。 优点： MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集. 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： 类别平均分配的前提假设现实几率不大&amp;对K太过敏感 对此，也有改进算法，我没有深入研究。 CCP(Cost-Complexity Pruning)方法 CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤: (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。 ​ (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。 缺点： 生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的. 对比四种方法 剪枝名称 剪枝方式 计算复杂度 误差估计 REP 自底向上 0(n) 剪枝集上误差估计 PEP 自顶向下 o(n) 使用连续纠正 CCP 自底向上 o(n2) 标准误差 MEP 自底向上 o(n) 使用连续纠正 ① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。 ② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。 ③ 如果数据集多（REP&amp;CCP←复杂但树小） ④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定） 3.总结决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。 剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。 预剪枝： 预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。 简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。 这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。 后剪枝： 后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。 同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。 后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。 剪枝方法的选择 如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。 后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。 有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。 经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。 无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。 函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点 函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。 函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值 上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"机器学习系列之决策树算法（02）：决策树的生成","slug":"机器学习系列之决策树算法（02）：决策树的生成","date":"2020-04-11T11:31:53.052Z","updated":"2020-04-11T11:31:53.052Z","comments":true,"path":"2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/","excerpt":"","text":"1. 前言上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。 2. 决策树的生成 示例数据表格 文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中 2.1 相关概念阐述2.1.1 决策树 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。 2.1.2 ID3、C4.5、CART在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【机器学习系列之决策树算法（01）：决策树特征选择】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。 信息熵 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。 ​ Entopy(t)=-Σp(i|t)log2p(i|t) 信息增益信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为： ​ Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di) 信息增益率 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7： ​ H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) 基尼系数 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下基尼系数 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低. CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分. 基尼系数的计算公式如下: ​ Gini = 1 – Σ (Pi)2 for i=1 to number of classes 2.2 完整生成过程 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例 2.2.1 根节点的选择 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为： ​ Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985 天气 其数据表格如下: 信息增益计算如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下： D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981 D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为： 3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965 其信息增益为： Gain(天气)=0.985 - 0.965 = 0.020 信息增益率计算 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为： H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556 则其信息增益率为： Gain_ratio(天气)=0.020/1.556=0.012 基尼系数计算 Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9 Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21 温度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 作为子节点的归一化信息熵为： 4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857 其信息增益为： Gain(温度)=0.985 - 0.857 = 0.128 信息增益率计算 属性熵为： H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378 则其信息增益率为： Gain_ratio(温度)=0.128/1.378=0.0928 基尼系数计算 Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 4/7 * 1.0 + 3/7 * 0.918 = 0.964 其信息增益为： Gain(湿度)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(湿度)=0.021/0.985=0.021 基尼系数计算 Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 作为子节点的归一化信息熵为： 3/7 * 1.0 + 4/7 * 0.918 = 0.964 其信息增益为： Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数计算 Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 根节点的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度 信息增益 Gain(天气)=0.985 - 0.965 = 0.020 Gain(温度)=0.985 - 0.857 = 0.128 Gain(湿度)=0.985 - 0.964 = 0.021 Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率 Gain_ratio(天气)=0.020/1.556=0.012 Gain_ratio(温度)=0.128/1.378=0.0928 Gain_ratio(湿度)=0.021/0.985=0.021 Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择: 根节点 子节点温度高:(待进一步进行选择) 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.2 子节点温度高的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： ​ Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 归一化信息熵为： 2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5 其信息增益为： Gain(天气)=1.0 - 0.5 = 0.5 信息增益率计算 对应的属性熵为： H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5 则其信息增益率为： Gain_ratio(天气)=0.5/1.5=0.33333 基尼系数计算 Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0 D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0 作为子节点的归一化信息熵为： 2/4 * 0.0 + 2/4 * 0.0 = 0.0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0 Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0 D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 1/4 * 0.0 + 3/4 * 0.918 = 0.688 其信息增益为： Gain(刮风)=1.0 - 0.688 = 0.312 信息增益率计算 属性熵为： H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918 则其信息增益率为： Gain_ratio(刮风)=0.312/0.918=0.349 基尼系数计算 Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 子节点温度高的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度 Gain(天气)=1.0 - 0.5 = 0.5 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 0.688 = 0.312 Gain_ratio(天气)=0.5/1.5=0.33333 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0.312/0.918=0.349 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.3 子节点温度中的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D (阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(天气)=1.0 - 0 = 1.0 信息增益率计算 对应的属性熵为： H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(天气)=1.0/1.0=1.0 基尼系数计算 Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 作为子节点的归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 作为子节点的归一化信息熵为： 1/1 * 1.0 = 1.0 其信息增益为： Gain(刮风)=1.0 - 1.0 = 0 信息增益率计算 属性熵为： H(刮风)=-(2/2 * log2(2/2) = 0.0 则其信息增益率为： Gain_ratio(刮风)=0/0 = 0 基尼系数计算 Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5 Gini(刮风)=2/2 * 0.5 = 0.5 子节点温度中的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧 Gain(天气)=1.0 - 0 = 1.0 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 1.0 = 0 Gain_ratio(天气)=1.0/1.0=1.0 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0/0 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(刮风)=2/2 * 0.5 = 0.5 确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.4 最终的决策树 在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树： 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 3. 思考 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？ 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"数据存储之MySQL系列（01）：MySQL体系结构","slug":"数据存储之MySQL系列（01）：MySQL体系结构","date":"2020-04-11T11:31:10.255Z","updated":"2020-04-11T11:31:10.255Z","comments":true,"path":"2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"DataBase","slug":"DataBase","permalink":"https://dataquaner.github.io/categories/DataBase/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dataquaner.github.io/tags/MySQL/"}]},{"title":"xgboost算法模型输出的解释","slug":"xgboost算模型输出的解释","date":"2020-04-11T11:30:52.923Z","updated":"2020-04-11T11:30:52.923Z","comments":true,"path":"2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/","excerpt":"","text":"1. 问题描述 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。 2. 数据集 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据. from sklearn import datasets iris = datasets.load_iris() data = iris.data[:100] print data.shape #(100L, 4L) #一共有100个样本数据, 维度为4维 label = iris.target[:100] print label #正好选取label为0和1的数据 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 3. 训练集与测试集from sklearn.cross_validation import train_test_split train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=0) 4. Xgboost建模4.1 模型初始化设置import xgboost as xgb dtrain=xgb.DMatrix(train_x,label=train_y) dtest=xgb.DMatrix(test_x) params={'booster':'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'auc', 'max_depth':4, 'lambda':10, 'subsample':0.75, 'colsample_bytree':0.75, 'min_child_weight':2, 'eta': 0.025, 'seed':0, 'nthread':8, 'silent':1} watchlist = [(dtrain,'train')] 4.2 建模与预测bst=xgb.train(params,dtrain,num_boost_round=100,evals=watchlist) ypred=bst.predict(dtest) # 设置阈值, 输出一些评价指标 y_pred = (ypred >= 0.5)*1 from sklearn import metrics print 'AUC: %.4f' % metrics.roc_auc_score(test_y,ypred) print 'ACC: %.4f' % metrics.accuracy_score(test_y,y_pred) print 'Recall: %.4f' % metrics.recall_score(test_y,y_pred) print 'F1-score: %.4f' %metrics.f1_score(test_y,y_pred) print 'Precesion: %.4f' %metrics.precision_score(test_y,y_pred) metrics.confusion_matrix(test_y,y_pred) Out[23]: AUC: 1.0000 ACC: 1.0000 Recall: 1.0000 F1-score: 1.0000 Precesion: 1.0000 array([[13, 0], [ 0, 12]], dtype=int64) Yeah, 完美的模型, 完美的预测! 4.3 可视化输出#对于预测的输出有三种方式 ?bst.predict Signature: bst.predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False) pred_leaf : bool When this option is on, the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree. Note that the leaf index of a tree is unique per tree, so you may find leaf 1 in both tree 1 and tree 0. pred_contribs : bool When this option is on, the output will be a matrix of (nsample, nfeats+1) with each record indicating the feature contributions (SHAP values) for that prediction. The sum of all feature contributions is equal to the prediction. Note that the bias is added as the final column, on top of the regular features. 4.3.1 得分默认的输出就是得分, 这没什么好说的, 直接上code. ypred = bst.predict(dtest) ypred Out[32]: array([ 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.20081411], dtype=float32) 在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出. 4.3.2 所属的叶子节点当设置pred_leaf=True的时候, 这时就会输出每个样本在所有树中的叶子节点 ypred_leaf = bst.predict(dtest, pred_leaf=True) ypred_leaf Out[33]: array([[1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1]]) 输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以ypred_leaf的维度为[100*100]. 对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树). 那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型. xgb.to_graphviz(bst, num_trees=0) #可视化第一棵树的生成情况 #直接输出模型的迭代工程 bst.dump_model(\"model.txt\") booster[0]: 0:[f3&lt;0.75] yes=1,no=2,missing=1 1:leaf=-0.019697 2:leaf=0.0214286 booster[1]: 0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0212184 2:leaf=0.0212 booster[2]: 0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0197404 2:leaf=0.0197235 booster[3]: …… 通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值. 在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值. 同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值. leaf1 -1.381214 leaf2 1.410950 在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了'objective': 'binary:logistic', 因此使用函数将累加的值转换为实际的打分: f(x)=1/(1+exp(−x)) 1/float(1+np.exp(1.38121416)) Out[24]: 0.20081407112186503 1/float(1+np.exp(-1.410950)) Out[25]: 0.8039157403338895 这就与ypred = bst.predict(dtest) 的分值相对应上了. 4.3.2 特征重要性接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性. ypred_contribs = bst.predict(dtest, pred_contribs=True) ypred_contribs Out[37]: array([[ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663]], dtype=float32) 输出的ypred_contribs的维度为[100,5], 通过阅读前面的文档注释就可以知道, 最后一列是bias, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的. 通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例. score_a = sum(ypred_contribs[0]) print score_a # -1.38121373579 score_b = sum(ypred_contribs[1]) print score_b # 1.41094945744 相同的分值, 相同的处理情况. 到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"LightGBM算法基础系列之基础理论篇（1）","slug":"LightGBM算法基础系列之基础理论篇（1）","date":"2020-04-11T11:30:37.965Z","updated":"2020-04-11T11:30:37.965Z","comments":true,"path":"2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/","excerpt":"","text":"这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"零基础自学人工智能路径规划，附资源，亲身经验","slug":"零基础自学人工智能路径规划附资源亲身经验","date":"2020-04-11T01:25:00.000Z","updated":"2020-04-11T12:08:44.555Z","comments":true,"path":"2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/","excerpt":"","text":"0. 前言下面的每个资源都是我亲身学过的，且是网上公开公认最优质的资源。 下面的每个学习步骤也是我一步步走过来的。希望大家以我为参考，少走弯路。 请大家不要浪费时间找非常多的资料，只看最精华的！ 综述，机器学习的自学简单来说分为三个步骤 前期：知识储备包括数学知识，机器学习经典算法知识，编程技术（python）的掌握 中期：算法的代码实现 后期：实战水平提升 机器学习路径规划图 1. 数学基础很多人看到数学知识的时候就望而却步，数学是需要的，但是作为入门水平，对数学的要求没有那么的高。假设你上过大学的数学课（忘了也没事），需要的数学知识啃一啃还是基本能理解下来的。 1.1 数学内容线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然(MLE)和最大后验估计(MAP)等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的如梯度下降、牛顿法等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等 上面的看不太懂没事，不是特别难，学习一下就能理解了。 1.2 数学资源网上有很多人会列举大量大量的课程资源，这是非常不负责任的事，学完那些我头发都得白了。实际上，我们只需要学习其中的一部分就够了。 1.2.1 吴恩达的斯坦福大学机器学习王牌课程CS229课后就有对学生数学知识的要求和补充，这些数学知识是完全符合机器学习要求的，不多也不少。墙裂推荐要看，不过只有英文版的。 链接：https://pan.baidu.com/s/1NrCAW38C9lXFqPwqTlrVRA密码：3k3m 1.2.2 深度学习的三大开山鼻祖之一Yoshua Bengio写的深度学习（包含了机器学习）领域的教科书现在以开源的形式在网上公开。这部书被誉为深度学习的圣经。在这里我们只看这本书的第一部分，也就是数学基础。囊括了机器学习所需的所有必备数学基础，而且是从最基础的说起，也不多，必读的。 链接：https://pan.baidu.com/s/1GmmbqFewyCuEA7blXNC-7g密码：6qqm 1.2.3 跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。链接：https://zhuanlan.zhihu.com/p/25197792，知乎专栏上的一篇好文章，囊括了所有的应用知识点。 好了，数学方面我只推荐上面三个资源，三个都是必看的。里面很多可能你现在看不太懂，没关系。先大概过一遍，知道自己的数学水平在哪。在看到算法知识的时候，不懂的再回来补就好。后期需要更多的数学资料我会再更新的。 2 编程技术编程语言：python3.5及以上，python易学，这个这期先不细讲。 3 经典算法知识算法包括机器学习和深度学习，机器学习是深度学习的基础。所以务必先学机器学习的经典算法，再学深度学习的算法。 3.1 机器学习3.1.1 课程资料首推吴恩达的CS229，经典中的经典，在网易公开课里有视频，翻译，课程讲义，笔记是非常非常完备的。墙裂推荐。这个课程对数学有一定的要求，但我觉得只要你上过大学的数学，然后补一下上面的数学，完全可以直接来看这个CS229。 假设你的数学真的很差的话，怎么办？吴恩达在coursera上也开了一门跟CS229完全匹配的课程，coursera机器学习课。这门课是CS229的翻版，唯一不同的是它对数学基本是没有要求了，如果你对数学真的不懂的话，那就先看这个的教程吧。它跟CS229的关系就是同样的广度，但是深度浅很多，不过你学完coursera还是要回过头来看CS229的。这个也是免费的。 CS229课程视频：http://open.163.com/special/opencourse/machinelearning.html 课程讲义和中文笔记：https://pan.baidu.com/s/1MC_yWjcz_m5YoZFNBcsRSQ密码：6rw6 3.1.2 配套书籍：机器学习实战，必看。用代码实现了一遍各大经典机器学习算法，必须看，对你理解算法有很大帮助，同时里面也有应用。如果大家看上面纯理论的部分太枯燥了，就可以来看看这本书来知道在现实中机器学习算法是怎么应用的，会很大程度提升你的学习兴趣，当初我可是看了好几遍。 书籍及课后代码：链接：https://pan.baidu.com/s/15XtFOH18si316076GLKYfg密码：sawb 李航《统计学习方法》，配合着看 链接：https://pan.baidu.com/s/1Mk_O71k-H8GHeaivWbzM-Q密码：adep，配合着看 周志华《机器学习》，机器学习的百科全书，配合着看。 链接：https://pan.baidu.com/s/1lJoQnWToonvBU6cYwjrRKg密码：7rzl 3.2 深度学习说到深度学习，我们不得不提斯坦福的另一门王牌课程CS231，李飞飞教授的。这门课的课程，课后习题，堪称完美。必须必须看。整个系列下来，特别是课后的习题要做，做完之后你会发现，哇哦！它的课后习题就是写代码来实现算法的。这个在网易云课堂上有。 视频地址：http://study.163.com/course/introduction.htm?courseId=1004697005 课程笔记翻译，知乎专栏：https://zhuanlan.zhihu.com/p/21930884 墙裂建议要阅读这个知乎专栏，关于怎么学这门课，这个专栏写的很清楚。 课后作业配套答案：https://blog.csdn.net/bigdatadigest/article/category/7425092 3.3 学习时间到这里了，你的机器学习和深度学习算是入门了。学完上面这些，按一天6小时，一周六天的话，起码也得三个月吧。上面是基本功一定要认真学。但是，还找不了工作。因为你还没把这些知识应用到实际当中。 3.4 实战部分3.4.1 实战基础这一个阶段，你要开始用tensorflow（谷歌的深度学习框架）、scikit-learn（python的机器学习框架），这两个框架极大程度地集成了各大算法。其实上面在学习cs231n的时候你就会用到一部分。 scikit-learn的学习：http://sklearn.apachecn.org/cn/0.19.0/ 这是scikit-learn的官方文档中文版翻译，有理论有实战，最好的库学习资源，没有之一。认真看，传统的机器学习就是用这个库来实现的。 Tensorflow的学习：https://tensorflow.google.cn/api_docs/python/?hl=zh-cn 官方文档很详尽，还有实战例子，学习tensorflow的不二之选 3.4.2 实战进阶仅仅看这两个教程是不够的，你需要更多地去应用这两个库。 接下来推荐一部神书，机器学习和深度学习的实战教学，非常非常的棒，网上有很多号称实战的书或者例子，我看了基本就是照搬官网的，只有这一本书，是完全按照工业界的流程解决方案进行实战，你不仅能学习到库的应用，还能深入了解工业界的流程解决方案，最好的实战教学书，没有之一。书名是hands-on-ml-with-sklearn-and-tf 链接：https://pan.baidu.com/s/1x318qTHGt9oZKQwHkoUvKA密码：xssj 3.4.3 实战最终阶段kaggle数据竞赛，如果你已经学到了这一步，恭喜你离梦想越来越近了：对于我们初学者来说，没有机会接触到机器学习真正的应用项目，所以一些比赛平台是我们不错的选择。参加kaggle竞赛可以给你的简历增分不少，里面有入门级别到专家级别的实战案例，满足你的各方面需求。如果你能学到这一步了，我相信也不需要再看这个了。 上述所有资料的合集：https://pan.baidu.com/s/1tPqsSmSMZa6qLyD0ng87IQ密码：ve75 补充： 学到这个水平，应该是能够实习的水平了，还有很多后面再说吧。比如深度学习和机器学习的就业方向，深度学习得看论文，找工作还得对你得编程基础进行加强，具体就是数据结构与算法，我当年在这个上面可是吃了很大的亏。 这里面关于深度学习和机器学习的就业其实是两个方向，上面的其实也没有说全。一般来说，你得选择一个方向专攻。我建议的是，自学的最好在后期侧重机器学习方向，而不是深度学习。深度学习的岗位实在是太少，要求太高。机器学习还算稍微好点。 重点：上面的学习路径是主要框架，但是不意味着仅仅学习这些就够了。根据每个人基础的不同，你有可能需要另外的学习资料补充。但是，我希望大家可以按照上面的主框架走，先按上面我推荐的资源学，有需要的再去看别的（我之后也会推荐），上面的我能列出来的都是最经典的，最有效率，而且我亲身学过的。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"LearnPath","slug":"LearnPath","permalink":"https://dataquaner.github.io/categories/LearnPath/"}],"tags":[{"name":"LearnPath","slug":"LearnPath","permalink":"https://dataquaner.github.io/tags/LearnPath/"},{"name":"AI","slug":"AI","permalink":"https://dataquaner.github.io/tags/AI/"}]},{"title":"机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化","slug":"机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化","date":"2020-01-16T06:08:00.000Z","updated":"2020-04-11T11:34:10.409Z","comments":true,"path":"2020/01/16/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-10-jue-ce-shu-mo-xing-xgboost-lightgbm-he-catboost-mo-xing-ke-shi-hua/","link":"","permalink":"https://dataquaner.github.io/2020/01/16/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-10-jue-ce-shu-mo-xing-xgboost-lightgbm-he-catboost-mo-xing-ke-shi-hua/","excerpt":"","text":"安装 graphviz 参考文档 http://graphviz.readthedocs.io/en/stable/manual.html#installation graphviz安装包下载地址 https://www.graphviz.org/download/ 将graphviz的安装位置添加到系统环境变量 使用pip install graphviz安装graphviz python包 使用pip install pydotplus安装pydotplus python包 决策树模型可视化 以iris数据为例。训练一个分类决策树，调用export_graphviz函数导出DOT格式的文件。并用pydotplus包绘制图片。 #在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from sklearn import tree from sklearn.datasets import load_iris iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) import pydotplus from IPython.display import Image dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data) Image(graph.create_png()) XGBoost模型可视化参考文档 https://xgboost.readthedocs.io/en/latest/python/python_api.htmlxgboost中，对应的可视化函数是xgboost.to_graphviz。以iris数据为例，训练一个xgb分类模型并可视化 # 在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' import xgboost as xgb from sklearn.datasets import load_iris iris = load_iris() xgb_clf = xgb.XGBClassifier() xgb_clf.fit(iris.data, iris.target) xgb.to_graphviz(xgb_clf, num_trees=1) 也可以通过Digraph对象可以将保存文件并查看 digraph = xgb.to_graphviz(xgb_clf, num_trees=1) digraph.format = 'png' digraph.view('./iris_xgb') xgboost中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。 import matplotlib.pyplot as plt fig = plt.figure(figsize=(10, 10)) ax = fig.subplots() xgb.plot_tree(xgb_clf, num_trees=1, ax=ax) plt.show() LightGBM模型可视化参考文档 https://lightgbm.readthedocs.io/en/latest/Python-API.html#plottinglgb中，对应的可视化函数是lightgbm.create_tree_digraph。以iris数据为例，训练一个lgb分类模型并可视化 # 在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from sklearn.datasets import load_iris import lightgbm as lgb iris = load_iris() lgb_clf = lgb.LGBMClassifier() lgb_clf.fit(iris.data, iris.target) lgb.create_tree_digraph(lgb_clf, tree_index=1) lgb中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。 import matplotlib.pyplot as plt fig2 = plt.figure(figsize=(20, 20)) ax = fig2.subplots() lgb.plot_tree(lgb_clf, tree_index=1, ax=ax) plt.show() CatBoost模型可视化参考文档 catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)以iris数据为例，训练一个catboost模型。 参考文档 https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)以iris数据为例，训练一个catboost模型。 from sklearn.datasets import load_iris from catboost import CatBoostClassifier iris = load_iris() cat_clf = CatBoostClassifier(iterations=100) cat_clf.fit(iris.data, iris.target) 以python代码格式保存模型文件 cat_clf.save_model('catboost_model_file.py', format=\"python\", export_parameters=None) 也可以保存以C++代码格式保存模型文件 cat_clf.save_model('catboost_model_file.cpp', format=\"cpp\", export_parameters=None) 查看保存到的python代码，部分信息如下 需要自己解析出文件了树的结构，再用 graphviz 绘制图像 导出的Python文件首先第一个for循环部分 binary_feature_index = 0 binary_features = [0] * model.binary_feature_count for i in range(model.float_feature_count): for j in range(model.border_counts[i]): binary_features[binary_feature_index] = 1 if (float_features[i] > model.borders[binary_feature_index]) else 0 binary_feature_index += 1 输入的参数float_features存储输入的数值型特征值。model.binary_feature_count表示booster中所有树的节点总数。model.border_counts存储每个feature对应的节点数量，model.borders存储所有节点的判断边界。显然，CatBoost并没有按照二叉树结构从左到右，从上到下的存储结构。此段代码的功能，生成所有节点的判断结果。如果特征值大于判断边界，表示为1，否则为0。存储在binary_features中。 第二个for循环部分 # Extract and sum values from trees result = 0.0 tree_splits_index = 0 current_tree_leaf_values_index = 0 for tree_id in range(model.tree_count): current_tree_depth = model.tree_depth[tree_id] index = 0 for depth in range(current_tree_depth): index |= (binary_features[model.tree_splits[tree_splits_index + depth]] &lt;&lt; depth) result += model.leaf_values[current_tree_leaf_values_index + index] tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) return result 这段点代码功能是生成模型的预测结果result。model.tree_count表示决策树的数量，遍历每棵决策树。model.tree_depth存储每棵决策树的深度，取当前树的深度，存储在current_tree_depth。model.tree_splits存储决策树当前深度的节点在binary_features中的索引，每棵树有current_tree_depth个节点。看似CatBoost模型存储了都是完全二叉树，而且每一层的节点以及该节点的判断边界一致。如一棵6层的决策，可以从binary_features中得到一个长度为6，值为0和1组成的list。model.leaf_values存储所有叶子节点的值，每棵树的叶子节点有(1 &lt;&lt; current_tree_depth)个。将之前得到的list，倒序之后，看出一个2进制表示的数index，加上current_tree_leaf_values_index后，即是值在model.leaf_values的索引。将所有树得到的值相加，得到CatBoost模型的结果。 还原CatBoost模型树先从第二个for循环开始，打印每棵树序号，树的深度，当前树节点索引在tree_splits的便宜了，已经每个节点对应在tree_splits中的值。这个值对应的是在第一个for循环中生成的binary_features的索引。 tree_splits_index = 0 current_tree_leaf_values_index = 0 for tree_id in range(tree_count): current_tree_depth = tree_depth[tree_id] tree_splits_list = [] for depth in range(current_tree_depth): tree_splits_list.append(tree_splits[tree_splits_index + depth]) print tree_id, current_tree_depth, tree_splits_index, tree_splits_list tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) 0 6 0 [96, 61, 104, 2, 52, 81] 1 6 6 [95, 99, 106, 44, 91, 14] 2 6 12 [96, 31, 81, 102, 16, 34] 3 6 18 [95, 105, 15, 106, 57, 111] 4 6 24 [95, 51, 30, 8, 75, 57] 5 6 30 [94, 96, 103, 104, 25, 33] 6 6 36 [60, 8, 25, 39, 15, 99] 7 6 42 [96, 27, 48, 50, 69, 111] 8 6 48 [61, 80, 71, 3, 45, 2] 9 4 54 [61, 21, 90, 37] 从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。 12345678910从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。 split_list = [0] for i in range(len(border_counts)): split_list.append(split_list[-1] + border_counts[i]) print border_counts print zip(split_list[:-1], split_list[1:]) [32, 21, 39, 20] [(0, 32), (32, 53), (53, 92), (92, 112)] 在拿到一个binary_features的索引后即可知道该索引对应的节点使用的特征序号（float_features的索引）。 def find_feature(tree_splits_index): for i in range(len(split_list) - 1): if split_list[i] &lt;= tree_splits_index &lt; split_list[i+1]: return i 有了节点在binary_features中的索引，该索引也对应特征的判断边界数值索引，也知道了如何根据索引获取特征序号。决策树索引信息都的得到了，现在可以绘制树了。 绘制单棵决策树首先修改一下代码，便于获取单棵树的节点 class CatBoostTree(object): def __init__(self, CatboostModel): self.model = CatboostModel self.split_list = [0] for i in range(self.model.float_feature_count): self.split_list.append(self.split_list[-1] + self.model.border_counts[i]) def find_feature(self, splits_index): # 可优化成二分查找 for i in range(self.model.float_feature_count): if self.split_list[i] &lt;= splits_index &lt; self.split_list[i + 1]: return i def get_split_index(self, tree_id): tree_splits_index = 0 current_tree_leaf_values_index = 0 for index in range(tree_id): current_tree_depth = self.model.tree_depth[index] tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) return tree_splits_index, current_tree_leaf_values_index def get_tree_info(self, tree_id): tree_splits_index, current_tree_leaf_values_index = self.get_split_index(tree_id) current_tree_depth = self.model.tree_depth[tree_id] tree_splits_list = [] for depth in range(current_tree_depth): tree_splits_list.append(self.model.tree_splits[tree_splits_index + depth]) node_feature_list = [self.find_feature(index) for index in tree_splits_list] node_feature_borders = [self.model.borders[index] for index in tree_splits_list] end_tree_leaf_values_index = current_tree_leaf_values_index + (1 &lt;&lt; current_tree_depth) tree_leaf_values = self.model.leaf_values[current_tree_leaf_values_index: end_tree_leaf_values_index] return current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values 下面是绘制一棵决策树的函数，CatBoost导出的python代码文件通过model_file参数传入。 import imp import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from graphviz import Digraph def draw_tree(model_file, tree_id): fp, pathname, description = imp.find_module(model_file) CatboostModel = imp.load_module('CatboostModel', fp, pathname, description) catboost_tree = CatBoostTree(CatboostModel.CatboostModel) current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values = catboost_tree.get_tree_info(tree_id) dot = Digraph(name='tree_'+str(tree_id)) for depth in range(current_tree_depth): node_name = str(node_feature_list[current_tree_depth - 1 - depth]) node_border = str(node_feature_borders[current_tree_depth - 1 - depth]) label = 'column_' + node_name + '>' + node_border if depth == 0: dot.node(str(depth) + '_0', label) else: for j in range(1 &lt;&lt; depth): dot.node(str(depth) + '_' + str(j), label) dot.edge(str(depth-1) + '_' + str(j//2), str(depth) + '_' + str(j), label='No' if j%2 == 0 else 'Yes') depth = current_tree_depth for j in range(1 &lt;&lt; depth): dot.node(str(depth) + '_' + str(j), str(tree_leaf_values[j])) dot.edge(str(depth-1) + '_' + str(j//2), str(depth) + '_' + str(j), label='No' if j%2 == 0 else 'Yes') # dot.format = 'png' path = dot.render('./' + str(tree_id), cleanup=True) print path 例如绘制第11棵树（序数从0开始）。draw_tree(‘catboost_model_file’, 11)。 为了验证这个对不对，需要对一个测试特征生成每棵树的路径和结果，抽查一两个测试用例以及其中的一两颗树，观察结果是否相同。 def test_tree(model_file, float_features): fp, pathname, description = imp.find_module(model_file) CatboostModel = imp.load_module('CatboostModel', fp, pathname, description) model = CatboostModel.CatboostModel catboost_tree = CatBoostTree(CatboostModel.CatboostModel) result = 0 for tree_id in range(model.tree_count): current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values = catboost_tree.get_tree_info(tree_id) route = [] for depth in range(current_tree_depth): route.append(1 if float_features[node_feature_list[depth]] > node_feature_borders[depth] else 0) index = 0 for depth in range(current_tree_depth): index |= route[depth] &lt;&lt; depth tree_value = tree_leaf_values[index] print route, index, tree_value result += tree_value return result 如我们生成了第11棵树的图像，根据测试测试特征，手动在图像上查找可以得到一个值A。test_tree函数会打印一系列值，其中第11行对应的结果为值B。值A与值B相同，则测试为问题。其次还需要测试所有树的结果和导出文件中apply_catboost_model函数得到的结果相同。这个可以写个脚本，拿训练数据集跑一边。 from catboost_model_file import apply_catboost_model from CatBoostModelInfo import test_tree from sklearn.datasets import load_iris def main(): iris = load_iris() # print iris.data # print iris.target for feature in iris.data: if apply_catboost_model(feature) != test_tree('catboost_model_file', feature): print False print 'End.' if name == 'main': main() 至此，CatBoost模型的可视化完成了。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"},{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"},{"name":"CatBoost","slug":"CatBoost","permalink":"https://dataquaner.github.io/tags/CatBoost/"}]},{"title":"DBSCAN算法python实现（附完整数据集和代码）","slug":"DBSCAN算法python实现（附完整数据集和代码）","date":"2020-01-07T08:05:00.000Z","updated":"2020-04-11T11:30:29.639Z","comments":true,"path":"2020/01/07/dbscan-suan-fa-python-shi-xian-fu-wan-zheng-shu-ju-ji-he-dai-ma/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/dbscan-suan-fa-python-shi-xian-fu-wan-zheng-shu-ju-ji-he-dai-ma/","excerpt":"","text":"目录[TOC] 1. 算法思路DBSCAN算法的核心是“延伸”。先找到一个未访问的点p，若该点是核心点，则创建一个新的簇C，将其邻域中的点放入该簇，并遍历其邻域中的点，若其邻域中有点q为核心点，则将q的邻域内的点也划入簇C，直到C不再扩展。直到最后所有的点都标记为已访问。 点p通过密度可达来扩大自己的“地盘”，实际上就是簇在“延伸”。 图示网站：https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ 可以看一下簇是如何延伸的。 2. 算法实现2.1 计算两点之间的距离# 计算两个点之间的欧式距离，参数为两个元组 def dist(t1, t2): dis = math.sqrt((np.power((t1[0]-t2[0]),2) + np.power((t1[1]-t2[1]),2))) # print(\"两点之间的距离为：\"+str(dis)) return dis 2.2 读取文件，加载数据集def loadDataSet(fileName, splitChar='\\t'): dataSet = [] with open(fileName) as fr: for line in fr.readlines(): curline = line.strip().split(splitChar) fltline = list(map(float, curline)) dataSet.append(fltline) return dataSet 2.3 DBSCAN算法实现1、标记点是否被访问：我设置了两个列表，一个存放未访问的点unvisited，一个存放已访问的点visited。每次访问一个点，unvisited列表remove该点，visited列表append该点，以此来实现点的标记改变。 2、C作为输出结果，初始时是一个长度为所有点的个数的值全为-1的列表。之后修改点对应的索引的值来设置点属于哪个簇 # DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值 def dbscan(Data, Eps, MinPts): num = len(Data) # 点的个数 # print(\"点的个数：\"+str(num)) unvisited = [i for i in range(num)] # 没有访问到的点的列表 # print(unvisited) visited = [] # 已经访问的点的列表 C = [-1 for i in range(num)] # C为输出结果，默认是一个长度为num的值全为-1的列表 # 用k来标记不同的簇，k = -1表示噪声点 k = -1 # 如果还有没访问的点 while len(unvisited) > 0: # 随机选择一个unvisited对象 p = random.choice(unvisited) unvisited.remove(p) visited.append(p) # N为p的epsilon邻域中的对象的集合 N = [] for i in range(num): if (dist(Data[i], Data[p]) &lt;= Eps):# and (i!=p): N.append(i) # 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象 if len(N) >= MinPts: k = k+1 # print(k) C[p] = k # 对于p的epsilon邻域中的每个对象pi for pi in N: if pi in unvisited: unvisited.remove(pi) visited.append(pi) # 找到pi的邻域中的核心对象，将这些对象放入N中 # M是位于pi的邻域中的点的列表 M = [] for j in range(num): if (dist(Data[j], Data[pi])&lt;=Eps): #and (j!=pi): M.append(j) if len(M)>=MinPts: for t in M: if t not in N: N.append(t) # 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动 if C[pi] == -1: C[pi] = k # 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点 else: C[p] = -1 return C 3. 问题记录代码思路非常简单，让我以为实现起来也很简单。结果拖拖拉拉半个多月才终于将算法改好。 算法实现过程中遇到的问题其实是小问题，但是导致的结果非常严重。因为不起眼所以才难以察觉。 这是刚开始我运行算法得到的结果（Eps为10，MinPts为10）： Eps为2，MinPts为10（我改了点的大小）： 可以看出图中颜色特别多，实际上就是聚成的簇太多，可实际上目测应该只有七八个簇。这是为什么呢？ 原来是变量k的重复使用问题。 前面我用k来标识不同的簇，后面（如下图）我又将k变成了循环变量，注意M列表中都是整数，代表点在数据集中的索引，所以实际上是k在整数列表中遍历，覆盖掉了前面用来标识不同簇的k值，导致每次运行出来k取值特别多（如下下图）。 4. 运行结果 5. 完整代码5.1 源数据附数据集 链接：数据集788个点 提取码：rv06 5.2 源代码# encoding:utf-8 import matplotlib.pyplot as plt import random import numpy as np import math from sklearn import datasets list_1 = [] list_2 = [] # 数据集一：随机生成散点图,参数为点的个数 # def scatter(num): # for i in range(num): # x = random.randint(0, 100) # list_1.append(x) # y = random.randint(0, 100) # list_2.append(y) # print(list_1) # print(list_2) # data = list(zip(list_1, list_2)) # print(data) # #plt.scatter(list_1, list_2) # #plt.show() # return data #scatter(50) def loadDataSet(fileName, splitChar='\\t'): dataSet = [] with open(fileName) as fr: for line in fr.readlines(): curline = line.strip().split(splitChar) fltline = list(map(float, curline)) dataSet.append(fltline) return dataSet # 计算两个点之间的欧式距离，参数为两个元组 def dist(t1, t2): dis = math.sqrt((np.power((t1[0]-t2[0]),2) + np.power((t1[1]-t2[1]),2))) # print(\"两点之间的距离为：\"+str(dis)) return dis # dis = dist((1,1),(3,4)) # print(dis) # DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值 def dbscan(Data, Eps, MinPts): num = len(Data) # 点的个数 # print(\"点的个数：\"+str(num)) unvisited = [i for i in range(num)] # 没有访问到的点的列表 # print(unvisited) visited = [] # 已经访问的点的列表 C = [-1 for i in range(num)] # C为输出结果，默认是一个长度为num的值全为-1的列表 # 用k来标记不同的簇，k = -1表示噪声点 k = -1 # 如果还有没访问的点 while len(unvisited) > 0: # 随机选择一个unvisited对象 p = random.choice(unvisited) unvisited.remove(p) visited.append(p) # N为p的epsilon邻域中的对象的集合 N = [] for i in range(num): if (dist(Data[i], Data[p]) &lt;= Eps):# and (i!=p): N.append(i) # 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象 if len(N) >= MinPts: k = k+1 # print(k) C[p] = k # 对于p的epsilon邻域中的每个对象pi for pi in N: if pi in unvisited: unvisited.remove(pi) visited.append(pi) # 找到pi的邻域中的核心对象，将这些对象放入N中 # M是位于pi的邻域中的点的列表 M = [] for j in range(num): if (dist(Data[j], Data[pi])&lt;=Eps): #and (j!=pi): M.append(j) if len(M)>=MinPts: for t in M: if t not in N: N.append(t) # 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动 if C[pi] == -1: C[pi] = k # 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点 else: C[p] = -1 return C # 数据集二：788个点 dataSet = loadDataSet('788points.txt', splitChar=',') C = dbscan(dataSet, 2, 14) print(C) x = [] y = [] for data in dataSet: x.append(data[0]) y.append(data[1]) plt.figure(figsize=(8, 6), dpi=80) plt.scatter(x,y, c=C, marker='o') plt.show() # print(x) # print(y) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://dataquaner.github.io/tags/DBSCAN/"}]},{"title":"短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示","slug":"短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示","date":"2020-01-07T08:05:00.000Z","updated":"2020-04-11T11:34:53.017Z","comments":true,"path":"2020/01/07/duan-wen-ben-ju-lei-dbscan-suan-fa-yuan-li-python-dai-ma-shi-xian-ju-lei-jie-guo-zhan-shi/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/duan-wen-ben-ju-lei-dbscan-suan-fa-yuan-li-python-dai-ma-shi-xian-ju-lei-jie-guo-zhan-shi/","excerpt":"","text":"目录[TOC] 1. 算法原理1.1 常见的聚类算法聚类算法属于常见的无监督分类算法，在很多场景下都有应用，如用户聚类，文本聚类等。常见的聚类算法可以分成两类： 以 k-means 为代表的基于分区的算法 以层次聚类为代表的基于层次划分的算法 对于第一类方法，有以下几个缺点： 1）需要事先确定聚类的个数，当数据集比较大时，很难事先给出一个合适的值； 2）只适用于具有凸形状的簇，不适用于具有任意形状的簇； 3）对内存的占用资源比较大，难以推广至大规模数据集； 对于第二类方法，有以下缺点： 1）需要确定停止分裂的条件 2）计算速度慢 1.2 DBSCAN聚类 A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise （Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu） DBSCAN是一类基于密度的算法，能有效解决上述两类算法的问题。 DBSCAN的基本假设是一个集群的密度要显著高于噪声点的密度。因此，其基本思想是对于集群中的每一个点，在给定的半径范围内，相邻点的数量必须超过预先设定的某一个阈值。 因此，DBSCAN算法中包含两个重要的参数： eps：聚类类别中样本的相似度衡量，与类别内样本相似度成反比。可以理解为同一个类别当中，对两个样本之间距离的最大值限定。 min_samples：每个聚类类别中的最小样本数，会对未分类样本数量造成影响，与未分类样本数量成正比。当相似样本数量少于该参数时，不会聚到一起。 在实际应用过程中，根据样本的大小，以及样本的大致分布，了解聚类结果会随着这两个参数如何变化之后，可以根据自己的经验对两个参数进行调整。只有两个模型参数需要调整，因此调参过程也不会太麻烦。 2. 代码实现2.1 import需要的包# === import packages === # import jieba.posseg as pseg from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import CountVectorizer import numpy as np from sklearn.cluster import DBSCAN 2.2 载入数据根据数据文件的不同存在不同的数据载入方法，我当时使用的是两种类型的数据，分别是直接包含目标短文本的txt，以json格式存储的txt。如果有用到这两种类型的文件可以参考这部分的数据载入代码，其他的请根据文件类型和数据样式自行载入。首先是载入以json格式存储的txt文件，可以用正则表达式，也可以根据数据存储的方式提取出对应的字段。先展示一下数据的存储格式： { \"code\": \"200\", \"data\": { \"result\": [ { \"updateDate\": 1551923786433, \"ensureIntentName\": \"新意图\", \"corpus\": \"怎么查询之前的小微提醒\", \"recommendResult\": 0, \"remark\": \"\", \"source\": 2, \"result\": 2, \"eventName\": \"\", \"id\": \"b07328fc-8383-44b7-b466-15b063b8544a\", \"state\": 0, \"tag\": \"\", \"isHandle\": 1, \"createDate\": 1551669751334, \"eventId\": \"\", \"corpusTagId\": \"3335d2d8-a16e-46a2-9ed7-76739108d684\", \"intentName\": \"\", \"ensureIntent\": \"newIntent\", \"recommendIntent\": [ \"setmsgnotifications\" ], \"uploadTime\": 1551669751333, \"w3account\": \"x00286769\", \"createBy\": \"x00286769\", \"intentCode\": \"\", \"isBotSupport\": 0, \"userRole\": \"0\", \"welinkVersion\": \"3.9.13\" } ], \"pagination\": { \"pageCount\": 1, \"pageSizes\": 50, \"pageNumber\": 1, \"offset\": 0, \"pageTotal\": 1, \"pageNumbers\": 1, \"pageSize\": 50 } }, \"error\": \"\", \"stack\": \"\", \"message\": \"ok\" } 我的目标是对上述数据当中，字典中key “data” 对应的字典中的 “result” 中每一个item 的 “corpus” 进行提取，于是就有了下列代码。 # === Data loading === # data = [] corpus = [] for line in open(\"新意图语料.txt\", 'r+', encoding='UTF-8'): data.append(eval(line)) for i in range(len(data)): tmp = data[i]['data']['result'] for j in range(len(tmp)): corpus.append(tmp[j]['corpus']) 然后是载入包含目标短文本的txt，也就是说该txt直接存储了上面的 “corpus” 对应的内容，但是每一行的内容都加上了双引号和逗号，就通过strip把这些不需要的部分去掉了，最后得到所有 “corpus” 组成的list。 for line in open(\"未识别语料.txt\", 'r+'): line = line.strip('\\n') line = line.strip('\\t') line = line.rstrip(',') line = line.lstrip('\"') line = line.rstrip('\"') corpus.append(line) 2.3 对文本进行分词，并记录词性调用结巴词库对语料进行分词，并记录分词结果中每个词的词性。我的数据集在处理之后得到了5316条短文本，分词得到20640个不重复的词汇及其对应的词性，并建立了两者之间的字典联系。 # === Record the text cut and POS === # part_of_speech = [] word_after_cut = [] cut_corpus_iter = corpus.copy() cut_corpus = corpus.copy() for i in range(len(corpus)): cut_corpus_iter[i] = pseg.cut(corpus[i]) # 5316 cut_corpus[i] = \"\" for every in cut_corpus_iter[i]: cut_corpus[i] = (cut_corpus[i] + \" \" + str(every.word)).strip() part_of_speech.append(every.flag) # 20640 word_after_cut.append(every.word) # 20640 word_pos_dict = {word_after_cut[i]: part_of_speech[i] for i in range(len(word_after_cut))} 2.4 文本向量化–TF-IDF权重使用TF-IDF对文本进行向量化，得到文本的TF-IDF权重。 # === Get the TF-IDF weights === # Count_vectorizer = CountVectorizer() transformer = TfidfTransformer() # 用于统计每个词语的tf-idf权值 tf_idf = transformer.fit_transform(Count_vectorizer.fit_transform(cut_corpus)) # （5316，2039）第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵 word = Count_vectorizer.get_feature_names() # 2039，获取词袋模型中的所有词语 weight = tf_idf.toarray() # （5316，2039）将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重 2.5 基于词性的新权重前面得到了分词的结果，并对词性进行了记录，接下来可以针对不同词汇的词性码，给与其TF-IDF权重以不同的乘数，这样可以突出某些类型的词汇的重要性，在一定程度上有助于聚类的效果。 具体的乘数构造规则可以根据需求自行调整。 # === Get new weight with POS considered === # word_weight = [1 for i in range(len(word))] for i in range(len(word)): if word[i] not in word_pos_dict.keys(): continue if word_pos_dict[word[i]] == 'n': word_weight[i] = 1.2 elif word_pos_dict[word[i]] == \"vn\": word_weight[i] = 1.1 elif word_pos_dict[word[i]] == \"m\": word_weight[i] = 0 else: # 权重调整可以根据实际情况进行更改 continue word_weight = np.array(word_weight) new_weight = weight.copy() for i in range(len(weight)): for j in range(len(word)): new_weight[i][j] = weight[i][j] * word_weight[j] 2.6 DBSCAN模型得到了文本的向量化表示之后就可以将其投喂到模型当中了，eps和min_samples都是可以调整的参数。 # === Fit the DBSCAN model and get the classify labels === # DBS_clf = DBSCAN(eps=1, min_samples=4) DBS_clf.fit(new_weight) print(DBS_clf.labels_) 3. 聚类结果DBSCAN模型实现聚类之后，聚类的结果会存储在 labels_ 中，将 labels_ 与原来的文本一一对应，可以得到最终的聚类结果： # === Define the function of classify the original corpus according to the labels === # def labels_to_original(labels, original_corpus): assert len(labels) == len(original_corpus) max_label = max(labels) number_label = [i for i in range(0, max_label + 1, 1)] number_label.append(-1) result = [[] for i in range(len(number_label))] for i in range(len(labels)): index = number_label.index(labels[i]) result[index].append(original_corpus[i]) return result labels_original = labels_to_original(DBS_clf.labels_, corpus) for i in range(5): print(labels_original[i]) # 聚类结果展示（部分） ['社保卡', '社保卡', '社保卡。', '社保卡办理', '社保卡', '社保卡', '社保卡挂失', '社保卡。', '社保卡', '领取社保卡。'] ['五险一金', '五险一金。', '五险一金。', '五险一金介绍', '看看二月份五险一金情况'] ['打开汇钱。', '打开汇钱。', '我要汇钱', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。'] ['车辆通行证。', '车辆通行证。', '我要办车辆通行证。', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证。', '车辆通行证', '车辆通行证。', '车辆通行证。', '车辆通行证'] ['邮件附件权限', '等等邮件附件权限。', '邮件附件权限', '邮件附件权限', '邮件附件权限', '邮件附件权限', '您好，请问怎样申请图片查看权限和邮件附件查看权限？'] 4 附件：完整代码# === import packages === # import jieba.posseg as pseg from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import CountVectorizer import numpy as np from sklearn.cluster import DBSCAN # === Data loading === # data = [] corpus = [] for line in open(\"新意图语料.txt\", 'r+', encoding='UTF-8'): data.append(eval(line)) for i in range(len(data)): tmp = data[i]['data']['result'] for j in range(len(tmp)): corpus.append(tmp[j]['corpus']) for line in open(\"未识别语料.txt\", 'r+'): line = line.strip('\\n') line = line.strip('\\t') line = line.rstrip(',') line = line.lstrip('\"') line = line.rstrip('\"') corpus.append(line) # === Record the text cut and POS === # part_of_speech = [] word_after_cut = [] cut_corpus_iter = corpus.copy() cut_corpus = corpus.copy() for i in range(len(corpus)): cut_corpus_iter[i] = pseg.cut(corpus[i]) # 5316 cut_corpus[i] = \"\" for every in cut_corpus_iter[i]: cut_corpus[i] = (cut_corpus[i] + \" \" + str(every.word)).strip() part_of_speech.append(every.flag) # 20640 word_after_cut.append(every.word) # 20640 word_pos_dict = {word_after_cut[i]: part_of_speech[i] for i in range(len(word_after_cut))} # === Get new weight with POS considered === # word_weight = [1 for i in range(len(word))] for i in range(len(word)): if word[i] not in word_pos_dict.keys(): continue if word_pos_dict[word[i]] == 'n': word_weight[i] = 1.2 elif word_pos_dict[word[i]] == \"vn\": word_weight[i] = 1.1 elif word_pos_dict[word[i]] == \"m\": word_weight[i] = 0 else: # 权重调整可以根据实际情况进行更改 continue word_weight = np.array(word_weight) new_weight = weight.copy() for i in range(len(weight)): for j in range(len(word)): new_weight[i][j] = weight[i][j] * word_weight[j] # === Fit the DBSCAN model and get the classify labels === # DBS_clf = DBSCAN(eps=1, min_samples=4) DBS_clf.fit(new_weight) print(DBS_clf.labels_) # === Define the function of classify the original corpus according to the labels === # def labels_to_original(labels, original_corpus): assert len(labels) == len(original_corpus) max_label = max(labels) number_label = [i for i in range(0, max_label + 1, 1)] number_label.append(-1) result = [[] for i in range(len(number_label))] for i in range(len(labels)): index = number_label.index(labels[i]) result[index].append(original_corpus[i]) return result labels_original = labels_to_original(DBS_clf.labels_, corpus) for i in range(5): print(labels_original[i]) # 聚类结果展示（部分） ['社保卡', '社保卡', '社保卡。', '社保卡办理', '社保卡', '社保卡', '社保卡挂失', '社保卡。', '社保卡', '领取社保卡。'] ['五险一金', '五险一金。', '五险一金。', '五险一金介绍', '看看二月份五险一金情况'] ['打开汇钱。', '打开汇钱。', '我要汇钱', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。'] ['车辆通行证。', '车辆通行证。', '我要办车辆通行证。', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证。', '车辆通行证', '车辆通行证。', '车辆通行证。', '车辆通行证'] ['邮件附件权限', '等等邮件附件权限。', '邮件附件权限', '邮件附件权限', '邮件附件权限', '邮件附件权限', '您好，请问怎样申请图片查看权限和邮件附件查看权限？'] document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://dataquaner.github.io/tags/DBSCAN/"}]},{"title":"机器学习系列之决策树算法(08):梯度提升树算法LightGBM","slug":"机器学习系列之决策树算法（08）：梯度提升树算法LightGBM","date":"2020-01-07T02:30:00.000Z","updated":"2020-04-11T11:33:39.995Z","comments":true,"path":"2020/01/07/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-08-ti-du-ti-sheng-shu-suan-fa-lightgbm/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-08-ti-du-ti-sheng-shu-suan-fa-lightgbm/","excerpt":"","text":"1. LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。而LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。 1.1 LightGBM提出的动机常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。 LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。 1.2 XGBoost的缺点及LightGBM的优化（1）XGBoost的缺点在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是： 首先，对所有特征都按照特征的数值进行预排序。 其次，在遍历分割点的时候用的代价找到一个特征上的最好分割点。 最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。 这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显： 首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。 最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。 （2）LightGBM的优化为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化： 基于Histogram的决策树算法。 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 直接支持类别特征(Categorical Feature) 支持高效并行 Cache命中率优化 下面我们就详细介绍以上提到的lightGBM优化算法。 2. LightGBM的基本原理2.1 基于Histogram的决策树算法（1）直方图算法Histogram algorithm应该翻译为直方图算法，直方图算法的基本思想是： 先把连续的浮点特征值离散化成 k个整数，同时构造一个宽度为 k 的 直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。 图：直方图算法 直方图算法简单理解为： 首先确定对于每一个特征需要多少个箱子（bin）并为每一个箱子分配一个整数； 然后将浮点数的范围均分成若干区间，区间个数与箱子个数相等，将属于该箱子的样本数据更新为箱子的值； 最后用直方图（#bins）表示。看起来很高大上，其实就是直方图统计，将大规模的数据放在了直方图中。 我们知道特征离散化具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等。对于直方图算法来说最直接的有以下两个优点： 内存占用更小： 直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8 。也就是说XGBoost需要用32位的浮点数去存储特征值，并用32位的整形去存储索引，而 LightGBM只需要用8位去存储直方图，内存相当于减少为 ； 图：内存占用优化为预排序算法的1/8 计算代价更小： 预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算 k次（ 可以认为是常数），直接将时间复杂度从O(#data * #feature )降低到 O(k * #feature )，而我们知道#data &gt;&gt;k。 当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 （2）直方图做差加速LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。 注意： XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。 2.2 带深度限制的 Leaf-wise 算法在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。 LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。 2.3 单边梯度采样算法Gradient-based One-Side Sampling 应该被翻译为单边梯度采样（GOSS）。GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 AdaBoost中，样本权重是数据重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的是，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用。即梯度小的样本，训练误差也比较小，说明数据已经被模型学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练模型的精确度，为了避免此问题，提出了GOSS算法。 GOSS是一个样本的采样算法，目的是丢弃一些对计算信息增益没有帮助的样本留下有帮助的。根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。因此，GOSS在进行数据采样的时候只保留了梯度较大的数据，但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。所以，GOSS首先将要进行分裂的特征的所有取值按照绝对值大小降序排序（XGBoost一样也进行了排序，但是LightGBM不用保存排序后的结果），选取绝对值最大的 个数据。然后在剩下的较小梯度数据中随机选择 个数据。接着将这 个数据乘以一个常数 ，这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这 个数据来计算信息增益。下图是GOSS的具体算法。 2.4 互斥特征捆绑算法高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从 变为 ，这里 指特征融合绑定后特征包的个数，且 远小于 。 针对这种想法，我们会遇到两个问题： 怎么判定哪些特征应该绑在一起（build bundled）？ 怎么把特征绑为一个（merge feature）？ （1）解决哪些特征应该绑在一起将相互独立的特征进行绑定是一个 NP-Hard 问题，LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。此外，我们注意到通常有很多特征，尽管不是％相互排斥，但也很少同时取非零值。如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。经过简单的计算，随机污染小部分特征值将影响精度最多 ， 是每个绑定中的最大冲突比率，当其相对较小时，能够完成精度和效率之间的平衡。具体步骤可以总结如下： 构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关； 根据节点的度进行降序排序，度越大，与其它特征的冲突越大； 遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。 算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 来平衡算法的精度和效率。EFB 算法的伪代码如下所示： 算法3的时间复杂度是 ，训练之前只处理一次，其时间复杂度在特征不是特别多的情况下是可以接受的，但难以应对百万维度的特征。为了继续提高效率，LightGBM提出了一种更加高效的无图的排序策略：将特征按照非零值个数排序，这和使用图节点的度排序相似，因为更多的非零值通常会导致冲突，新算法在算法3基础上改变了排序策略。 （2）解决怎么把特征绑为一捆特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间 ，B特征的原始取值为区间，我们可以在B特征的取值上加一个偏置常量，将其取值范围变为，绑定后的特征取值范围为 ，这样就可以放心的融合特征A和B了。具体的特征合并算法如下所示： 3. LightGBM的工程优化我们将论文《Lightgbm: A highly efficient gradient boosting decision tree》中没有提到的优化方案，而在其相关论文《A communication-efficient parallel algorithm for decision tree》中提到的优化方案，放到本节作为LightGBM的工程优化来向大家介绍。 3.1 直接支持类别特征实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的特征，降低了空间和时间的效率。但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题： 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为1 ，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别。 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，如下图左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差。但如果使用下图右边的切分方法，数据会被切分到两个比较大的空间，进一步的学习也会更好。下图右边叶子节点的含义是或者放到左孩子，其余放到右孩子。 图：左图为基于 one-hot 编码进行分裂，右图为 LightGBM 基于 many-vs-many 进行分裂 而类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 个类别，则有 种可能，时间复杂度为 ，LightGBM 基于 Fisher的《On Grouping For Maximum Homogeneity》论文实现了 的时间复杂度。 算法流程如下图所示，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。从下图可以看到， 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。 图：LightGBM求解类别特征的最优切分算法 在Expo数据集上的实验结果表明，相比展开的方法，使用LightGBM支持的类别特征可以使训练速度加速倍，并且精度一致。更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。 3.2 支持高效并行（1）特征并行特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法。这种特征并行方法有个很大的缺点：就是对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度。 LightGBM 则不进行数据垂直划分，而是在每台机器上保存全部训练数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。具体过程如下图所示。 （2）数据并行传统的数据并行策略主要为水平划分数据，让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 ；如果使用集成的通信，则通讯开销为 。 LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。具体过程如下图所示。 图：数据并行 （3）投票并行基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行的方式只合并部分特征的直方图从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。 大致步骤为两步： 本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征； 合并时只合并每个机器选出来的特征。 图：投票并行 3.3 Cache命中率优化XGBoost对cache优化不友好，如下图所示。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问算法进行改进。 图：随机访问会造成cache miss 而 LightGBM 所使用直方图算法对 Cache 天生友好： 首先，所有的特征都采用相同的方式获得梯度（区别于XGBoost的不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率； 其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss的问题。 图：LightGBM增加缓存命中率 4. LightGBM的优缺点4.1 优点这部分主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。 （1）速度更快 LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度； LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算； LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量； LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略； LightGBM 对缓存也进行了优化，增加了缓存命中率； （2）内存更小 XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 降低为 ，极大的减少了内存消耗； LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗； LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。 4.2 缺点 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合； Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感； 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去； 5. LightGBM实例本篇文章所有数据集和代码均在我的GitHub中，地址：https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/LightGBM 5.1 安装LightGBM依赖包pip install lightgbm 5.2 LightGBM分类和回归LightGBM有两大类接口：LightGBM原生接口 和 scikit-learn接口 ，并且LightGBM能够实现分类和回归两种任务。 （1）基于LightGBM原生接口的分类import lightgbm as lgb from sklearn import datasets from sklearn.model_selection import train_test_split import numpy as np from sklearn.metrics import roc_auc_score, accuracy_score # 加载数据 iris = datasets.load_iris() # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3) # 转换为Dataset数据格式 train_data = lgb.Dataset(X_train, label=y_train) validation_data = lgb.Dataset(X_test, label=y_test) # 参数 params = { 'learning_rate': 0.1, 'lambda_l1': 0.1, 'lambda_l2': 0.2, 'max_depth': 4, 'objective': 'multiclass', # 目标函数 'num_class': 3, } # 模型训练 gbm = lgb.train(params, train_data, valid_sets=[validation_data]) # 模型预测 y_pred = gbm.predict(X_test) y_pred = [list(x).index(max(x)) for x in y_pred] print(y_pred) # 模型评估 print(accuracy_score(y_test, y_pred)) （2）基于Scikit-learn接口的分类 from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.externals import joblib # 加载数据 iris = load_iris() data = iris.data target = iris.target # 划分训练数据和测试数据 X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2) # 模型训练 gbm = LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=20) gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5) # 模型存储 joblib.dump(gbm, 'loan_model.pkl') # 模型加载 gbm = joblib.load('loan_model.pkl') # 模型预测 y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_) # 模型评估 print('The accuracy of prediction is:', accuracy_score(y_test, y_pred)) # 特征重要度 print('Feature importances:', list(gbm.feature_importances_)) # 网格搜索，参数优化 estimator = LGBMClassifier(num_leaves=31) param_grid = { 'learning_rate': [0.01, 0.1, 1], 'n_estimators': [20, 40] } gbm = GridSearchCV(estimator, param_grid) gbm.fit(X_train, y_train) print('Best parameters found by grid search are:', gbm.best_params_) （3）基于LightGBM原生接口的回归对于LightGBM解决回归问题，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.转换为Dataset数据格式 lgb_train = lgb.Dataset(train_X, train_y) lgb_eval = lgb.Dataset(test_X, test_y, reference=lgb_train) # 6.参数 params = { 'task': 'train', 'boosting_type': 'gbdt', # 设置提升类型 'objective': 'regression', # 目标函数 'metric': {'l2', 'auc'}, # 评估函数 'num_leaves': 31, # 叶子节点数 'learning_rate': 0.05, # 学习速率 'feature_fraction': 0.9, # 建树的特征选择比例 'bagging_fraction': 0.8, # 建树的样本采样比例 'bagging_freq': 5, # k 意味着每 k 次迭代执行bagging 'verbose': 1 # &lt;0 显示致命的, =0 显示错误 (警告), >0 显示信息 } # 7.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5) # 8.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X, num_iteration=my_model.best_iteration) # 9.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) （4）基于Scikit-learn接口的回归import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=20, verbosity=2) my_model.fit(train_X, train_y, verbose=False) # 6.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X) # 7.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 5.3 LightGBM调参在上一部分中，LightGBM模型的参数有一部分进行了简单的设置，但大都使用了模型的默认参数，但默认参数并不是最好的。要想让LightGBM表现的更好，需要对LightGBM模型进行参数微调。下图展示的是回归模型需要调节的参数，分类模型需要调节的参数与此类似。 6. 关于LightGBM若干问题的思考6.1 LightGBM与XGBoost的联系和区别有哪些？（1）LightGBM使用了基于histogram的决策树算法，这一点不同于XGBoost中的贪心算法和近似算法，histogram算法在内存和计算代价上都有不小优势。1）内存上优势：很明显，直方图算法的内存消耗为 (因为对特征分桶后只需保存特征离散化之后的值)，而XGBoost的贪心算法内存消耗为： ，因为XGBoost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要位的浮点数来保存。2）计算上的优势：预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为，而直方图算法只需要遍历桶就行了，时间为。 （2）XGBoost采用的是level-wise的分裂策略，而LightGBM采用了leaf-wise的策略，区别是XGBoost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGBoost也进行了分裂，带来了不必要的开销。leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。 （3）XGBoost在每一层都动态构建直方图，因为XGBoost的直方图算法不是针对某个特定的特征，而是所有特征共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而LightGBM中对每个特征都有一个直方图，所以构建一次直方图就够了。 （4）LightGBM使用直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。 （5）LightGBM支持类别特征，不需要进行独热编码处理。 （6）LightGBM优化了特征并行和数据并行算法，除此之外还添加了投票并行方案。 （7）LightGBM采用基于梯度的单边采样来减少训练样本并保持数据分布不变，减少模型因数据分布发生变化而造成的模型精度下降。 （8）特征捆绑转化为图着色问题，减少特征数量。 7. Reference由于参考的文献较多，我把每篇参考文献按照自己的学习思路，进行了详细的归类和标注。 LightGBM论文解读： 【1】Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]//Advances in Neural Information Processing Systems. 2017: 3146-3154. 【2】Taifeng Wang分享LightGBM的视频，地址：https://v.qq.com/x/page/k0362z6lqix.html 【3】开源|LightGBM：三天内收获GitHub 1000+ 星，地址：https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog 【4】Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/anshuai_aw1/article/details/83048709 【5】快的不要不要的lightGBM - 王乐的文章 - 知乎 https://zhuanlan.zhihu.com/p/31986189 【6】『 论文阅读』LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/shine19930820/article/details/79123216 LightGBM算法讲解： 【7】【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/87885678 【8】入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同，地址：https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw 【9】CatBoost vs. Light GBM vs. XGBoost，地址：https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db 【10】机器学习算法之LightGBM，地址：https://www.biaodianfu.com/lightgbm.html LightGBM工程优化： 【11】Meng Q, Ke G, Wang T, et al. A communication-efficient parallel algorithm for decision tree[C]//Advances in Neural Information Processing Systems. 2016: 1279-1287. 【12】Zhang H, Si S, Hsieh C J. GPU-acceleration for Large-scale Tree Boosting[J]. arXiv preprint arXiv:1706.08359, 2017. 【13】LightGBM的官方GitHub代码库，地址：https://github.com/microsoft/LightGBM 【14】关于sklearn中的决策树是否应该用one-hot编码？- 柯国霖的回答 - 知乎 https://www.zhihu.com/question/266195966/answer/306104444 LightGBM实例： 【15】LightGBM使用，地址：https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/ 【16】LightGBM两种使用方式 ，地址：https://www.cnblogs.com/chenxiangzhen/p/10894306.html LightGBM若干问题的思考： 【17】GBDT、XGBoost、LightGBM的区别和联系，地址：https://www.jianshu.com/p/765efe2b951a 【18】xgboost和lightgbm的区别和适用场景，地址：https://www.nowcoder.com/ta/review-ml/review?page=101 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战：原生接口和sklearn接口区别","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST实战：原生接口和sklearn接口的区别","date":"2019-12-26T16:00:00.000Z","updated":"2020-04-11T11:33:22.949Z","comments":true,"path":"2019/12/27/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan-yuan-sheng-jie-kou-he-sklearn-jie-kou-de-qu-bie/","link":"","permalink":"https://dataquaner.github.io/2019/12/27/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan-yuan-sheng-jie-kou-he-sklearn-jie-kou-de-qu-bie/","excerpt":"","text":"1 前言2 官方文档英文官方文档 中文文档 3 sklearn接口from xgboost.sklearn import XGBClassifier xgbc = XGBClassifier(n_jobs=-1) # 新建xgboost sklearn的分类class # xgboost的sklearn接口默认只使用cpu单线程，设置n_jobs=-1使用所有线程 print(\"开始xgboost classifier训练\") xgbc.fit(train_vector,np.array(train_label)) # 喂给分类器训练numpy形式的训练特征向量和标签向量 print(\"完成xgboost classifier训练，开始预测\") pre_train_Classifier = xgbc.predict(test_vector) # 喂给分类器numpy形式的测试特征向量 np.save(os.path.join(model_path,\"pre_train_Classifier.npy\"),pre_train_Classifier) # 保存结果 xgboost的sklearn接口，可以不经过标签标准化(即将标签编码为0~n_class-1)，直接喂给分类器特征向量和标签向量，使用fit训练后调用predict就能得到预测向量的预测标签，它会在内部调用sklearn.preprocessing.LabelEncoder()将标签在分类器使用时transform，在输出结果时inverse_transform。 优点：使用简单，无需对标签进行标准化处理，直接得到预测标签； 缺点：在模型保存后重新载入，丢失LabelEncoder，不能增量训练只能用一次. 4 xgboost的原生接口vector_matrix,label_single_new = get_data(data_path) # 获取得到特征矩阵、标签向量 print(\"标签总数为：%d；数据量总数为：%d\"%(len(list(set(label_single_new))),len(vector_matrix))) # 将标签标准化为0~class number-1,则xgboost概率最大的下标即为该位置数对应的标签 from sklearn import preprocessing label_coder = preprocessing.LabelEncoder() label_single_code = label_coder.fit_transform(label_single_new) # 切割训练集、测试集 from sklearn.model_selection import train_test_split train_matrix,test_matrix,train_label,test_label = train_test_split( vector_matrix,label_single_code,test_size=0.1,random_state=0) import xgboost as xgb # 参数设置见 http://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/xgboost/chapters/xgboost_usage.html params = { 'booster': 'gbtree', 'silent':0, # 如果为 0（默认值），则表示打印运行时的信息；如果为 1，则表示不打印这些信息 'objective': 'multi:softprob', # 基于softmax 的多分类模型，但是它的输出是一个矩阵：ndata*nclass，给出了每个样本属于每个类别的概率。 'num_class':len(set(label_single_new)),#指定类别数量 } dtrain = xgb.DMatrix(train_matrix, label=train_label, nthread=-1) # xgboost原生接口需要使用DMatrix格式的数据，这里与sklearn接口不同 print(\"开始xgboost训练\") xgbc = xgb.train(params,dtrain) # 初始化xgboost分类器，原生接口默认启用全部线程 xgbc.save_model(model_path+save_name+'xgbc_0.9.model') # 保存模型 # ============================================================================= # xgbc = xgb.Booster() # 重新载入模型 # xgbc.load_model(fname=model_path+save_name+'xgbc_0.9.model') # ============================================================================= print(\"xgboost训练完成，得到概率矩阵\") pre_train = xgbc.predict(xgb.DMatrix(train_matrix, nthread=-1)) # 训练数据的预测概率矩阵，启用全部线程 pre_test = xgbc.predict(xgb.DMatrix(test_matrix, nthread=-1)) # 测试数据的预测概率矩阵，启用全部线程 # 概率矩阵各行的数据为各条数据的预测概率，各行数据之和为1； # 概率矩阵各行的下标即为标准化后的label标签(0~class number-1) # 数据保存 np.save(model_path+save_name+'pre_train.npy',pre_train) np.save(model_path+save_name+'train_label.npy',train_label) np.save(model_path+save_name+'pre_test.npy',pre_test) np.save(model_path+save_name+'test_label.npy',test_label) # 数据载入 # ============================================================================= # pre_train = np.load(model_path+save_name+'pre_train.npy') # train_label = np.load(model_path+save_name+'train_label.npy') # pre_test = np.load(model_path+save_name+'pre_test.npy') # test_label = np.load(model_path+save_name+'test_label.npy') # ============================================================================= # narray_target.argsort(axis=1)，获得按行(排序对象为各行数值)升序后的下标矩阵，axis=0为按列升序; # np.fliplr(narray_target)获取矩阵的左右翻转，narray_target[::-1]获取矩阵的上下翻转 # narray_target[:,-5:]获取矩阵的后5列; top_k = 5 # 获取预测概率最大的5个标签 # 获取概率矩阵排序信息，得到按行升序的下标矩阵,切割得到各行的后5个下标, # 将其左右翻转后，得到各行降序的前5个下标，即标准化后的标签 pre_test_index = np.fliplr(pre_test.argsort(axis=1)[:,-1*top_k:]) pre_test_label = label_coder.inverse_transform(pre_test_index) # 调用label标准化工具inverse_transform将下标转化为真实标签 pre_train_index = np.fliplr(pre_train.argsort(axis=1)[:,-1*top_k:]) pre_train_label = label_coder.inverse_transform(pre_train_index) xgboost原生接口，数据需要经过标签标准化(LabelEncoder().fit_transform)、输入数据标准化(xgboost.DMatrix)和输出结果反标签标准化(LabelEncoder().inverse_transform)，训练调用train预测调用predict. 需要注意的是，xgboost原生接口输出的预测标签概率矩阵各行的下标即为标准化后的label标签(0~class number-1). 5 结论优先考虑使用原生接口形式，便于模型保存后的复用。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST实战","date":"2019-12-25T16:00:00.000Z","updated":"2020-04-11T11:33:03.342Z","comments":true,"path":"2019/12/26/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan/","link":"","permalink":"https://dataquaner.github.io/2019/12/26/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan/","excerpt":"","text":"1 前言上一篇从数据原理角度深入介绍了XGBoost的实现原理及优化，参考《梯度提升树算法XGBoost》。本篇主要介绍XGBoost的工程实战，参数调优等内容。 学习一个算法实战，一般按照以下几步，第一步能够基于某个平台、某种语言构建一个模型，第二步是能够优化一个模型 。我们将学习以下内容 如果使用xgboost构建分类器 xgboost 的参数含义，以及如何调参 xgboost 的如何做cv xgboost的可视化 2 XGBoost模型构建回归模型准备数据我们使用房价数据 ，做的是一个回归任务，预测房价，分类任务类似。 导入包 import pandas as pd from xgboost import XGBRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import Imputer from sklearn.metrics import mean_absolute_error 读入和展示数据 data = pd.read_csv('../input/train.csv') data.dropna(axis=0, subset=['SalePrice'], inplace=True) y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25) my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) print(train_X.shape) print(test_X.shape) print(train_y.shape) print(test_y.shape) --- ##执行结果 (1095, 37) (365, 37) (1095,) (365,) --- 创建并训练XGBoost模型随机选取默认参数进行初始化建模 my_model = XGBRegressor() # Add silent=True to avoid printing out updates with each cycle my_model.fit(train_X, train_y, verbose=False) 评估并预测模型# make predictions predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 模型调优XGBoost有一些参数可以显著影响模型的准确性和训练速度。 n_estimatorsn_estimators 指定训练循环次数。在 欠拟合 vs 过拟合 图表, n_estimators让训练沿着图表向右移动。 值太低会导致欠拟合，这对训练数据和新数据的预测都是不准确的。 太大的值会导致过度拟合，这是对训练数据的准确预测，但对新数据的预测不准确（这是我们关心的）。 通过实际实验来找到理想的n_estimators。 典型值范围为100-1000，但这很大程度上取决于下面讨论的 early_stopping_roundsearly_stopping_rounds 提供了一种自动查找理想值的方法。 early_stopping_rounds会导致模型在validation score停止改善时停止迭代，即使迭代次数还没有到n_estimators。为n_estimators设置一个高值然后使用early_stopping_rounds来找到停止迭代的最佳时间是明智的。 存在随机的情况有时会导致validation score无法改善，因此需要指定一个数字，以确定在停止前允许多少轮退化。early_stopping_rounds = 5是一个合理的值。 因此，在五轮validation score无法改善之后训练将停止。 以下是early_stopping的代码： my_model = XGBRegressor(n_estimators=1000) my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 当使用early_stopping_rounds时，需要留出一些数据来检查要使用的轮数。 如果以后想要使所有数据拟合模型，请将n_estimators设置为在早期停止运行时发现的最佳值。 learning_rate对于更好的XGBoost模型，这是一个微妙但重要的技巧： XGBoost模型不是通过简单地将每个组件模型中的预测相加来获得预测，而是在将它们添加之前将每个模型的预测乘以一个小数字。这意味着我们添加到集合中的每个树都不会对最后结果有决定性的影响。在实践中，这降低了模型过度拟合的倾向。 因此，使用一个较大的n_estimators值并不会造成过拟合。如果使用early_stopping_rounds，树的数量会被设置成一个合适的值。 通常，较小的learning rate（以及大量的estimators）将产生更准确的XGBoost模型，但是由于它在整个循环中进行更多迭代，因此也将使模型更长时间进行训练。 包含学习率的代码如下： my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 小结XGBoost目前是用于在传统数据（也称为表格或结构数据）上构建精确模型的主要算法 from xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model1 = XGBRegressor() my_model1.fit(train_X, train_y, verbose=False) predictions = my_model1.predict(test_X) print(\"Mean Absolute Error 1: \" + str(mean_absolute_error(predictions, test_y))) my_model2 = XGBRegressor(n_estimators=1000) my_model2.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model2.predict(test_X) print(\"Mean Absolute Error 2: \" + str(mean_absolute_error(predictions, test_y))) my_model3 = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model3.fit(train_X, train_y, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model3.predict(test_X) print(\"Mean Absolute Error 3: \" + str(mean_absolute_error(predictions, test_y))) 分类模型以天池竞赛中的《快来一起挖掘幸福感！》中的数据为例，开始一个多分类模型的的实例 导入包import pandas as pd from matplotlib import pyplot as plt import xgboost as xgb from sklearn.model_selection import learning_curve, train_test_split,GridSearchCV from sklearn.metrics import accuracy_score from sklearn.metrics import mean_absolute_error 导入数据''' ## 准备训练集和测试集 ''' data = pd.read_csv('happiness_train_abbr.csv') y=data['happiness'] data.drop('happiness',axis=1,inplace=True) data.drop('survey_time',axis=1,inplace=True)#survey_time格式不能直接识别 X=data 数据集划分train_x, test_x, train_y, test_y = train_test_split (X, y, test_size =0.30, early_stopping_rounds=10,random_state = 33) XGBoost模型训练''' ## xgboost训练 ''' params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1 } #第一次设置300次的迭代，评测的指标是\"merror\",\"mlogloss\"，这是一个多分类问题。 model = xgb.XGBClassifier(params) eval_set = [(train_x, train_y), (test_x, test_y)] model.fit(train_x, train_y, eval_set=eval_set, eval_metric=[\"merror\", \"mlogloss\"],verbose=True) predictions = model.predict(test_x) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) accuracy = accuracy_score(test_y, predictions) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) 模型可视化''' ## 可视化训练过程 ''' results = model.evals_result() epochs = len(results['validation_0']['merror']) x_axis = range(0, epochs) from matplotlib import pyplot fig, ax = pyplot.subplots(1,2,figsize=(10,5)) ax[0].plot(x_axis, results['validation_0']['mlogloss'], label='Train') ax[0].plot(x_axis, results['validation_1']['mlogloss'], label='Test') ax[0].legend() ax[0].set_title('XGBoost Log Loss') ax[0].set_ylabel('Log Loss') ax[0].set_xlabel('epochs') ax[1].plot(x_axis, results['validation_0']['merror'], label='Train') ax[1].plot(x_axis, results['validation_1']['merror'], label='Test') ax[1].legend() ax[1].set_title('XGBoost Classification Error') ax[1].set_ylabel('Classification Error') ax[1].set_xlabel('epochs') pyplot.show() 实际训练效果，在第146次迭代就停止了，说明最好的效果实在136次左右。根据许多大牛的实践经验，选择early_stopping_rounds = 10% * n_estimators。 最终输出模型最佳状态下的结果： print (\"best iteration:\",model.best_iteration) limit = model.best_iteration predictions = model.predict(test_x,ntree_limit=limit) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) accuracy = accuracy_score(test_y, predictions) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) 3 参考资料https://www.kaggle.com/dansbecker/xgboost https://blog.csdn.net/lujiandong1/article/details/52777168 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST","date":"2019-12-24T16:00:00.000Z","updated":"2020-04-11T11:32:44.329Z","comments":true,"path":"2019/12/25/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost/","link":"","permalink":"https://dataquaner.github.io/2019/12/25/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost/","excerpt":"","text":"前言XGBoost的全称是eXtreme Gradient Boosting，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。XGBoost是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用XGBoost进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，XGBoost的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从XGBoost的数学原理和工程实现上进行介绍，然后介绍XGBoost的优缺点。 数学原理生成一棵树Boosting Tree回顾XGBoost模型是大规模并行boosting tree的工具，它是目前较好的开源boosting tree工具包。因此，在了解XGBoost算法基本原理之前，需要首先了解Boosting Tree算法基本原理。Boosting方法是一类应用广泛且非常有效的统计学习方法。它是基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比任何一个专家单独的判断要好。这种思想整体上可以分为两种： 强可学习：如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称为强可学习，直接单个模型就搞定常规问题。就好比专家给出的意见都很接近且都是正确率很高的结果，那么一个专家的结论就可以用了，这种情况非常少见。 弱可学习：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。这种情况是比较常见的。 boosting算法主要是针对弱可学习的分类器来开展优化工作。其关心的问题包括两方面内容： （1）在每一轮如何改变训练数据的权值和概率分布； （2）如何将弱分类器组合成一个强分类器，这种思路较好的就是AdaBoost算法，以前在遥感图像地物识别中得到过应用。 Boosting Tree模型采用加法模型与前向分步算法，而且基模型都是决策树模型。前向分步算法（Forward stage wise additive model）是指在叠加新的基模型的基础上同步进行优化，具体而言，就是每一次叠加的模型都去拟合上一次模型拟合后产生的残差（Residual）。从算法模型解释上来说，Boosting Tree是决策树的加法模型： （1） 上式中M为决策树的数量； 为某个决策树； 为对应决策树的参数。 Boosting Tree模型采用前向分步算法，其中假设 ，则第m步的模型是： （2） 为求解对应的参数 ，需要最小化相应损失函数来确定，具体公式如下： （3） 由前向分步算法得到M棵决策树 后，再进行加和，就得到了提升树模型 。在xgboost论文中提到的一个明显的boosting tree的加和应用案例如图3所示。 图2 boosting tree的累加效果示意图 相关树模型的参数值求解主要依据于损失函数的定义。 一般来言对于分类问题，选择指数损失函数作为损失函数时，将形成AdaBoost模型； 对于回归问题，损失函数常利用平方损失函数。为了扩展Boosting Tree的应用范围，需要构建一种可以广泛适用的残差描述方式来满足于任意损失函数的形式，为解决分类问题的Gradient Boosting Decision Tree算法应运而生。 带正则项的Boosting Tree模型和带梯度的Boosting Tree推导过程 目标函数我们知道 XGBoost 是由 个基模型组成的一个加法运算式： 其中 为第 个基模型， 为第 个样本的预测值。 损失函数可由预测值 与真实值 进行表示： 其中 为样本数量。 我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的损失函数 与抑制模型复杂度的正则项 组成，所以我们有： 为模型的正则项，由于 XGBoost 支持决策树也支持线性模型，所以这里再不展开描述。 我们知道 boosting 模型是前向加法，以第 步的模型为例，模型对第 个样本 的预测为： 其中 由第 步的模型给出的预测值，是已知常数， 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成： 求此时最优化目标函数，就相当于求解 。 泰勒公式是将一个在 处具有 阶导数的函数 利用关于 的 次多项式来逼近函数的方法，若函数 在包含 的某个闭区间 上具有 阶导数，且在开区间 上具有 阶导数，则对闭区间 上任意一点 有 ，其中的多项式称为函数在 处的泰勒展开式， 是泰勒公式的余项且是 的高阶无穷小。 根据泰勒公式我们把函数 在点 处进行泰勒的二阶展开，可得到如下等式： 我们把 视为 ， 视为 ，故可以将目标函数写为： 其中 为损失函数的一阶导， 为损失函数的二阶导，注意这里的导是对 求导。 我们以平方损失函数为例： 则： 由于在第 步时 其实是一个已知的值，所以 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成： 所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 ，最后根据加法模型得到一个整体模型。 基于决策树的目标函数损失函数可由预测值 与真实值 进行表示： 其中， 为样本的数量。 我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要在目标函数中添加正则项，用于防止过拟合。所以目标函数由模型的损失函数 与抑制模型复杂度的正则项 组成，目标函数的定义如下： 其中， 是将全部 棵树的复杂度进行求和，添加到目标函数中作为正则化项，用于防止模型过度拟合。 由于XGBoost是boosting族中的算法，所以遵从前向分步加法，以第 步的模型为例，模型对第 个样本 的预测值为： 其中， 是由第 步的模型给出的预测值，是已知常数， 是这次需要加入的新模型的预测值。此时，目标函数就可以写成： 注意上式中，只有一个变量，那就是第 棵树 ，其余都是已知量或可通过已知量可以计算出来的。细心的同学可能会问，上式中的第二行到第三行是如何得到的呢？这里我们将正则化项进行拆分，由于前 棵树的结构已经确定，因此前 棵树的复杂度之和可以用一个常量表示，如下所示： 泰勒公式展开泰勒公式是将一个在 处具有 阶导数的函数 利用关于 的 次多项式来逼近函数的方法。若函数 在包含 的某个闭区间 上具有 阶导数，且在开区间 上具有 阶导数，则对闭区间 上任意一点 有： 其中的多项式称为函数在 处的泰勒展开式， 是泰勒公式的余项且是 的高阶无穷小。 根据泰勒公式，把函数 在点 处进行泰勒的二阶展开，可得如下等式： 回到XGBoost的目标函数上来， 对应损失函数 ， 对应前 棵树的预测值 ， 对应于我们正在训练的第 棵树 ，则可以将损失函数写为： 其中， 为损失函数的一阶导， 为损失函数的二阶导，注意这里的求导是对 求导。 我们以平方损失函数为例： 则： 将上述的二阶展开式，带入到XGBoost的目标函数中，可以得到目标函数的近似值： 由于在第 步时 其实是一个已知的值，所以 是一个常数，其对函数的优化不会产生影响。因此，去掉全部的常数项，得到目标函数为： 所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 ，最后根据加法模型得到一个整体模型。 一棵树的生长细节分裂结点在实际训练过程中，当建立第 t 棵树时，XGBoost采用贪心法进行树结点的分裂： 从树深为0时开始： 对树中的每个叶子结点尝试进行分裂； 每次分裂后，原来的一个叶子结点继续分裂为左右两个子叶子结点，原叶子结点中的样本集将根据该结点的判断规则分散到左右两个叶子结点中； 新分裂一个结点后，我们需要检测这次分裂是否会给损失函数带来增益，增益的定义如下： 如果增益Gain&gt;0，即分裂为两个叶子节点后，目标函数下降了，那么我们会考虑此次分裂的结果。 但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？接下来会讲到。 寻找最佳分裂点 在实际训练过程中，当建立第 棵树时，一个非常关键的问题是如何找到叶子节点的最优切分点，XGBoost支持两种分裂节点的方法——贪心算法和近似算法。 贪心算法 从树的深度为0开始： 对每个叶节点枚举所有的可用特征； 针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益； 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集； 回到第1步，递归执行直到满足特定条件为止； 那么如何计算每个特征的分裂收益呢？ 假设我们在某一节点完成特征分裂，则分裂前的目标函数可以写为： 分裂后的目标函数为： 则对于目标函数来说，分裂后的收益为： 注意：该特征收益也可作为特征重要性输出的重要依据。 对于每次分裂，我们都需要枚举所有特征可能的分割方案，如何高效地枚举所有的分割呢？ 假设我们要枚举某个特征所有 这样条件的样本，对于某个特定的分割点 我们要计算 左边和右边的导数和。 我们可以发现对于所有的分裂点 ，只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 、 。然后用上面的公式计算每个分割方案的收益就可以了。 观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入新叶子的惩罚项，也就是说引入的分割带来的增益如果小于一个阀值的时候，我们可以剪掉这个分割。 上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。 但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。 基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案： 特征预排序+缓存：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。 分位点近似法：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。 并行查找：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。 近似算法 贪心算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪心算法这一缺点给出了近似最优解。 对于每个特征，只考察分位点可以减少计算复杂度。 该算法首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。 在提出候选切分点时有两种策略： Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割； Local：每次分裂前将重新提出候选切分点。 直观上来看，Local策略需要更多的计算步骤，而Global策略因为节点已有划分所以需要更多的候选点。 下图给出不同种分裂策略的AUC变化曲线，横坐标为迭代次数，纵坐标为测试集AUC，eps为近似算法的精度，其倒数为桶的数量。 从上图我们可以看到， Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在eps取值合理的情况下，分位数策略可以获得与贪心算法相同的精度。 近似算法简单来说，就是根据特征 的分布来确定 个候选切分点 ，然后根据这些候选切分点把相应的样本放入对应的桶中，对每个桶的 进行累加。最后在候选切分点集合上贪心查找。该算法描述如下： 算法讲解： 第一个for循环：对特征k根据该特征分布的分位数找到切割点的候选集合 。这样做的目的是提取出部分的切分点不用遍历所有的切分点。其中获取某个特征k的候选切割点的方式叫proposal(策略)。XGBoost 支持 Global 策略和 Local 策略。 第二个for循环：将每个特征的取值映射到由该特征对应的候选点集划分的分桶区间，即 。对每个桶区间内的样本统计值 G,H并进行累加，最后在这些累计的统计量上寻找最佳分裂点。这样做的目的是获取每个特征的候选分割点的 G,H值。 下图给出近似算法的具体例子，以三分位为例： 根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 G,H 值，最终求解节点划分的增益。 停止生长一棵树不会一直生长下去，下面是一些常见的限制条件。 (1) 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 (2) 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 (3) 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。 每个叶子结点的样本权值和计算方式如下： 总结推导过程： 算法工程优化对内存的优化：列块并行学习在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而 XGBoost 在训练之前会根据特征对数据进行排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。 作者提出通过按特征进行分块并排序，在块里面保存排序后的特征值及对应样本的引用，以便于获取样本的一阶、二阶导数值。具体流程为： 整体训练数据可以看做一个 的超大规模稀疏矩阵 按照mini-batch的方式横向分割，可以切成很多个“Block” 每一个“Block”内部采用一种Compress Sparse Column的稀疏短阵格式，每一列特征分别做好升序排列，便于搜索切分点，整体的时间复杂度有效降低。 通过Block的设置，可以采用并行计算，从而提升模型训练速度。 具体方式如图： 通过顺序访问排序后的块遍历样本特征的特征值，方便进行切分点的查找。此外分块存储后多个特征之间互不干涉，可以使用多线程同时对不同的特征进行切分点查找，即特征的并行化处理。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 XGBoost 能够实现分布式或者多线程计算的原因。 对CPU Cache的优化：缓存优化针对一个具体的块(block)，其中存储了排序好的特征值，以及指向特征值所属样本的索引指针，算法需要间接地利用索引指针来获得样本的梯度值。列块并行学习的设计可以减少节点分裂时的计算量，在顺序访问特征值时，访问的是一块连续的内存空间，但通过特征值持有的索引（样本索引）访问样本获取一阶、二阶导数时，这个访问操作访问的内存空间并不连续，这样可能造成cpu缓存命中率低，影响算法效率。由于块中数据是按特征值来排序的，当索引指针指向内存中不连续的样本时，无法充分利用CPU缓存来提速。 为了解决缓存命中率低的问题，XGBoost 提出了两种优化思路。 （1）提前取数（Prefetching） 对于精确搜索，利用多线程的方式，给每个线程划分一个连续的缓存空间，当training线程在按特征值的顺序计算梯度的累加时，prefetching线程可以提前将接下来的一批特征值对应的梯度加载到CPU缓存中。为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就实现了非连续空间到连续空间的转换，提高了算法效率。 （2）合理设置分块大小 对于近似分桶搜索，按行分块时需要准确地选择块的大小。块太小会导致每个线程的工作量太少，切换线程的成本过高，不利于并行计算；块太大导致缓存命中率低，需要花费更多时间在读取数据上。经过反复实验，作者找到一个合理的block_size为 。 对IO的优化：核外块计算当数据量非常大时，我们不能把所有的数据都加载到内存中。那么就必须将一部分需要加载进内存的数据先存放在硬盘中，当需要时再加载进内存。这样操作具有很明显的瓶颈，即硬盘的IO操作速度远远低于内存的处理速度，肯定会存在大量等待硬盘IO操作的情况。针对这个问题作者提出了“核外”计算的优化方法。具体操作为，将数据集分成多个块存放在硬盘中，使用一个独立的线程专门从硬盘读取数据，加载到内存中，这样算法在内存中处理数据就可以和从硬盘读取数据同时进行。此外，XGBoost 还用了两种方法来降低硬盘读写的开销： 块压缩（Block Compression）。论文使用的是按列进行压缩，读取的时候用另外的线程解压。对于行索引，只保存第一个索引值，然后用16位的整数保存与该block第一个索引的差值。作者通过测试在block设置为 个样本大小时，压缩比率几乎达到26% 29%。 块分区（Block Sharding ）。块分区是将特征block分区存放在不同的硬盘上，以此来增加硬盘IO的吞吐量。 优缺点优点 精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数； 灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导； 正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是XGBoost优于传统GBDT的一个特性。 Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统GBDT的实现也有学习速率； 列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是XGBoost异于传统GBDT的一个特性； 缺失值处理：对于特征的值有缺失的样本，XGBoost 采用的稀疏感知算法可以自动学习出它的分裂方向； XGBoost工具支持并行：boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 可并行的近似算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，用于高效地生成候选的分割点。 缺点 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集； 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。 XGBoost与GBDT的差异在分析XGBooting优缺点的时候，通过比较该算法与GBDT的差异，即可有较清楚的描述，具体表现在如下方面。 （1）基分类器的差异 GBDT算法只能利用CART树作为基学习器，满足分类应用； XGBoost算法除了回归树之外还支持线性的基学习器，因此其一方面可以解决带L1与L2正则化项的逻辑回归分类问题，也可以解决线性回问题。 （2）节点分类方法的差异 GBDT算法主要是利用Gini impurity针对特征进行节点划分； XGBoost经过公式推导，提出的weighted quantile sketch（加权分位数缩略图）划分方法，依据影响Loss的程度来确定连续特征的切分值。 （3）模型损失函数的差异 传统GBDT在优化时只用到一阶导数信息； xgboost则对代价函数进行了二阶泰勒展开，二阶导数有利于梯度下降的更快更准。 （4）模型防止过拟合的差异 GBDT算法无正则项，可能出现过拟合； Xgboost在代价函数里加入了正则项，用于控制模型的复杂度，降低了过拟合的可能性。 （5）模型实现上的差异 决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。其能够实现在特征粒度的并行。 XGBoost代码实现安装XGBoost依赖包pip install xgboost XGBoost分类和回归XGBoost有两大类接口：XGBoost原生接口 和 scikit-learn接口 ，并且XGBoost能够实现分类和回归两种任务。 （1）基于XGBoost原生接口的分类 from sklearn.datasets import load_iris import xgboost as xgb from xgboost import plot_importance from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split # read in the iris data iris = load_iris() X = iris.data y = iris.target # split train data and test data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234565) # set XGBoost's parameters params = { 'booster': 'gbtree', 'objective': 'multi:softmax', # 回归任务设置为：'objective': 'reg:gamma', 'num_class': 3, # 回归任务没有这个参数 'gamma': 0.1, 'max_depth': 6, 'lambda': 2, 'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 3, 'silent': 1, 'eta': 0.1, 'seed': 1000, 'nthread': 4, } plst = params.items() dtrain = xgb.DMatrix(X_train, y_train) num_rounds = 500 model = xgb.train(plst, dtrain, num_rounds) # 对测试集进行预测 dtest = xgb.DMatrix(X_test) ans = model.predict(dtest) # 计算准确率 cnt1 = 0 cnt2 = 0 for i in range(len(y_test)): if ans[i] == y_test[i]: cnt1 += 1 else: cnt2 += 1 print(\"Accuracy: %.2f %% \" % (100 * cnt1 / (cnt1 + cnt2))) # 显示重要特征 plot_importance(model) plt.show() （2）基于Scikit-learn接口的回归 这里，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer import xgboost as xgb from sklearn.metrics import mean_absolute_error # 1.读文件 data = pd.read_csv('./dataset/train.csv') data.dropna(axis=0, subset=['SalePrice'], inplace=True) # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = SimpleImputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.调用XGBoost模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = xgb.XGBRegressor(objective='reg:squarederror', verbosity=2) # xgb.XGBClassifier() XGBoost分类模型 my_model.fit(train_X, train_y, verbose=False) # 6.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X) # 7.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) XGBoost调参在上一部分中，XGBoot模型的参数都使用了模型的默认参数，但默认参数并不是最好的。要想让XGBoost表现的更好，需要对XGBoost模型进行参数微调。XGBoost需要调的参数不算多，他们可以分成三个部分： 1、General Parameters，即与整个模型属基调相关的参数； 2、Booster Parameters，即与单颗树生成有关的参数； 3、Learning Task Parameters，与模型调优相关的参数； General Parameters1、booster [default=gbtree] 即xgboost中基学习器类型，有两种选择，分别是树模型（gbtree）和线性模型（linear models） 2、silent [default=0] 即控制迭代日志的是否输出，默认输出； 3、nthread [default to maximum number of threads available if not set] 即控制模型训练调用机器的核心数，与sklearn中n_jobs的含义相似； Booster parameters因为booster有两种类型，常用的一般是树模型，这里只列树模型相关的参数： 1、eta [default=0.3] ：学习率 学习率，这个相当于sklearn中的learning_rate，常见的设置范围在0.01-0.2之间 2、min_child_weight [default=1]：叶节点的最小权重值 这个参数与GBM（sklearn）中的“min_samples_leaf”很相似，只不过这里不是样本数，而是权重值，如果样本的权重都是1，这两个参数是等同的；这个值设置较大时，通常树不会太深，可以控制过拟合，但太大时，容易造成欠拟合的现象，具体调参需要cv； 3、max_depth：树的最大深度 树的最大深度，含义很直白，控制树的复杂性；通常取值范围在3-10； 4、max_leaf_nodes：最大叶节点数 一般这个参数与max_depth二选一控制即可； 5、gamma [default=0]：分裂收益阈值 即用来比较每次节点分裂带来的收益，有效控制节点的过度分裂； 这个参数的变化范围受损失函数的选取影响； 6、max_delta_step [default=0] 这个参数暂时不是很理解它的作用范围，一般可以忽略它； 7、subsample [default=1]：采样比例 与sklearn中的参数一样，即每颗树的生成可以不去全部样本，这样可以控制模型的过拟合；通常取值范围0.5-1； 8、colsample_bytree [default=1]：特征采样的比例（每棵树） 即每棵树不使用全部的特征，控制模型的过拟合； 通常取值范围0.5-1； 9、colsample_bylevel [default=1] 特征采样的比例（每次分裂）； 这个与随机森林的思想很相似，即每次分裂都不取全部变量； 当7、8的参数设置较好时，该参数可以不用在意； 10、lambda [default=1] L2范数的惩罚系数，叶子结点的分数？； 11、alpha [default=0] L1范数的惩罚系数，叶子结点数？； 12、scale_pos_weight [default=1] 这个参数也不是很理解，貌似与类别不平衡的问题相关； Learning Task Parameters1、objective [default=reg:linear]：目标函数 通常的选项分别是：binary:logistic，用于二分类，产生每类的概率值；multi:softmax，用于多分类，但不产生概率值，直接产生类别结果；multi:softprob，类似softmax，但产生多分类的概率值； 2、eval_metric [ default according to objective ]：评价指标 当你给模型一个验证集时，会输出对应的评价指标值； 一般有：rmse ，均方误差；mae ，绝对平均误差；logloss ，对数似然值；error ，二分类错误率；merror ，多分类错误率；mlogloss ；auc 3、seed：即随机种子 关于XGBoost若干问题的思考XGBoost与GBDT的联系和区别有哪些？（1）GBDT是机器学习算法，XGBoost是该算法的工程实现。 （2）正则项：在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。 （3）导数信息：GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。 （4）基分类器：传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。 （5）子采样：传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。 （6）缺失值处理：传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。 （7）并行化：传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 为什么XGBoost泰勒二阶展开后效果就比较好呢？（1）从为什么会想到引入泰勒二阶的角度来说（可扩展性）：XGBoost官网上有说，当目标函数是MSE时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如logistic loss的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把MSE推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与MSE统一？是因为MSE是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与MSE统一了，那就只用推导MSE就好了。 （2）从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。 XGBoost对缺失值是怎么处理的？在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。 XGBoost为什么可以并行训练？（1）XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。 （2）XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 20道XGBoost面试题简单介绍一下XGBoost首先需要说一说GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。 XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。 XGBoost与GBDT有什么不同 基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。 导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。 正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。 列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。 并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 XGBoost为什么使用泰勒二阶展开 精准性：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导。 XGBoost为什么可以并行训练 XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。 XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 XGBoost为什么快 分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点 候选分位点：每个特征采用常数个分位点作为候选分割点 CPU cache 命中优化： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。 Block 处理优化：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐 XGBoost防止过拟合的方法XGBoost在设计时，为了防止过拟合做了很多优化，具体如下： 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化 列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可） 子采样：每轮计算可以不使用全部样本，使算法更加保守 shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间 XGBoost如何处理缺失值XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下： 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。 find_split时，缺失值处理的伪代码 XGBoost中叶子结点的权重如何计算出来XGBoost目标函数最终推导形式如下： 利用一元二次函数求最值的知识，当目标函数达到最小值Obj时，每个叶子结点的权重为wj。 具体公式如下： XGBoost中的一棵树的停止生长条件 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。 RF和GBDT的区别相同点： 都是由多棵树组成，最终的结果都是由多棵树一起决定。 不同点： 集成学习：RF属于bagging思想，而GBDT是boosting思想 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成) 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感 泛化能力：RF不易过拟合，而GBDT容易过拟合 XGBoost如何处理不平衡数据对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决： 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10； 第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。 原话是这么说的： For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of xgboost model, and there are two ways to improve it. If you care only about the ranking order (AUC) of your prediction Balance the positive and negative weights, via scale_pos_weight Use AUC for evaluation If you care about predicting the right probability In such a case, you cannot re-balance the dataset In such a case, set parameter max_delta_step to a finite number (say 1) will help convergence 那么，源码到底是怎么利用scale_pos_weight来平衡样本的呢，是调节权重还是过采样呢？请看源码： if (info.labels[i] == 1.0f) w *= param_.scale_pos_weight 可以看出，应该是增大了少数样本的权重。 除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。 比较LR和GBDT，说说什么情景下GBDT不如LR先说说LR和GBDT的区别： LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程 GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合； 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下： 先看一个例子： 假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。 那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1f1 + Wifi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？ 仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。 XGBoost中如何对树进行剪枝 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。 XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。 XGBoost如何选择最佳分裂点？XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。 因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。 如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。 XGBoost的Scalable性如何体现 基分类器的scalability：弱分类器可以支持CART决策树，也可以支持LR和Linear。 目标函数的scalability：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。 学习方法的scalability：Block结构支持并行化，支持 Out-of-core计算。 XGBoost如何评价特征的重要性我们采用三种方法来评判XGBoost模型中特征的重要程度： 官方文档：（1）weight - the number of times a feature is used to split the data across all trees. （2）gain - the average gain of the feature when it is used in trees. （3）cover - the average coverage of the feature when it is used in trees. weight ：该特征在所有树中被用作分割样本的特征的总次数。 gain ：该特征在其出现过的所有树中产生的平均增益。 cover ：该特征在其出现过的所有树中的平均覆盖范围。 注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。 XGBooost参数调优的一般步骤首先需要初始化一些基本变量，例如： max_depth = 5 min_child_weight = 1 gamma = 0 subsample, colsample_bytree = 0.8 scale_pos_weight = 1 (1) 确定learning rate和estimator的数量 learning rate可以先用0.1，用cv来寻找最优的estimators (2) max_depth和 min_child_weight 我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。 max_depth，每棵子树的最大深度，check from range(3,10,2)。 min_child_weight，子节点的权重阈值，check from range(1,6,2)。 如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。 (3) gamma 也称作最小划分损失min_split_loss，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。 如果大于该阈值，则该叶子节点值得继续划分 如果小于该阈值，则该叶子节点不值得继续划分 (4) subsample, colsample_bytree subsample是对训练的采样比例 colsample_bytree是对特征的采样比例 both check from 0.6 to 0.9 (5) 正则化参数 alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100 lambda 是L2正则化系数 (6) 降低学习率 降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1 XGBoost模型如果过拟合了怎么解决当出现过拟合时，有两类参数可以缓解： 第一类参数：用于直接控制模型的复杂度。包括max_depth,min_child_weight,gamma 等参数 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample,colsample_bytree 还有就是直接减小learning rate，但需要同时增加estimator 参数。 为什么XGBoost相比某些模型对缺失值不敏感对存在缺失值的特征，一般的解决方法是： 离散型变量：用出现次数最多的特征值填充； 连续型变量：用中位数或均值填充； 一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。 而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。 XGBoost对缺失数据有特定的处理方法，详情参考上篇文章第7题。 因此，对于有缺失值的数据在经过缺失处理后： 当数据量很小时，优先用朴素贝叶斯 数据量适中或者较大，用树模型，优先XGBoost 数据量较大，也可以用神经网络 避免使用距离度量相关的模型，如KNN和SVM XGBoost和LightGBM的区别 （1）树生长策略：XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 （2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下： 减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。 计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑏𝑖𝑛𝑠)。 LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算 但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？ xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。 （3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。 （4）缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。 （5）LightGBM 与 XGboost 的并行策略不同： 特征并行 ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。 数据并行 ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。 投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。 参考资料XGBoost论文解读： 【1】Chen T , Guestrin C . XGBoost: A Scalable Tree Boosting System[J]. 2016. 【2】Tianqi Chen的XGBoost的Slides 【3】对xgboost的理解 - 金贵涛的文章 - 知乎 【4】CTR预估 论文精读(一)–XGBoost 【5】XGBoost论文阅读及其原理 - Salon sai的文章 - 知乎 【6】XGBoost 论文翻译+个人注释 XGBoost算法讲解： 【7】XGBoost超详细推导，终于有人讲明白了！ 【8】终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！ 【9】机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - wepon的回答 - 知乎 【10】GBDT算法原理与系统设计简介，wepon XGBoost实例： 【11】Kaggle 神器 xgboost 【12】干货 | XGBoost在携程搜索排序中的应用 【13】史上最详细的XGBoost实战 - 章华燕的文章 - 知乎 【14】XGBoost模型构建流程及模型参数微调（房价预测附代码讲解） - 人工智能学术前沿的文章 - 知乎 XGBoost面试题： 【15】珍藏版 | 20道XGBoost面试题，你会几个？(上篇) 【16】珍藏版 | 20道XGBoost面试题，你会几个？(下篇) 【17】推荐收藏 | 10道XGBoost面试题送给你 【18】面试题：xgboost怎么给特征评分？ 【19】[校招-基础算法]GBDT/XGBoost常见问题 - Jack Stark的文章 - 知乎 【20】《百面机器学习》诸葛越主编、葫芦娃著，P295-P297。 【21】灵魂拷问，你看过Xgboost原文吗？ - 小雨姑娘的文章 - 知乎 【22】为什么xgboost泰勒二阶展开后效果就比较好了呢？ - Zsank的回答 - 知乎 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","slug":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","date":"2019-12-24T06:08:00.000Z","updated":"2020-04-11T11:32:24.599Z","comments":true,"path":"2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-05-ti-du-ti-sheng-shu-suan-fa-gbdt/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-05-ti-du-ti-sheng-shu-suan-fa-gbdt/","excerpt":"","text":"1 前言前面讲述了《决策树的特征选择》、《决策树的生成》、《决策树的剪枝》，熟悉了单棵决策树的的实现细节，在实际应用时，往往采用多棵决策树组合的形式完成目标任务。那么如何组合单棵决策树可以使得模型效果更优呢？目前主要有两种思想：bagging和boosting，分别对应的典型算法随机森林和Adaboost、GBDT等。 Bagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。典型实现算法随机森林 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。典型实现算法是GBDT boosting的思想如下图： 基于boosting思想的经典算法是Adaboost和GBDT。关于Adaboost的介绍可以参考《Adaboost算法》，本文重点介绍GBDT。 2 什么是GBDT GBDT(Gradient Boosting Decision Tree) 是一种迭代的决策树算法，是回归树，而不是分类树。该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性。 GBDT用来做回归预测，调整后也可以用于分类。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。具体训练过程如下图示意： 3 GBDT算法原理GBDT算法的核心思想 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。即所有弱分类器相加等于预测值，下一个弱分类器去拟合误差函数对预测值的梯度。 GBDT加入了简单的数值优化思想。 Xgboost更加有效应用了数值优化。相比于gbdt，最重要是对损失函数变得更复杂。目标函数依然是所有树想加等于预测值。损失函数引入了一阶导数，二阶导数。 不同于随机森林所有树的预测求均值，GBDT所有的树的预测值加起来是最终的预测值，可以不断接近真实值。 GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想的通俗解释 假如有个人30岁， 第一棵树，我们首先用20岁去拟合，发现损失有10岁， 第二颗，这时我们用6岁去拟合剩下的损失，发现差距还有4岁， 第三颗，我们用3岁拟合剩下的差距，差距就只有一岁了。 三棵树加起来为29岁，距离30最近。 从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？ 4 负梯度拟合在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为 利用(xi,rti)(i=1,2,..m),我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域Rtj,j=1,2,…,J。其中J为叶子节点的个数。 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj如下： 这样就得到了本轮的决策树拟合函数如下： 从而本轮最终得到的强学习器的表达式如下： 通过损失函数的负梯度来拟合，找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 传统模型中，我们定义一个固定结构的函数，然后通过样本训练拟合更新该函数的参数，获得最后的最优函数。 GBDT提升树并非如此。它是加法模型，是不定结构的函数，通过不断加入新的子函数来使得模型能更加拟合训练数据，直到最优。函数更新的迭代方式可以写作：)。所以如果要更快逼近最优的函数，我们就需要在正确的方向上添加子函数，这个“正确的方向”当然就是损失减少最快的方向。所以我们需要用损失函数)对函数)求导（注意不是对x求导），求得的导数，就是接下来)需要弥补的方向。在上式中就是表示导数的拟合。 导数值跟损失函数的选择有关系。如果选择平方损失误差，那么它的导数就是： 令人惊喜的是这正是真实值和估计值之间的残差！ 这就是为什么谈到GBDT的时候，很多文章都提到“残差”的拟合，却没有说“梯度”的拟合。其实它们在平方损失误差条件下是一个意思！BTW，上面之所以用了是为了计算方便，常数项并不会影响平方损失误差，以及残差的比较。 现在让我们重新理解这个式子： 1）先求取一个拟合函数Fm-1(x) 2）用Fm-1(x)进行预测，计算预测值和实际值的残差 3）为了弥补上面的残差，用一个函数△F(x)来拟合这个残差 4）这样最终的函数就变成了)，其中Fm-1(x)用来拟合原数据，△F(x)用来拟合残差 5）如果目前还有较大的残差，则循环2)~4)，更新函数到Fm+1(x) , Fm+2(x), …..直到残差满足条件。 针对以上流程，我们用实例来说明 5 提升树的生成过程有以下数据需要用回归，并要求平方损失误差小于0.2（这0.2就是我们人为设置的最优条件，否则训练可能会无休止地进行下去）时，可以停止建树： 第一棵树 1） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 比如s=1.5,分割成了两个子集： 通过公式求平方损失误差 而其中)为各自子集的平均值时，可以使得每个子集的平方损失误差最小。 求平均值为：)，进而求得平方损失误差为 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=6.5时,为所有切分点里平方损失误差最小的 2) 选择切分点s=6.5构建第一颗回归树，各分支数值使用 ： 第一轮过后，我们提升树为: 3) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果： 提升树拟合数据的平方损失误差计算： 大于0.2，则还需要继续建树。 第二棵树 4) 确定需要拟合的训练数据为上一棵树的残差： 5） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=3.5时,为所有切分点里平方损失误差最小的 6) 选择切分点s=3.5构建第二颗回归树，各分支数值使用 ： 第二轮过后，我们提升树为: 7) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果，同时对比初始值和上一颗树的残差： 可以看见，随着树的增多，残差一直在减少。 到目前为止，提升树拟合数据的平方损失误差计算： 多说一句，这里是从全局提升树的角度去计算损失，其实和上面第5）步中从最后一颗树的角度去计算损失，结果是一样的 目前损失大于0.2的阈值，还需要继续建树 … … 第六棵树 到第六颗树的时候，我们已经累计获得了： 此时提升树为： 此时用拟合训练数据的平方损失误差为： 平方损失误差小于0.2的阈值，停止建树。 为我们最终所求的提升树。 6 回归算法输入： 最大迭代次数T, 损失函数L，训练样本集 输出： 强学习器f(x) 1） 初始化弱学习器 2）对迭代轮数t=1,2,…T有： a) 对样本i=1,2，…m，计算负梯度 b) 利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,…,J。其中J为回归树t的叶子节点的个数。 c) 对叶子区域j =1,2,..J,计算最佳拟合值 (d) 更新强学习器 3） 得到强学习器f(x)的表达式 7 分类算法GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。 为了解决这个问题，主要有两个方法， 1）一个是用指数损失函数，此时GBDT退化为Adaboost算法。 2）另一种方法是用类似于逻辑回归的对数似然损失函数的方法。 也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。 7.1 二元分类算法对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为： 其中y∈{−1,+1}。则此时的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。 7.2 多元分类算法多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为： 集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。 8 正则化和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。 GBDT的正则化主要有三种方式。 第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代 如果我们加上了正则化项，则有 ν的取值范围为0&lt;ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了 9 总结GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。 优点 1) 可以灵活处理各种类型的数据，包括连续值和离散值。 2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 缺点 1) 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"}]},{"title":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","slug":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","date":"2019-12-24T06:08:00.000Z","updated":"2020-04-11T11:33:49.104Z","comments":true,"path":"2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-09-id3-c4.5-cart-sui-ji-sen-lin-bagging-boosting-adaboost-gbdt-xgboost-suan-fa-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-09-id3-c4.5-cart-sui-ji-sen-lin-bagging-boosting-adaboost-gbdt-xgboost-suan-fa-zong-jie/","excerpt":"","text":"最近心血来潮，整理了一下和树有关的方法和模型，请多担待！ 决策树首先，决策树是一个有监督的分类模型，其本质是选择一个能带来最大信息增益的特征值进行树的分割，直到到达结束条件或者叶子结点纯度到达一定阈值。下图是决策树的一个简单例子 按照分割指标和分割方法，决策树的经典模型可以分为ID3、C4.5以及CART ID3：以信息增益为准则来选择最优划分属性信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 C4.5：基于信息增益率准则选择最优分割属性信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 CART：以基尼系数为准则选择最优划分属性CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 随机森林在讲随机森林之前，我们需要补充一点组合分类器的概念，将多个分类器的结果进行多票表决或者是取平均值，以此作为最终的结果。 1、构建组合分类器的好处： （1）、提升模型精度：整合各个模型的分类结果，得到更合理的决策边界，减少整体错误，实现更好的分类效果； （2）、处理过大或过小的数据集：数据集较大时，可以将数据集划分成多个子集，对子集构建分类器；数据集较小时，可通过多种抽样方式（bootstrap）从原始数据集抽样产生多组不同的数据集，构建分类器。 （3）、若决策边界过于复杂，则线性模型不能很好地描述真实情况。因此先对于特定区域的数据集，训练多个线性分类器，再将它们集成。 （4）、比较适合处理多源异构数据（存储方式不同（关系型、非关系型），类别不同（时序型、离散型、连续型、网络结构数据）） 随机森林是一个典型的多个决策树的组合分类器。主要包括两个方面：数据的随机性选取，以及待选特征的随机选取。 （1）、数据的随机选取：第一，从原始的数据集中采取有放回的抽样（bootstrap），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。 （2）、待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。 组合树示例图 GBDT和xgboostbagging和boostingBagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。 boosting算法结构图 拿著名的Adaboost算法举例： 我们有一个数据集，样本大小为N，每一个样本对应一个原始标签起初，我们初始化样本的权重为1/N em计算的是当前数据下，模型的分类误差率，模型的系数值是基于分类误差率的 根据模型的分类结果，更新原始数据中数据的分布，增加被错分的数据被抽中的概率，以便下一次迭代的时候能被模型重新训练 最终的分类器是各个基分类器的组合 GBDTGBDT是以决策树（CART）为基学习器的GB算法，是迭代树，而不是分类树。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。有了前面Adaboost的铺垫，大家应该能很容易理解大体思想。 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。 xgboostXgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。 损失函数如下，引入了一阶导数，二阶导数。： 好的模型需要具备两个基本要素：一是要有好的精度（即好的拟合程度），二是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项） 常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。 每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差 目标函数如上图，最后一行画圈部分实际上就是预测值和真实值之间的残差 先对训练误差进行展开： xgboost则对代价函数进行了二阶泰勒展开，同时用到了残差平方和的一阶和二阶导数 再研究目标函数中的正则项： 树的复杂度可以用树的分支数目来衡量，树的分支我们可以用叶子结点的数量来表示 那么树的复杂度式子：右边第一项是叶子结点的数量T，第二项是树的叶子结点权重w的l2正则化，正则化是为了防止叶子结点过多 此时，每一次迭代，相当于在原有模型中增加一棵树，目标函数中，我们用wq（x）表示一棵树，包括了树的结构以及叶子结点的权重，w表示权重（反映预测的概率），q表示样本所在的索引号（反映树的结构） 将最终得到的目标函数对参数w求导，带回目标函数，可知目标函数值由红色方框部分决定： 因此，xgboost的迭代是以下图中gain式子定义的指标选择最优分割点的： 那么如何得到优秀的组合树呢？ 一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。 另一种方法：XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照特征值的密度分布，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将bucket边界上的特征值作为splitpoint的候选，遍历所有的候选分裂点来找到最佳分裂点。 上图近似算法公式的解释：将特征k的特征值进行排序，计算特征值分布，rk（z）表示的是对于特征k而言，其特征值小于z的权重之和占总权重的比例，代表了这些特征值的重要程度，我们按照这个比例计算公式，将特征值分成若干个bucket，每个bucket的比例相同，选取这几类特征值的边界作为划分候选点，构成候选集；选择候选集的条件是要使得相邻的两个候选分裂节点差值小于某个阈值。 综合以上的解说，我们可以得到xgboost相比于GBDT的创新之处： 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象； 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销 指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。 并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"极客时间《数据分析45讲总结》","slug":"极客时间《数据分析45讲总结》","date":"2019-12-17T08:05:00.000Z","updated":"2020-04-11T11:34:35.238Z","comments":true,"path":"2019/12/17/ji-ke-shi-jian-shu-ju-fen-xi-45-jiang-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/ji-ke-shi-jian-shu-ju-fen-xi-45-jiang-zong-jie/","excerpt":"","text":"1.前言该讲主要引导读者从全局去了解什么是数据分析？为什么做数据分析？怎么去做数据分析？答案就是：掌握数据，就是掌握规律。当你了解了市场数据，对它进行分析，就可以得到市场规律。当你掌握了产品自身的数据，对它进行分析，就可以了解产品的用户来源、用户画像等等。所以说数据是个全新的视角。数据分析如此重要，它不仅是新时代的“数据结构 + 算法”，也更是企业争夺人才的高地。 谈到数据分析，我们一般都会从3个方面入手： 数据采集 – 数据源，我们要用的原材料 数据挖掘 – 它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值（所谓的商业智能BI） 数据的可视化 – 数据领域中的万金油，直观了解数据分析结构 数据分析的三驾马车的关系如下： 下面来大致认识下这三驾马车： 2.数据采集：数据的采集，主要是和数据打交道，用工具对数据进行采集，常用的数据源，如何获取它们。在专栏里，后续会将介绍如何掌握“八爪鱼”这个自动抓取的神器，它可以帮你抓取 99% 的页面源。也会教读者如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如“王祖贤”的海报，还能自动给微博加粉丝，让你掌握自动化的快感。 3.数据挖掘：数据挖掘，它可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。 掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的。 4.数据可视化 为什么说数据要可视化，因为数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。 数据可视化的两种方法： Python ：在 Python 对数据进行清洗、挖掘的过程中，很多的库可以使用，像 Matplotlib、Seaborn 等第三方库进行呈现。 第三方工具：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。 数据分析包括数据采集、数据挖掘、数据可视化这三个部分。乍看你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点“高深莫测”，掌握起来是不是会吃力。其实这些都是不必要的烦恼。个人觉得只要内心笃定，认为自己一定能做成，学成，其他一切都是“纸老虎”哈。 再说下，陈博在文章中提到的如何来快速掌握数据分析，核心就是认知。我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程就是认知升级的过程。 我本人也是很赞同这种说法，简单一句就是“知行合一” 总结 记录下你每天的认知 这些认知对应工具的哪些操作 做更多练习来巩固你的认知 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/categories/Data-Analysis/"}],"tags":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/tags/Data-Analysis/"}]},{"title":"机器学习系列之决策树算法（01）：决策树特征选择","slug":"机器学习系列之决策树算法（01）：决策树特征选择","date":"2019-12-17T07:07:00.000Z","updated":"2020-04-11T11:31:34.243Z","comments":true,"path":"2019/12/17/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-01-jue-ce-shu-te-zheng-xuan-ze/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-01-jue-ce-shu-te-zheng-xuan-ze/","excerpt":"","text":"1.什么是特征选择【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。 【那么选择的是什么样的特征呢？】这里我们选择的特征需要是对训练数据有分类能力的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征没有分类能力，舍去这个特征对学习的精度不会有特别大的影响。 特征选择是决定用哪个特征来划分特征空间。 比如女生找男朋友，可能这个女生首先会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了 【那怎么判断哪个特征有更好的分类能力呢？】这时候【信息增益】就要出场了。 2.信息增益为了解释什么是信息增益，我们首先要讲解一下什么是【熵（entropy）】 熵（Entropy） 在热力学与化学中： 熵是一种测量在动力学方面【不能做功的能量的总数】，当总体熵增加，其做功能力也下降，熵的度量是能量退化的指标。 1948 年，香农把热力学中的熵引入到信息论中，称为香农熵。根据维基百科的描述： 在信息论中，熵是接收的每条消息中包含的信息的平均量。 更一般的，【熵表示随机变量的不确定性】。假设一个有限取值的离散随机变量 X 的概率分布如下： 那么它的熵定义为： 上式中的 b 通常取 2 或者自然对数 e，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。 从上式中，我们可以看到，熵与 X 的取值是没有关系的，它只与 X 的分布有关，所以 H 也可以写作 p 的函数： 我们现在来看两个随机变量的情况。 假设随机变量 (X, Y) 的联合概率分布如下： 我们使用条件熵（conditional entropy）H(Y|X)来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。 条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。 是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法： 有了上面的公式以后，条件熵的定义就非常容易理解了。 那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？ 信息增益的定义与信息增益算法既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？ 没错，由于熵表示不确定性，严格来说，信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」。 我们给出信息增益的最终定义： 特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。 这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。 假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。 很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要 信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大 条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定 信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类 下面我们给出信息增益的算法。 首先对数据做一些介绍： 假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 ， 为属于这个类的样本的个数，很显然 。 再假设我们有一个特征叫 A，A 有 n 个不同的取值 ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 ， 是这个子集的样本个数，很显然 。 我们把 中属于类别 的集合称作 ， 是其样本个数。 信息增益的计算就分为如下几个步骤： 计算 D 的经验熵 H(D)： \\2. 计算 A 对 D 的经验条件熵 H(D|A)： \\3. 计算信息增益 g(D, A)： 3.信息增益比看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？ 按照经验来看，【以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差】。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。类别越多代表特征越不确定，即熵越多，类别的信息增益越小。 为了校正这一误差，我们引入了【信息增益比（information gain ratio）】，又叫做信息增益率，它的定义如下： 特征 A 对训练数据集 D 的信息增益比 定义为其信息增益 与训练数据集 D 关于特征 A 的值的熵 之比。 其中， ，n 是 A 取值的个数。 两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。 4. ID3 ： 最大信息增益 ID3以信息增益为准则来选择最优划分属性 信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 5. C4.5 ：最大信息增益率 C4.5基于信息增益率准则选择最优分割属性的算法 信息增益比率通过引入一个被称作【分裂信息(Split information)】的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 6.CART ：最小基尼指数 CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归 CART是一棵二叉树，采用【二元切分法】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 7.分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]}]}