{"meta":{"title":"DataQuaner","subtitle":"","description":"Data & Algorithm & Business & Value","author":"Leon","url":"https://dataquaner.github.io","root":"/"},"pages":[{"title":"关于","date":"2019-11-14T14:27:21.000Z","updated":"2019-11-14T14:28:04.822Z","comments":true,"path":"about/index.html","permalink":"https://dataquaner.github.io/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-11-10T09:22:35.000Z","updated":"2019-11-10T09:23:03.650Z","comments":true,"path":"categories/index.html","permalink":"https://dataquaner.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-11-10T09:17:30.000Z","updated":"2019-11-10T09:20:41.842Z","comments":true,"path":"link/index.html","permalink":"https://dataquaner.github.io/link/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-10T09:21:38.000Z","updated":"2019-11-10T09:22:17.868Z","comments":true,"path":"tags/index.html","permalink":"https://dataquaner.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LightGBM算法基础系列之基础理论篇（1）","slug":"LightGBM算法基础系列之基础理论篇（1）","date":"2019-12-19T10:39:35.388Z","updated":"2019-12-19T10:39:35.388Z","comments":true,"path":"2019/12/19/LightGBM算法基础系列之基础理论篇（1）/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/","excerpt":"","text":"这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"LightGBM","slug":"Machine-Learning/LightGBM","permalink":"https://dataquaner.github.io/categories/Machine-Learning/LightGBM/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"数据存储之MySQL系列（01）：MySQL体系结构","slug":"数据存储之MySQL系列（01）：MySQL体系结构","date":"2019-12-19T10:39:12.063Z","updated":"2019-12-19T10:39:12.063Z","comments":true,"path":"2019/12/19/数据存储之MySQL系列（01）：MySQL体系结构/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/","excerpt":"","text":"","categories":[{"name":"DataBase","slug":"DataBase","permalink":"https://dataquaner.github.io/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"https://dataquaner.github.io/categories/DataBase/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dataquaner.github.io/tags/MySQL/"}]},{"title":"xgboost算法模型输出的解释","slug":"xgboost算模型输出的解释","date":"2019-12-19T10:38:11.639Z","updated":"2019-12-19T10:38:11.639Z","comments":true,"path":"2019/12/19/xgboost算模型输出的解释/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/","excerpt":"","text":"1. 问题描述 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。 2. 数据集 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据. 1234567891011121314from sklearn import datasets iris = datasets.load_iris()data = iris.data[:100]print data.shape#(100L, 4L)#一共有100个样本数据, 维度为4维 label = iris.target[:100]print label#正好选取label为0和1的数据[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 3. 训练集与测试集123from sklearn.cross_validation import train_test_split train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=0) 4. Xgboost建模4.1 模型初始化设置123456789101112131415161718import xgboost as xgbdtrain=xgb.DMatrix(train_x,label=train_y)dtest=xgb.DMatrix(test_x) params=&#123;'booster':'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'auc', 'max_depth':4, 'lambda':10, 'subsample':0.75, 'colsample_bytree':0.75, 'min_child_weight':2, 'eta': 0.025, 'seed':0, 'nthread':8, 'silent':1&#125; watchlist = [(dtrain,'train')] 4.2 建模与预测1234567891011121314bst=xgb.train(params,dtrain,num_boost_round=100,evals=watchlist) ypred=bst.predict(dtest) # 设置阈值, 输出一些评价指标y_pred = (ypred &gt;= 0.5)*1 from sklearn import metricsprint 'AUC: %.4f' % metrics.roc_auc_score(test_y,ypred)print 'ACC: %.4f' % metrics.accuracy_score(test_y,y_pred)print 'Recall: %.4f' % metrics.recall_score(test_y,y_pred)print 'F1-score: %.4f' %metrics.f1_score(test_y,y_pred)print 'Precesion: %.4f' %metrics.precision_score(test_y,y_pred)metrics.confusion_matrix(test_y,y_pred) Out[23]: 1234567AUC: 1.0000ACC: 1.0000Recall: 1.0000F1-score: 1.0000Precesion: 1.0000array([[13, 0], [ 0, 12]], dtype=int64) Yeah, 完美的模型, 完美的预测! 4.3 可视化输出123456789101112131415#对于预测的输出有三种方式?bst.predictSignature: bst.predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False) pred_leaf : bool When this option is on, the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree. Note that the leaf index of a tree is unique per tree, so you may find leaf 1 in both tree 1 and tree 0. pred_contribs : bool When this option is on, the output will be a matrix of (nsample, nfeats+1) with each record indicating the feature contributions (SHAP values) for that prediction. The sum of all feature contributions is equal to the prediction. Note that the bias is added as the final column, on top of the regular features. 4.3.1 得分默认的输出就是得分, 这没什么好说的, 直接上code. 12ypred = bst.predict(dtest)ypred Out[32]: 12345array([ 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.20081411], dtype=float32) 在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出. 4.3.2 所属的叶子节点当设置pred_leaf=True的时候, 这时就会输出每个样本在所有树中的叶子节点 12ypred_leaf = bst.predict(dtest, pred_leaf=True)ypred_leaf Out[33]: 1234567array([[1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1]]) 输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以ypred_leaf的维度为[100*100]. 对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树). 那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型. 12xgb.to_graphviz(bst, num_trees=0)#可视化第一棵树的生成情况 12#直接输出模型的迭代工程bst.dump_model(\"model.txt\") 12345678910111213booster[0]:0:[f3&lt;0.75] yes=1,no=2,missing=1 1:leaf=-0.019697 2:leaf=0.0214286booster[1]:0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0212184 2:leaf=0.0212booster[2]:0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0197404 2:leaf=0.0197235booster[3]: …… 通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值. 在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值. 同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值. 12leaf1 -1.381214leaf2 1.410950 在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了&#39;objective&#39;: &#39;binary:logistic&#39;, 因此使用函数将累加的值转换为实际的打分: f(x)=1/(1+exp(−x)) 12341/float(1+np.exp(1.38121416))Out[24]: 0.200814071121865031/float(1+np.exp(-1.410950))Out[25]: 0.8039157403338895 这就与ypred = bst.predict(dtest) 的分值相对应上了. 4.3.2 特征重要性接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性. 12ypred_contribs = bst.predict(dtest, pred_contribs=True)ypred_contribs Out[37]: 12345678910111213141516171819202122232425array([[ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663]], dtype=float32) 输出的ypred_contribs的维度为[100,5], 通过阅读前面的文档注释就可以知道, 最后一列是bias, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的. 通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例. 123456score_a = sum(ypred_contribs[0])print score_a# -1.38121373579score_b = sum(ypred_contribs[1])print score_b# 1.41094945744 相同的分值, 相同的处理情况. 到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"XGBboost","slug":"Machine-Learning/XGBboost","permalink":"https://dataquaner.github.io/categories/Machine-Learning/XGBboost/"}],"tags":[{"name":"XGBboost","slug":"XGBboost","permalink":"https://dataquaner.github.io/tags/XGBboost/"}]},{"title":"机器学习系列之决策树算法（02）：决策树的剪枝","slug":"机器学习系列之决策树算法（03）：决策树的剪枝","date":"2019-12-19T10:28:10.883Z","updated":"2019-12-19T10:27:50.378Z","comments":true,"path":"2019/12/19/机器学习系列之决策树算法（03）：决策树的剪枝/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8803%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D/","excerpt":"","text":"1. 前言上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现过拟合现象。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为剪枝(pruning)。 具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss funtion)或代价函数(cost funtion)来实现。 2.剪枝算法2.1 为什么要剪枝现象 接上一次讲的生成决策树，下面给出一张图。 横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。 实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。 可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。 原因 原因1：噪声、样本冲突，即错误的样本数据。 原因2：特征即属性不能完全作为分类标准。 原因3：巧合的规律性，数据量不够大。 这个时候，就需要对生成树进行修剪，也就是剪枝。 2.2 如何进行剪枝预剪枝预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法: (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。 (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。 (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。 (4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。 优点&amp;缺点 由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【怎样精确地估计何时停止树的增长是相当困难的】。 预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。 后剪枝后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。 这里主要介绍四种： REP-错误率降低剪枝 PEP-悲观剪枝 CCP-代价复杂度剪枝 MEP-最小错误剪枝 REP(Reduced Error Pruning)方法 对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S 优点： REP 是当前最简单的事后剪枝方法之一。 它的计算复杂性是线性的。 和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。 缺点： 但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意. PEP(Pessimistic Error Pruning)方法 为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, PEP 方法对误差估计增加了连续性校正(continuity correction)。关于PEP方法的数据解释待后续开专题梳理。 优点： PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一 PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。 TIPS： 个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。 MEP(Minimum Error Pruning)方法 MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。 优点： MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集. 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： 类别平均分配的前提假设现实几率不大&amp;对K太过敏感 对此，也有改进算法，我没有深入研究。 CCP(Cost-Complexity Pruning)方法 CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤: (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。 ​ (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。 缺点： 生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的. 对比四种方法 剪枝名称 剪枝方式 计算复杂度 误差估计 REP 自底向上 0(n) 剪枝集上误差估计 PEP 自顶向下 o(n) 使用连续纠正 CCP 自底向上 o(n2) 标准误差 MEP 自底向上 o(n) 使用连续纠正 ① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。 ② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。 ③ 如果数据集多（REP&amp;CCP←复杂但树小） ④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定） 3.总结决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。 剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。 预剪枝： 预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。 简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。 这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。 后剪枝： 后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。 同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。 后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。 剪枝方法的选择 如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。 后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。 有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。 经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。 无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。 函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点 函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。 函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值 上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"Decision Tree","slug":"Machine-Learning/Decision-Tree","permalink":"https://dataquaner.github.io/categories/Machine-Learning/Decision-Tree/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"机器学习系列之决策树算法（02）：决策树的生成","slug":"机器学习系列之决策树算法（02）：决策树的生成","date":"2019-12-19T10:28:04.764Z","updated":"2019-12-19T10:27:49.949Z","comments":true,"path":"2019/12/19/机器学习系列之决策树算法（02）：决策树的生成/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8802%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/","excerpt":"","text":"1. 前言上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。 2. 决策树的生成 示例数据表格 文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中 2.1 相关概念阐述2.1.1 决策树 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。 2.1.2 ID3、C4.5、CART在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【机器学习系列之决策树算法（01）：决策树特征选择】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。 信息熵 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。 ​ Entopy(t)=-Σp(i|t)log2p(i|t) 信息增益信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为： ​ Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di) 信息增益率 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7： ​ H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) 基尼系数 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下基尼系数 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低. CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分. 基尼系数的计算公式如下: ​ Gini = 1 – Σ (Pi)2 for i=1 to number of classes 2.2 完整生成过程 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例 2.2.1 根节点的选择 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为： ​ Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985 天气 其数据表格如下: 信息增益计算如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下： D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981 D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为： 3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965 其信息增益为： Gain(天气)=0.985 - 0.965 = 0.020 信息增益率计算 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为： H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556 则其信息增益率为： Gain_ratio(天气)=0.020/1.556=0.012 基尼系数计算 Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9 Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21 温度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 作为子节点的归一化信息熵为： 4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857 其信息增益为： Gain(温度)=0.985 - 0.857 = 0.128 信息增益率计算 属性熵为： H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378 则其信息增益率为： Gain_ratio(温度)=0.128/1.378=0.0928 基尼系数计算 Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 4/7 * 1.0 + 3/7 * 0.918 = 0.964 其信息增益为： Gain(湿度)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(湿度)=0.021/0.985=0.021 基尼系数计算 Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 作为子节点的归一化信息熵为： 3/7 * 1.0 + 4/7 * 0.918 = 0.964 其信息增益为： Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数计算 Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 根节点的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度 信息增益 Gain(天气)=0.985 - 0.965 = 0.020 Gain(温度)=0.985 - 0.857 = 0.128 Gain(湿度)=0.985 - 0.964 = 0.021 Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率 Gain_ratio(天气)=0.020/1.556=0.012 Gain_ratio(温度)=0.128/1.378=0.0928 Gain_ratio(湿度)=0.021/0.985=0.021 Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择: 根节点 子节点温度高:(待进一步进行选择) 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.2 子节点温度高的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： ​ Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 归一化信息熵为： 2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5 其信息增益为： Gain(天气)=1.0 - 0.5 = 0.5 信息增益率计算 对应的属性熵为： H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5 则其信息增益率为： Gain_ratio(天气)=0.5/1.5=0.33333 基尼系数计算 Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0 D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0 作为子节点的归一化信息熵为： 2/4 * 0.0 + 2/4 * 0.0 = 0.0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0 Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0 D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 1/4 * 0.0 + 3/4 * 0.918 = 0.688 其信息增益为： Gain(刮风)=1.0 - 0.688 = 0.312 信息增益率计算 属性熵为： H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918 则其信息增益率为： Gain_ratio(刮风)=0.312/0.918=0.349 基尼系数计算 Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 子节点温度高的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度 Gain(天气)=1.0 - 0.5 = 0.5 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 0.688 = 0.312 Gain_ratio(天气)=0.5/1.5=0.33333 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0.312/0.918=0.349 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.3 子节点温度中的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D (阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(天气)=1.0 - 0 = 1.0 信息增益率计算 对应的属性熵为： H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(天气)=1.0/1.0=1.0 基尼系数计算 Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 作为子节点的归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 作为子节点的归一化信息熵为： 1/1 * 1.0 = 1.0 其信息增益为： Gain(刮风)=1.0 - 1.0 = 0 信息增益率计算 属性熵为： H(刮风)=-(2/2 * log2(2/2) = 0.0 则其信息增益率为： Gain_ratio(刮风)=0/0 = 0 基尼系数计算 Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5 Gini(刮风)=2/2 * 0.5 = 0.5 子节点温度中的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧 Gain(天气)=1.0 - 0 = 1.0 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 1.0 = 0 Gain_ratio(天气)=1.0/1.0=1.0 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0/0 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(刮风)=2/2 * 0.5 = 0.5 确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.4 最终的决策树 在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树： 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 3. 思考 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？ 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n)","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"Decision Tree","slug":"Machine-Learning/Decision-Tree","permalink":"https://dataquaner.github.io/categories/Machine-Learning/Decision-Tree/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"极客时间《数据分析45讲总结》","slug":"极客时间《数据分析45讲总结》","date":"2019-12-17T08:05:00.000Z","updated":"2019-12-17T08:08:35.390Z","comments":true,"path":"2019/12/17/极客时间《数据分析45讲总结》/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4%E3%80%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%9045%E8%AE%B2%E6%80%BB%E7%BB%93%E3%80%8B/","excerpt":"","text":"1.前言该讲主要引导读者从全局去了解什么是数据分析？为什么做数据分析？怎么去做数据分析？答案就是：掌握数据，就是掌握规律。当你了解了市场数据，对它进行分析，就可以得到市场规律。当你掌握了产品自身的数据，对它进行分析，就可以了解产品的用户来源、用户画像等等。所以说数据是个全新的视角。数据分析如此重要，它不仅是新时代的“数据结构 + 算法”，也更是企业争夺人才的高地。 谈到数据分析，我们一般都会从3个方面入手： 数据采集 – 数据源，我们要用的原材料 数据挖掘 – 它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值（所谓的商业智能BI） 数据的可视化 – 数据领域中的万金油，直观了解数据分析结构 数据分析的三驾马车的关系如下： 下面来大致认识下这三驾马车： 2.数据采集：数据的采集，主要是和数据打交道，用工具对数据进行采集，常用的数据源，如何获取它们。在专栏里，后续会将介绍如何掌握“八爪鱼”这个自动抓取的神器，它可以帮你抓取 99% 的页面源。也会教读者如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如“王祖贤”的海报，还能自动给微博加粉丝，让你掌握自动化的快感。 3.数据挖掘：数据挖掘，它可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。 掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的。 4.数据可视化 为什么说数据要可视化，因为数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。 数据可视化的两种方法： Python ：在 Python 对数据进行清洗、挖掘的过程中，很多的库可以使用，像 Matplotlib、Seaborn 等第三方库进行呈现。 第三方工具：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。 数据分析包括数据采集、数据挖掘、数据可视化这三个部分。乍看你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点“高深莫测”，掌握起来是不是会吃力。其实这些都是不必要的烦恼。个人觉得只要内心笃定，认为自己一定能做成，学成，其他一切都是“纸老虎”哈。 再说下，陈博在文章中提到的如何来快速掌握数据分析，核心就是认知。我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程就是认知升级的过程。 我本人也是很赞同这种说法，简单一句就是“知行合一” 总结 记录下你每天的认知 这些认知对应工具的哪些操作 做更多练习来巩固你的认知","categories":[{"name":"Learning Path","slug":"Learning-Path","permalink":"https://dataquaner.github.io/categories/Learning-Path/"},{"name":"Data Analysis","slug":"Learning-Path/Data-Analysis","permalink":"https://dataquaner.github.io/categories/Learning-Path/Data-Analysis/"}],"tags":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/tags/Data-Analysis/"}]},{"title":"机器学习系列之决策树算法（01）：决策树特征选择","slug":"机器学习系列之决策树算法（01）：决策树特征选择","date":"2019-12-17T07:07:00.000Z","updated":"2019-12-19T10:36:47.474Z","comments":true,"path":"2019/12/17/机器学习系列之决策树算法（01）：决策树特征选择/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8801%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"","text":"1.什么是特征选择【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。 【那么选择的是什么样的特征呢？】这里我们选择的特征需要是对训练数据有分类能力的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征没有分类能力，舍去这个特征对学习的精度不会有特别大的影响。 特征选择是决定用哪个特征来划分特征空间。 比如女生找男朋友，可能这个女生首先会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了 【那怎么判断哪个特征有更好的分类能力呢？】这时候【信息增益】就要出场了。 2.信息增益为了解释什么是信息增益，我们首先要讲解一下什么是【熵（entropy）】 熵（Entropy） 在热力学与化学中： 熵是一种测量在动力学方面【不能做功的能量的总数】，当总体熵增加，其做功能力也下降，熵的度量是能量退化的指标。 1948 年，香农把热力学中的熵引入到信息论中，称为香农熵。根据维基百科的描述： 在信息论中，熵是接收的每条消息中包含的信息的平均量。 更一般的，【熵表示随机变量的不确定性】。假设一个有限取值的离散随机变量 X 的概率分布如下： 那么它的熵定义为： 上式中的 b 通常取 2 或者自然对数 e，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。 从上式中，我们可以看到，熵与 X 的取值是没有关系的，它只与 X 的分布有关，所以 H 也可以写作 p 的函数： 我们现在来看两个随机变量的情况。 假设随机变量 (X, Y) 的联合概率分布如下： 我们使用条件熵（conditional entropy）H(Y|X)来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。 条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。 是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法： 有了上面的公式以后，条件熵的定义就非常容易理解了。 那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？ 信息增益的定义与信息增益算法既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？ 没错，由于熵表示不确定性，严格来说，信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」。 我们给出信息增益的最终定义： 特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。 这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。 假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。 很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要 信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大 条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定 信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类 下面我们给出信息增益的算法。 首先对数据做一些介绍： 假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 ， 为属于这个类的样本的个数，很显然 。 再假设我们有一个特征叫 A，A 有 n 个不同的取值 ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 ， 是这个子集的样本个数，很显然 。 我们把 中属于类别 的集合称作 ， 是其样本个数。 信息增益的计算就分为如下几个步骤： 计算 D 的经验熵 H(D)： \\2. 计算 A 对 D 的经验条件熵 H(D|A)： \\3. 计算信息增益 g(D, A)： 3.信息增益比看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？ 按照经验来看，【以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差】。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。类别越多代表特征越不确定，即熵越多，类别的信息增益越小。 为了校正这一误差，我们引入了【信息增益比（information gain ratio）】，又叫做信息增益率，它的定义如下： 特征 A 对训练数据集 D 的信息增益比 定义为其信息增益 与训练数据集 D 关于特征 A 的值的熵 之比。 其中， ，n 是 A 取值的个数。 两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。 4. ID3 ： 最大信息增益 ID3以信息增益为准则来选择最优划分属性 信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 5. C4.5 ：最大信息增益率 C4.5基于信息增益率准则选择最优分割属性的算法 信息增益比率通过引入一个被称作【分裂信息(Split information)】的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 6.CART ：最小基尼指数 CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归 CART是一棵二叉树，采用【二元切分法】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 7.分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"Decision Tree","slug":"Machine-Learning/Decision-Tree","permalink":"https://dataquaner.github.io/categories/Machine-Learning/Decision-Tree/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]}]}