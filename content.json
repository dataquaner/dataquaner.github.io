{"meta":{"title":"DataQuaner","subtitle":"DataQuaner","description":"Data|Algorithm|Business","author":"Leon","url":"https://dataquaner.github.io","root":"/"},"pages":[{"title":"关于","date":"2019-11-14T14:27:21.000Z","updated":"2020-04-11T13:58:33.412Z","comments":true,"path":"about/index.html","permalink":"https://dataquaner.github.io/about/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"分类","date":"2019-11-10T09:22:35.000Z","updated":"2020-04-11T11:01:07.476Z","comments":true,"path":"categories/index.html","permalink":"https://dataquaner.github.io/categories/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"contact","date":"2020-04-11T02:31:05.000Z","updated":"2020-04-11T02:31:41.680Z","comments":true,"path":"contact/index.html","permalink":"https://dataquaner.github.io/contact/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"friends","date":"2020-04-11T02:33:44.000Z","updated":"2020-04-11T02:34:02.597Z","comments":true,"path":"friends/index.html","permalink":"https://dataquaner.github.io/friends/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"友情链接","date":"2019-11-10T09:17:30.000Z","updated":"2019-11-10T09:20:41.842Z","comments":true,"path":"link/index.html","permalink":"https://dataquaner.github.io/link/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"medias","date":"2020-04-11T10:11:04.000Z","updated":"2020-04-11T10:11:04.927Z","comments":true,"path":"medias/index.html","permalink":"https://dataquaner.github.io/medias/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"标签","date":"2019-11-10T09:21:38.000Z","updated":"2020-04-11T11:00:23.510Z","comments":true,"path":"tags/index.html","permalink":"https://dataquaner.github.io/tags/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"}],"posts":[{"title":"Flink面试问题梳理(基础+进阶+源码)","slug":"Flink面试问题梳理基础进阶源码","date":"2020-04-18T13:40:00.000Z","updated":"2020-04-18T14:39:18.053Z","comments":true,"path":"2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/","link":"","permalink":"https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/","excerpt":"","text":"第一部分：Flink 面试基础篇1. 简单介绍一下 Flink​ Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。 ​ Flink提供了诸多高抽象层的API以便用户编写分布式任务： DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。 DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。 Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。 ​ 此外，Flink 还针对特定的应用领域提供了领域库，例如：Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。 根据官网的介绍，Flink 的特性包含： 支持高吞吐、低延迟、高性能的流处理支持带有事件时间的窗口 （Window） 操作支持有状态计算的 Exactly-once 语义支持高度灵活的窗口 （Window） 操作，支持基于 time、count、session 以及 data-driven 的窗口操作支持具有 Backpressure 功能的持续流模型支持基于轻量级分布式快照（Snapshot）实现的容错一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理Flink 在 JVM 内部实现了自己的内存管理支持迭代计算支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存 2. Flink 相比传统的 Spark Streaming 有什么区别?​ 这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来： Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。 下面我们就分几个方面介绍两个框架的主要区别： [1] 架构模型​ Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。 [2] 任务调度​ Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。 ​ Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。 [3] 时间机制​ Spark Streaming 支持的时间机制有限，只支持处理时间。Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。 [4] 容错机制​ 对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。 ​ Flink 则使用两阶段提交协议来解决这个问题。 3. Flink 的组件栈有哪些？​ 根据 Flink 官网描述，Flink 是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。 图片来源于：https://flink.apache.org ​ 自下而上，每一层分别代表： Deploy 层：该层主要涉及了Flink的部署模式，在上图中我们可以看出，Flink 支持包括local、Standalone、Cluster、Cloud等多种部署模式。 Runtime 层：Runtime层提供了支持 Flink 计算的核心实现，比如：支持分布式 Stream 处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。 API层：API 层主要实现了面向流（Stream）处理和批（Batch）处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API，后续版本，Flink有计划将DataStream和DataSet API进行统一。 Libraries层：该层称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。 4. Flink 的运行必须依赖 Hadoop组件吗？​ Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadoop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点。 5. 你们的Flink集群规模多大？​ 大家注意，这个问题看起来是问你实际应用中的Flink集群规模，其实还隐藏着另一个问题：Flink可以支持多少节点的集群规模？ ​ 在回答这个问题时候，可以将自己生产环节中的集群规模、节点、内存情况说明，同时说明部署模式（一般是Flink on Yarn），除此之外，用户也可以同时在小集群（少于5个节点）和拥有 TB 级别状态的上千个节点上运行 Flink 任务。 6. Flink的基础编程模型了解吗？ ​ 上图是来自Flink官网的运行流程图。通过上图我们可以得知，Flink 程序的基本构建是数据输入来自一个 Source，Source 代表数据的输入端，经过 Transformation 进行转换，然后在一个或者多个Sink接收器中结束。数据流（stream）就是一组永远不会停止的数据记录流，而转换（transformation）是将一个或多个流作为输入，并生成一个或多个输出流的操作。执行时，Flink程序映射到 streaming dataflows，由流（streams）和转换操作（transformation operators）组成。 7. Flink集群有哪些角色？各自有什么作用？ ​ Flink 程序在运行时主要有 TaskManager，JobManager，Client三种角色。其中JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。 ​ TaskManager是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。 ​ Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。 8. 说说 Flink 资源管理中 Task Slot 的概念 ​ 在Flink架构角色中我们提到，TaskManager是实际负责执行计算的Worker，TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 Task Slot 的概念。 ​ 简单的说，TaskManager会将自己节点上管理的资源分为不同的Slot：固定大小的资源子集。这样就避免了不同Job的Task互相竞争内存资源，但是需要主要的是，Slot只会做内存的隔离。没有做CPU的隔离。 9. 说说 Flink 的常用算子？​ Flink 最常用的常用算子包括：Map：DataStream → DataStream，输入一个参数产生一个参数，map的功能是对输入的参数进行转换操作。Filter：过滤掉指定条件的数据。KeyBy：按照指定的key进行分组。Reduce：用来进行结果汇总合并。Window：窗口函数，根据某些特性将每个key的数据进行分组（例如：在5s内到达的数据） 10. 说说你知道的Flink分区策略？​ 什么要搞懂什么是分区策略。分区策略是用来决定数据如何发送至下游。目前 Flink 支持了8中分区策略的实现。 上图是整个Flink实现的分区策略继承图： GlobalPartitioner数据会被分发到下游算子的第一个实例中进行处理。 ShufflePartitioner数据会被随机分发到下游算子的每一个实例中进行处理。 RebalancePartitioner数据会被循环发送到下游的每一个实例中进行处理。 RescalePartitioner这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为2，编号为A和B。下游并行度为4，编号为1，2，3，4。那么A则把数据循环发送给1和2，B则把数据循环发送给3和4。假设上游并行度为4，编号为A，B，C，D。下游并行度为2，编号为1，2。那么A和B则把数据发送给1，C和D则把数据发送给2。 BroadcastPartitioner广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做join的场景。 ForwardPartitioner 用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。 KeyGroupStreamPartitioner Hash分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。 CustomPartitionerWrapper用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑。 11. Flink的并行度了解吗？Flink的并行度设置是怎样的？​ Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。这些并行实例的数量被称为并行度。 ​ 我们在实际生产环境中可以从四个不同层面设置并行度： 操作算子层面(Operator Level) 执行环境层面(Execution Environment Level) 客户端层面(Client Level) 系统层面(System Level) 需要注意的优先级：算子层面&gt;环境层面&gt;客户端层面&gt;系统层面。 12. Flink的Slot和parallelism有什么区别？官网上十分经典的图： slot是指taskmanager的并发执行能力，假设我们将 taskmanager.numberOfTaskSlots 配置为3那么每一个 taskmanager 中分配3个 TaskSlot, 3个 taskmanager 一共有9个TaskSlot。 parallelism是指taskmanager实际使用的并发能力。假设我们把 parallelism.default 设置为1，那么9个 TaskSlot 只能用1个，有8个空闲。 13. Flink有没有重启策略？说说有哪几种？Flink 实现了多种重启策略。 固定延迟重启策略（Fixed Delay Restart Strategy） 故障率重启策略（Failure Rate Restart Strategy） 没有重启策略（No Restart Strategy） Fallback重启策略（Fallback Restart Strategy） 14. 用过Flink中的分布式缓存吗？如何使用？​ Flink实现的分布式缓存和Hadoop有异曲同工之妙。目的是在本地读取文件，并把他放在 taskmanager 节点中，防止task重复拉取。 val env = ExecutionEnvironment.getExecutionEnvironment // register a file from HDFS env.registerCachedFile(\"hdfs:///path/to/your/file\", \"hdfsFile\") // register a local executable file (script, executable, ...) env.registerCachedFile(\"file:///path/to/exec/file\", \"localExecFile\", true) // define your program and execute ... val input: DataSet[String] = ... val result: DataSet[Integer] = input.map(new MyMapper()) ... env.execute() 15. 说说Flink中的广播变量，使用时需要注意什么？​ 我们知道Flink是并行的，计算过程可能不在一个 Slot 中进行，那么有一种情况即：当我们需要访问同一份数据。那么Flink中的广播变量就是为了解决这种情况。 ​ 我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。 16. 说说Flink中的窗口？​ 来一张官网经典的图： ​ Flink 支持两种划分窗口的方式，按照time和count。如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window。 ​ flink支持窗口的两个重要属性（size和interval） ​ 如果size=interval,那么就会形成tumbling-window(无重叠数据)如果size&gt;interval,那么就会形成sliding-window(有重叠数据)如果size&lt; interval, 那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。 ​ 通过组合可以得出四种基本窗口： time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5)) time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3)) count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5) count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3) 17. 说说Flink中的状态存储？​ Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和 checkpoint 交互。 ​ Flink提供了三种状态存储方式：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。 18. Flink 中的时间有哪几类​ Flink 中的时间和其他流式计算系统的时间一样分为三类：事件时间，摄入时间，处理时间三种。 如果以 EventTime 为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime。 如果以 IngesingtTime 为基准来定义时间窗口将形成 IngestingTimeWindow,以 source 的systemTime为准。 如果以 ProcessingTime 基准来定义时间窗口将形成 ProcessingTimeWindow，以 operator 的systemTime 为准。 19. Flink 中水印是什么概念，起到什么作用？​ Watermark 是 Apache Flink 为了处理 EventTime 窗口计算提出的一种机制, 本质上是一种时间戳。一般来讲Watermark经常和Window一起被用来处理乱序事件。 20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用​ TableEnvironment是Table API和SQL集成的核心概念。 这个类主要用来： 在内部catalog中注册表 注册外部catalog 执行SQL查询 注册用户定义（标量，表或聚合）函数 将DataStream或DataSet转换为表 持有对ExecutionEnvironment或StreamExecutionEnvironment的引用 21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？​ 首先大家要知道 Flink 的SQL解析是基于Apache Calcite这个开源框架。 基于此，一次完整的SQL解析过程如下： 用户使用对外提供Stream SQL的语法开发业务应用 用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划 采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划 对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行 第二部分：Flink 面试进阶篇1. Flink是如何支持批流一体的？ ​ 本道面试题考察的其实就是一句话：Flink的开发者认为批处理是流处理的一种特殊情况。批处理是有限的流处理。Flink 使用一个引擎支持了DataSet API 和 DataStream API。 2. Flink是如何做到高效的数据交换的？​ 在一个Flink Job中，数据需要在不同的task中进行交换，整个数据交换是有 TaskManager 负责的，TaskManager 的网络组件首先从缓冲buffer中收集records，然后再发送。Records 并不是一个一个被发送的，二是积累一个批次再发送，batch 技术可以更加高效的利用网络资源。 3. Flink是如何做容错的？​ Flink 实现容错主要靠强大的CheckPoint机制和State机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态。 4. Flink 分布式快照的原理是什么？​ Flink的分布式快照是根据Chandy-Lamport算法量身定做的。简单来说就是持续创建分布式数据流及其状态的一致快照。 ​ 核心思想是在 input source 端插入 barrier，控制 barrier 的同步来实现 snapshot 的备份和 exactly-once 语义。 5. Flink 是如何保证Exactly-once语义的？​ Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。分为以下几个步骤： 开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面 预提交（preCommit）将内存中缓存的数据写入文件并关闭 正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟 丢弃（abort）丢弃临时文件 ​ 若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。 6. Flink 的 kafka 连接器有什么特别的地方？​ Flink源码中有一个独立的connector模块，所有的其他connector都依赖于此模块，Flink 在1.9版本发布的全新kafka连接器，摒弃了之前连接不同版本的kafka集群需要依赖不同版本的connector这种做法，只需要依赖一个connector即可。 7. 说说 Flink的内存管理是如何做的?​ Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。 理论上Flink的内存管理分为三部分： Network Buffers：这个是在TaskManager启动的时候分配的，这是一组用于缓存网络数据的内存，每个块是32K，默认分配2048个，可以通过“taskmanager.network.numberOfBuffers”修改 Memory Manage pool：大量的Memory Segment块，用于运行时的算法（Sort/Join/Shuffle等），这部分启动的时候就会分配。下面这段代码，根据配置文件中的各种参数来计算内存的分配方法。（heap or off-heap，这个放到下节谈），内存的分配支持预分配和lazy load，默认懒加载的方式。 User Code，这部分是除了Memory Manager之外的内存用于User code和TaskManager本身的数据结构。 8. 说说 Flink的序列化如何做的?​ Java本身自带的序列化和反序列化的功能，但是辅助信息占用空间比较大，在序列化对象时记录了过多的类信息。 ​ Apache Flink摒弃了Java原生的序列化方法，以独特的方式处理数据类型和序列化，包含自己的类型描述符，泛型类型提取和类型序列化框架。 ​ TypeInformation 是所有类型描述符的基类。它揭示了该类型的一些基本属性，并且可以生成序列化器。TypeInformation 支持以下几种类型： BasicTypeInfo: 任意Java 基本类型或 String 类型 BasicArrayTypeInfo: 任意Java基本类型数组或 String 数组 WritableTypeInfo: 任意 Hadoop Writable 接口的实现类 TupleTypeInfo: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现 CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples) PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法 GenericTypeInfo: 任意无法匹配之前几种类型的类 ​ 针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。 9. Flink中的Window出现了数据倾斜，你有什么解决办法？​ window产生数据倾斜指的是数据在不同的窗口内堆积的数据量相差过多。本质上产生这种情况的原因是数据源头发送的数据量速度不同导致的。出现这种情况一般通过两种方式来解决： 在数据进入窗口前做预聚合 重新设计窗口聚合的key 10. Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？​ 数据倾斜和数据热点是所有大数据框架绕不过去的问题。处理这类问题主要从3个方面入手： 在业务上规避这类问题 ​ 例如一个假设订单场景，北京和上海两个城市订单量增长几十倍，其余城市的数据量不变。这时候我们在进行聚合的时候，北京和上海就会出现数据堆积，我们可以单独数据北京和上海的数据。 Key的设计上 ​ 把热key进行拆分，比如上个例子中的北京和上海，可以把北京和上海按照地区进行拆分聚合。 参数设置 ​ Flink 1.9.0 SQL(Blink Planner) 性能优化中一项重要的改进就是升级了微批模型，即 MiniBatch。原理是缓存一定的数据后再触发处理，以减少对State的访问，从而提升吞吐和减少数据的输出量。 11. Flink任务延迟高，想解决这个问题，你会如何入手？​ 在Flink的后台任务管理中，我们可以看到Flink的哪个算子和task出现了反压。最主要的手段是资源调优和算子调优。资源调优即是对作业中的Operator的并发数（parallelism）、CPU（core）、堆内存（heap_memory）等参数进行调优。作业参数调优包括：并行度的设置，State的设置，checkpoint的设置。 12. Flink是如何处理反压的？​ Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。下游消费者消费变慢，上游就会受到阻塞。 13. Flink的反压和Storm有哪些不同？​ Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。 ​ Flink中的反压使用了高效有界的分布式阻塞队列，下游消费变慢会导致发送端阻塞。 ​ 二者最大的区别是Flink是逐级反压，而Storm是直接从源头降速。 14. Operator Chains（算子链）这个概念你了解吗？​ 为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。这就是我们所说的算子链。 15. Flink什么情况下才会把Operator chain在一起形成算子链？​ 两个operator chain在一起的的条件： 上下游的并行度一致 下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入） 上下游节点都在同一个 slot group 中（下面会解释 slot group） 下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS） 上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD） 两个节点间数据分区方式是 forward（参考理解数据流的分区） 用户没有禁用 chain 16. 说说Flink1.9的新特性？ 支持hive读写，支持UDF Flink SQL TopN和GroupBy等优化 Checkpoint跟savepoint针对实际业务场景做了优化 Flink state查询 17. 消费kafka数据的时候，如何处理脏数据？ 可以在处理前加一个fliter算子，将不符合规则的数据过滤出去。 第三部分：Flink 面试源码篇1. Flink Job的提交流程​ 用户提交的Flink Job会被转化成一个DAG任务运行，分别是：StreamGraph、JobGraph、ExecutionGraph，Flink中JobManager与TaskManager，JobManager与Client的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。 2. Flink所谓”三层图”结构是哪几个”图”？ 一个Flink任务的DAG生成计算图大致经历以下三个过程： StreamGraph最接近代码所表达的逻辑层面的计算拓扑结构，按照用户代码的执行顺序向StreamExecutionEnvironment添加StreamTransformation构成流式图。 JobGraph从StreamGraph生成，将可以串联合并的节点进行合并，设置节点之间的边，安排资源共享slot槽位和放置相关联的节点，上传任务所需的文件，设置检查点配置等。相当于经过部分初始化和优化处理的任务图。 ExecutionGraph由JobGraph转换而来，包含了任务具体执行所需的内容，是最贴近底层实现的执行图。 3. JobManger在集群中扮演了什么角色？​ JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。 ​ JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务管理和资源管理。 ​ JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。 ​ 同时在任务执行的过程中，Flink JobManager 会触发 Checkpoint 操作，每个 TaskManager 节点 收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Fink JobManager 中完成。 ​ 当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。 4. JobManger在集群启动过程中起到什么作用？​ JobManager的职责主要是接收Flink作业，调度Task，收集作业状态和管理TaskManager。它包含一个Actor，并且做如下操作： RegisterTaskManager: 它由想要注册到JobManager的TaskManager发送。注册成功会通过AcknowledgeRegistration消息进行Ack。 SubmitJob: 由提交作业到系统的Client发送。提交的信息是JobGraph形式的作业描述信息。 CancelJob: 请求取消指定id的作业。成功会返回CancellationSuccess，否则返回CancellationFailure。 UpdateTaskExecutionState: 由TaskManager发送，用来更新执行节点(ExecutionVertex)的状态。成功则返回true，否则返回false。 RequestNextInputSplit: TaskManager上的Task请求下一个输入split，成功则返回NextInputSplit，否则返回null。 JobStatusChanged： 它意味着作业的状态(RUNNING, CANCELING, FINISHED,等)发生变化。这个消息由ExecutionGraph发送。 5. TaskManager在集群中扮演了什么角色？​ TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。 ​ 客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。 ​ TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。 ​ 可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进行的方式有很大的区别，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。 6. TaskManager在集群启动过程中起到什么作用？​ TaskManager的启动流程较为简单：启动类：org.apache.flink.runtime.taskmanager.TaskManager核心启动方法 ： selectNetworkInterfaceAndRunTaskManager启动后直接向JobManager注册自己，注册完成后，进行部分模块的初始化。 7. Flink 计算资源的调度是如何实现的？​ TaskManager中最细粒度的资源是Task slot，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。 ​ 通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。 ​ 而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot： 8. 简述Flink的数据抽象及数据交换过程？​ Flink 为了避免JVM的固有缺陷例如java对象存储密度低，FGC影响吞吐和响应等，实现了自主管理内存。MemorySegment就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。 ​ 在MemorySegment这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是Buffer。 ​ 对接从Java对象转为Buffer的中间对象是另一个抽象StreamRecord。 9. Flink 中的分布式快照机制是如何实现的？​ Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。 ​ barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。 ​ 然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。 一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。在所有sink确认快照后，意味快照着已完成。 ​ 一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。 10. 简单说说FlinkSQL的是如何实现的？​ Flink 将 SQL 校验、SQL 解析以及 SQL 优化交给了Apache Calcite。Calcite 在其他很多开源项目里也都应用到了，譬如 Apache Hive, Apache Drill, Apache Kylin, Cascading。Calcite 在新的架构中处于核心的地位，如下图所示。 构建抽象语法树的事情交给了 Calcite 去做。SQL query 会经过 Calcite 解析器转变成 SQL 节点树，通过验证后构建成 Calcite 的抽象语法树（也就是图中的 Logical Plan）。另一边，Table API 上的调用会构建成 Table API 的抽象语法树，并通过 Calcite 提供的 RelBuilder 转变成 Calcite 的抽象语法树。然后依次被转换成逻辑执行计划和物理执行计划。 在提交任务后会分发到各个 TaskManager 中运行，在运行时会使用 Janino 编译器编译代码后运行。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/tags/Flink/"},{"name":"面试必备","slug":"面试必备","permalink":"https://dataquaner.github.io/tags/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87/"}]},{"title":"机器学习系列之决策树算法（03）：决策树的剪枝","slug":"机器学习系列之决策树算法（03）：决策树的剪枝","date":"2020-04-11T11:32:09.079Z","updated":"2020-04-11T11:32:09.079Z","comments":true,"path":"2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/","excerpt":"","text":"1. 前言上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现过拟合现象。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为剪枝(pruning)。 具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss funtion)或代价函数(cost funtion)来实现。 2.剪枝算法2.1 为什么要剪枝现象 接上一次讲的生成决策树，下面给出一张图。 横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。 实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。 可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。 原因 原因1：噪声、样本冲突，即错误的样本数据。 原因2：特征即属性不能完全作为分类标准。 原因3：巧合的规律性，数据量不够大。 这个时候，就需要对生成树进行修剪，也就是剪枝。 2.2 如何进行剪枝预剪枝预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法: (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。 (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。 (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。 (4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。 优点&amp;缺点 由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【怎样精确地估计何时停止树的增长是相当困难的】。 预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。 后剪枝后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。 这里主要介绍四种： REP-错误率降低剪枝 PEP-悲观剪枝 CCP-代价复杂度剪枝 MEP-最小错误剪枝 REP(Reduced Error Pruning)方法 对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S 优点： REP 是当前最简单的事后剪枝方法之一。 它的计算复杂性是线性的。 和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。 缺点： 但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意. PEP(Pessimistic Error Pruning)方法 为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, PEP 方法对误差估计增加了连续性校正(continuity correction)。关于PEP方法的数据解释待后续开专题梳理。 优点： PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一 PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。 TIPS： 个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。 MEP(Minimum Error Pruning)方法 MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。 优点： MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集. 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： 类别平均分配的前提假设现实几率不大&amp;对K太过敏感 对此，也有改进算法，我没有深入研究。 CCP(Cost-Complexity Pruning)方法 CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤: (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。 ​ (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。 缺点： 生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的. 对比四种方法 剪枝名称 剪枝方式 计算复杂度 误差估计 REP 自底向上 0(n) 剪枝集上误差估计 PEP 自顶向下 o(n) 使用连续纠正 CCP 自底向上 o(n2) 标准误差 MEP 自底向上 o(n) 使用连续纠正 ① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。 ② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。 ③ 如果数据集多（REP&amp;CCP←复杂但树小） ④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定） 3.总结决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。 剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。 预剪枝： 预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。 简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。 这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。 后剪枝： 后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。 同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。 后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。 剪枝方法的选择 如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。 后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。 有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。 经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。 无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。 函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点 函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。 函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值 上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"机器学习系列之决策树算法（02）：决策树的生成","slug":"机器学习系列之决策树算法（02）：决策树的生成","date":"2020-04-11T11:31:53.052Z","updated":"2020-04-11T11:31:53.052Z","comments":true,"path":"2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/","excerpt":"","text":"1. 前言上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。 2. 决策树的生成 示例数据表格 文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中 2.1 相关概念阐述2.1.1 决策树 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。 2.1.2 ID3、C4.5、CART在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【机器学习系列之决策树算法（01）：决策树特征选择】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。 信息熵 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。 ​ Entopy(t)=-Σp(i|t)log2p(i|t) 信息增益信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为： ​ Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di) 信息增益率 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7： ​ H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) 基尼系数 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下基尼系数 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低. CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分. 基尼系数的计算公式如下: ​ Gini = 1 – Σ (Pi)2 for i=1 to number of classes 2.2 完整生成过程 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例 2.2.1 根节点的选择 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为： ​ Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985 天气 其数据表格如下: 信息增益计算如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下： D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981 D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为： 3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965 其信息增益为： Gain(天气)=0.985 - 0.965 = 0.020 信息增益率计算 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为： H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556 则其信息增益率为： Gain_ratio(天气)=0.020/1.556=0.012 基尼系数计算 Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9 Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21 温度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 作为子节点的归一化信息熵为： 4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857 其信息增益为： Gain(温度)=0.985 - 0.857 = 0.128 信息增益率计算 属性熵为： H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378 则其信息增益率为： Gain_ratio(温度)=0.128/1.378=0.0928 基尼系数计算 Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 4/7 * 1.0 + 3/7 * 0.918 = 0.964 其信息增益为： Gain(湿度)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(湿度)=0.021/0.985=0.021 基尼系数计算 Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 作为子节点的归一化信息熵为： 3/7 * 1.0 + 4/7 * 0.918 = 0.964 其信息增益为： Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数计算 Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 根节点的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度 信息增益 Gain(天气)=0.985 - 0.965 = 0.020 Gain(温度)=0.985 - 0.857 = 0.128 Gain(湿度)=0.985 - 0.964 = 0.021 Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率 Gain_ratio(天气)=0.020/1.556=0.012 Gain_ratio(温度)=0.128/1.378=0.0928 Gain_ratio(湿度)=0.021/0.985=0.021 Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择: 根节点 子节点温度高:(待进一步进行选择) 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.2 子节点温度高的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： ​ Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 归一化信息熵为： 2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5 其信息增益为： Gain(天气)=1.0 - 0.5 = 0.5 信息增益率计算 对应的属性熵为： H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5 则其信息增益率为： Gain_ratio(天气)=0.5/1.5=0.33333 基尼系数计算 Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0 D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0 作为子节点的归一化信息熵为： 2/4 * 0.0 + 2/4 * 0.0 = 0.0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0 Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0 D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 1/4 * 0.0 + 3/4 * 0.918 = 0.688 其信息增益为： Gain(刮风)=1.0 - 0.688 = 0.312 信息增益率计算 属性熵为： H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918 则其信息增益率为： Gain_ratio(刮风)=0.312/0.918=0.349 基尼系数计算 Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 子节点温度高的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度 Gain(天气)=1.0 - 0.5 = 0.5 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 0.688 = 0.312 Gain_ratio(天气)=0.5/1.5=0.33333 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0.312/0.918=0.349 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.3 子节点温度中的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D (阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(天气)=1.0 - 0 = 1.0 信息增益率计算 对应的属性熵为： H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(天气)=1.0/1.0=1.0 基尼系数计算 Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 作为子节点的归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 作为子节点的归一化信息熵为： 1/1 * 1.0 = 1.0 其信息增益为： Gain(刮风)=1.0 - 1.0 = 0 信息增益率计算 属性熵为： H(刮风)=-(2/2 * log2(2/2) = 0.0 则其信息增益率为： Gain_ratio(刮风)=0/0 = 0 基尼系数计算 Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5 Gini(刮风)=2/2 * 0.5 = 0.5 子节点温度中的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧 Gain(天气)=1.0 - 0 = 1.0 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 1.0 = 0 Gain_ratio(天气)=1.0/1.0=1.0 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0/0 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(刮风)=2/2 * 0.5 = 0.5 确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.4 最终的决策树 在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树： 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 3. 思考 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？ 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"数据存储之MySQL系列（01）：MySQL体系结构","slug":"数据存储之MySQL系列（01）：MySQL体系结构","date":"2020-04-11T11:31:10.255Z","updated":"2020-04-11T11:31:10.255Z","comments":true,"path":"2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"DataBase","slug":"DataBase","permalink":"https://dataquaner.github.io/categories/DataBase/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dataquaner.github.io/tags/MySQL/"}]},{"title":"xgboost算法模型输出的解释","slug":"xgboost算模型输出的解释","date":"2020-04-11T11:30:52.923Z","updated":"2020-04-11T11:30:52.923Z","comments":true,"path":"2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/","excerpt":"","text":"1. 问题描述 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。 2. 数据集 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据. from sklearn import datasets iris = datasets.load_iris() data = iris.data[:100] print data.shape #(100L, 4L) #一共有100个样本数据, 维度为4维 label = iris.target[:100] print label #正好选取label为0和1的数据 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 3. 训练集与测试集from sklearn.cross_validation import train_test_split train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=0) 4. Xgboost建模4.1 模型初始化设置import xgboost as xgb dtrain=xgb.DMatrix(train_x,label=train_y) dtest=xgb.DMatrix(test_x) params={'booster':'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'auc', 'max_depth':4, 'lambda':10, 'subsample':0.75, 'colsample_bytree':0.75, 'min_child_weight':2, 'eta': 0.025, 'seed':0, 'nthread':8, 'silent':1} watchlist = [(dtrain,'train')] 4.2 建模与预测bst=xgb.train(params,dtrain,num_boost_round=100,evals=watchlist) ypred=bst.predict(dtest) # 设置阈值, 输出一些评价指标 y_pred = (ypred >= 0.5)*1 from sklearn import metrics print 'AUC: %.4f' % metrics.roc_auc_score(test_y,ypred) print 'ACC: %.4f' % metrics.accuracy_score(test_y,y_pred) print 'Recall: %.4f' % metrics.recall_score(test_y,y_pred) print 'F1-score: %.4f' %metrics.f1_score(test_y,y_pred) print 'Precesion: %.4f' %metrics.precision_score(test_y,y_pred) metrics.confusion_matrix(test_y,y_pred) Out[23]: AUC: 1.0000 ACC: 1.0000 Recall: 1.0000 F1-score: 1.0000 Precesion: 1.0000 array([[13, 0], [ 0, 12]], dtype=int64) Yeah, 完美的模型, 完美的预测! 4.3 可视化输出#对于预测的输出有三种方式 ?bst.predict Signature: bst.predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False) pred_leaf : bool When this option is on, the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree. Note that the leaf index of a tree is unique per tree, so you may find leaf 1 in both tree 1 and tree 0. pred_contribs : bool When this option is on, the output will be a matrix of (nsample, nfeats+1) with each record indicating the feature contributions (SHAP values) for that prediction. The sum of all feature contributions is equal to the prediction. Note that the bias is added as the final column, on top of the regular features. 4.3.1 得分默认的输出就是得分, 这没什么好说的, 直接上code. ypred = bst.predict(dtest) ypred Out[32]: array([ 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.20081411], dtype=float32) 在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出. 4.3.2 所属的叶子节点当设置pred_leaf=True的时候, 这时就会输出每个样本在所有树中的叶子节点 ypred_leaf = bst.predict(dtest, pred_leaf=True) ypred_leaf Out[33]: array([[1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1]]) 输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以ypred_leaf的维度为[100*100]. 对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树). 那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型. xgb.to_graphviz(bst, num_trees=0) #可视化第一棵树的生成情况 #直接输出模型的迭代工程 bst.dump_model(\"model.txt\") booster[0]: 0:[f3&lt;0.75] yes=1,no=2,missing=1 1:leaf=-0.019697 2:leaf=0.0214286 booster[1]: 0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0212184 2:leaf=0.0212 booster[2]: 0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0197404 2:leaf=0.0197235 booster[3]: …… 通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值. 在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值. 同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值. leaf1 -1.381214 leaf2 1.410950 在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了'objective': 'binary:logistic', 因此使用函数将累加的值转换为实际的打分: f(x)=1/(1+exp(−x)) 1/float(1+np.exp(1.38121416)) Out[24]: 0.20081407112186503 1/float(1+np.exp(-1.410950)) Out[25]: 0.8039157403338895 这就与ypred = bst.predict(dtest) 的分值相对应上了. 4.3.2 特征重要性接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性. ypred_contribs = bst.predict(dtest, pred_contribs=True) ypred_contribs Out[37]: array([[ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663]], dtype=float32) 输出的ypred_contribs的维度为[100,5], 通过阅读前面的文档注释就可以知道, 最后一列是bias, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的. 通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例. score_a = sum(ypred_contribs[0]) print score_a # -1.38121373579 score_b = sum(ypred_contribs[1]) print score_b # 1.41094945744 相同的分值, 相同的处理情况. 到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"LightGBM算法基础系列之基础理论篇（1）","slug":"LightGBM算法基础系列之基础理论篇（1）","date":"2020-04-11T11:30:37.965Z","updated":"2020-04-11T11:30:37.965Z","comments":true,"path":"2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/","excerpt":"","text":"这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"零基础自学人工智能路径规划，附资源，亲身经验","slug":"零基础自学人工智能路径规划附资源亲身经验","date":"2020-04-11T01:25:00.000Z","updated":"2020-04-11T12:08:44.555Z","comments":true,"path":"2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/","excerpt":"","text":"0. 前言下面的每个资源都是我亲身学过的，且是网上公开公认最优质的资源。 下面的每个学习步骤也是我一步步走过来的。希望大家以我为参考，少走弯路。 请大家不要浪费时间找非常多的资料，只看最精华的！ 综述，机器学习的自学简单来说分为三个步骤 前期：知识储备包括数学知识，机器学习经典算法知识，编程技术（python）的掌握 中期：算法的代码实现 后期：实战水平提升 机器学习路径规划图 1. 数学基础很多人看到数学知识的时候就望而却步，数学是需要的，但是作为入门水平，对数学的要求没有那么的高。假设你上过大学的数学课（忘了也没事），需要的数学知识啃一啃还是基本能理解下来的。 1.1 数学内容线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然(MLE)和最大后验估计(MAP)等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的如梯度下降、牛顿法等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等 上面的看不太懂没事，不是特别难，学习一下就能理解了。 1.2 数学资源网上有很多人会列举大量大量的课程资源，这是非常不负责任的事，学完那些我头发都得白了。实际上，我们只需要学习其中的一部分就够了。 1.2.1 吴恩达的斯坦福大学机器学习王牌课程CS229课后就有对学生数学知识的要求和补充，这些数学知识是完全符合机器学习要求的，不多也不少。墙裂推荐要看，不过只有英文版的。 链接：https://pan.baidu.com/s/1NrCAW38C9lXFqPwqTlrVRA密码：3k3m 1.2.2 深度学习的三大开山鼻祖之一Yoshua Bengio写的深度学习（包含了机器学习）领域的教科书现在以开源的形式在网上公开。这部书被誉为深度学习的圣经。在这里我们只看这本书的第一部分，也就是数学基础。囊括了机器学习所需的所有必备数学基础，而且是从最基础的说起，也不多，必读的。 链接：https://pan.baidu.com/s/1GmmbqFewyCuEA7blXNC-7g密码：6qqm 1.2.3 跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。链接：https://zhuanlan.zhihu.com/p/25197792，知乎专栏上的一篇好文章，囊括了所有的应用知识点。 好了，数学方面我只推荐上面三个资源，三个都是必看的。里面很多可能你现在看不太懂，没关系。先大概过一遍，知道自己的数学水平在哪。在看到算法知识的时候，不懂的再回来补就好。后期需要更多的数学资料我会再更新的。 2 编程技术编程语言：python3.5及以上，python易学，这个这期先不细讲。 3 经典算法知识算法包括机器学习和深度学习，机器学习是深度学习的基础。所以务必先学机器学习的经典算法，再学深度学习的算法。 3.1 机器学习3.1.1 课程资料首推吴恩达的CS229，经典中的经典，在网易公开课里有视频，翻译，课程讲义，笔记是非常非常完备的。墙裂推荐。这个课程对数学有一定的要求，但我觉得只要你上过大学的数学，然后补一下上面的数学，完全可以直接来看这个CS229。 假设你的数学真的很差的话，怎么办？吴恩达在coursera上也开了一门跟CS229完全匹配的课程，coursera机器学习课。这门课是CS229的翻版，唯一不同的是它对数学基本是没有要求了，如果你对数学真的不懂的话，那就先看这个的教程吧。它跟CS229的关系就是同样的广度，但是深度浅很多，不过你学完coursera还是要回过头来看CS229的。这个也是免费的。 CS229课程视频：http://open.163.com/special/opencourse/machinelearning.html 课程讲义和中文笔记：https://pan.baidu.com/s/1MC_yWjcz_m5YoZFNBcsRSQ密码：6rw6 3.1.2 配套书籍：机器学习实战，必看。用代码实现了一遍各大经典机器学习算法，必须看，对你理解算法有很大帮助，同时里面也有应用。如果大家看上面纯理论的部分太枯燥了，就可以来看看这本书来知道在现实中机器学习算法是怎么应用的，会很大程度提升你的学习兴趣，当初我可是看了好几遍。 书籍及课后代码：链接：https://pan.baidu.com/s/15XtFOH18si316076GLKYfg密码：sawb 李航《统计学习方法》，配合着看 链接：https://pan.baidu.com/s/1Mk_O71k-H8GHeaivWbzM-Q密码：adep，配合着看 周志华《机器学习》，机器学习的百科全书，配合着看。 链接：https://pan.baidu.com/s/1lJoQnWToonvBU6cYwjrRKg密码：7rzl 3.2 深度学习说到深度学习，我们不得不提斯坦福的另一门王牌课程CS231，李飞飞教授的。这门课的课程，课后习题，堪称完美。必须必须看。整个系列下来，特别是课后的习题要做，做完之后你会发现，哇哦！它的课后习题就是写代码来实现算法的。这个在网易云课堂上有。 视频地址：http://study.163.com/course/introduction.htm?courseId=1004697005 课程笔记翻译，知乎专栏：https://zhuanlan.zhihu.com/p/21930884 墙裂建议要阅读这个知乎专栏，关于怎么学这门课，这个专栏写的很清楚。 课后作业配套答案：https://blog.csdn.net/bigdatadigest/article/category/7425092 3.3 学习时间到这里了，你的机器学习和深度学习算是入门了。学完上面这些，按一天6小时，一周六天的话，起码也得三个月吧。上面是基本功一定要认真学。但是，还找不了工作。因为你还没把这些知识应用到实际当中。 3.4 实战部分3.4.1 实战基础这一个阶段，你要开始用tensorflow（谷歌的深度学习框架）、scikit-learn（python的机器学习框架），这两个框架极大程度地集成了各大算法。其实上面在学习cs231n的时候你就会用到一部分。 scikit-learn的学习：http://sklearn.apachecn.org/cn/0.19.0/ 这是scikit-learn的官方文档中文版翻译，有理论有实战，最好的库学习资源，没有之一。认真看，传统的机器学习就是用这个库来实现的。 Tensorflow的学习：https://tensorflow.google.cn/api_docs/python/?hl=zh-cn 官方文档很详尽，还有实战例子，学习tensorflow的不二之选 3.4.2 实战进阶仅仅看这两个教程是不够的，你需要更多地去应用这两个库。 接下来推荐一部神书，机器学习和深度学习的实战教学，非常非常的棒，网上有很多号称实战的书或者例子，我看了基本就是照搬官网的，只有这一本书，是完全按照工业界的流程解决方案进行实战，你不仅能学习到库的应用，还能深入了解工业界的流程解决方案，最好的实战教学书，没有之一。书名是hands-on-ml-with-sklearn-and-tf 链接：https://pan.baidu.com/s/1x318qTHGt9oZKQwHkoUvKA密码：xssj 3.4.3 实战最终阶段kaggle数据竞赛，如果你已经学到了这一步，恭喜你离梦想越来越近了：对于我们初学者来说，没有机会接触到机器学习真正的应用项目，所以一些比赛平台是我们不错的选择。参加kaggle竞赛可以给你的简历增分不少，里面有入门级别到专家级别的实战案例，满足你的各方面需求。如果你能学到这一步了，我相信也不需要再看这个了。 上述所有资料的合集：https://pan.baidu.com/s/1tPqsSmSMZa6qLyD0ng87IQ密码：ve75 补充： 学到这个水平，应该是能够实习的水平了，还有很多后面再说吧。比如深度学习和机器学习的就业方向，深度学习得看论文，找工作还得对你得编程基础进行加强，具体就是数据结构与算法，我当年在这个上面可是吃了很大的亏。 这里面关于深度学习和机器学习的就业其实是两个方向，上面的其实也没有说全。一般来说，你得选择一个方向专攻。我建议的是，自学的最好在后期侧重机器学习方向，而不是深度学习。深度学习的岗位实在是太少，要求太高。机器学习还算稍微好点。 重点：上面的学习路径是主要框架，但是不意味着仅仅学习这些就够了。根据每个人基础的不同，你有可能需要另外的学习资料补充。但是，我希望大家可以按照上面的主框架走，先按上面我推荐的资源学，有需要的再去看别的（我之后也会推荐），上面的我能列出来的都是最经典的，最有效率，而且我亲身学过的。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"LearnPath","slug":"LearnPath","permalink":"https://dataquaner.github.io/categories/LearnPath/"}],"tags":[{"name":"LearnPath","slug":"LearnPath","permalink":"https://dataquaner.github.io/tags/LearnPath/"},{"name":"AI","slug":"AI","permalink":"https://dataquaner.github.io/tags/AI/"}]},{"title":"机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化","slug":"机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化","date":"2020-01-16T06:08:00.000Z","updated":"2020-04-11T11:34:10.409Z","comments":true,"path":"2020/01/16/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-10-jue-ce-shu-mo-xing-xgboost-lightgbm-he-catboost-mo-xing-ke-shi-hua/","link":"","permalink":"https://dataquaner.github.io/2020/01/16/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-10-jue-ce-shu-mo-xing-xgboost-lightgbm-he-catboost-mo-xing-ke-shi-hua/","excerpt":"","text":"安装 graphviz 参考文档 http://graphviz.readthedocs.io/en/stable/manual.html#installation graphviz安装包下载地址 https://www.graphviz.org/download/ 将graphviz的安装位置添加到系统环境变量 使用pip install graphviz安装graphviz python包 使用pip install pydotplus安装pydotplus python包 决策树模型可视化 以iris数据为例。训练一个分类决策树，调用export_graphviz函数导出DOT格式的文件。并用pydotplus包绘制图片。 #在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from sklearn import tree from sklearn.datasets import load_iris iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) import pydotplus from IPython.display import Image dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data) Image(graph.create_png()) XGBoost模型可视化参考文档 https://xgboost.readthedocs.io/en/latest/python/python_api.htmlxgboost中，对应的可视化函数是xgboost.to_graphviz。以iris数据为例，训练一个xgb分类模型并可视化 # 在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' import xgboost as xgb from sklearn.datasets import load_iris iris = load_iris() xgb_clf = xgb.XGBClassifier() xgb_clf.fit(iris.data, iris.target) xgb.to_graphviz(xgb_clf, num_trees=1) 也可以通过Digraph对象可以将保存文件并查看 digraph = xgb.to_graphviz(xgb_clf, num_trees=1) digraph.format = 'png' digraph.view('./iris_xgb') xgboost中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。 import matplotlib.pyplot as plt fig = plt.figure(figsize=(10, 10)) ax = fig.subplots() xgb.plot_tree(xgb_clf, num_trees=1, ax=ax) plt.show() LightGBM模型可视化参考文档 https://lightgbm.readthedocs.io/en/latest/Python-API.html#plottinglgb中，对应的可视化函数是lightgbm.create_tree_digraph。以iris数据为例，训练一个lgb分类模型并可视化 # 在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from sklearn.datasets import load_iris import lightgbm as lgb iris = load_iris() lgb_clf = lgb.LGBMClassifier() lgb_clf.fit(iris.data, iris.target) lgb.create_tree_digraph(lgb_clf, tree_index=1) lgb中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。 import matplotlib.pyplot as plt fig2 = plt.figure(figsize=(20, 20)) ax = fig2.subplots() lgb.plot_tree(lgb_clf, tree_index=1, ax=ax) plt.show() CatBoost模型可视化参考文档 catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)以iris数据为例，训练一个catboost模型。 参考文档 https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)以iris数据为例，训练一个catboost模型。 from sklearn.datasets import load_iris from catboost import CatBoostClassifier iris = load_iris() cat_clf = CatBoostClassifier(iterations=100) cat_clf.fit(iris.data, iris.target) 以python代码格式保存模型文件 cat_clf.save_model('catboost_model_file.py', format=\"python\", export_parameters=None) 也可以保存以C++代码格式保存模型文件 cat_clf.save_model('catboost_model_file.cpp', format=\"cpp\", export_parameters=None) 查看保存到的python代码，部分信息如下 需要自己解析出文件了树的结构，再用 graphviz 绘制图像 导出的Python文件首先第一个for循环部分 binary_feature_index = 0 binary_features = [0] * model.binary_feature_count for i in range(model.float_feature_count): for j in range(model.border_counts[i]): binary_features[binary_feature_index] = 1 if (float_features[i] > model.borders[binary_feature_index]) else 0 binary_feature_index += 1 输入的参数float_features存储输入的数值型特征值。model.binary_feature_count表示booster中所有树的节点总数。model.border_counts存储每个feature对应的节点数量，model.borders存储所有节点的判断边界。显然，CatBoost并没有按照二叉树结构从左到右，从上到下的存储结构。此段代码的功能，生成所有节点的判断结果。如果特征值大于判断边界，表示为1，否则为0。存储在binary_features中。 第二个for循环部分 # Extract and sum values from trees result = 0.0 tree_splits_index = 0 current_tree_leaf_values_index = 0 for tree_id in range(model.tree_count): current_tree_depth = model.tree_depth[tree_id] index = 0 for depth in range(current_tree_depth): index |= (binary_features[model.tree_splits[tree_splits_index + depth]] &lt;&lt; depth) result += model.leaf_values[current_tree_leaf_values_index + index] tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) return result 这段点代码功能是生成模型的预测结果result。model.tree_count表示决策树的数量，遍历每棵决策树。model.tree_depth存储每棵决策树的深度，取当前树的深度，存储在current_tree_depth。model.tree_splits存储决策树当前深度的节点在binary_features中的索引，每棵树有current_tree_depth个节点。看似CatBoost模型存储了都是完全二叉树，而且每一层的节点以及该节点的判断边界一致。如一棵6层的决策，可以从binary_features中得到一个长度为6，值为0和1组成的list。model.leaf_values存储所有叶子节点的值，每棵树的叶子节点有(1 &lt;&lt; current_tree_depth)个。将之前得到的list，倒序之后，看出一个2进制表示的数index，加上current_tree_leaf_values_index后，即是值在model.leaf_values的索引。将所有树得到的值相加，得到CatBoost模型的结果。 还原CatBoost模型树先从第二个for循环开始，打印每棵树序号，树的深度，当前树节点索引在tree_splits的便宜了，已经每个节点对应在tree_splits中的值。这个值对应的是在第一个for循环中生成的binary_features的索引。 tree_splits_index = 0 current_tree_leaf_values_index = 0 for tree_id in range(tree_count): current_tree_depth = tree_depth[tree_id] tree_splits_list = [] for depth in range(current_tree_depth): tree_splits_list.append(tree_splits[tree_splits_index + depth]) print tree_id, current_tree_depth, tree_splits_index, tree_splits_list tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) 0 6 0 [96, 61, 104, 2, 52, 81] 1 6 6 [95, 99, 106, 44, 91, 14] 2 6 12 [96, 31, 81, 102, 16, 34] 3 6 18 [95, 105, 15, 106, 57, 111] 4 6 24 [95, 51, 30, 8, 75, 57] 5 6 30 [94, 96, 103, 104, 25, 33] 6 6 36 [60, 8, 25, 39, 15, 99] 7 6 42 [96, 27, 48, 50, 69, 111] 8 6 48 [61, 80, 71, 3, 45, 2] 9 4 54 [61, 21, 90, 37] 从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。 12345678910从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。 split_list = [0] for i in range(len(border_counts)): split_list.append(split_list[-1] + border_counts[i]) print border_counts print zip(split_list[:-1], split_list[1:]) [32, 21, 39, 20] [(0, 32), (32, 53), (53, 92), (92, 112)] 在拿到一个binary_features的索引后即可知道该索引对应的节点使用的特征序号（float_features的索引）。 def find_feature(tree_splits_index): for i in range(len(split_list) - 1): if split_list[i] &lt;= tree_splits_index &lt; split_list[i+1]: return i 有了节点在binary_features中的索引，该索引也对应特征的判断边界数值索引，也知道了如何根据索引获取特征序号。决策树索引信息都的得到了，现在可以绘制树了。 绘制单棵决策树首先修改一下代码，便于获取单棵树的节点 class CatBoostTree(object): def __init__(self, CatboostModel): self.model = CatboostModel self.split_list = [0] for i in range(self.model.float_feature_count): self.split_list.append(self.split_list[-1] + self.model.border_counts[i]) def find_feature(self, splits_index): # 可优化成二分查找 for i in range(self.model.float_feature_count): if self.split_list[i] &lt;= splits_index &lt; self.split_list[i + 1]: return i def get_split_index(self, tree_id): tree_splits_index = 0 current_tree_leaf_values_index = 0 for index in range(tree_id): current_tree_depth = self.model.tree_depth[index] tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) return tree_splits_index, current_tree_leaf_values_index def get_tree_info(self, tree_id): tree_splits_index, current_tree_leaf_values_index = self.get_split_index(tree_id) current_tree_depth = self.model.tree_depth[tree_id] tree_splits_list = [] for depth in range(current_tree_depth): tree_splits_list.append(self.model.tree_splits[tree_splits_index + depth]) node_feature_list = [self.find_feature(index) for index in tree_splits_list] node_feature_borders = [self.model.borders[index] for index in tree_splits_list] end_tree_leaf_values_index = current_tree_leaf_values_index + (1 &lt;&lt; current_tree_depth) tree_leaf_values = self.model.leaf_values[current_tree_leaf_values_index: end_tree_leaf_values_index] return current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values 下面是绘制一棵决策树的函数，CatBoost导出的python代码文件通过model_file参数传入。 import imp import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from graphviz import Digraph def draw_tree(model_file, tree_id): fp, pathname, description = imp.find_module(model_file) CatboostModel = imp.load_module('CatboostModel', fp, pathname, description) catboost_tree = CatBoostTree(CatboostModel.CatboostModel) current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values = catboost_tree.get_tree_info(tree_id) dot = Digraph(name='tree_'+str(tree_id)) for depth in range(current_tree_depth): node_name = str(node_feature_list[current_tree_depth - 1 - depth]) node_border = str(node_feature_borders[current_tree_depth - 1 - depth]) label = 'column_' + node_name + '>' + node_border if depth == 0: dot.node(str(depth) + '_0', label) else: for j in range(1 &lt;&lt; depth): dot.node(str(depth) + '_' + str(j), label) dot.edge(str(depth-1) + '_' + str(j//2), str(depth) + '_' + str(j), label='No' if j%2 == 0 else 'Yes') depth = current_tree_depth for j in range(1 &lt;&lt; depth): dot.node(str(depth) + '_' + str(j), str(tree_leaf_values[j])) dot.edge(str(depth-1) + '_' + str(j//2), str(depth) + '_' + str(j), label='No' if j%2 == 0 else 'Yes') # dot.format = 'png' path = dot.render('./' + str(tree_id), cleanup=True) print path 例如绘制第11棵树（序数从0开始）。draw_tree(‘catboost_model_file’, 11)。 为了验证这个对不对，需要对一个测试特征生成每棵树的路径和结果，抽查一两个测试用例以及其中的一两颗树，观察结果是否相同。 def test_tree(model_file, float_features): fp, pathname, description = imp.find_module(model_file) CatboostModel = imp.load_module('CatboostModel', fp, pathname, description) model = CatboostModel.CatboostModel catboost_tree = CatBoostTree(CatboostModel.CatboostModel) result = 0 for tree_id in range(model.tree_count): current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values = catboost_tree.get_tree_info(tree_id) route = [] for depth in range(current_tree_depth): route.append(1 if float_features[node_feature_list[depth]] > node_feature_borders[depth] else 0) index = 0 for depth in range(current_tree_depth): index |= route[depth] &lt;&lt; depth tree_value = tree_leaf_values[index] print route, index, tree_value result += tree_value return result 如我们生成了第11棵树的图像，根据测试测试特征，手动在图像上查找可以得到一个值A。test_tree函数会打印一系列值，其中第11行对应的结果为值B。值A与值B相同，则测试为问题。其次还需要测试所有树的结果和导出文件中apply_catboost_model函数得到的结果相同。这个可以写个脚本，拿训练数据集跑一边。 from catboost_model_file import apply_catboost_model from CatBoostModelInfo import test_tree from sklearn.datasets import load_iris def main(): iris = load_iris() # print iris.data # print iris.target for feature in iris.data: if apply_catboost_model(feature) != test_tree('catboost_model_file', feature): print False print 'End.' if name == 'main': main() 至此，CatBoost模型的可视化完成了。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"},{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"},{"name":"CatBoost","slug":"CatBoost","permalink":"https://dataquaner.github.io/tags/CatBoost/"}]},{"title":"DBSCAN算法python实现（附完整数据集和代码）","slug":"DBSCAN算法python实现（附完整数据集和代码）","date":"2020-01-07T08:05:00.000Z","updated":"2020-04-11T11:30:29.639Z","comments":true,"path":"2020/01/07/dbscan-suan-fa-python-shi-xian-fu-wan-zheng-shu-ju-ji-he-dai-ma/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/dbscan-suan-fa-python-shi-xian-fu-wan-zheng-shu-ju-ji-he-dai-ma/","excerpt":"","text":"目录[TOC] 1. 算法思路DBSCAN算法的核心是“延伸”。先找到一个未访问的点p，若该点是核心点，则创建一个新的簇C，将其邻域中的点放入该簇，并遍历其邻域中的点，若其邻域中有点q为核心点，则将q的邻域内的点也划入簇C，直到C不再扩展。直到最后所有的点都标记为已访问。 点p通过密度可达来扩大自己的“地盘”，实际上就是簇在“延伸”。 图示网站：https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ 可以看一下簇是如何延伸的。 2. 算法实现2.1 计算两点之间的距离# 计算两个点之间的欧式距离，参数为两个元组 def dist(t1, t2): dis = math.sqrt((np.power((t1[0]-t2[0]),2) + np.power((t1[1]-t2[1]),2))) # print(\"两点之间的距离为：\"+str(dis)) return dis 2.2 读取文件，加载数据集def loadDataSet(fileName, splitChar='\\t'): dataSet = [] with open(fileName) as fr: for line in fr.readlines(): curline = line.strip().split(splitChar) fltline = list(map(float, curline)) dataSet.append(fltline) return dataSet 2.3 DBSCAN算法实现1、标记点是否被访问：我设置了两个列表，一个存放未访问的点unvisited，一个存放已访问的点visited。每次访问一个点，unvisited列表remove该点，visited列表append该点，以此来实现点的标记改变。 2、C作为输出结果，初始时是一个长度为所有点的个数的值全为-1的列表。之后修改点对应的索引的值来设置点属于哪个簇 # DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值 def dbscan(Data, Eps, MinPts): num = len(Data) # 点的个数 # print(\"点的个数：\"+str(num)) unvisited = [i for i in range(num)] # 没有访问到的点的列表 # print(unvisited) visited = [] # 已经访问的点的列表 C = [-1 for i in range(num)] # C为输出结果，默认是一个长度为num的值全为-1的列表 # 用k来标记不同的簇，k = -1表示噪声点 k = -1 # 如果还有没访问的点 while len(unvisited) > 0: # 随机选择一个unvisited对象 p = random.choice(unvisited) unvisited.remove(p) visited.append(p) # N为p的epsilon邻域中的对象的集合 N = [] for i in range(num): if (dist(Data[i], Data[p]) &lt;= Eps):# and (i!=p): N.append(i) # 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象 if len(N) >= MinPts: k = k+1 # print(k) C[p] = k # 对于p的epsilon邻域中的每个对象pi for pi in N: if pi in unvisited: unvisited.remove(pi) visited.append(pi) # 找到pi的邻域中的核心对象，将这些对象放入N中 # M是位于pi的邻域中的点的列表 M = [] for j in range(num): if (dist(Data[j], Data[pi])&lt;=Eps): #and (j!=pi): M.append(j) if len(M)>=MinPts: for t in M: if t not in N: N.append(t) # 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动 if C[pi] == -1: C[pi] = k # 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点 else: C[p] = -1 return C 3. 问题记录代码思路非常简单，让我以为实现起来也很简单。结果拖拖拉拉半个多月才终于将算法改好。 算法实现过程中遇到的问题其实是小问题，但是导致的结果非常严重。因为不起眼所以才难以察觉。 这是刚开始我运行算法得到的结果（Eps为10，MinPts为10）： Eps为2，MinPts为10（我改了点的大小）： 可以看出图中颜色特别多，实际上就是聚成的簇太多，可实际上目测应该只有七八个簇。这是为什么呢？ 原来是变量k的重复使用问题。 前面我用k来标识不同的簇，后面（如下图）我又将k变成了循环变量，注意M列表中都是整数，代表点在数据集中的索引，所以实际上是k在整数列表中遍历，覆盖掉了前面用来标识不同簇的k值，导致每次运行出来k取值特别多（如下下图）。 4. 运行结果 5. 完整代码5.1 源数据附数据集 链接：数据集788个点 提取码：rv06 5.2 源代码# encoding:utf-8 import matplotlib.pyplot as plt import random import numpy as np import math from sklearn import datasets list_1 = [] list_2 = [] # 数据集一：随机生成散点图,参数为点的个数 # def scatter(num): # for i in range(num): # x = random.randint(0, 100) # list_1.append(x) # y = random.randint(0, 100) # list_2.append(y) # print(list_1) # print(list_2) # data = list(zip(list_1, list_2)) # print(data) # #plt.scatter(list_1, list_2) # #plt.show() # return data #scatter(50) def loadDataSet(fileName, splitChar='\\t'): dataSet = [] with open(fileName) as fr: for line in fr.readlines(): curline = line.strip().split(splitChar) fltline = list(map(float, curline)) dataSet.append(fltline) return dataSet # 计算两个点之间的欧式距离，参数为两个元组 def dist(t1, t2): dis = math.sqrt((np.power((t1[0]-t2[0]),2) + np.power((t1[1]-t2[1]),2))) # print(\"两点之间的距离为：\"+str(dis)) return dis # dis = dist((1,1),(3,4)) # print(dis) # DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值 def dbscan(Data, Eps, MinPts): num = len(Data) # 点的个数 # print(\"点的个数：\"+str(num)) unvisited = [i for i in range(num)] # 没有访问到的点的列表 # print(unvisited) visited = [] # 已经访问的点的列表 C = [-1 for i in range(num)] # C为输出结果，默认是一个长度为num的值全为-1的列表 # 用k来标记不同的簇，k = -1表示噪声点 k = -1 # 如果还有没访问的点 while len(unvisited) > 0: # 随机选择一个unvisited对象 p = random.choice(unvisited) unvisited.remove(p) visited.append(p) # N为p的epsilon邻域中的对象的集合 N = [] for i in range(num): if (dist(Data[i], Data[p]) &lt;= Eps):# and (i!=p): N.append(i) # 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象 if len(N) >= MinPts: k = k+1 # print(k) C[p] = k # 对于p的epsilon邻域中的每个对象pi for pi in N: if pi in unvisited: unvisited.remove(pi) visited.append(pi) # 找到pi的邻域中的核心对象，将这些对象放入N中 # M是位于pi的邻域中的点的列表 M = [] for j in range(num): if (dist(Data[j], Data[pi])&lt;=Eps): #and (j!=pi): M.append(j) if len(M)>=MinPts: for t in M: if t not in N: N.append(t) # 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动 if C[pi] == -1: C[pi] = k # 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点 else: C[p] = -1 return C # 数据集二：788个点 dataSet = loadDataSet('788points.txt', splitChar=',') C = dbscan(dataSet, 2, 14) print(C) x = [] y = [] for data in dataSet: x.append(data[0]) y.append(data[1]) plt.figure(figsize=(8, 6), dpi=80) plt.scatter(x,y, c=C, marker='o') plt.show() # print(x) # print(y) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://dataquaner.github.io/tags/DBSCAN/"}]},{"title":"短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示","slug":"短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示","date":"2020-01-07T08:05:00.000Z","updated":"2020-04-11T11:34:53.017Z","comments":true,"path":"2020/01/07/duan-wen-ben-ju-lei-dbscan-suan-fa-yuan-li-python-dai-ma-shi-xian-ju-lei-jie-guo-zhan-shi/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/duan-wen-ben-ju-lei-dbscan-suan-fa-yuan-li-python-dai-ma-shi-xian-ju-lei-jie-guo-zhan-shi/","excerpt":"","text":"目录[TOC] 1. 算法原理1.1 常见的聚类算法聚类算法属于常见的无监督分类算法，在很多场景下都有应用，如用户聚类，文本聚类等。常见的聚类算法可以分成两类： 以 k-means 为代表的基于分区的算法 以层次聚类为代表的基于层次划分的算法 对于第一类方法，有以下几个缺点： 1）需要事先确定聚类的个数，当数据集比较大时，很难事先给出一个合适的值； 2）只适用于具有凸形状的簇，不适用于具有任意形状的簇； 3）对内存的占用资源比较大，难以推广至大规模数据集； 对于第二类方法，有以下缺点： 1）需要确定停止分裂的条件 2）计算速度慢 1.2 DBSCAN聚类 A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise （Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu） DBSCAN是一类基于密度的算法，能有效解决上述两类算法的问题。 DBSCAN的基本假设是一个集群的密度要显著高于噪声点的密度。因此，其基本思想是对于集群中的每一个点，在给定的半径范围内，相邻点的数量必须超过预先设定的某一个阈值。 因此，DBSCAN算法中包含两个重要的参数： eps：聚类类别中样本的相似度衡量，与类别内样本相似度成反比。可以理解为同一个类别当中，对两个样本之间距离的最大值限定。 min_samples：每个聚类类别中的最小样本数，会对未分类样本数量造成影响，与未分类样本数量成正比。当相似样本数量少于该参数时，不会聚到一起。 在实际应用过程中，根据样本的大小，以及样本的大致分布，了解聚类结果会随着这两个参数如何变化之后，可以根据自己的经验对两个参数进行调整。只有两个模型参数需要调整，因此调参过程也不会太麻烦。 2. 代码实现2.1 import需要的包# === import packages === # import jieba.posseg as pseg from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import CountVectorizer import numpy as np from sklearn.cluster import DBSCAN 2.2 载入数据根据数据文件的不同存在不同的数据载入方法，我当时使用的是两种类型的数据，分别是直接包含目标短文本的txt，以json格式存储的txt。如果有用到这两种类型的文件可以参考这部分的数据载入代码，其他的请根据文件类型和数据样式自行载入。首先是载入以json格式存储的txt文件，可以用正则表达式，也可以根据数据存储的方式提取出对应的字段。先展示一下数据的存储格式： { \"code\": \"200\", \"data\": { \"result\": [ { \"updateDate\": 1551923786433, \"ensureIntentName\": \"新意图\", \"corpus\": \"怎么查询之前的小微提醒\", \"recommendResult\": 0, \"remark\": \"\", \"source\": 2, \"result\": 2, \"eventName\": \"\", \"id\": \"b07328fc-8383-44b7-b466-15b063b8544a\", \"state\": 0, \"tag\": \"\", \"isHandle\": 1, \"createDate\": 1551669751334, \"eventId\": \"\", \"corpusTagId\": \"3335d2d8-a16e-46a2-9ed7-76739108d684\", \"intentName\": \"\", \"ensureIntent\": \"newIntent\", \"recommendIntent\": [ \"setmsgnotifications\" ], \"uploadTime\": 1551669751333, \"w3account\": \"x00286769\", \"createBy\": \"x00286769\", \"intentCode\": \"\", \"isBotSupport\": 0, \"userRole\": \"0\", \"welinkVersion\": \"3.9.13\" } ], \"pagination\": { \"pageCount\": 1, \"pageSizes\": 50, \"pageNumber\": 1, \"offset\": 0, \"pageTotal\": 1, \"pageNumbers\": 1, \"pageSize\": 50 } }, \"error\": \"\", \"stack\": \"\", \"message\": \"ok\" } 我的目标是对上述数据当中，字典中key “data” 对应的字典中的 “result” 中每一个item 的 “corpus” 进行提取，于是就有了下列代码。 # === Data loading === # data = [] corpus = [] for line in open(\"新意图语料.txt\", 'r+', encoding='UTF-8'): data.append(eval(line)) for i in range(len(data)): tmp = data[i]['data']['result'] for j in range(len(tmp)): corpus.append(tmp[j]['corpus']) 然后是载入包含目标短文本的txt，也就是说该txt直接存储了上面的 “corpus” 对应的内容，但是每一行的内容都加上了双引号和逗号，就通过strip把这些不需要的部分去掉了，最后得到所有 “corpus” 组成的list。 for line in open(\"未识别语料.txt\", 'r+'): line = line.strip('\\n') line = line.strip('\\t') line = line.rstrip(',') line = line.lstrip('\"') line = line.rstrip('\"') corpus.append(line) 2.3 对文本进行分词，并记录词性调用结巴词库对语料进行分词，并记录分词结果中每个词的词性。我的数据集在处理之后得到了5316条短文本，分词得到20640个不重复的词汇及其对应的词性，并建立了两者之间的字典联系。 # === Record the text cut and POS === # part_of_speech = [] word_after_cut = [] cut_corpus_iter = corpus.copy() cut_corpus = corpus.copy() for i in range(len(corpus)): cut_corpus_iter[i] = pseg.cut(corpus[i]) # 5316 cut_corpus[i] = \"\" for every in cut_corpus_iter[i]: cut_corpus[i] = (cut_corpus[i] + \" \" + str(every.word)).strip() part_of_speech.append(every.flag) # 20640 word_after_cut.append(every.word) # 20640 word_pos_dict = {word_after_cut[i]: part_of_speech[i] for i in range(len(word_after_cut))} 2.4 文本向量化–TF-IDF权重使用TF-IDF对文本进行向量化，得到文本的TF-IDF权重。 # === Get the TF-IDF weights === # Count_vectorizer = CountVectorizer() transformer = TfidfTransformer() # 用于统计每个词语的tf-idf权值 tf_idf = transformer.fit_transform(Count_vectorizer.fit_transform(cut_corpus)) # （5316，2039）第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵 word = Count_vectorizer.get_feature_names() # 2039，获取词袋模型中的所有词语 weight = tf_idf.toarray() # （5316，2039）将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重 2.5 基于词性的新权重前面得到了分词的结果，并对词性进行了记录，接下来可以针对不同词汇的词性码，给与其TF-IDF权重以不同的乘数，这样可以突出某些类型的词汇的重要性，在一定程度上有助于聚类的效果。 具体的乘数构造规则可以根据需求自行调整。 # === Get new weight with POS considered === # word_weight = [1 for i in range(len(word))] for i in range(len(word)): if word[i] not in word_pos_dict.keys(): continue if word_pos_dict[word[i]] == 'n': word_weight[i] = 1.2 elif word_pos_dict[word[i]] == \"vn\": word_weight[i] = 1.1 elif word_pos_dict[word[i]] == \"m\": word_weight[i] = 0 else: # 权重调整可以根据实际情况进行更改 continue word_weight = np.array(word_weight) new_weight = weight.copy() for i in range(len(weight)): for j in range(len(word)): new_weight[i][j] = weight[i][j] * word_weight[j] 2.6 DBSCAN模型得到了文本的向量化表示之后就可以将其投喂到模型当中了，eps和min_samples都是可以调整的参数。 # === Fit the DBSCAN model and get the classify labels === # DBS_clf = DBSCAN(eps=1, min_samples=4) DBS_clf.fit(new_weight) print(DBS_clf.labels_) 3. 聚类结果DBSCAN模型实现聚类之后，聚类的结果会存储在 labels_ 中，将 labels_ 与原来的文本一一对应，可以得到最终的聚类结果： # === Define the function of classify the original corpus according to the labels === # def labels_to_original(labels, original_corpus): assert len(labels) == len(original_corpus) max_label = max(labels) number_label = [i for i in range(0, max_label + 1, 1)] number_label.append(-1) result = [[] for i in range(len(number_label))] for i in range(len(labels)): index = number_label.index(labels[i]) result[index].append(original_corpus[i]) return result labels_original = labels_to_original(DBS_clf.labels_, corpus) for i in range(5): print(labels_original[i]) # 聚类结果展示（部分） ['社保卡', '社保卡', '社保卡。', '社保卡办理', '社保卡', '社保卡', '社保卡挂失', '社保卡。', '社保卡', '领取社保卡。'] ['五险一金', '五险一金。', '五险一金。', '五险一金介绍', '看看二月份五险一金情况'] ['打开汇钱。', '打开汇钱。', '我要汇钱', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。'] ['车辆通行证。', '车辆通行证。', '我要办车辆通行证。', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证。', '车辆通行证', '车辆通行证。', '车辆通行证。', '车辆通行证'] ['邮件附件权限', '等等邮件附件权限。', '邮件附件权限', '邮件附件权限', '邮件附件权限', '邮件附件权限', '您好，请问怎样申请图片查看权限和邮件附件查看权限？'] 4 附件：完整代码# === import packages === # import jieba.posseg as pseg from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import CountVectorizer import numpy as np from sklearn.cluster import DBSCAN # === Data loading === # data = [] corpus = [] for line in open(\"新意图语料.txt\", 'r+', encoding='UTF-8'): data.append(eval(line)) for i in range(len(data)): tmp = data[i]['data']['result'] for j in range(len(tmp)): corpus.append(tmp[j]['corpus']) for line in open(\"未识别语料.txt\", 'r+'): line = line.strip('\\n') line = line.strip('\\t') line = line.rstrip(',') line = line.lstrip('\"') line = line.rstrip('\"') corpus.append(line) # === Record the text cut and POS === # part_of_speech = [] word_after_cut = [] cut_corpus_iter = corpus.copy() cut_corpus = corpus.copy() for i in range(len(corpus)): cut_corpus_iter[i] = pseg.cut(corpus[i]) # 5316 cut_corpus[i] = \"\" for every in cut_corpus_iter[i]: cut_corpus[i] = (cut_corpus[i] + \" \" + str(every.word)).strip() part_of_speech.append(every.flag) # 20640 word_after_cut.append(every.word) # 20640 word_pos_dict = {word_after_cut[i]: part_of_speech[i] for i in range(len(word_after_cut))} # === Get new weight with POS considered === # word_weight = [1 for i in range(len(word))] for i in range(len(word)): if word[i] not in word_pos_dict.keys(): continue if word_pos_dict[word[i]] == 'n': word_weight[i] = 1.2 elif word_pos_dict[word[i]] == \"vn\": word_weight[i] = 1.1 elif word_pos_dict[word[i]] == \"m\": word_weight[i] = 0 else: # 权重调整可以根据实际情况进行更改 continue word_weight = np.array(word_weight) new_weight = weight.copy() for i in range(len(weight)): for j in range(len(word)): new_weight[i][j] = weight[i][j] * word_weight[j] # === Fit the DBSCAN model and get the classify labels === # DBS_clf = DBSCAN(eps=1, min_samples=4) DBS_clf.fit(new_weight) print(DBS_clf.labels_) # === Define the function of classify the original corpus according to the labels === # def labels_to_original(labels, original_corpus): assert len(labels) == len(original_corpus) max_label = max(labels) number_label = [i for i in range(0, max_label + 1, 1)] number_label.append(-1) result = [[] for i in range(len(number_label))] for i in range(len(labels)): index = number_label.index(labels[i]) result[index].append(original_corpus[i]) return result labels_original = labels_to_original(DBS_clf.labels_, corpus) for i in range(5): print(labels_original[i]) # 聚类结果展示（部分） ['社保卡', '社保卡', '社保卡。', '社保卡办理', '社保卡', '社保卡', '社保卡挂失', '社保卡。', '社保卡', '领取社保卡。'] ['五险一金', '五险一金。', '五险一金。', '五险一金介绍', '看看二月份五险一金情况'] ['打开汇钱。', '打开汇钱。', '我要汇钱', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。'] ['车辆通行证。', '车辆通行证。', '我要办车辆通行证。', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证。', '车辆通行证', '车辆通行证。', '车辆通行证。', '车辆通行证'] ['邮件附件权限', '等等邮件附件权限。', '邮件附件权限', '邮件附件权限', '邮件附件权限', '邮件附件权限', '您好，请问怎样申请图片查看权限和邮件附件查看权限？'] document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://dataquaner.github.io/tags/DBSCAN/"}]},{"title":"机器学习系列之决策树算法(08):梯度提升树算法LightGBM","slug":"机器学习系列之决策树算法（08）：梯度提升树算法LightGBM","date":"2020-01-07T02:30:00.000Z","updated":"2020-04-11T11:33:39.995Z","comments":true,"path":"2020/01/07/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-08-ti-du-ti-sheng-shu-suan-fa-lightgbm/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-08-ti-du-ti-sheng-shu-suan-fa-lightgbm/","excerpt":"","text":"1. LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。而LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。 1.1 LightGBM提出的动机常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。 LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。 1.2 XGBoost的缺点及LightGBM的优化（1）XGBoost的缺点在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是： 首先，对所有特征都按照特征的数值进行预排序。 其次，在遍历分割点的时候用的代价找到一个特征上的最好分割点。 最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。 这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显： 首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。 最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。 （2）LightGBM的优化为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化： 基于Histogram的决策树算法。 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 直接支持类别特征(Categorical Feature) 支持高效并行 Cache命中率优化 下面我们就详细介绍以上提到的lightGBM优化算法。 2. LightGBM的基本原理2.1 基于Histogram的决策树算法（1）直方图算法Histogram algorithm应该翻译为直方图算法，直方图算法的基本思想是： 先把连续的浮点特征值离散化成 k个整数，同时构造一个宽度为 k 的 直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。 图：直方图算法 直方图算法简单理解为： 首先确定对于每一个特征需要多少个箱子（bin）并为每一个箱子分配一个整数； 然后将浮点数的范围均分成若干区间，区间个数与箱子个数相等，将属于该箱子的样本数据更新为箱子的值； 最后用直方图（#bins）表示。看起来很高大上，其实就是直方图统计，将大规模的数据放在了直方图中。 我们知道特征离散化具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等。对于直方图算法来说最直接的有以下两个优点： 内存占用更小： 直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8 。也就是说XGBoost需要用32位的浮点数去存储特征值，并用32位的整形去存储索引，而 LightGBM只需要用8位去存储直方图，内存相当于减少为 ； 图：内存占用优化为预排序算法的1/8 计算代价更小： 预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算 k次（ 可以认为是常数），直接将时间复杂度从O(#data * #feature )降低到 O(k * #feature )，而我们知道#data &gt;&gt;k。 当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 （2）直方图做差加速LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。 注意： XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。 2.2 带深度限制的 Leaf-wise 算法在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。 LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。 2.3 单边梯度采样算法Gradient-based One-Side Sampling 应该被翻译为单边梯度采样（GOSS）。GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 AdaBoost中，样本权重是数据重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的是，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用。即梯度小的样本，训练误差也比较小，说明数据已经被模型学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练模型的精确度，为了避免此问题，提出了GOSS算法。 GOSS是一个样本的采样算法，目的是丢弃一些对计算信息增益没有帮助的样本留下有帮助的。根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。因此，GOSS在进行数据采样的时候只保留了梯度较大的数据，但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。所以，GOSS首先将要进行分裂的特征的所有取值按照绝对值大小降序排序（XGBoost一样也进行了排序，但是LightGBM不用保存排序后的结果），选取绝对值最大的 个数据。然后在剩下的较小梯度数据中随机选择 个数据。接着将这 个数据乘以一个常数 ，这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这 个数据来计算信息增益。下图是GOSS的具体算法。 2.4 互斥特征捆绑算法高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从 变为 ，这里 指特征融合绑定后特征包的个数，且 远小于 。 针对这种想法，我们会遇到两个问题： 怎么判定哪些特征应该绑在一起（build bundled）？ 怎么把特征绑为一个（merge feature）？ （1）解决哪些特征应该绑在一起将相互独立的特征进行绑定是一个 NP-Hard 问题，LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。此外，我们注意到通常有很多特征，尽管不是％相互排斥，但也很少同时取非零值。如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。经过简单的计算，随机污染小部分特征值将影响精度最多 ， 是每个绑定中的最大冲突比率，当其相对较小时，能够完成精度和效率之间的平衡。具体步骤可以总结如下： 构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关； 根据节点的度进行降序排序，度越大，与其它特征的冲突越大； 遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。 算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 来平衡算法的精度和效率。EFB 算法的伪代码如下所示： 算法3的时间复杂度是 ，训练之前只处理一次，其时间复杂度在特征不是特别多的情况下是可以接受的，但难以应对百万维度的特征。为了继续提高效率，LightGBM提出了一种更加高效的无图的排序策略：将特征按照非零值个数排序，这和使用图节点的度排序相似，因为更多的非零值通常会导致冲突，新算法在算法3基础上改变了排序策略。 （2）解决怎么把特征绑为一捆特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间 ，B特征的原始取值为区间，我们可以在B特征的取值上加一个偏置常量，将其取值范围变为，绑定后的特征取值范围为 ，这样就可以放心的融合特征A和B了。具体的特征合并算法如下所示： 3. LightGBM的工程优化我们将论文《Lightgbm: A highly efficient gradient boosting decision tree》中没有提到的优化方案，而在其相关论文《A communication-efficient parallel algorithm for decision tree》中提到的优化方案，放到本节作为LightGBM的工程优化来向大家介绍。 3.1 直接支持类别特征实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的特征，降低了空间和时间的效率。但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题： 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为1 ，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别。 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，如下图左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差。但如果使用下图右边的切分方法，数据会被切分到两个比较大的空间，进一步的学习也会更好。下图右边叶子节点的含义是或者放到左孩子，其余放到右孩子。 图：左图为基于 one-hot 编码进行分裂，右图为 LightGBM 基于 many-vs-many 进行分裂 而类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 个类别，则有 种可能，时间复杂度为 ，LightGBM 基于 Fisher的《On Grouping For Maximum Homogeneity》论文实现了 的时间复杂度。 算法流程如下图所示，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。从下图可以看到， 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。 图：LightGBM求解类别特征的最优切分算法 在Expo数据集上的实验结果表明，相比展开的方法，使用LightGBM支持的类别特征可以使训练速度加速倍，并且精度一致。更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。 3.2 支持高效并行（1）特征并行特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法。这种特征并行方法有个很大的缺点：就是对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度。 LightGBM 则不进行数据垂直划分，而是在每台机器上保存全部训练数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。具体过程如下图所示。 （2）数据并行传统的数据并行策略主要为水平划分数据，让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 ；如果使用集成的通信，则通讯开销为 。 LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。具体过程如下图所示。 图：数据并行 （3）投票并行基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行的方式只合并部分特征的直方图从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。 大致步骤为两步： 本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征； 合并时只合并每个机器选出来的特征。 图：投票并行 3.3 Cache命中率优化XGBoost对cache优化不友好，如下图所示。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问算法进行改进。 图：随机访问会造成cache miss 而 LightGBM 所使用直方图算法对 Cache 天生友好： 首先，所有的特征都采用相同的方式获得梯度（区别于XGBoost的不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率； 其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss的问题。 图：LightGBM增加缓存命中率 4. LightGBM的优缺点4.1 优点这部分主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。 （1）速度更快 LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度； LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算； LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量； LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略； LightGBM 对缓存也进行了优化，增加了缓存命中率； （2）内存更小 XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 降低为 ，极大的减少了内存消耗； LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗； LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。 4.2 缺点 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合； Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感； 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去； 5. LightGBM实例本篇文章所有数据集和代码均在我的GitHub中，地址：https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/LightGBM 5.1 安装LightGBM依赖包pip install lightgbm 5.2 LightGBM分类和回归LightGBM有两大类接口：LightGBM原生接口 和 scikit-learn接口 ，并且LightGBM能够实现分类和回归两种任务。 （1）基于LightGBM原生接口的分类import lightgbm as lgb from sklearn import datasets from sklearn.model_selection import train_test_split import numpy as np from sklearn.metrics import roc_auc_score, accuracy_score # 加载数据 iris = datasets.load_iris() # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3) # 转换为Dataset数据格式 train_data = lgb.Dataset(X_train, label=y_train) validation_data = lgb.Dataset(X_test, label=y_test) # 参数 params = { 'learning_rate': 0.1, 'lambda_l1': 0.1, 'lambda_l2': 0.2, 'max_depth': 4, 'objective': 'multiclass', # 目标函数 'num_class': 3, } # 模型训练 gbm = lgb.train(params, train_data, valid_sets=[validation_data]) # 模型预测 y_pred = gbm.predict(X_test) y_pred = [list(x).index(max(x)) for x in y_pred] print(y_pred) # 模型评估 print(accuracy_score(y_test, y_pred)) （2）基于Scikit-learn接口的分类 from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.externals import joblib # 加载数据 iris = load_iris() data = iris.data target = iris.target # 划分训练数据和测试数据 X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2) # 模型训练 gbm = LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=20) gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5) # 模型存储 joblib.dump(gbm, 'loan_model.pkl') # 模型加载 gbm = joblib.load('loan_model.pkl') # 模型预测 y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_) # 模型评估 print('The accuracy of prediction is:', accuracy_score(y_test, y_pred)) # 特征重要度 print('Feature importances:', list(gbm.feature_importances_)) # 网格搜索，参数优化 estimator = LGBMClassifier(num_leaves=31) param_grid = { 'learning_rate': [0.01, 0.1, 1], 'n_estimators': [20, 40] } gbm = GridSearchCV(estimator, param_grid) gbm.fit(X_train, y_train) print('Best parameters found by grid search are:', gbm.best_params_) （3）基于LightGBM原生接口的回归对于LightGBM解决回归问题，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.转换为Dataset数据格式 lgb_train = lgb.Dataset(train_X, train_y) lgb_eval = lgb.Dataset(test_X, test_y, reference=lgb_train) # 6.参数 params = { 'task': 'train', 'boosting_type': 'gbdt', # 设置提升类型 'objective': 'regression', # 目标函数 'metric': {'l2', 'auc'}, # 评估函数 'num_leaves': 31, # 叶子节点数 'learning_rate': 0.05, # 学习速率 'feature_fraction': 0.9, # 建树的特征选择比例 'bagging_fraction': 0.8, # 建树的样本采样比例 'bagging_freq': 5, # k 意味着每 k 次迭代执行bagging 'verbose': 1 # &lt;0 显示致命的, =0 显示错误 (警告), >0 显示信息 } # 7.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5) # 8.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X, num_iteration=my_model.best_iteration) # 9.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) （4）基于Scikit-learn接口的回归import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=20, verbosity=2) my_model.fit(train_X, train_y, verbose=False) # 6.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X) # 7.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 5.3 LightGBM调参在上一部分中，LightGBM模型的参数有一部分进行了简单的设置，但大都使用了模型的默认参数，但默认参数并不是最好的。要想让LightGBM表现的更好，需要对LightGBM模型进行参数微调。下图展示的是回归模型需要调节的参数，分类模型需要调节的参数与此类似。 6. 关于LightGBM若干问题的思考6.1 LightGBM与XGBoost的联系和区别有哪些？（1）LightGBM使用了基于histogram的决策树算法，这一点不同于XGBoost中的贪心算法和近似算法，histogram算法在内存和计算代价上都有不小优势。1）内存上优势：很明显，直方图算法的内存消耗为 (因为对特征分桶后只需保存特征离散化之后的值)，而XGBoost的贪心算法内存消耗为： ，因为XGBoost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要位的浮点数来保存。2）计算上的优势：预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为，而直方图算法只需要遍历桶就行了，时间为。 （2）XGBoost采用的是level-wise的分裂策略，而LightGBM采用了leaf-wise的策略，区别是XGBoost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGBoost也进行了分裂，带来了不必要的开销。leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。 （3）XGBoost在每一层都动态构建直方图，因为XGBoost的直方图算法不是针对某个特定的特征，而是所有特征共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而LightGBM中对每个特征都有一个直方图，所以构建一次直方图就够了。 （4）LightGBM使用直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。 （5）LightGBM支持类别特征，不需要进行独热编码处理。 （6）LightGBM优化了特征并行和数据并行算法，除此之外还添加了投票并行方案。 （7）LightGBM采用基于梯度的单边采样来减少训练样本并保持数据分布不变，减少模型因数据分布发生变化而造成的模型精度下降。 （8）特征捆绑转化为图着色问题，减少特征数量。 7. Reference由于参考的文献较多，我把每篇参考文献按照自己的学习思路，进行了详细的归类和标注。 LightGBM论文解读： 【1】Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]//Advances in Neural Information Processing Systems. 2017: 3146-3154. 【2】Taifeng Wang分享LightGBM的视频，地址：https://v.qq.com/x/page/k0362z6lqix.html 【3】开源|LightGBM：三天内收获GitHub 1000+ 星，地址：https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog 【4】Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/anshuai_aw1/article/details/83048709 【5】快的不要不要的lightGBM - 王乐的文章 - 知乎 https://zhuanlan.zhihu.com/p/31986189 【6】『 论文阅读』LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/shine19930820/article/details/79123216 LightGBM算法讲解： 【7】【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/87885678 【8】入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同，地址：https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw 【9】CatBoost vs. Light GBM vs. XGBoost，地址：https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db 【10】机器学习算法之LightGBM，地址：https://www.biaodianfu.com/lightgbm.html LightGBM工程优化： 【11】Meng Q, Ke G, Wang T, et al. A communication-efficient parallel algorithm for decision tree[C]//Advances in Neural Information Processing Systems. 2016: 1279-1287. 【12】Zhang H, Si S, Hsieh C J. GPU-acceleration for Large-scale Tree Boosting[J]. arXiv preprint arXiv:1706.08359, 2017. 【13】LightGBM的官方GitHub代码库，地址：https://github.com/microsoft/LightGBM 【14】关于sklearn中的决策树是否应该用one-hot编码？- 柯国霖的回答 - 知乎 https://www.zhihu.com/question/266195966/answer/306104444 LightGBM实例： 【15】LightGBM使用，地址：https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/ 【16】LightGBM两种使用方式 ，地址：https://www.cnblogs.com/chenxiangzhen/p/10894306.html LightGBM若干问题的思考： 【17】GBDT、XGBoost、LightGBM的区别和联系，地址：https://www.jianshu.com/p/765efe2b951a 【18】xgboost和lightgbm的区别和适用场景，地址：https://www.nowcoder.com/ta/review-ml/review?page=101 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战：原生接口和sklearn接口区别","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST实战：原生接口和sklearn接口的区别","date":"2019-12-26T16:00:00.000Z","updated":"2020-04-11T11:33:22.949Z","comments":true,"path":"2019/12/27/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan-yuan-sheng-jie-kou-he-sklearn-jie-kou-de-qu-bie/","link":"","permalink":"https://dataquaner.github.io/2019/12/27/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan-yuan-sheng-jie-kou-he-sklearn-jie-kou-de-qu-bie/","excerpt":"","text":"1 前言2 官方文档英文官方文档 中文文档 3 sklearn接口from xgboost.sklearn import XGBClassifier xgbc = XGBClassifier(n_jobs=-1) # 新建xgboost sklearn的分类class # xgboost的sklearn接口默认只使用cpu单线程，设置n_jobs=-1使用所有线程 print(\"开始xgboost classifier训练\") xgbc.fit(train_vector,np.array(train_label)) # 喂给分类器训练numpy形式的训练特征向量和标签向量 print(\"完成xgboost classifier训练，开始预测\") pre_train_Classifier = xgbc.predict(test_vector) # 喂给分类器numpy形式的测试特征向量 np.save(os.path.join(model_path,\"pre_train_Classifier.npy\"),pre_train_Classifier) # 保存结果 xgboost的sklearn接口，可以不经过标签标准化(即将标签编码为0~n_class-1)，直接喂给分类器特征向量和标签向量，使用fit训练后调用predict就能得到预测向量的预测标签，它会在内部调用sklearn.preprocessing.LabelEncoder()将标签在分类器使用时transform，在输出结果时inverse_transform。 优点：使用简单，无需对标签进行标准化处理，直接得到预测标签； 缺点：在模型保存后重新载入，丢失LabelEncoder，不能增量训练只能用一次. 4 xgboost的原生接口vector_matrix,label_single_new = get_data(data_path) # 获取得到特征矩阵、标签向量 print(\"标签总数为：%d；数据量总数为：%d\"%(len(list(set(label_single_new))),len(vector_matrix))) # 将标签标准化为0~class number-1,则xgboost概率最大的下标即为该位置数对应的标签 from sklearn import preprocessing label_coder = preprocessing.LabelEncoder() label_single_code = label_coder.fit_transform(label_single_new) # 切割训练集、测试集 from sklearn.model_selection import train_test_split train_matrix,test_matrix,train_label,test_label = train_test_split( vector_matrix,label_single_code,test_size=0.1,random_state=0) import xgboost as xgb # 参数设置见 http://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/xgboost/chapters/xgboost_usage.html params = { 'booster': 'gbtree', 'silent':0, # 如果为 0（默认值），则表示打印运行时的信息；如果为 1，则表示不打印这些信息 'objective': 'multi:softprob', # 基于softmax 的多分类模型，但是它的输出是一个矩阵：ndata*nclass，给出了每个样本属于每个类别的概率。 'num_class':len(set(label_single_new)),#指定类别数量 } dtrain = xgb.DMatrix(train_matrix, label=train_label, nthread=-1) # xgboost原生接口需要使用DMatrix格式的数据，这里与sklearn接口不同 print(\"开始xgboost训练\") xgbc = xgb.train(params,dtrain) # 初始化xgboost分类器，原生接口默认启用全部线程 xgbc.save_model(model_path+save_name+'xgbc_0.9.model') # 保存模型 # ============================================================================= # xgbc = xgb.Booster() # 重新载入模型 # xgbc.load_model(fname=model_path+save_name+'xgbc_0.9.model') # ============================================================================= print(\"xgboost训练完成，得到概率矩阵\") pre_train = xgbc.predict(xgb.DMatrix(train_matrix, nthread=-1)) # 训练数据的预测概率矩阵，启用全部线程 pre_test = xgbc.predict(xgb.DMatrix(test_matrix, nthread=-1)) # 测试数据的预测概率矩阵，启用全部线程 # 概率矩阵各行的数据为各条数据的预测概率，各行数据之和为1； # 概率矩阵各行的下标即为标准化后的label标签(0~class number-1) # 数据保存 np.save(model_path+save_name+'pre_train.npy',pre_train) np.save(model_path+save_name+'train_label.npy',train_label) np.save(model_path+save_name+'pre_test.npy',pre_test) np.save(model_path+save_name+'test_label.npy',test_label) # 数据载入 # ============================================================================= # pre_train = np.load(model_path+save_name+'pre_train.npy') # train_label = np.load(model_path+save_name+'train_label.npy') # pre_test = np.load(model_path+save_name+'pre_test.npy') # test_label = np.load(model_path+save_name+'test_label.npy') # ============================================================================= # narray_target.argsort(axis=1)，获得按行(排序对象为各行数值)升序后的下标矩阵，axis=0为按列升序; # np.fliplr(narray_target)获取矩阵的左右翻转，narray_target[::-1]获取矩阵的上下翻转 # narray_target[:,-5:]获取矩阵的后5列; top_k = 5 # 获取预测概率最大的5个标签 # 获取概率矩阵排序信息，得到按行升序的下标矩阵,切割得到各行的后5个下标, # 将其左右翻转后，得到各行降序的前5个下标，即标准化后的标签 pre_test_index = np.fliplr(pre_test.argsort(axis=1)[:,-1*top_k:]) pre_test_label = label_coder.inverse_transform(pre_test_index) # 调用label标准化工具inverse_transform将下标转化为真实标签 pre_train_index = np.fliplr(pre_train.argsort(axis=1)[:,-1*top_k:]) pre_train_label = label_coder.inverse_transform(pre_train_index) xgboost原生接口，数据需要经过标签标准化(LabelEncoder().fit_transform)、输入数据标准化(xgboost.DMatrix)和输出结果反标签标准化(LabelEncoder().inverse_transform)，训练调用train预测调用predict. 需要注意的是，xgboost原生接口输出的预测标签概率矩阵各行的下标即为标准化后的label标签(0~class number-1). 5 结论优先考虑使用原生接口形式，便于模型保存后的复用。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST实战","date":"2019-12-25T16:00:00.000Z","updated":"2020-04-11T11:33:03.342Z","comments":true,"path":"2019/12/26/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan/","link":"","permalink":"https://dataquaner.github.io/2019/12/26/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan/","excerpt":"","text":"1 前言上一篇从数据原理角度深入介绍了XGBoost的实现原理及优化，参考《梯度提升树算法XGBoost》。本篇主要介绍XGBoost的工程实战，参数调优等内容。 学习一个算法实战，一般按照以下几步，第一步能够基于某个平台、某种语言构建一个模型，第二步是能够优化一个模型 。我们将学习以下内容 如果使用xgboost构建分类器 xgboost 的参数含义，以及如何调参 xgboost 的如何做cv xgboost的可视化 2 XGBoost模型构建回归模型准备数据我们使用房价数据 ，做的是一个回归任务，预测房价，分类任务类似。 导入包 import pandas as pd from xgboost import XGBRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import Imputer from sklearn.metrics import mean_absolute_error 读入和展示数据 data = pd.read_csv('../input/train.csv') data.dropna(axis=0, subset=['SalePrice'], inplace=True) y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25) my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) print(train_X.shape) print(test_X.shape) print(train_y.shape) print(test_y.shape) --- ##执行结果 (1095, 37) (365, 37) (1095,) (365,) --- 创建并训练XGBoost模型随机选取默认参数进行初始化建模 my_model = XGBRegressor() # Add silent=True to avoid printing out updates with each cycle my_model.fit(train_X, train_y, verbose=False) 评估并预测模型# make predictions predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 模型调优XGBoost有一些参数可以显著影响模型的准确性和训练速度。 n_estimatorsn_estimators 指定训练循环次数。在 欠拟合 vs 过拟合 图表, n_estimators让训练沿着图表向右移动。 值太低会导致欠拟合，这对训练数据和新数据的预测都是不准确的。 太大的值会导致过度拟合，这是对训练数据的准确预测，但对新数据的预测不准确（这是我们关心的）。 通过实际实验来找到理想的n_estimators。 典型值范围为100-1000，但这很大程度上取决于下面讨论的 early_stopping_roundsearly_stopping_rounds 提供了一种自动查找理想值的方法。 early_stopping_rounds会导致模型在validation score停止改善时停止迭代，即使迭代次数还没有到n_estimators。为n_estimators设置一个高值然后使用early_stopping_rounds来找到停止迭代的最佳时间是明智的。 存在随机的情况有时会导致validation score无法改善，因此需要指定一个数字，以确定在停止前允许多少轮退化。early_stopping_rounds = 5是一个合理的值。 因此，在五轮validation score无法改善之后训练将停止。 以下是early_stopping的代码： my_model = XGBRegressor(n_estimators=1000) my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 当使用early_stopping_rounds时，需要留出一些数据来检查要使用的轮数。 如果以后想要使所有数据拟合模型，请将n_estimators设置为在早期停止运行时发现的最佳值。 learning_rate对于更好的XGBoost模型，这是一个微妙但重要的技巧： XGBoost模型不是通过简单地将每个组件模型中的预测相加来获得预测，而是在将它们添加之前将每个模型的预测乘以一个小数字。这意味着我们添加到集合中的每个树都不会对最后结果有决定性的影响。在实践中，这降低了模型过度拟合的倾向。 因此，使用一个较大的n_estimators值并不会造成过拟合。如果使用early_stopping_rounds，树的数量会被设置成一个合适的值。 通常，较小的learning rate（以及大量的estimators）将产生更准确的XGBoost模型，但是由于它在整个循环中进行更多迭代，因此也将使模型更长时间进行训练。 包含学习率的代码如下： my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 小结XGBoost目前是用于在传统数据（也称为表格或结构数据）上构建精确模型的主要算法 from xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model1 = XGBRegressor() my_model1.fit(train_X, train_y, verbose=False) predictions = my_model1.predict(test_X) print(\"Mean Absolute Error 1: \" + str(mean_absolute_error(predictions, test_y))) my_model2 = XGBRegressor(n_estimators=1000) my_model2.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model2.predict(test_X) print(\"Mean Absolute Error 2: \" + str(mean_absolute_error(predictions, test_y))) my_model3 = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model3.fit(train_X, train_y, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model3.predict(test_X) print(\"Mean Absolute Error 3: \" + str(mean_absolute_error(predictions, test_y))) 分类模型以天池竞赛中的《快来一起挖掘幸福感！》中的数据为例，开始一个多分类模型的的实例 导入包import pandas as pd from matplotlib import pyplot as plt import xgboost as xgb from sklearn.model_selection import learning_curve, train_test_split,GridSearchCV from sklearn.metrics import accuracy_score from sklearn.metrics import mean_absolute_error 导入数据''' ## 准备训练集和测试集 ''' data = pd.read_csv('happiness_train_abbr.csv') y=data['happiness'] data.drop('happiness',axis=1,inplace=True) data.drop('survey_time',axis=1,inplace=True)#survey_time格式不能直接识别 X=data 数据集划分train_x, test_x, train_y, test_y = train_test_split (X, y, test_size =0.30, early_stopping_rounds=10,random_state = 33) XGBoost模型训练''' ## xgboost训练 ''' params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1 } #第一次设置300次的迭代，评测的指标是\"merror\",\"mlogloss\"，这是一个多分类问题。 model = xgb.XGBClassifier(params) eval_set = [(train_x, train_y), (test_x, test_y)] model.fit(train_x, train_y, eval_set=eval_set, eval_metric=[\"merror\", \"mlogloss\"],verbose=True) predictions = model.predict(test_x) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) accuracy = accuracy_score(test_y, predictions) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) 模型可视化''' ## 可视化训练过程 ''' results = model.evals_result() epochs = len(results['validation_0']['merror']) x_axis = range(0, epochs) from matplotlib import pyplot fig, ax = pyplot.subplots(1,2,figsize=(10,5)) ax[0].plot(x_axis, results['validation_0']['mlogloss'], label='Train') ax[0].plot(x_axis, results['validation_1']['mlogloss'], label='Test') ax[0].legend() ax[0].set_title('XGBoost Log Loss') ax[0].set_ylabel('Log Loss') ax[0].set_xlabel('epochs') ax[1].plot(x_axis, results['validation_0']['merror'], label='Train') ax[1].plot(x_axis, results['validation_1']['merror'], label='Test') ax[1].legend() ax[1].set_title('XGBoost Classification Error') ax[1].set_ylabel('Classification Error') ax[1].set_xlabel('epochs') pyplot.show() 实际训练效果，在第146次迭代就停止了，说明最好的效果实在136次左右。根据许多大牛的实践经验，选择early_stopping_rounds = 10% * n_estimators。 最终输出模型最佳状态下的结果： print (\"best iteration:\",model.best_iteration) limit = model.best_iteration predictions = model.predict(test_x,ntree_limit=limit) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) accuracy = accuracy_score(test_y, predictions) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) 3 参考资料https://www.kaggle.com/dansbecker/xgboost https://blog.csdn.net/lujiandong1/article/details/52777168 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST","date":"2019-12-24T16:00:00.000Z","updated":"2020-04-11T11:32:44.329Z","comments":true,"path":"2019/12/25/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost/","link":"","permalink":"https://dataquaner.github.io/2019/12/25/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost/","excerpt":"","text":"前言XGBoost的全称是eXtreme Gradient Boosting，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。XGBoost是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用XGBoost进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，XGBoost的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从XGBoost的数学原理和工程实现上进行介绍，然后介绍XGBoost的优缺点。 数学原理生成一棵树Boosting Tree回顾XGBoost模型是大规模并行boosting tree的工具，它是目前较好的开源boosting tree工具包。因此，在了解XGBoost算法基本原理之前，需要首先了解Boosting Tree算法基本原理。Boosting方法是一类应用广泛且非常有效的统计学习方法。它是基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比任何一个专家单独的判断要好。这种思想整体上可以分为两种： 强可学习：如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称为强可学习，直接单个模型就搞定常规问题。就好比专家给出的意见都很接近且都是正确率很高的结果，那么一个专家的结论就可以用了，这种情况非常少见。 弱可学习：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。这种情况是比较常见的。 boosting算法主要是针对弱可学习的分类器来开展优化工作。其关心的问题包括两方面内容： （1）在每一轮如何改变训练数据的权值和概率分布； （2）如何将弱分类器组合成一个强分类器，这种思路较好的就是AdaBoost算法，以前在遥感图像地物识别中得到过应用。 Boosting Tree模型采用加法模型与前向分步算法，而且基模型都是决策树模型。前向分步算法（Forward stage wise additive model）是指在叠加新的基模型的基础上同步进行优化，具体而言，就是每一次叠加的模型都去拟合上一次模型拟合后产生的残差（Residual）。从算法模型解释上来说，Boosting Tree是决策树的加法模型： （1） 上式中M为决策树的数量； 为某个决策树； 为对应决策树的参数。 Boosting Tree模型采用前向分步算法，其中假设 ，则第m步的模型是： （2） 为求解对应的参数 ，需要最小化相应损失函数来确定，具体公式如下： （3） 由前向分步算法得到M棵决策树 后，再进行加和，就得到了提升树模型 。在xgboost论文中提到的一个明显的boosting tree的加和应用案例如图3所示。 图2 boosting tree的累加效果示意图 相关树模型的参数值求解主要依据于损失函数的定义。 一般来言对于分类问题，选择指数损失函数作为损失函数时，将形成AdaBoost模型； 对于回归问题，损失函数常利用平方损失函数。为了扩展Boosting Tree的应用范围，需要构建一种可以广泛适用的残差描述方式来满足于任意损失函数的形式，为解决分类问题的Gradient Boosting Decision Tree算法应运而生。 带正则项的Boosting Tree模型和带梯度的Boosting Tree推导过程 目标函数我们知道 XGBoost 是由 个基模型组成的一个加法运算式： 其中 为第 个基模型， 为第 个样本的预测值。 损失函数可由预测值 与真实值 进行表示： 其中 为样本数量。 我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的损失函数 与抑制模型复杂度的正则项 组成，所以我们有： 为模型的正则项，由于 XGBoost 支持决策树也支持线性模型，所以这里再不展开描述。 我们知道 boosting 模型是前向加法，以第 步的模型为例，模型对第 个样本 的预测为： 其中 由第 步的模型给出的预测值，是已知常数， 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成： 求此时最优化目标函数，就相当于求解 。 泰勒公式是将一个在 处具有 阶导数的函数 利用关于 的 次多项式来逼近函数的方法，若函数 在包含 的某个闭区间 上具有 阶导数，且在开区间 上具有 阶导数，则对闭区间 上任意一点 有 ，其中的多项式称为函数在 处的泰勒展开式， 是泰勒公式的余项且是 的高阶无穷小。 根据泰勒公式我们把函数 在点 处进行泰勒的二阶展开，可得到如下等式： 我们把 视为 ， 视为 ，故可以将目标函数写为： 其中 为损失函数的一阶导， 为损失函数的二阶导，注意这里的导是对 求导。 我们以平方损失函数为例： 则： 由于在第 步时 其实是一个已知的值，所以 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成： 所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 ，最后根据加法模型得到一个整体模型。 基于决策树的目标函数损失函数可由预测值 与真实值 进行表示： 其中， 为样本的数量。 我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要在目标函数中添加正则项，用于防止过拟合。所以目标函数由模型的损失函数 与抑制模型复杂度的正则项 组成，目标函数的定义如下： 其中， 是将全部 棵树的复杂度进行求和，添加到目标函数中作为正则化项，用于防止模型过度拟合。 由于XGBoost是boosting族中的算法，所以遵从前向分步加法，以第 步的模型为例，模型对第 个样本 的预测值为： 其中， 是由第 步的模型给出的预测值，是已知常数， 是这次需要加入的新模型的预测值。此时，目标函数就可以写成： 注意上式中，只有一个变量，那就是第 棵树 ，其余都是已知量或可通过已知量可以计算出来的。细心的同学可能会问，上式中的第二行到第三行是如何得到的呢？这里我们将正则化项进行拆分，由于前 棵树的结构已经确定，因此前 棵树的复杂度之和可以用一个常量表示，如下所示： 泰勒公式展开泰勒公式是将一个在 处具有 阶导数的函数 利用关于 的 次多项式来逼近函数的方法。若函数 在包含 的某个闭区间 上具有 阶导数，且在开区间 上具有 阶导数，则对闭区间 上任意一点 有： 其中的多项式称为函数在 处的泰勒展开式， 是泰勒公式的余项且是 的高阶无穷小。 根据泰勒公式，把函数 在点 处进行泰勒的二阶展开，可得如下等式： 回到XGBoost的目标函数上来， 对应损失函数 ， 对应前 棵树的预测值 ， 对应于我们正在训练的第 棵树 ，则可以将损失函数写为： 其中， 为损失函数的一阶导， 为损失函数的二阶导，注意这里的求导是对 求导。 我们以平方损失函数为例： 则： 将上述的二阶展开式，带入到XGBoost的目标函数中，可以得到目标函数的近似值： 由于在第 步时 其实是一个已知的值，所以 是一个常数，其对函数的优化不会产生影响。因此，去掉全部的常数项，得到目标函数为： 所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 ，最后根据加法模型得到一个整体模型。 一棵树的生长细节分裂结点在实际训练过程中，当建立第 t 棵树时，XGBoost采用贪心法进行树结点的分裂： 从树深为0时开始： 对树中的每个叶子结点尝试进行分裂； 每次分裂后，原来的一个叶子结点继续分裂为左右两个子叶子结点，原叶子结点中的样本集将根据该结点的判断规则分散到左右两个叶子结点中； 新分裂一个结点后，我们需要检测这次分裂是否会给损失函数带来增益，增益的定义如下： 如果增益Gain&gt;0，即分裂为两个叶子节点后，目标函数下降了，那么我们会考虑此次分裂的结果。 但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？接下来会讲到。 寻找最佳分裂点 在实际训练过程中，当建立第 棵树时，一个非常关键的问题是如何找到叶子节点的最优切分点，XGBoost支持两种分裂节点的方法——贪心算法和近似算法。 贪心算法 从树的深度为0开始： 对每个叶节点枚举所有的可用特征； 针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益； 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集； 回到第1步，递归执行直到满足特定条件为止； 那么如何计算每个特征的分裂收益呢？ 假设我们在某一节点完成特征分裂，则分裂前的目标函数可以写为： 分裂后的目标函数为： 则对于目标函数来说，分裂后的收益为： 注意：该特征收益也可作为特征重要性输出的重要依据。 对于每次分裂，我们都需要枚举所有特征可能的分割方案，如何高效地枚举所有的分割呢？ 假设我们要枚举某个特征所有 这样条件的样本，对于某个特定的分割点 我们要计算 左边和右边的导数和。 我们可以发现对于所有的分裂点 ，只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 、 。然后用上面的公式计算每个分割方案的收益就可以了。 观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入新叶子的惩罚项，也就是说引入的分割带来的增益如果小于一个阀值的时候，我们可以剪掉这个分割。 上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。 但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。 基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案： 特征预排序+缓存：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。 分位点近似法：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。 并行查找：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。 近似算法 贪心算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪心算法这一缺点给出了近似最优解。 对于每个特征，只考察分位点可以减少计算复杂度。 该算法首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。 在提出候选切分点时有两种策略： Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割； Local：每次分裂前将重新提出候选切分点。 直观上来看，Local策略需要更多的计算步骤，而Global策略因为节点已有划分所以需要更多的候选点。 下图给出不同种分裂策略的AUC变化曲线，横坐标为迭代次数，纵坐标为测试集AUC，eps为近似算法的精度，其倒数为桶的数量。 从上图我们可以看到， Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在eps取值合理的情况下，分位数策略可以获得与贪心算法相同的精度。 近似算法简单来说，就是根据特征 的分布来确定 个候选切分点 ，然后根据这些候选切分点把相应的样本放入对应的桶中，对每个桶的 进行累加。最后在候选切分点集合上贪心查找。该算法描述如下： 算法讲解： 第一个for循环：对特征k根据该特征分布的分位数找到切割点的候选集合 。这样做的目的是提取出部分的切分点不用遍历所有的切分点。其中获取某个特征k的候选切割点的方式叫proposal(策略)。XGBoost 支持 Global 策略和 Local 策略。 第二个for循环：将每个特征的取值映射到由该特征对应的候选点集划分的分桶区间，即 。对每个桶区间内的样本统计值 G,H并进行累加，最后在这些累计的统计量上寻找最佳分裂点。这样做的目的是获取每个特征的候选分割点的 G,H值。 下图给出近似算法的具体例子，以三分位为例： 根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 G,H 值，最终求解节点划分的增益。 停止生长一棵树不会一直生长下去，下面是一些常见的限制条件。 (1) 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 (2) 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 (3) 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。 每个叶子结点的样本权值和计算方式如下： 总结推导过程： 算法工程优化对内存的优化：列块并行学习在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而 XGBoost 在训练之前会根据特征对数据进行排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。 作者提出通过按特征进行分块并排序，在块里面保存排序后的特征值及对应样本的引用，以便于获取样本的一阶、二阶导数值。具体流程为： 整体训练数据可以看做一个 的超大规模稀疏矩阵 按照mini-batch的方式横向分割，可以切成很多个“Block” 每一个“Block”内部采用一种Compress Sparse Column的稀疏短阵格式，每一列特征分别做好升序排列，便于搜索切分点，整体的时间复杂度有效降低。 通过Block的设置，可以采用并行计算，从而提升模型训练速度。 具体方式如图： 通过顺序访问排序后的块遍历样本特征的特征值，方便进行切分点的查找。此外分块存储后多个特征之间互不干涉，可以使用多线程同时对不同的特征进行切分点查找，即特征的并行化处理。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 XGBoost 能够实现分布式或者多线程计算的原因。 对CPU Cache的优化：缓存优化针对一个具体的块(block)，其中存储了排序好的特征值，以及指向特征值所属样本的索引指针，算法需要间接地利用索引指针来获得样本的梯度值。列块并行学习的设计可以减少节点分裂时的计算量，在顺序访问特征值时，访问的是一块连续的内存空间，但通过特征值持有的索引（样本索引）访问样本获取一阶、二阶导数时，这个访问操作访问的内存空间并不连续，这样可能造成cpu缓存命中率低，影响算法效率。由于块中数据是按特征值来排序的，当索引指针指向内存中不连续的样本时，无法充分利用CPU缓存来提速。 为了解决缓存命中率低的问题，XGBoost 提出了两种优化思路。 （1）提前取数（Prefetching） 对于精确搜索，利用多线程的方式，给每个线程划分一个连续的缓存空间，当training线程在按特征值的顺序计算梯度的累加时，prefetching线程可以提前将接下来的一批特征值对应的梯度加载到CPU缓存中。为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就实现了非连续空间到连续空间的转换，提高了算法效率。 （2）合理设置分块大小 对于近似分桶搜索，按行分块时需要准确地选择块的大小。块太小会导致每个线程的工作量太少，切换线程的成本过高，不利于并行计算；块太大导致缓存命中率低，需要花费更多时间在读取数据上。经过反复实验，作者找到一个合理的block_size为 。 对IO的优化：核外块计算当数据量非常大时，我们不能把所有的数据都加载到内存中。那么就必须将一部分需要加载进内存的数据先存放在硬盘中，当需要时再加载进内存。这样操作具有很明显的瓶颈，即硬盘的IO操作速度远远低于内存的处理速度，肯定会存在大量等待硬盘IO操作的情况。针对这个问题作者提出了“核外”计算的优化方法。具体操作为，将数据集分成多个块存放在硬盘中，使用一个独立的线程专门从硬盘读取数据，加载到内存中，这样算法在内存中处理数据就可以和从硬盘读取数据同时进行。此外，XGBoost 还用了两种方法来降低硬盘读写的开销： 块压缩（Block Compression）。论文使用的是按列进行压缩，读取的时候用另外的线程解压。对于行索引，只保存第一个索引值，然后用16位的整数保存与该block第一个索引的差值。作者通过测试在block设置为 个样本大小时，压缩比率几乎达到26% 29%。 块分区（Block Sharding ）。块分区是将特征block分区存放在不同的硬盘上，以此来增加硬盘IO的吞吐量。 优缺点优点 精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数； 灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导； 正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是XGBoost优于传统GBDT的一个特性。 Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统GBDT的实现也有学习速率； 列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是XGBoost异于传统GBDT的一个特性； 缺失值处理：对于特征的值有缺失的样本，XGBoost 采用的稀疏感知算法可以自动学习出它的分裂方向； XGBoost工具支持并行：boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 可并行的近似算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，用于高效地生成候选的分割点。 缺点 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集； 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。 XGBoost与GBDT的差异在分析XGBooting优缺点的时候，通过比较该算法与GBDT的差异，即可有较清楚的描述，具体表现在如下方面。 （1）基分类器的差异 GBDT算法只能利用CART树作为基学习器，满足分类应用； XGBoost算法除了回归树之外还支持线性的基学习器，因此其一方面可以解决带L1与L2正则化项的逻辑回归分类问题，也可以解决线性回问题。 （2）节点分类方法的差异 GBDT算法主要是利用Gini impurity针对特征进行节点划分； XGBoost经过公式推导，提出的weighted quantile sketch（加权分位数缩略图）划分方法，依据影响Loss的程度来确定连续特征的切分值。 （3）模型损失函数的差异 传统GBDT在优化时只用到一阶导数信息； xgboost则对代价函数进行了二阶泰勒展开，二阶导数有利于梯度下降的更快更准。 （4）模型防止过拟合的差异 GBDT算法无正则项，可能出现过拟合； Xgboost在代价函数里加入了正则项，用于控制模型的复杂度，降低了过拟合的可能性。 （5）模型实现上的差异 决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。其能够实现在特征粒度的并行。 XGBoost代码实现安装XGBoost依赖包pip install xgboost XGBoost分类和回归XGBoost有两大类接口：XGBoost原生接口 和 scikit-learn接口 ，并且XGBoost能够实现分类和回归两种任务。 （1）基于XGBoost原生接口的分类 from sklearn.datasets import load_iris import xgboost as xgb from xgboost import plot_importance from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split # read in the iris data iris = load_iris() X = iris.data y = iris.target # split train data and test data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234565) # set XGBoost's parameters params = { 'booster': 'gbtree', 'objective': 'multi:softmax', # 回归任务设置为：'objective': 'reg:gamma', 'num_class': 3, # 回归任务没有这个参数 'gamma': 0.1, 'max_depth': 6, 'lambda': 2, 'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 3, 'silent': 1, 'eta': 0.1, 'seed': 1000, 'nthread': 4, } plst = params.items() dtrain = xgb.DMatrix(X_train, y_train) num_rounds = 500 model = xgb.train(plst, dtrain, num_rounds) # 对测试集进行预测 dtest = xgb.DMatrix(X_test) ans = model.predict(dtest) # 计算准确率 cnt1 = 0 cnt2 = 0 for i in range(len(y_test)): if ans[i] == y_test[i]: cnt1 += 1 else: cnt2 += 1 print(\"Accuracy: %.2f %% \" % (100 * cnt1 / (cnt1 + cnt2))) # 显示重要特征 plot_importance(model) plt.show() （2）基于Scikit-learn接口的回归 这里，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer import xgboost as xgb from sklearn.metrics import mean_absolute_error # 1.读文件 data = pd.read_csv('./dataset/train.csv') data.dropna(axis=0, subset=['SalePrice'], inplace=True) # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = SimpleImputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.调用XGBoost模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = xgb.XGBRegressor(objective='reg:squarederror', verbosity=2) # xgb.XGBClassifier() XGBoost分类模型 my_model.fit(train_X, train_y, verbose=False) # 6.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X) # 7.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) XGBoost调参在上一部分中，XGBoot模型的参数都使用了模型的默认参数，但默认参数并不是最好的。要想让XGBoost表现的更好，需要对XGBoost模型进行参数微调。XGBoost需要调的参数不算多，他们可以分成三个部分： 1、General Parameters，即与整个模型属基调相关的参数； 2、Booster Parameters，即与单颗树生成有关的参数； 3、Learning Task Parameters，与模型调优相关的参数； General Parameters1、booster [default=gbtree] 即xgboost中基学习器类型，有两种选择，分别是树模型（gbtree）和线性模型（linear models） 2、silent [default=0] 即控制迭代日志的是否输出，默认输出； 3、nthread [default to maximum number of threads available if not set] 即控制模型训练调用机器的核心数，与sklearn中n_jobs的含义相似； Booster parameters因为booster有两种类型，常用的一般是树模型，这里只列树模型相关的参数： 1、eta [default=0.3] ：学习率 学习率，这个相当于sklearn中的learning_rate，常见的设置范围在0.01-0.2之间 2、min_child_weight [default=1]：叶节点的最小权重值 这个参数与GBM（sklearn）中的“min_samples_leaf”很相似，只不过这里不是样本数，而是权重值，如果样本的权重都是1，这两个参数是等同的；这个值设置较大时，通常树不会太深，可以控制过拟合，但太大时，容易造成欠拟合的现象，具体调参需要cv； 3、max_depth：树的最大深度 树的最大深度，含义很直白，控制树的复杂性；通常取值范围在3-10； 4、max_leaf_nodes：最大叶节点数 一般这个参数与max_depth二选一控制即可； 5、gamma [default=0]：分裂收益阈值 即用来比较每次节点分裂带来的收益，有效控制节点的过度分裂； 这个参数的变化范围受损失函数的选取影响； 6、max_delta_step [default=0] 这个参数暂时不是很理解它的作用范围，一般可以忽略它； 7、subsample [default=1]：采样比例 与sklearn中的参数一样，即每颗树的生成可以不去全部样本，这样可以控制模型的过拟合；通常取值范围0.5-1； 8、colsample_bytree [default=1]：特征采样的比例（每棵树） 即每棵树不使用全部的特征，控制模型的过拟合； 通常取值范围0.5-1； 9、colsample_bylevel [default=1] 特征采样的比例（每次分裂）； 这个与随机森林的思想很相似，即每次分裂都不取全部变量； 当7、8的参数设置较好时，该参数可以不用在意； 10、lambda [default=1] L2范数的惩罚系数，叶子结点的分数？； 11、alpha [default=0] L1范数的惩罚系数，叶子结点数？； 12、scale_pos_weight [default=1] 这个参数也不是很理解，貌似与类别不平衡的问题相关； Learning Task Parameters1、objective [default=reg:linear]：目标函数 通常的选项分别是：binary:logistic，用于二分类，产生每类的概率值；multi:softmax，用于多分类，但不产生概率值，直接产生类别结果；multi:softprob，类似softmax，但产生多分类的概率值； 2、eval_metric [ default according to objective ]：评价指标 当你给模型一个验证集时，会输出对应的评价指标值； 一般有：rmse ，均方误差；mae ，绝对平均误差；logloss ，对数似然值；error ，二分类错误率；merror ，多分类错误率；mlogloss ；auc 3、seed：即随机种子 关于XGBoost若干问题的思考XGBoost与GBDT的联系和区别有哪些？（1）GBDT是机器学习算法，XGBoost是该算法的工程实现。 （2）正则项：在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。 （3）导数信息：GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。 （4）基分类器：传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。 （5）子采样：传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。 （6）缺失值处理：传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。 （7）并行化：传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 为什么XGBoost泰勒二阶展开后效果就比较好呢？（1）从为什么会想到引入泰勒二阶的角度来说（可扩展性）：XGBoost官网上有说，当目标函数是MSE时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如logistic loss的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把MSE推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与MSE统一？是因为MSE是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与MSE统一了，那就只用推导MSE就好了。 （2）从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。 XGBoost对缺失值是怎么处理的？在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。 XGBoost为什么可以并行训练？（1）XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。 （2）XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 20道XGBoost面试题简单介绍一下XGBoost首先需要说一说GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。 XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。 XGBoost与GBDT有什么不同 基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。 导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。 正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。 列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。 并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 XGBoost为什么使用泰勒二阶展开 精准性：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导。 XGBoost为什么可以并行训练 XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。 XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 XGBoost为什么快 分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点 候选分位点：每个特征采用常数个分位点作为候选分割点 CPU cache 命中优化： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。 Block 处理优化：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐 XGBoost防止过拟合的方法XGBoost在设计时，为了防止过拟合做了很多优化，具体如下： 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化 列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可） 子采样：每轮计算可以不使用全部样本，使算法更加保守 shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间 XGBoost如何处理缺失值XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下： 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。 find_split时，缺失值处理的伪代码 XGBoost中叶子结点的权重如何计算出来XGBoost目标函数最终推导形式如下： 利用一元二次函数求最值的知识，当目标函数达到最小值Obj时，每个叶子结点的权重为wj。 具体公式如下： XGBoost中的一棵树的停止生长条件 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。 RF和GBDT的区别相同点： 都是由多棵树组成，最终的结果都是由多棵树一起决定。 不同点： 集成学习：RF属于bagging思想，而GBDT是boosting思想 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成) 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感 泛化能力：RF不易过拟合，而GBDT容易过拟合 XGBoost如何处理不平衡数据对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决： 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10； 第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。 原话是这么说的： For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of xgboost model, and there are two ways to improve it. If you care only about the ranking order (AUC) of your prediction Balance the positive and negative weights, via scale_pos_weight Use AUC for evaluation If you care about predicting the right probability In such a case, you cannot re-balance the dataset In such a case, set parameter max_delta_step to a finite number (say 1) will help convergence 那么，源码到底是怎么利用scale_pos_weight来平衡样本的呢，是调节权重还是过采样呢？请看源码： if (info.labels[i] == 1.0f) w *= param_.scale_pos_weight 可以看出，应该是增大了少数样本的权重。 除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。 比较LR和GBDT，说说什么情景下GBDT不如LR先说说LR和GBDT的区别： LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程 GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合； 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下： 先看一个例子： 假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。 那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1f1 + Wifi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？ 仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。 XGBoost中如何对树进行剪枝 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。 XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。 XGBoost如何选择最佳分裂点？XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。 因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。 如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。 XGBoost的Scalable性如何体现 基分类器的scalability：弱分类器可以支持CART决策树，也可以支持LR和Linear。 目标函数的scalability：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。 学习方法的scalability：Block结构支持并行化，支持 Out-of-core计算。 XGBoost如何评价特征的重要性我们采用三种方法来评判XGBoost模型中特征的重要程度： 官方文档：（1）weight - the number of times a feature is used to split the data across all trees. （2）gain - the average gain of the feature when it is used in trees. （3）cover - the average coverage of the feature when it is used in trees. weight ：该特征在所有树中被用作分割样本的特征的总次数。 gain ：该特征在其出现过的所有树中产生的平均增益。 cover ：该特征在其出现过的所有树中的平均覆盖范围。 注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。 XGBooost参数调优的一般步骤首先需要初始化一些基本变量，例如： max_depth = 5 min_child_weight = 1 gamma = 0 subsample, colsample_bytree = 0.8 scale_pos_weight = 1 (1) 确定learning rate和estimator的数量 learning rate可以先用0.1，用cv来寻找最优的estimators (2) max_depth和 min_child_weight 我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。 max_depth，每棵子树的最大深度，check from range(3,10,2)。 min_child_weight，子节点的权重阈值，check from range(1,6,2)。 如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。 (3) gamma 也称作最小划分损失min_split_loss，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。 如果大于该阈值，则该叶子节点值得继续划分 如果小于该阈值，则该叶子节点不值得继续划分 (4) subsample, colsample_bytree subsample是对训练的采样比例 colsample_bytree是对特征的采样比例 both check from 0.6 to 0.9 (5) 正则化参数 alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100 lambda 是L2正则化系数 (6) 降低学习率 降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1 XGBoost模型如果过拟合了怎么解决当出现过拟合时，有两类参数可以缓解： 第一类参数：用于直接控制模型的复杂度。包括max_depth,min_child_weight,gamma 等参数 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample,colsample_bytree 还有就是直接减小learning rate，但需要同时增加estimator 参数。 为什么XGBoost相比某些模型对缺失值不敏感对存在缺失值的特征，一般的解决方法是： 离散型变量：用出现次数最多的特征值填充； 连续型变量：用中位数或均值填充； 一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。 而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。 XGBoost对缺失数据有特定的处理方法，详情参考上篇文章第7题。 因此，对于有缺失值的数据在经过缺失处理后： 当数据量很小时，优先用朴素贝叶斯 数据量适中或者较大，用树模型，优先XGBoost 数据量较大，也可以用神经网络 避免使用距离度量相关的模型，如KNN和SVM XGBoost和LightGBM的区别 （1）树生长策略：XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 （2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下： 减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。 计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑏𝑖𝑛𝑠)。 LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算 但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？ xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。 （3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。 （4）缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。 （5）LightGBM 与 XGboost 的并行策略不同： 特征并行 ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。 数据并行 ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。 投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。 参考资料XGBoost论文解读： 【1】Chen T , Guestrin C . XGBoost: A Scalable Tree Boosting System[J]. 2016. 【2】Tianqi Chen的XGBoost的Slides 【3】对xgboost的理解 - 金贵涛的文章 - 知乎 【4】CTR预估 论文精读(一)–XGBoost 【5】XGBoost论文阅读及其原理 - Salon sai的文章 - 知乎 【6】XGBoost 论文翻译+个人注释 XGBoost算法讲解： 【7】XGBoost超详细推导，终于有人讲明白了！ 【8】终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！ 【9】机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - wepon的回答 - 知乎 【10】GBDT算法原理与系统设计简介，wepon XGBoost实例： 【11】Kaggle 神器 xgboost 【12】干货 | XGBoost在携程搜索排序中的应用 【13】史上最详细的XGBoost实战 - 章华燕的文章 - 知乎 【14】XGBoost模型构建流程及模型参数微调（房价预测附代码讲解） - 人工智能学术前沿的文章 - 知乎 XGBoost面试题： 【15】珍藏版 | 20道XGBoost面试题，你会几个？(上篇) 【16】珍藏版 | 20道XGBoost面试题，你会几个？(下篇) 【17】推荐收藏 | 10道XGBoost面试题送给你 【18】面试题：xgboost怎么给特征评分？ 【19】[校招-基础算法]GBDT/XGBoost常见问题 - Jack Stark的文章 - 知乎 【20】《百面机器学习》诸葛越主编、葫芦娃著，P295-P297。 【21】灵魂拷问，你看过Xgboost原文吗？ - 小雨姑娘的文章 - 知乎 【22】为什么xgboost泰勒二阶展开后效果就比较好了呢？ - Zsank的回答 - 知乎 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","slug":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","date":"2019-12-24T06:08:00.000Z","updated":"2020-04-11T11:32:24.599Z","comments":true,"path":"2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-05-ti-du-ti-sheng-shu-suan-fa-gbdt/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-05-ti-du-ti-sheng-shu-suan-fa-gbdt/","excerpt":"","text":"1 前言前面讲述了《决策树的特征选择》、《决策树的生成》、《决策树的剪枝》，熟悉了单棵决策树的的实现细节，在实际应用时，往往采用多棵决策树组合的形式完成目标任务。那么如何组合单棵决策树可以使得模型效果更优呢？目前主要有两种思想：bagging和boosting，分别对应的典型算法随机森林和Adaboost、GBDT等。 Bagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。典型实现算法随机森林 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。典型实现算法是GBDT boosting的思想如下图： 基于boosting思想的经典算法是Adaboost和GBDT。关于Adaboost的介绍可以参考《Adaboost算法》，本文重点介绍GBDT。 2 什么是GBDT GBDT(Gradient Boosting Decision Tree) 是一种迭代的决策树算法，是回归树，而不是分类树。该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性。 GBDT用来做回归预测，调整后也可以用于分类。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。具体训练过程如下图示意： 3 GBDT算法原理GBDT算法的核心思想 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。即所有弱分类器相加等于预测值，下一个弱分类器去拟合误差函数对预测值的梯度。 GBDT加入了简单的数值优化思想。 Xgboost更加有效应用了数值优化。相比于gbdt，最重要是对损失函数变得更复杂。目标函数依然是所有树想加等于预测值。损失函数引入了一阶导数，二阶导数。 不同于随机森林所有树的预测求均值，GBDT所有的树的预测值加起来是最终的预测值，可以不断接近真实值。 GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想的通俗解释 假如有个人30岁， 第一棵树，我们首先用20岁去拟合，发现损失有10岁， 第二颗，这时我们用6岁去拟合剩下的损失，发现差距还有4岁， 第三颗，我们用3岁拟合剩下的差距，差距就只有一岁了。 三棵树加起来为29岁，距离30最近。 从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？ 4 负梯度拟合在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为 利用(xi,rti)(i=1,2,..m),我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域Rtj,j=1,2,…,J。其中J为叶子节点的个数。 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj如下： 这样就得到了本轮的决策树拟合函数如下： 从而本轮最终得到的强学习器的表达式如下： 通过损失函数的负梯度来拟合，找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 传统模型中，我们定义一个固定结构的函数，然后通过样本训练拟合更新该函数的参数，获得最后的最优函数。 GBDT提升树并非如此。它是加法模型，是不定结构的函数，通过不断加入新的子函数来使得模型能更加拟合训练数据，直到最优。函数更新的迭代方式可以写作：)。所以如果要更快逼近最优的函数，我们就需要在正确的方向上添加子函数，这个“正确的方向”当然就是损失减少最快的方向。所以我们需要用损失函数)对函数)求导（注意不是对x求导），求得的导数，就是接下来)需要弥补的方向。在上式中就是表示导数的拟合。 导数值跟损失函数的选择有关系。如果选择平方损失误差，那么它的导数就是： 令人惊喜的是这正是真实值和估计值之间的残差！ 这就是为什么谈到GBDT的时候，很多文章都提到“残差”的拟合，却没有说“梯度”的拟合。其实它们在平方损失误差条件下是一个意思！BTW，上面之所以用了是为了计算方便，常数项并不会影响平方损失误差，以及残差的比较。 现在让我们重新理解这个式子： 1）先求取一个拟合函数Fm-1(x) 2）用Fm-1(x)进行预测，计算预测值和实际值的残差 3）为了弥补上面的残差，用一个函数△F(x)来拟合这个残差 4）这样最终的函数就变成了)，其中Fm-1(x)用来拟合原数据，△F(x)用来拟合残差 5）如果目前还有较大的残差，则循环2)~4)，更新函数到Fm+1(x) , Fm+2(x), …..直到残差满足条件。 针对以上流程，我们用实例来说明 5 提升树的生成过程有以下数据需要用回归，并要求平方损失误差小于0.2（这0.2就是我们人为设置的最优条件，否则训练可能会无休止地进行下去）时，可以停止建树： 第一棵树 1） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 比如s=1.5,分割成了两个子集： 通过公式求平方损失误差 而其中)为各自子集的平均值时，可以使得每个子集的平方损失误差最小。 求平均值为：)，进而求得平方损失误差为 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=6.5时,为所有切分点里平方损失误差最小的 2) 选择切分点s=6.5构建第一颗回归树，各分支数值使用 ： 第一轮过后，我们提升树为: 3) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果： 提升树拟合数据的平方损失误差计算： 大于0.2，则还需要继续建树。 第二棵树 4) 确定需要拟合的训练数据为上一棵树的残差： 5） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=3.5时,为所有切分点里平方损失误差最小的 6) 选择切分点s=3.5构建第二颗回归树，各分支数值使用 ： 第二轮过后，我们提升树为: 7) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果，同时对比初始值和上一颗树的残差： 可以看见，随着树的增多，残差一直在减少。 到目前为止，提升树拟合数据的平方损失误差计算： 多说一句，这里是从全局提升树的角度去计算损失，其实和上面第5）步中从最后一颗树的角度去计算损失，结果是一样的 目前损失大于0.2的阈值，还需要继续建树 … … 第六棵树 到第六颗树的时候，我们已经累计获得了： 此时提升树为： 此时用拟合训练数据的平方损失误差为： 平方损失误差小于0.2的阈值，停止建树。 为我们最终所求的提升树。 6 回归算法输入： 最大迭代次数T, 损失函数L，训练样本集 输出： 强学习器f(x) 1） 初始化弱学习器 2）对迭代轮数t=1,2,…T有： a) 对样本i=1,2，…m，计算负梯度 b) 利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,…,J。其中J为回归树t的叶子节点的个数。 c) 对叶子区域j =1,2,..J,计算最佳拟合值 (d) 更新强学习器 3） 得到强学习器f(x)的表达式 7 分类算法GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。 为了解决这个问题，主要有两个方法， 1）一个是用指数损失函数，此时GBDT退化为Adaboost算法。 2）另一种方法是用类似于逻辑回归的对数似然损失函数的方法。 也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。 7.1 二元分类算法对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为： 其中y∈{−1,+1}。则此时的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。 7.2 多元分类算法多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为： 集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。 8 正则化和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。 GBDT的正则化主要有三种方式。 第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代 如果我们加上了正则化项，则有 ν的取值范围为0&lt;ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了 9 总结GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。 优点 1) 可以灵活处理各种类型的数据，包括连续值和离散值。 2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 缺点 1) 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"}]},{"title":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","slug":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","date":"2019-12-24T06:08:00.000Z","updated":"2020-04-11T11:33:49.104Z","comments":true,"path":"2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-09-id3-c4.5-cart-sui-ji-sen-lin-bagging-boosting-adaboost-gbdt-xgboost-suan-fa-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-09-id3-c4.5-cart-sui-ji-sen-lin-bagging-boosting-adaboost-gbdt-xgboost-suan-fa-zong-jie/","excerpt":"","text":"最近心血来潮，整理了一下和树有关的方法和模型，请多担待！ 决策树首先，决策树是一个有监督的分类模型，其本质是选择一个能带来最大信息增益的特征值进行树的分割，直到到达结束条件或者叶子结点纯度到达一定阈值。下图是决策树的一个简单例子 按照分割指标和分割方法，决策树的经典模型可以分为ID3、C4.5以及CART ID3：以信息增益为准则来选择最优划分属性信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 C4.5：基于信息增益率准则选择最优分割属性信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 CART：以基尼系数为准则选择最优划分属性CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 随机森林在讲随机森林之前，我们需要补充一点组合分类器的概念，将多个分类器的结果进行多票表决或者是取平均值，以此作为最终的结果。 1、构建组合分类器的好处： （1）、提升模型精度：整合各个模型的分类结果，得到更合理的决策边界，减少整体错误，实现更好的分类效果； （2）、处理过大或过小的数据集：数据集较大时，可以将数据集划分成多个子集，对子集构建分类器；数据集较小时，可通过多种抽样方式（bootstrap）从原始数据集抽样产生多组不同的数据集，构建分类器。 （3）、若决策边界过于复杂，则线性模型不能很好地描述真实情况。因此先对于特定区域的数据集，训练多个线性分类器，再将它们集成。 （4）、比较适合处理多源异构数据（存储方式不同（关系型、非关系型），类别不同（时序型、离散型、连续型、网络结构数据）） 随机森林是一个典型的多个决策树的组合分类器。主要包括两个方面：数据的随机性选取，以及待选特征的随机选取。 （1）、数据的随机选取：第一，从原始的数据集中采取有放回的抽样（bootstrap），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。 （2）、待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。 组合树示例图 GBDT和xgboostbagging和boostingBagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。 boosting算法结构图 拿著名的Adaboost算法举例： 我们有一个数据集，样本大小为N，每一个样本对应一个原始标签起初，我们初始化样本的权重为1/N em计算的是当前数据下，模型的分类误差率，模型的系数值是基于分类误差率的 根据模型的分类结果，更新原始数据中数据的分布，增加被错分的数据被抽中的概率，以便下一次迭代的时候能被模型重新训练 最终的分类器是各个基分类器的组合 GBDTGBDT是以决策树（CART）为基学习器的GB算法，是迭代树，而不是分类树。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。有了前面Adaboost的铺垫，大家应该能很容易理解大体思想。 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。 xgboostXgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。 损失函数如下，引入了一阶导数，二阶导数。： 好的模型需要具备两个基本要素：一是要有好的精度（即好的拟合程度），二是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项） 常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。 每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差 目标函数如上图，最后一行画圈部分实际上就是预测值和真实值之间的残差 先对训练误差进行展开： xgboost则对代价函数进行了二阶泰勒展开，同时用到了残差平方和的一阶和二阶导数 再研究目标函数中的正则项： 树的复杂度可以用树的分支数目来衡量，树的分支我们可以用叶子结点的数量来表示 那么树的复杂度式子：右边第一项是叶子结点的数量T，第二项是树的叶子结点权重w的l2正则化，正则化是为了防止叶子结点过多 此时，每一次迭代，相当于在原有模型中增加一棵树，目标函数中，我们用wq（x）表示一棵树，包括了树的结构以及叶子结点的权重，w表示权重（反映预测的概率），q表示样本所在的索引号（反映树的结构） 将最终得到的目标函数对参数w求导，带回目标函数，可知目标函数值由红色方框部分决定： 因此，xgboost的迭代是以下图中gain式子定义的指标选择最优分割点的： 那么如何得到优秀的组合树呢？ 一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。 另一种方法：XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照特征值的密度分布，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将bucket边界上的特征值作为splitpoint的候选，遍历所有的候选分裂点来找到最佳分裂点。 上图近似算法公式的解释：将特征k的特征值进行排序，计算特征值分布，rk（z）表示的是对于特征k而言，其特征值小于z的权重之和占总权重的比例，代表了这些特征值的重要程度，我们按照这个比例计算公式，将特征值分成若干个bucket，每个bucket的比例相同，选取这几类特征值的边界作为划分候选点，构成候选集；选择候选集的条件是要使得相邻的两个候选分裂节点差值小于某个阈值。 综合以上的解说，我们可以得到xgboost相比于GBDT的创新之处： 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象； 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销 指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。 并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"极客时间《数据分析45讲总结》","slug":"极客时间《数据分析45讲总结》","date":"2019-12-17T08:05:00.000Z","updated":"2020-04-11T11:34:35.238Z","comments":true,"path":"2019/12/17/ji-ke-shi-jian-shu-ju-fen-xi-45-jiang-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/ji-ke-shi-jian-shu-ju-fen-xi-45-jiang-zong-jie/","excerpt":"","text":"1.前言该讲主要引导读者从全局去了解什么是数据分析？为什么做数据分析？怎么去做数据分析？答案就是：掌握数据，就是掌握规律。当你了解了市场数据，对它进行分析，就可以得到市场规律。当你掌握了产品自身的数据，对它进行分析，就可以了解产品的用户来源、用户画像等等。所以说数据是个全新的视角。数据分析如此重要，它不仅是新时代的“数据结构 + 算法”，也更是企业争夺人才的高地。 谈到数据分析，我们一般都会从3个方面入手： 数据采集 – 数据源，我们要用的原材料 数据挖掘 – 它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值（所谓的商业智能BI） 数据的可视化 – 数据领域中的万金油，直观了解数据分析结构 数据分析的三驾马车的关系如下： 下面来大致认识下这三驾马车： 2.数据采集：数据的采集，主要是和数据打交道，用工具对数据进行采集，常用的数据源，如何获取它们。在专栏里，后续会将介绍如何掌握“八爪鱼”这个自动抓取的神器，它可以帮你抓取 99% 的页面源。也会教读者如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如“王祖贤”的海报，还能自动给微博加粉丝，让你掌握自动化的快感。 3.数据挖掘：数据挖掘，它可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。 掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的。 4.数据可视化 为什么说数据要可视化，因为数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。 数据可视化的两种方法： Python ：在 Python 对数据进行清洗、挖掘的过程中，很多的库可以使用，像 Matplotlib、Seaborn 等第三方库进行呈现。 第三方工具：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。 数据分析包括数据采集、数据挖掘、数据可视化这三个部分。乍看你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点“高深莫测”，掌握起来是不是会吃力。其实这些都是不必要的烦恼。个人觉得只要内心笃定，认为自己一定能做成，学成，其他一切都是“纸老虎”哈。 再说下，陈博在文章中提到的如何来快速掌握数据分析，核心就是认知。我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程就是认知升级的过程。 我本人也是很赞同这种说法，简单一句就是“知行合一” 总结 记录下你每天的认知 这些认知对应工具的哪些操作 做更多练习来巩固你的认知 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/categories/Data-Analysis/"}],"tags":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/tags/Data-Analysis/"}]},{"title":"机器学习系列之决策树算法（01）：决策树特征选择","slug":"机器学习系列之决策树算法（01）：决策树特征选择","date":"2019-12-17T07:07:00.000Z","updated":"2020-04-11T11:31:34.243Z","comments":true,"path":"2019/12/17/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-01-jue-ce-shu-te-zheng-xuan-ze/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-01-jue-ce-shu-te-zheng-xuan-ze/","excerpt":"","text":"1.什么是特征选择【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。 【那么选择的是什么样的特征呢？】这里我们选择的特征需要是对训练数据有分类能力的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征没有分类能力，舍去这个特征对学习的精度不会有特别大的影响。 特征选择是决定用哪个特征来划分特征空间。 比如女生找男朋友，可能这个女生首先会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了 【那怎么判断哪个特征有更好的分类能力呢？】这时候【信息增益】就要出场了。 2.信息增益为了解释什么是信息增益，我们首先要讲解一下什么是【熵（entropy）】 熵（Entropy） 在热力学与化学中： 熵是一种测量在动力学方面【不能做功的能量的总数】，当总体熵增加，其做功能力也下降，熵的度量是能量退化的指标。 1948 年，香农把热力学中的熵引入到信息论中，称为香农熵。根据维基百科的描述： 在信息论中，熵是接收的每条消息中包含的信息的平均量。 更一般的，【熵表示随机变量的不确定性】。假设一个有限取值的离散随机变量 X 的概率分布如下： 那么它的熵定义为： 上式中的 b 通常取 2 或者自然对数 e，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。 从上式中，我们可以看到，熵与 X 的取值是没有关系的，它只与 X 的分布有关，所以 H 也可以写作 p 的函数： 我们现在来看两个随机变量的情况。 假设随机变量 (X, Y) 的联合概率分布如下： 我们使用条件熵（conditional entropy）H(Y|X)来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。 条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。 是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法： 有了上面的公式以后，条件熵的定义就非常容易理解了。 那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？ 信息增益的定义与信息增益算法既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？ 没错，由于熵表示不确定性，严格来说，信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」。 我们给出信息增益的最终定义： 特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。 这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。 假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。 很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要 信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大 条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定 信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类 下面我们给出信息增益的算法。 首先对数据做一些介绍： 假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 ， 为属于这个类的样本的个数，很显然 。 再假设我们有一个特征叫 A，A 有 n 个不同的取值 ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 ， 是这个子集的样本个数，很显然 。 我们把 中属于类别 的集合称作 ， 是其样本个数。 信息增益的计算就分为如下几个步骤： 计算 D 的经验熵 H(D)： \\2. 计算 A 对 D 的经验条件熵 H(D|A)： \\3. 计算信息增益 g(D, A)： 3.信息增益比看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？ 按照经验来看，【以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差】。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。类别越多代表特征越不确定，即熵越多，类别的信息增益越小。 为了校正这一误差，我们引入了【信息增益比（information gain ratio）】，又叫做信息增益率，它的定义如下： 特征 A 对训练数据集 D 的信息增益比 定义为其信息增益 与训练数据集 D 关于特征 A 的值的熵 之比。 其中， ，n 是 A 取值的个数。 两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。 4. ID3 ： 最大信息增益 ID3以信息增益为准则来选择最优划分属性 信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 5. C4.5 ：最大信息增益率 C4.5基于信息增益率准则选择最优分割属性的算法 信息增益比率通过引入一个被称作【分裂信息(Split information)】的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 6.CART ：最小基尼指数 CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归 CART是一棵二叉树，采用【二元切分法】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 7.分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]}]}