{"meta":{"title":"DataQuaner","subtitle":"DataQuaner","description":"Data|Algorithm|Business","author":"Leon","url":"https://dataquaner.github.io","root":"/"},"pages":[{"title":"关于","date":"2019-11-14T14:27:21.000Z","updated":"2020-04-11T13:58:33.412Z","comments":true,"path":"about/index.html","permalink":"https://dataquaner.github.io/about/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"contact","date":"2020-04-11T02:31:05.000Z","updated":"2020-04-11T02:31:41.680Z","comments":true,"path":"contact/index.html","permalink":"https://dataquaner.github.io/contact/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"分类","date":"2019-11-10T09:22:35.000Z","updated":"2020-04-11T11:01:07.476Z","comments":true,"path":"categories/index.html","permalink":"https://dataquaner.github.io/categories/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"friends","date":"2020-04-11T02:33:44.000Z","updated":"2020-04-11T02:34:02.597Z","comments":true,"path":"friends/index.html","permalink":"https://dataquaner.github.io/friends/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"友情链接","date":"2019-11-10T09:17:30.000Z","updated":"2019-11-10T09:20:41.842Z","comments":true,"path":"link/index.html","permalink":"https://dataquaner.github.io/link/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"标签","date":"2019-11-10T09:21:38.000Z","updated":"2020-04-11T11:00:23.510Z","comments":true,"path":"tags/index.html","permalink":"https://dataquaner.github.io/tags/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"medias","date":"2020-04-11T10:11:04.000Z","updated":"2020-04-11T10:11:04.927Z","comments":true,"path":"medias/index.html","permalink":"https://dataquaner.github.io/medias/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"}],"posts":[{"title":"Spark面试问题梳理：选择题","slug":"Spark面试问题：选择题","date":"2020-06-21T06:35:00.000Z","updated":"2020-06-21T12:27:36.753Z","comments":true,"path":"2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/","link":"","permalink":"https://dataquaner.github.io/2020/06/21/spark-mian-shi-wen-ti-xuan-ze-ti/","excerpt":"","text":"1. Spark 的四大组件下面哪个不是 (D )A.Spark Streaming B. Mlib C Graphx D.Spark R 2. 下面哪个端口不是 spark 自带服务的端口 (C )A.8080 B.4040 C.8090 D.18080 备注：8080：spark集群web ui端口，4040：sparkjob监控端口，18080：jobhistory端口 3. spark 1.4 版本的最大变化 (B )A spark sql Release 版本 B .引入 Spark R C DataFrame D.支持动态资源分配 4. Spark Job 默认的调度模式 (A )A FIFO B FAIR C 无 D 运行时指定 备注：Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。 5.哪个不是本地模式运行的条件 ( D)A spark.localExecution.enabled=true B 显式指定本地运行 C finalStage 无父 Stage D partition默认值 备注：【问题】Spark在windows能跑集群模式吗？ 我认为是可以的，但是需要详细了解cmd命令行的写法。目前win下跑spark的单机模式是没有问题的。 【关键点】spark启动机制容易被windows的命令行cmd坑 1、带空格、奇怪字符的安装路径，cmd不能识别。最典型的坑就是安装在Program Files文件夹下的程序，因为Program和Files之间有个空格，所以cmd竟不能识别。之前就把JDK安装在了Program Files下面，然后启动spark的时候，总是提示我找不到JDK。我明明配置了环境变量了啊？这就是所谓了《已经配置环境变量，spark 仍然找不到Java》的错误问题。至于奇怪的字符，如感叹号!，我经常喜欢用来将重要的文件夹排在最前面，但cmd命令提示符不能识别。 2、是否需要配置hadoop的路径的问题——答案是需要用HDFS或者yarn就配，不需要用则不需配置。目前大多数的应用场景里面，Spark大规模集群基本安装在Linux服务器上，而自己用windows跑spark的情景，则大多基于学习或者实验性质，如果我们所要读取的数据文件从本地windows系统的硬盘读取（比如说d:\\data\\ml.txt），基本上不需要配置hadoop路径。我们都知道，在编spark程序的时候，可以指定spark的启动模式，而启动模式有这么三中（以python代码举例）： （2.1）本地情况，conf = SparkConf().setMaster(“local[*]”) ——&gt;也就是拿本机的spark来跑程序 （2.2）远程情况，conf = SparkConf().setMaster(“spark://remotehost:7077”) ——&gt;远程spark主机 （2.3）yarn情况，conf = SparkConf().setMaster(“yarn-client”) ——&gt;远程或本地 yarn集群代理spark 针对这3种情况，配置hadoop安装路径都有什么作用呢？（2.1）本地的情况，直接拿本机安装的spark来运行spark程序（比如d:\\spark-1.6.2），则配不配制hadoop路径取决于是否需要使用hdfs。java程序的情况就更为简单，只需要导入相应的hadoop的jar包即可，是否配置hadoop路径并不重要。（2.2）的情况大体跟（2.1）的情况相同，虽然使用的远程spark，但如果使用本地数据，则运算的元数据也是从本地上传到远程spark集群的，无需配置hdfs。而（2.3）的情况就大不相同，经过我搜遍baidu、google、bing引擎，均没找到SparkConf直接配置远程yarn地址的方法，唯一的一个帖子介绍可以使用yarn://remote:8032的形式，则会报错“无法解析 地址”。查看Spark的官方说明，Spark其实是通过hadoop路径下的etc\\hadoop文件夹中的配置文件来寻找yarn集群的。因此，需要使用yarn来运行spark的情况，在spark那配置好hadoop的目录就尤为重要。后期经过虚拟机的验证，表明，只要windows本地配置的host地址等信息与linux服务器端相同（注意应更改hadoop-2/etc/hadoop 下各种文件夹的配置路径，使其与windows本地一致），是可以直接在win下用yarn-client提交spark任务到远程集群的。 3、是否需要配置环境变量的问题，若初次配置，可以考虑在IDE里面配置，或者在程序本身用setProperty函数进行配置。因为配置windows下的hadoop、spark环境是个非常头疼的问题，有可能路径不对而导致无法找到相应要调用的程序。待实验多次成功率提高以后，再直接配置windows的全局环境变量不迟。 4、使用Netbeans这个IDE的时候，有遇到Netbeans不能清理构建的问题。原因，极有可能是导入了重复的库，spark里面含有hadoop包，记得检查冲突。同时，在清理构建之前，记得重新编译一遍程序，再进行清理并构建。 ５、经常遇到WARN YarnClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources资源不足无法运行的问题，添加conf.set(“spark.executor.memory”, “512m”);语句进行资源限制。先前在虚拟机跑spark，由于本身机子性能不高，给虚拟机设置的内存仅仅2G，导致hadoop和spark双开之后系统资源严重不足。因此可以缩小每个executor的运算规模。其他资源缺乏问题的解决方法参考http://blog.sina.com.cn/s/blog_4b1452dd0102wyzo.html 6.下面哪个不是 RDD 的特点 (C )A. 可分区 B 可序列化 C 可修改 D 可持久化 7. 关于广播变量，下面哪个是错误的 (D )A 任何函数调用 B 是只读的 C 存储在各个节点 D 存储在磁盘或 HDFS 8. 关于累加器，下面哪个是错误的 (D )A 支持加法 B 支持数值类型 C 可并行 D 不支持自定义类型 9.Spark 支持的分布式部署方式中哪个是错误的 (D )A standalone B spark on mesos C spark on YARN D Spark on local 10.Stage 的 Task 的数量由什么决定 (A )A Partition B Job C Stage D TaskScheduler 11.下面哪个操作是窄依赖 (B )A join B filter C group D sort 12.下面哪个操作肯定是宽依赖 (C )A map B flatMap C reduceByKey D sample 13.spark 的 master 和 worker 通过什么方式进行通信的？ (D )A http B nio C netty D Akka 备注：从spark1.3.1之后，netty完全代替 了akka 一直以来，基于Akka实现的RPC通信框架是Spark引以为豪的主要特性，也是与Hadoop等分布式计算框架对比过程中一大亮点，但是时代和技术都在演化，从Spark1.3.1版本开始，为了解决大数据块（如shuffle）的传输问题，Spark引入了Netty通信框架，到了1.6.0版本，Netty居然完全取代了Akka，承担Spark内部所有的RPC通信以及数据流传输。 那么Akka又是什么东西？从Akka出现背景来说，它是基于Actor的RPC通信系统，它的核心概念也是Message，它是基于协程的，性能不容置疑；基于scala的偏函数，易用性也没有话说，但是它毕竟只是RPC通信，无法适用大的package/stream的数据传输，这也是Spark早期引入Netty的原因。 那么Netty为什么可以取代Akka？首先不容置疑的是Akka可以做到的，Netty也可以做到，但是Netty可以做到，Akka却无法做到，原因是啥？在软件栈中，Akka相比Netty要Higher一点，它专门针对RPC做了很多事情，而Netty相比更加基础一点，可以为不同的应用层通信协议（RPC，FTP，HTTP等）提供支持，在早期的Akka版本，底层的NIO通信就是用的Netty；其次一个优雅的工程师是不会允许一个系统中容纳两套通信框架，恶心！最后，虽然Netty没有Akka协程级的性能优势，但是Netty内部高效的Reactor线程模型，无锁化的串行设计，高效的序列化，零拷贝，内存池等特性也保证了Netty不会存在性能问题。 那么Spark是怎么用Netty来取代Akka呢？一句话，利用偏函数的特性，基于Netty“仿造”出一个简约版本的Actor模型！！ 14. 默认的存储级别 (A )A MEMORY_ONLY B MEMORY_ONLY_SER C MEMORY_AND_DISK D MEMORY_AND_DISK_SER 备注： //不会保存任务数据 val NONE = new StorageLevel(false, false, false, false) //直接将RDD的partition保存在该节点的Disk上 val DISK_ONLY = new StorageLevel(true, false, false, false) //直接将RDD的partition保存在该节点的Disk上,在其他节点上保存一个相同的备份 val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) //将RDD的partition对应的原生的Java Object保存在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //那么这些partition将不会缓存,并且需要的时候被重新计算,默认缓存的级别 val MEMORY_ONLY = new StorageLevel(false, true, false, true) //将RDD的partition对应的原生的Java Object保存在JVM中,在其他节点上保存一个相同的备份 val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) //将RDD的partition反序列化后的对象存储在JVM中,如果RDD太大导致它的部分partition不能存储在内存中 //超出的partition将被保存在Disk上,并且在需要时读取 val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) //在其他节点上保存一个相同的备份 val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) //将RDD的partition序列化后存储在Tachyon中 val OFF_HEAP = new StorageLevel(false, false, true, false) 15 spark.deploy.recoveryMode 不支持那种 (D )A.ZooKeeper B. FileSystem D NONE D Hadoop 16.下列哪个不是 RDD 的缓存方法 (C )A persist() B Cache() C Memory() 17.Task 运行在下来哪里个选项中 Executor 上的工作单元 (C )A Driver program B. spark master C.worker node D Cluster manager 18.hive 的元数据存储在 derby 和 MySQL 中有什么区别 (B )A.没区别 B.多会话 C.支持网络环境 D数据库的区别 备注： Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。 内置的derby主要问题是并发性能很差，可以理解为单线程操作。 Derby还有一个特性。更换目录执行操作，会找不到相关表等 19.DataFrame 和 RDD 最大的区别 (B )A.科学统计支持 B.多了 schema C.存储方式不一样 D.外部数据源支持 备注： 上图直观体现了RDD与DataFrame的区别：左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。 提升执行效率： RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。 减少数据读取：分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。 上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。 对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过（例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200）。 此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。 RDD和Dataset ​ DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 ​ DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为Spark SQl类型，然而RDD依赖于运行时反射机制。 DataFrame和Dataset ​ Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。因此具有如下三个特点： ​ DataSet可以在编译时检查类型 并且是面向对象的编程接口。 20.Master 的 ElectedLeader 事件后做了哪些操作 (D )A. 通知 driver B.通知 worker C.注册 application D.直接 ALIVE 34.cache后面能不能接其他算子,它是不是action操作？答：cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。 cache不是action操作 35.reduceByKey是不是action？答：不是，很多人都会以为是action，reduce rdd是action 36.数据本地性是在哪个环节确定的？具体的task运行在那他机器上，dag划分stage的时候确定的 37.RDD的弹性表现在哪几点？1）自动的进行内存和磁盘的存储切换； 2）基于Lingage的高效容错； 3）task如果失败会自动进行特定次数的重试； 4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片； 5）checkpoint和persist，数据计算之后持久化缓存 6）数据调度弹性，DAG TASK调度和资源无关 7）数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par 38.常规的容错方式有哪几种类型？1）.数据检查点,会发生拷贝，浪费资源 2）.记录数据的更新，每次更新都会记录下来，比较复杂且比较消耗性能 39.RDD通过Linage（记录数据更新）的方式为何很高效？1）lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且rDD 之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就 产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条 将复杂计算链条存储下来，计算的时候从后往前回溯 900步是上一个stage的结束，要么就checkpoint 2）记录原数据，是每次修改都记录，代价很大 如果修改一个集合，代价就很小，官方说rdd是 粗粒度的操作，是为了效率，为了简化，每次都是 操作数据集合，写或者修改操作，都是基于集合的 rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的 也可以是细粒度，读可以读其中的一条条的记录。 3）简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景 如网络爬虫，现实世界中，大多数写是粗粒度的场景 40.RDD有哪些缺陷？1）不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的 所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是 说可以一条条的读 2）不支持增量迭代计算，Flink支持 41.说一说Spark程序编写的一般步骤？答：初始化，资源，数据源，并行化，rdd转化，action算子打印输出结果或者也可以存至相应的数据存储介质，具体的可看下图： file:///E:/%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6/qq19B99AF2399E52F466CC3CF7E3B24ED5/069fa7b471f54e038440faf63233acce/640.webp 42. Spark有哪两种算子？答：Transformation（转化）算子和Action（执行）算子。 43. Spark提交你的jar包时所用的命令是什么？答：spark-submit。 44. Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？答：在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 45. 你所理解的Spark的shuffle过程？答：从下面三点去展开 1）shuffle过程的划分 2）shuffle的中间结果如何存储 3）shuffle的数据如何拉取过来 可以参考这篇博文：http://www.cnblogs.com/jxhd1/p/6528540.html Shuffle后续优化方向：通过上面的介绍，我们了解到，Shuffle过程的主要存储介质是磁盘，尽量的减少IO是Shuffle的主要优化方向。我们脑海中都有那个经典的存储金字塔体系，Shuffle过程为什么把结果都放在磁盘上，那是因为现在内存再大也大不过磁盘，内存就那么大，还这么多张嘴吃，当然是分配给最需要的了。如果具有“土豪”内存节点，减少Shuffle IO的最有效方式无疑是尽量把数据放在内存中。下面列举一些现在看可以优化的方面，期待经过我们不断的努力，TDW计算引擎运行地更好。 MapReduce Shuffle后续优化方向：压缩：对数据进行压缩，减少写读数据量； 减少不必要的排序：并不是所有类型的Reduce需要的数据都是需要排序的，排序这个nb的过程如果不需要最好还是不要的好；内存化：Shuffle的数据不放在磁盘而是尽量放在内存中，除非逼不得已往磁盘上放；当然了如果有性能和内存相当的第三方存储系统，那放在第三方存储系统上也是很好的；这个是个大招；网络框架：netty的性能据说要占优了；本节点上的数据不走网络框架：对于本节点上的Map输出，Reduce直接去读吧，不需要绕道网络框架。Spark Shuffle后续优化方向：Spark作为MapReduce的进阶架构，对于Shuffle过程已经是优化了的，特别是对于那些具有争议的步骤已经做了优化，但是Spark的Shuffle对于我们来说在一些方面还是需要优化的。 压缩：对数据进行压缩，减少写读数据量；内存化：Spark历史版本中是有这样设计的：Map写数据先把数据全部写到内存中，写完之后再把数据刷到磁盘上；考虑内存是紧缺资源，后来修改成把数据直接写到磁盘了；对于具有较大内存的集群来讲，还是尽量地往内存上写吧，内存放不下了再放磁盘。 46. 你如何从Kafka中获取数据？1) 基于Receiver的方式 这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。 2) 基于Direct的方式 这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据 47. 对于Spark中的数据倾斜问题你有什么好的方案？1）前提是定位数据倾斜，是OOM了，还是任务执行缓慢，看日志，看WebUI 2)解决方法，有多个方面 · 避免不必要的shuffle，如使用广播小表的方式，将reduce-side-join提升为map-side-join ·分拆发生数据倾斜的记录，分成几个部分进行，然后合并join后的结果 ·改变并行度，可能并行度太少了，导致个别task数据压力大 ·两阶段聚合，先局部聚合，再全局聚合 ·自定义paritioner，分散key的分布，使其更加均匀 详细解决方案参考博文《Spark数据倾斜优化方法》 48.RDD创建有哪几种方式？1).使用程序中的集合创建rdd 2).使用本地文件系统创建rdd 3).使用hdfs创建rdd， 4).基于数据库db创建rdd 5).基于Nosql创建rdd，如hbase 6).基于s3创建rdd， 7).基于数据流，如socket创建rdd 如果只回答了前面三种，是不够的，只能说明你的水平还是入门级的，实践过程中有很多种创建方式。 49.Spark并行度怎么设置比较合适答：spark并行度，每个core承载24个partition,如，32个core，那么64128之间的并行度，也就是 设置64~128个partion，并行读和数据规模无关，只和内存使用量和cpu使用 时间有关 50.Spark中数据的位置是被谁管理的？答：每个数据分片都对应具体物理位置，数据的位置是被blockManager，无论 数据是在磁盘，内存还是tacyan，都是由blockManager管理 答：Spark中的数据本地性有三种： a.PROCESS_LOCAL是指读取缓存在本地节点的数据 b.NODE_LOCAL是指读取本地节点硬盘数据 c.ANY是指读取非本地节点数据 通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。 52.rdd有几种操作类型？1）transformation，rdd由一种转为另一种rdd 2）action， 3）cronroller，crontroller是控制算子,cache,persist，对性能和效率的有很好的支持 三种类型，不要回答只有2中操作 53.Spark如何处理不能被序列化的对象？将不能序列化的内容封装成object 54.collect功能是什么，其底层是怎么实现的？答：driver通过collect把集群中各个节点的内容收集过来汇总成结果，collect返回结果是Array类型的，collect把各个节点上的数据抓过来，抓过来数据是Array型，collect对Array抓过来的结果进行合并，合并后Array中只有一个元素，是tuple类型（KV类型的）的。 55.Spaek程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？答： 1）因为输入数据有很多task，尤其是有很多小文件的时候，有多少个输入block就会有多少个task启动； 2）spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。 3）参数可以通过spark_home/conf/spark-default.conf配置文件设置: spark.sql.shuffle.partitions 50 spark.default.parallelism 10 第一个是针对spark sql的task数量 第二个是非spark sql程序设置生效 56.为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?答：会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑 否则很容易出现长时间分配不到资源，job一直不能运行的情况。 57.map与flatMap的区别 map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象 flatMap：对RDD每个元素转换，然后再扁平化 将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组 对象，会抛弃值为null的值 58.列举你常用的action？collect，reduce,take,count,saveAsTextFile等 59.Spark为什么要持久化，一般什么场景下要进行persist操作？ 为什么要进行持久化？ spark所有复杂一点的算法都会有persist身影,spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤 只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。 以下场景会使用persist 1）某个步骤计算非常耗时，需要进行persist持久化 2）计算链条非常长，重新恢复要算很多步骤，很好使，persist 3）checkpoint所在的rdd要持久化persist， lazy级别，框架发现有checnkpoint，checkpoint时单独触发一个job，需要重算一遍，checkpoint前 要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。 4）shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大 5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。 60.为什么要进行序列化 序列化可以减少数据的体积，减少存储空间，高效存储和传输数据，不好的是使用的时候要反序列化，非常消耗CPU 61.介绍一下join操作优化经验？ 答：join其实常见的就分为两类： map-side join 和 reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。 备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。 62.介绍一下cogroup rdd实现原理，你在什么场景下用过这个rdd？ 答：cogroup的函数实现:这个实现根据两个要进行合并的两个RDD操作,生成一个CoGroupedRDD的实例,这个RDD的返回结果是把相同的key中两个RDD分别进行合并操作,最后返回的RDD的value是一个Pair的实例,这个实例包含两个Iterable的值,第一个值表示的是RDD1中相同KEY的值,第二个值表示的是RDD2中相同key的值.由于做cogroup的操作,需要通过partitioner进行重新分区的操作,因此,执行这个流程时,需要执行一次shuffle的操作(如果要进行合并的两个RDD的都已经是shuffle后的rdd,同时他们对应的partitioner相同时,就不需要执行shuffle,)， 场景：表关联查询 63下面这段代码输出结果是什么？ def joinRdd(sc:SparkContext) { val name= Array( Tuple2(1,\"spark\"), Tuple2(2,\"tachyon\"), Tuple2(3,\"hadoop\") ) val score= Array( Tuple2(1,100), Tuple2(2,90), Tuple2(3,80) ) val namerdd=sc.parallelize(name); val scorerdd=sc.parallelize(score); val result = namerdd.join(scorerdd); result .collect.foreach(println); } -------------------------- 答案: (1,(Spark,100)) (2,(tachyon,90)) (3,(hadoop,80)) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/tags/Spark/"}]},{"title":"Spark面试问题梳理","slug":"史上最全的spark面试题","date":"2020-06-21T06:35:00.000Z","updated":"2020-06-21T12:25:46.358Z","comments":true,"path":"2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/","link":"","permalink":"https://dataquaner.github.io/2020/06/21/shi-shang-zui-quan-de-spark-mian-shi-ti/","excerpt":"","text":"问题一：Spark中的RDD是什么，有哪些特性？1.RDD是什么？ RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变，可分区，里面的元素可以并行计算的集合 Dataset：就是一个集合，用于存放数据的 Destributed：分布式，可以并行在集群计算 Resilient：表示弹性的，弹性表示 RDD中的数据可以存储在内存或者磁盘中； RDD中的分区是可以改变的； 2. 五大特性： A list of partitions：一个分区列表，RDD中的数据都存储在一个分区列表中 A function for computing each split：作用在每一个分区中的函数 A list of dependencies on other RDDs：一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的 Optionally,a Partitioner for key-value RDDs(eg:to say that the RDD is hash-partitioned)：可选的，针对于kv类型的RDD才有这个特性，作用是决定了数据的来源以及数据处理后的去向 可选项，数据本地性，数据位置最优 问题二：.概述一下spark中的常用算子区别（map,mapPartitions，foreach，foreachPatition）常用算子： map：用于遍历RDD，将函数应用于每一个元素，返回新的RDD（transformation算子） foreach：用于遍历RDD，将函数应用于每一个元素，无返回值（action算子） mapPatitions：用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子） foreachPatition：用于遍历操作RDD中的每一个分区，无返回值（action算子） 总结：一般使用mapPatitions和foreachPatition算子比map和foreach更加高效，推荐使用 问题三：.谈谈spark中的宽窄依赖：答：RDD和它的父RDD的关系有两种类型：窄依赖和宽依赖 宽依赖：指的是多个子RDD的Partition会依赖同一个父RDD的Partition，关系是一对多，父RDD的一个分区的数据去到子RDD的不同分区里面，会有shuffle的产生 窄依赖：指的是每一个父RDD的Partition最多被子RDD的一个partition使用，是一对一的，也就是父RDD的一个分区去到了子RDD的一个分区中，这个过程没有shuffle产生 区分的标准就是看父RDD的一个分区的数据的流向，要是流向一个partition的话就是窄依赖，否则就是宽依赖，如图所示： 问题四：spark中如何划分stage： 概念： ​ Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是宽窄依赖，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以taskSet的形式提交给TaskScheduler运行，stage是由一组并行的task组成。 ​ Spark程序中可以因为不同的action触发众多的job，一个程序中可以有很多的job，每一个job是由一个或者多个stage构成的，后面的stage依赖于前面的stage，也就是说只有前面依赖的stage计算完毕后，后面的stage才会运行； ​ stage 的划分标准就是宽依赖：何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生； ​ 切割规则：从后往前，遇到宽依赖就切割stage； 图解： 计算格式：pipeline管道计算模式，piepeline只是一种计算思想，一种模式。 ​ spark的pipeline管道计算模式相当于执行了一个高阶函数，也就是说来一条数据然后计算一条数据，会把所有的逻辑走完，然后落地，而MapReduce是1+1=2，2+1=3这样的计算模式，也就是计算完落地，然后再计算，然后再落地到磁盘或者内存，最后数据是落在计算节点上，按reduce的hash分区落地。管道计算模式完全基于内存计算，所以比MapReduce快的原因。 管道中的RDD何时落地：shuffle write的时候，对RDD进行持久化的时候。 ​ stage的task的并行度是由stage的最后一个RDD的分区数来决定的，一般来说，一个partition对应一个task，但最后reduce的时候可以手动改变reduce的个数，也就是改变最后一个RDD的分区数，也就改变了并行度。例如：reduceByKey(+,3) 优化：提高stage的并行度：reduceByKey(+,patition的个数) ，join(+,patition的个数) 问题五：DAGScheduler分析：答： 概述：DAGScheduler是一个面向stage 的调度器； 主要入参： dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,resultHandler, localProperties.get) rdd： final RDD； cleanedFunc： 计算每个分区的函数； resultHander： 结果侦听器； 主要功能： 接受用户提交的job； 将job根据类型划分为不同的stage，记录那些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset； 决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给TaskScheduler； 重新提交shuffle输出丢失的stage给taskScheduler； 注：一个stage内部的错误不是由shuffle输出丢失造成的，DAGScheduler是不管的，由TaskScheduler负责尝试重新提交task执行。 问题六：Job的生成：​ 一旦driver程序中出现action，就会生成一个job，比如count等，向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。 ​ 每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是宽依赖，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。 问题七：有向无环图：​ DAG，有向无环图，简单的来说，就是一个由顶点和有方向性的边构成的图中，从任意一个顶点出发，没有任意一条路径会将其带回到出发点的顶点位置，为每个spark job计算具有依赖关系的多个stage任务阶段，通常根据shuffle来划分stage，如reduceByKey,groupByKey等涉及到shuffle的transformation就会产生新的stage ，然后将每个stage划分为具体的一组任务，以TaskSets的形式提交给底层的任务调度模块来执行，其中不同stage之前的RDD为宽依赖关系，TaskScheduler任务调度模块负责具体启动任务，监控和汇报任务运行情况。 问题八：RDD是什么以及它的分类： 问题九：RDD的操作 问题十: RDD缓存：​ Spark可以使用 persist 和 cache 方法将任意 RDD 缓存到内存、磁盘文件系统中。缓存是容错的，如果一个 RDD 分片丢失，可以通过构建它的 transformation自动重构。被缓存的 RDD 被使用的时，存取速度会被大大加速。一般的executor内存60%做 cache， 剩下的40%做task。 ​ Spark中，RDD类可以使用cache() 和 persist() 方法来缓存。cache()是persist()的特例，将该RDD缓存到内存中。而persist可以指定一个StorageLevel。StorageLevel的列表可以在StorageLevel 伴生单例对象中找到。 ​ Spark的不同StorageLevel ，目的满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择： 如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。 如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。 如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。 如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。 在不会使用cached RDD的时候，及时使用unpersist方法来释放它。 问题十一：RDD共享变量：​ 在应用开发中，一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。 (1). 广播变量（Broadcast Variables） – 广播变量缓存到各个节点的内存中，而不是每个 Task – 广播变量被创建后，能在集群中运行的任何函数调用 – 广播变量是只读的，不能在被广播后修改 – 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本 val broadcastVar = sc.broadcast(Array(1, 2, 3))方法参数中是要广播的变量(2). 累加器 ​ 累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和。Spark 原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型。只有驱动程序才能获取累加器的值 问题十二：spark-submit的时候如何引入外部jar包：在通过spark-submit提交任务时，可以通过添加配置参数来指定 –driver-class-path 外部jar包–jars 外部jar包 问题十三：spark如何防止内存溢出： driver端的内存溢出 可以增大driver的内存参数：spark.driver.memory (default 1g)这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。 map过程产生大量对象导致内存溢出这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区， 不会有shuffle的过程。 数据不平衡导致内存溢出 数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。 shuffle后内存溢出 shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。 standalone模式下资源分配不均匀导致内存溢出 在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。​ 这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。 问题十四：spark中cache和persist的区别：cache：缓存数据，默认是缓存在内存中，其本质还是调用persistpersist: 缓存数据，有丰富的数据缓存策略。数据可以保存在内存也可以保存在磁盘中，使用的时候指定对应的缓存级别就可以了。 问题十五：spark中的数据倾斜的现象，原因，后果：(1)、数据倾斜的现象 多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。(2)、数据倾斜的原因 数据问题1、key本身分布不均衡（包括大量的key为空）2、key的设置不合理 spark使用问题1、shuffle时的并发度不够2、计算方式有误 (3)、数据倾斜的后果 spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的） 过多的数据在同一个task中运行，将会把executor撑爆。 问题十六：spark数据倾斜的处理：发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。 数据问题造成的数据倾斜找出异常的key如果任务长时间卡在最后最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个。比如: df.select(“key”).sample(false,0.1).(k=>(k,1)).reduceBykey(+).map(k=>(k._2,k._1)).sortByKey(false).take(10) 如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。 经过分析，倾斜的数据主要有以下三种情况: 1、null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。2、无效数据，大量重复的测试数据或是对结果影响不大的有效数据。3、有效数据，业务导致的正常数据分布。 解决办法 第1，2种情况，直接对数据进行过滤即可（因为该数据对当前业务不会产生影响）。第3种情况则需要进行一些特殊操作，常见的有以下几种做法(1) 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。(2) 对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。(3) 使用reduceByKey 代替 groupByKey(reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义.)(4) 使用map join。 案例 如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:(1) 将原始的 key 转化为 key + 随机值(例如Random.nextInt)(2) 对数据进行 reduceByKey(func)(3) 将 key + 随机值 转成 key(4) 再对数据进行 reduceByKey(func) 案例操作流程分析： 假设说有倾斜的Key，我们给所有的Key加上一个随机数，然后进行reduceByKey操作；此时同一个Key会有不同的随机数前缀，在进行reduceByKey操作的时候原来的一个非常大的倾斜的Key就分而治之变成若干个更小的Key，不过此时结果和原来不一样，怎么破？进行map操作，目的是把随机数前缀去掉，然后再次进行reduceByKey操作。（当然，如果你很无聊，可以再次做随机数前缀），这样我们就可以把原本倾斜的Key通过分而治之方案分散开来，最后又进行了全局聚合注意1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。注意2: 单独处理异常数据时，可以配合使用Map Join解决。 2.spark使用不当造成的数据倾斜 提高shuffle并行度dataFrame和sparkSql可以设置spark.sql.shuffle.partitions参数控制shuffle的并发度，默认为200。rdd操作可以设置spark.default.parallelism控制并发度，默认参数由不同的Cluster Manager控制。 局限性: 只是让每个task执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些key的大 小非常大，即使一个task单独执行它，也会受到数据倾斜的困扰。 使用map join 代替reduce join 在小表不是特别大(取决于你的executor大小)的情况下使用，可以使程序避免shuffle的过程，自然也就没有数据倾斜的困扰了.（详细见http://blog.csdn.net/lsshlsw/article/details/50834858、http://blog.csdn.net/lsshlsw/article/details/48694893） 局限性: 因为是先将小数据发送到每个executor上，所以数据量不能太大。 ​ 问题十七：spark中map-side-join关联优化： ​ 将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。 如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。 何时使用：在海量数据中匹配少量特定数据 原理：reduce-side-join 的缺陷在于会将key相同的数据发送到同一个partition中进行运算，大数据集的传输需要长时间的IO，同时任务并发度收到限制，还可能造成数据倾斜。 reduce-side-join 运行图如下 map-side-join 运行图如下： 将少量的数据转化为Map进行广播，广播会将此 Map 发送到每个节点中，如果不进行广播，每个task执行时都会去获取该Map数据，造成了性能浪费。对大数据进行遍历，使用mapPartition而不是map，因为mapPartition是在每个partition中进行操作，因此可以减少遍历时新建broadCastMap.value对象的空间消耗，同时匹配不到的数据也不会返回。 问题十八：kafka整合sparkStreaming问题：(1)、如何实现sparkStreaming读取kafka中的数据可以这样说：在kafka0.10版本之前有二种方式与sparkStreaming整合，一种是基于receiver，一种是direct,然后分别阐述这2种方式分别是什么 receiver：是采用了kafka高级api,利用receiver接收器来接受kafka topic中的数据，从kafka接收来的数据会存储在spark的executor中，之后spark streaming提交的job会处理这些数据，kafka中topic的偏移量是保存在zk中的。基本使用：还有几个需要注意的点： ​ 在Receiver的方式中，Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度.​ 对于不同的Group和topic我们可以使用多个Receiver创建不同的Dstream来并行接收数据，之后可以利用union来统一成一个Dstream。在默认配置下，这种方式可能会因为底层的失败而丢失数据. 因为receiver一直在接收数据,在其已经通知zookeeper数据接收完成但是还没有处理的时候,executor突然挂掉(或是driver挂掉通知executor关闭),缓存在其中的数据就会丢失. 如果希望做到高可靠, 让数据零丢失,如果我们启用了Write Ahead Logs(spark.streaming.receiver.writeAheadLog.enable=true）该机制会同步地将接收到的Kafka数据写入分布式文件系统(比如HDFS)上的预写日志中. 所以, 即使底层节点出现了失败, 也可以使用预写日志中的数据进行恢复. 复制到文件系统如HDFS，那么storage level需要设置成 StorageLevel.MEMORY_AND_DISK_SER，也就是KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER) direct: 在spark1.3之后，引入了Direct方式。不同于Receiver的方式，Direct方式没有receiver这一层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，之后根据设定的maxRatePerPartition来处理每个batch。（设置spark.streaming.kafka.maxRatePerPartition=10000。限制每秒钟从topic的每个partition最多消费的消息条数） (2) 对比这2中方式的优缺点： 采用receiver方式：这种方式可以保证数据不丢失，但是无法保证数据只被处理一次，WAL实现的是At-least-once语义（至少被处理一次），如果在写入到外部存储的数据还没有将offset更新到zookeeper就挂掉,这些数据将会被反复消费. 同时,降低了程序的吞吐量。 采用direct方式: 相比Receiver模式而言能够确保机制更加健壮. 区别于使用Receiver来被动接收数据, Direct模式会周期性地主动查询Kafka, 来获得每个topic+partition的最新的offset, 从而定义每个batch的offset的范围. 当处理数据的job启动时, 就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 **优点**： **1、简化并行读取** 如果要读取多个partition, 不需要创建多个输入DStream然后对它们进行union操作. Spark会创建跟Kafka partition一样多的RDD partition, 并且会并行从Kafka中读取数据. 所以在Kafka partition和RDD partition之间, 有一个一对一的映射关系. **2、高性能** 如果要保证零数据丢失, 在基于receiver的方式中, 需要开启WAL机制. 这种方式其实效率低下, 因为数据实际上被复制了两份, Kafka自己本身就有高可靠的机制, 会对数据复制一份, 而这里又会复制一份到WAL中. 而基于direct的方式, 不依赖Receiver, 不需要开启WAL机制, 只要Kafka中作了数据的复制, 那么就可以通过Kafka的副本进行恢复. **3、一次且仅一次的事务机制** 基于receiver的方式, 是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的. 这是消费Kafka数据的传统方式. 这种方式配合着WAL机制可以保证数据零丢失的高可靠性, 但是却无法保证数据被处理一次且仅一次, 可能会处理两次. 因为Spark和ZooKeeper之间可能是不同步的. 基于direct的方式, 使用kafka的简单api, Spark Streaming自己就负责追踪消费的offset, 并保存在checkpoint中. Spark自己一定是同步的, 因此可以保证数据是消费一次且仅消费一次。不过需要自己完成将offset写入zk的过程,在官方文档中都有相应介绍. -*简单代码实例： messages.foreachRDD(rdd=>{ val message = rdd.map(_._2)//对数据进行一些操作 message.map(method)//更新zk上的offset (自己实现) updateZKOffsets(rdd) }) sparkStreaming程序自己消费完成后，自己主动去更新zk上面的偏移量。也可以将zk中的偏移量保存在mysql或者redis数据库中，下次重启的时候，直接读取mysql或者redis中的偏移量，获取到上次消费的偏移量，接着读取数据。 问题十九：利用scala语言进行排序1.冒泡： 2.快读排序： 问题二十：spark master在使用zookeeper进行HA时，有哪些元数据保存在zookeeper？​ Spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括worker,master,application,executors.standby节点要从zk中获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的，另外，master切换需要注意两点： ​ 1. 在master切换的过程中，所有的已经在运行的程序皆正常运行，因为spark application在运行前就已经通过cluster manager获得了计算资源，所以在运行时job本身的调度和处理master是没有任何关系的； ​ 2. 在master的切换过程中唯一的影响是不能提交新的job，一方面不能提交新的应用程序给集群，因为只有Active master才能接受新的程序的提交请求，另外一方面，已经运行的程序也不能action操作触发新的job提交请求。 问题二十一：spark master HA主从切换过程不会影响集群已有的作业运行，为什么？ 答：因为程序在运行之前，已经向集群申请过资源，这些资源已经提交给driver了，也就是说已经分配好资源了，这是粗粒度分配，一次性分配好资源后不需要再关心资源分配，在运行时让driver和executor自动交互，弊端是如果资源分配太多，任务运行完不会很快释放，造成资源浪费，这里不适用细粒度分配的原因是因为任务提交太慢。 问题二十二：什么是粗粒度，什么是细粒度，各自的优缺点是什么？1.粗粒度：启动时就分配好资源，程序启动，后续具体使用就使用分配好的资源，不需要再分配资源。好处：作业特别多时，资源复用率较高，使用粗粒度。缺点：容易资源浪费，如果一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度。如果有999个资源闲置在那里，会造成资源大量浪费。 2.细粒度：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。 问题二十三：driver的功能是什么：1.一个spark作业运行时包括一个driver进程，也就是作业的主进程，具有main函数，并且有sparkContext的实例，是程序的入口； 2.功能：负责向集群申请资源，向master注册信息，负责了作业的调度，负责了作业的解析，生成stage并调度task到executor上，包括DAGScheduler，TaskScheduler。 问题二十四：spark的有几种部署模式，每种模式特点？1）本地模式 Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类 · local：只启动一个executor · local[k]:启动k个executor · local：启动跟cpu数目相同的 executor 2)standalone模式 分布式部署集群， 自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础， 3)Spark on yarn模式 分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端 4）Spark On Mesos模式。官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序： 1) 粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。 2) 细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。 问题二十五：Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？1）Spark core：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。 2）SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。 3）Spark sql：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析 4）BlinkDB ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。 5）MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib，MLI、ML Optimizer和MLRuntime。 6）GraphX是Spark中用于图和图并行计算 问题二十六：spark中worker 的主要工作是什么？主要功能：管理当前节点内存，CPU的使用情况，接受master发送过来的资源指令，通过executorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务，需要注意的是： 1.worker会不会汇报当前信息给master？worker心跳给master主要只有workid，不会以心跳的方式发送资源信息给master，这样master就知道worker是否存活，只有故障的时候才会发送资源信息； 2.worker不会运行代码，具体运行的是executor，可以运行具体application斜的业务逻辑代码，操作代码的节点，不会去运行代码。 问题二十七：简单说一下hadoop和spark的shuffle相同和差异？1）从 high-level 的角度来看，两者并没有大的差别。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。 2）从 low-level 的角度来看，两者差别不小。 Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。 3）从实现角度来看，两者也有不少差别。 Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。 如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？ Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。 问题二十八：Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别两者都是用mr模型来进行并行计算: 1) hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。 2) spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。 3) hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。 spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。 问题二十九：RDD机制？rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。 rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。 问题三十：spark有哪些组件？答：主要有如下组件： 1）master：管理集群和节点，不参与计算。 2）worker：计算节点，进程本身不参与计算，和master汇报。 3）Driver：运行程序的main方法，创建spark context对象。 4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。 5）client：用户提交程序的入口。 问题三十一：spark工作机制？答：用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。 执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。 task scheduler会将stage划分为task set分发到各个节点的executor中执行。 问题三十二：spark的优化怎么做？答： spark调优比较复杂，但是大体可以分为三个方面来进行， 1）平台层面的调优：防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet， 2）应用程序层面的调优：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等， 3）JVM层面的调优：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等 ​ 序列化在分布式系统中扮演着重要的角色，优化Spark程序时，首当其冲的就是对序列化方式的优化。Spark为使用者提供两种序列化方式： Java serialization: 默认的序列化方式。 Kryo serialization: 相较于 Java serialization 的方式，速度更快，空间占用更小，但并不支持所有的序列化格式，同时使用的时候需要注册class。spark-sql中默认使用的是kyro的序列化方式。可以在spark-default.conf设置全局参数，也可以代码中初始化时对SparkConf设置 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) ，该参数会同时作用于机器之间数据的shuffle操作以及序列化rdd到磁盘，内存。Spark不将Kyro设置成默认的序列化方式是因为它需要对类进行注册，官方强烈建议在一些网络数据传输很大的应用中使用kyro序列化。 如果你要序列化的对象比较大，可以增加参数spark.kryoserializer.buffer所设置的值。 如果你没有注册需要序列化的class，Kyro依然可以照常工作，但会存储每个对象的全类名(full class name)，这样的使用方式往往比默认的 Java serialization 还要浪费更多的空间。 可以设置 spark.kryo.registrationRequired 参数为 true，使用kyro时如果在应用中有类没有进行注册则会报错： 如上这个错误需要添加 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dataquaner.github.io/tags/Spark/"}]},{"title":"HiveSQL优化 hive参数版总结","slug":"hive参数优化","date":"2020-06-11T06:50:00.000Z","updated":"2020-06-11T10:22:08.086Z","comments":true,"path":"2020/06/11/hive-can-shu-you-hua/","link":"","permalink":"https://dataquaner.github.io/2020/06/11/hive-can-shu-you-hua/","excerpt":"","text":"Hive SQL基本上适用大数据领域离线数据处理的大部分场景。Hive SQL的优化也是我们必须掌握的技能，而且，面试一定会问。那么，我希望面试者能答出其中的80%优化点，在这个问题上才算过关。 1. Hive优化目标 在有限的资源下，执行效率更高 常见问题 数据倾斜 map数设置 reduce数设置 其他 2. Hive执行优化 HQL --&gt; Job --&gt; Map/Reduce 执行计划explain [extended] hql 样例 select col,count(1) from test2 group by col; explain select col,count(1) from test2 group by col; 3. Hive表优化 分区 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; ​ 静态分区 ​ 动态分区 分桶 set hive.enforce.bucketing=true; set hive.enforce.sorting=true; 数据 相同数据尽量聚集在一起 4. Hive Job优化并行化执行每个查询被hive转化成多个阶段，有些阶段关联性不大，则可以并行化执行，减少执行时间 set hive.exec.parallel= true; set hive.exec.parallel.thread.numbe=8; 本地化执行 ​ job的输入数据大小必须小于参数:hive.exec.mode.local.auto.inputbytes.max(默认128MB) ​ job的map数必须小于参数:hive.exec.mode.local.auto.tasks.max(默认4) ​ job的reduce数必须为0或者1 ​ set hive.exec.mode.local.auto=true; ​ 当一个job满足如上条件才能真正使用本地模式: job合并输入小文件 set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 合并文件数由mapred.max.split.size限制的大小决定 job合并输出小文件 set hive.merge.smallfiles.avgsize=256000000;当输出文件平均小于该值，启动新job合并文件 set hive.merge.size.per.task=64000000;合并之后的文件大小 JVM重利用 set mapred.job.reuse.jvm.num.tasks=20; JVM重利用可以使得JOB长时间保留slot,直到作业结束，这在对于有较多任务和较多小文件的任务是非常有意义的，减少执行时间。当然这个值不能设置过大，因为有些作业会有reduce任务，如果reduce任务没有完成，则map任务占用的slot不能释放，其他的作业可能就需要等待。 压缩数据set hive.exec.compress.output=true; set mapred.output.compreession.codec=org.apache.hadoop.io.compress.GzipCodec; set mapred.output.compression.type=BLOCK; set hive.exec.compress.intermediate=true; set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec; set hive.intermediate.compression.type=BLOCK; 中间压缩就是处理hive查询的多个job之间的数据，对于中间压缩，最好选择一个节省cpu耗时的压缩方式 hive查询最终的输出也可以压缩 5. Hive Map优化set mapred.map.tasks =10; 无效 (1) 默认map个数 default_num = total_size / block_size; 如果想增加map个数，则设置mapred.map.tasks为一个较大的值 如果想减小map个数，则设置mapred.min.split.size为一个较大的值 情况1：输入文件size巨大，但不是小文件 情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。这种情况通过增大mapred.min.split.size不可行，需要使用combineFileInputFormat将多个input path合并成一个InputSplit送给mapper处理，从而减少mapper的数量。 6. Hive Shuffle优化 Map端 ​ io.sort.mb ​ io.sort.spill.percent ​ min.num.spill.for.combine ​ io.sort.factor ​ io.sort.record.percent Reduce端 mapred.reduce.parallel.copies mapred.reduce.copy.backoff io.sort.factor mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent mapred.job.shuffle.input.buffer.percent 7. Hive Reduce优化 需要reduce操作的查询 group by, join, distribute by, cluster by… order by 比较特殊,只需要一个reduce sum,count,distinct… 聚合函数 高级查询 推测执行 mapred.reduce.tasks.speculative.execution hive.mapred.reduce.tasks.speculative.execution Reduce优化 numRTasks = min[maxReducers,input.size/perReducer] maxReducers=hive.exec.reducers.max perReducer = hive.exec.reducers.bytes.per.reducer hive.exec.reducers.max 默认 ：999 hive.exec.reducers.bytes.per.reducer 默认:1G set mapred.reduce.tasks=10;直接设置 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"}]},{"title":"6.Hadoop面试系列之UDF","slug":"6.Hadoop面试系列之UDF","date":"2020-06-10T14:57:19.767Z","updated":"2020-06-10T14:57:19.766Z","comments":true,"path":"2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/","link":"","permalink":"https://dataquaner.github.io/2020/06/10/6.hadoop-mian-shi-xi-lie-zhi-udf/","excerpt":"","text":"1. 开发步骤​ UDF简称自定义函数，它是Hive函数库的扩展，自定义函数UDF在MapReduce执行阶段发挥作用。开发步骤如下： 1） 给hive.ql.exec.UDF包开发一个自定义函数类，从UDF继承。自定义函数类实现evaluate方法。 2） 在FunctionRegistry类中注册开发的自定义函数类。 3） 打包发布至Hive客户端。 1.1 开发工具​ Eclipse是一款开源的、基于Java的可扩展开发平台。Hadoop开发人员可通过在Eclipse上面开发UDF。 1.2 UDF函数案例1）开发UDF函数类文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHelloWorld.java package org.apache.hadoop.hive.ql.udf; import org.apache.hadoop.hive.ql.exec.UDF; import org.apache.hadoop.io.Text; public class UDFHelloWorld extends UDF { public String evaluate(String str) { if (str == null) { return null; } return \"HelloWorld \" + str; } public static void main(String[] args) { helloUDF uf = new helloUDF();//Text t = new Text(\"gfsg\"); System.out.println(uf.evaluate(\"nihao\").toString()); } } 2）UDF类注册，注册方法文件名及路径：/hive-0.12.0/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java package org.apache.hadoop.hive.ql.exec; import org.apache.hadoop.hive.ql.udf.UDFHelloWorld;/*** FunctionRegistry.*/ public final class FunctionRegistry { static { registerGenericUDF(\"concat\", GenericUDFConcat.class); registerUDF(\"substr\", UDFSubstr.class, false); registerUDF(\"substring\", UDFSubstr.class, false); registerUDF(\"space\", UDFSpace.class, false); registerUDF(\"repeat\", UDFRepeat.class, false); registerUDF(\"ascii\", UDFAscii.class, false); registerUDF(\"lpad\", UDFLpad.class, false); registerUDF(\"rpad\", UDFRpad.class, false); registerUDF(\"Hello\", UDFHelloWorld.class, false); registerGenericUDF(\"size\", GenericUDFSize.class); 3）Jar包发布路径发布路径：/opt/boh/hive/lib/hive-exec-0.12.0-cdh5.0.0.jar 上传至hadoop集群执行脚本的hive客户端。 1.3Hive UDF函数1.3.1UDF函数列表 函数清单及其功能 TO_DATE(string date,'format') 格式化所需要的日期 ADD_MONTHS(Timestamp date,int n) 增加月数 date_tostring(Timestamp date,'format') 转换Date类型为指定格式字符串 MONTHS_BETWEEN（Timestamp date1，Timestamp date2） 返回两个日期之间的月数 f_age(string identityId) 验证身份证合法性并返回性别年龄 f_checkidcard(string identityId) 验证身份证合法性 1.3.2 UDF函数说明 TO_DATE函数 Select to_date('20140909111111','YYYYMMDDHH24miss') from test; 返回结果：2014-09-09 11:11:11 ADD_MONTHS函数 select add_months(to_date('20140909111111','YYYYMMDDHH24miss'),1) from test; 返回结果：2014-10-09 11:11:11 date_tostring函数 select date_tostring(to_date('20140909111111','YYYYMMDDHH24miss'),'YYYY-MM-DD') from test; 返回结果：2014-09-09 MONTHS_BETWEEN函数 select MONTHS_BETWEEN(to_date('20140909111111','YYYYMMDDHH24miss'),to_date('20140706111111','YYYYMMDDHH24miss')) from test; 返回结果：2.096774193548387 f_age函数 select f_age('511024198710148199') from test; 返回结果：127 f_checkidcard函数 select f_checkidcard('511024198710148199') from test; 返回结果：1 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[]},{"title":"Hive开窗函数梳理","slug":"Hive开窗函数总结","date":"2020-06-10T06:35:00.000Z","updated":"2020-06-10T11:15:10.879Z","comments":true,"path":"2020/06/10/hive-kai-chuang-han-shu-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2020/06/10/hive-kai-chuang-han-shu-zong-jie/","excerpt":"","text":"本文通过几个实际的查询例子，为大家介绍Hive SQL面试中最常问到的窗口函数。 假设有如下表格（loan）。表中包含贷款人的唯一标识，贷款日期，以及贷款金额。 1.SUM(), MIN(),MAX(),AVG()等聚合函数，可以直接使用 over() 进行分区计算。 SELECT *, /*前三次贷款的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv1, /*历史所有贷款 累加到下一次贷款 的金额之和*/ SUM(amount) OVER (PARTITION BY name ORDER BY orderdate ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) AS pv2 FROM loan ; 其中，窗口函数over()使得聚合函数sum()可以在限定的窗口中进行聚合。本例子中，第一条语句计算每个人当前记录的前三条贷款金额之和。第二条语句计算截至到下一次贷款，客户贷款的总额。 窗口的限定语法为：ROWS BETWEEN 一个时间点 AND 一个时间点。时间节点可以使用： n PRECEDING : 前n行 n preceding n FOLLOWING：后n行 CURRENT ROW ： 当前行 如果不想限制具体的行数，可以将 n 替换为 UNBOUNDED.比如从起始到当前，可以写为: ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. 窗口函数over()和group by 的最大区别，在于group by之后其余列也必须按照此分区进行计算，而over()函数使得单个特征可以进行分区。 2.NTILE(), ROW_NUMBER(), RANK(), DENSE_RANK()，可以为数据集新增加序列号。 SELECT *, #将数据按name切分成10区，并返回属于第几个分区 NTILE(10) OVER (PARTITION BY name ORDER BY orderdate) AS f1, #将数据按照name分区，并按照orderdate排序，返回排序序号 ROW_NUMBER() OVER (PARTITION BY name ORDER BY orderdate) AS f2, #将数据按照name分区，并按照orderdate排序，返回排序序号 RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f3, #将数据按照name分区，并按照orderdate排序，返回排序序号 DENSE_RANK() OVER (PARTITION BY name ORDER BY orderdate) AS f4 FROM loan; 其中第一个函数NTILE(10)是将数据按name切分成10区，并返回属于第几个分区。 可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。语法是： ntile (num) over ([partition_clause] order_by_clause) as your_bucket_num 然后可以根据桶号，选取前或后 n分之几的数据。 后面的三个函数的功能看起来很相似。区别在于当数据中出现相同值得时候，如何编号。 ROW_NUMBER()返回的是一列连续的序号。 RANK()对于数值相同的这一项会标记为相同的序号，而下一个序号跳过。比如{4，5，6}变成了{4，4，6}. DENSE_RANK()对于数值相同的这一项，也会标记为相同的序号，但下一个序号并不会跳过。比如{4，5，6}变成了{4，4，5}. 3.LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()函数返回一系列指定的点 SELECT *, #取上一笔贷款的日期,缺失默认填NULL LAG(orderdate, 1) OVER(PARTITION BY name ORDER BY orderdate) AS last_dt, #取下一笔贷款的日期,缺失指定填'1970-1-1' LEAD(orderdate, 1,'1970-1-1') OVER(PARTITION BY name ORDER BY orderdate) AS next_dt, #取最早一笔贷款的日期 FIRST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS first_dt, #取新一笔贷款的日期 LAST_VALUE(orderdate) OVER(PARTITION BY name ORDER BY orderdate) AS latest_dt FROM loan; LAG(n)将数据向前错位 n 行。LEAD(n)将数据向后错位 n 行。FIRST_VALUE()取当前分区中的第一个值。 LAST_VALUE()取当前分区最后一个值。注意：这四个函数取出的都是某个字段，不是整条记录 4.GROUPING SET(),with CUBE, with ROLLUP 对 group by 进行限制 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate GROUPING SETS(substring(orderdate,1,7), orderdate) ORDER BY GROUPING__ID; GROUPING__ID是GROUPING_SET()的操作之后自动生成的。生成GROUPING__ID是为了区分每条输出结果是属于哪一个group by的数据。它是根据group by后面声明的顺序字段，是否存在于当前group by中的一个二进制位组合数据。GROUPING SETS()必须先做GROUP BY操作。 比如（A,C）的group_id： group_id(A,C) = grouping(A)+grouping(B)+grouping (C) 的结果就是：二进制：101 也就是5. 如果解释器发现group by A,C 但是select A,B,C 那么运行时会将所有from 表取出的结果复制一份，B都置为null，也就是在结果中，B都为null. SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with CUBE ORDER BY GROUPING__ID; with CUBE 和GROUPING_SET()的区别就是，with CUBE 返回的是group by 字段的笛卡尔积。 SELECT A,B,C FROM loan #分别按照月份和日进行分区 GROUP BY substring(orderdate,1,7),orderdate with ROLLUP ORDER BY GROUPING__ID; with ROLLUP则不会产生第二列为键的聚合结果，在本例子中，只按照 substring(orderdate,1,7)进行展示。所以使用with ROLLUP时，要注意group by 后面字段的顺序。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"},{"name":"开窗函数","slug":"开窗函数","permalink":"https://dataquaner.github.io/tags/%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/"}]},{"title":"Hadoop核心知识之MapReduce原理","slug":"1.Hadoop面试系列之Mapreduce原理 ","date":"2020-06-08T13:14:00.000Z","updated":"2020-06-08T14:31:06.843Z","comments":true,"path":"2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/","link":"","permalink":"https://dataquaner.github.io/2020/06/08/1.hadoop-mian-shi-xi-lie-zhi-mapreduce-yuan-li/","excerpt":"","text":"MapReduce是一个基于集群的计算平台，是一个简化分布式编程的计算框架，是一个将分布式计算抽象为Map和Reduce两个阶段的编程模型。（这句话记住了是可以用来装逼的） 1.MapReduce工作流程0) 用户提交任务 （含数据） 1) 集群首先对输入数据源进行切片 2) master 调度 worker 执行 map 任务 3) worker 读取输入源片段 4) worker 执行 map 任务，将任务输出保存在本地5) master 调度 worker 执行 reduce 任务，reduce worker 读取 map 任务的输出文件 6） 执行 reduce 任务，将任务输出保存到 HDFS 由上至下依次执行 过程 过程描述 用户提交任务job 给集群 切片 集群查找源数据 对源数据做基本处理 分词(每行执行一次map函数) 集群(yarn的appliction)分配map任务节点worker 映射 其中间数据(存在本地) 分区(partition) 中间数据 排序 (或二次排序) 中间数据 聚合(combine有无key聚合后key无序) 中间数据 分组(group) 发生在排序后混洗前(个人理解) 混洗(shuffle后key有序) 混洗横跨mapper和reducer，其发生在mapper的输出和reducer的输入阶段 规约(reduce) 集群(yarn的appliction)分配reduce任务节点worker 下面针对具体过程详细介绍： 2.切片split​ HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map任务的数量：Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 Reduce任务的数量： 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误） 3.Map()阶段 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数 重写map()，对第一步产生的&lt;k,v&gt;进行处理，转换为新的&lt;k,v&gt;输出 对输出的key、value进行分区 对不同分区的数据，按照key进行排序、分组。相同key的value放到一个集合中 4. Reduce阶段 多个map任务的输出，按照不同的分区，通过网络复制到不同的reduce节点上 对多个map的输出进行合并、排序。 重写reduce函数实现自己的逻辑，对输入的key、value处理，转换成新的key、value输出 把reduce的输出保存到文件中 特别说明： 切片 不属于map阶段，但却是map阶段的输入，是集群对输入数据的解析处理 分词，映射，分区，排序，聚合 都属map阶段 混洗 横跨map阶段和reduce阶段，其发生在map阶段的输出和reduce的输入阶段 规约 属reduce阶段 规约结果是reduce阶段的输出，输出格式由集群默认或用户自定义 分词即map()函数的输入与map阶段的输入略有差别，他的输入是切片结果的kv形式，行号（偏移量）与行内容 5.总结执行步骤： 1）map任务处理——&gt;切片 读取输入文件内容，解析成key、value对，输入文件的每一行，就是一个key、value对，对应调用一次map函数。 写自己的逻辑，对输入的key、value（k1,v1）处理，转换成新的key、value(k2,v2)输出。 2）reduce任务处理——&gt;计算 在reduce之前，有一个shuffle的过程对多个map任务的输出进行合并、排序、分组等操作。 写reduce函数自己的逻辑，对输入的key、value（k2,{v2,…}）处理，转换成新的key、value(k3,v3)输出。 把reduce的输出保存到文件中。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"https://dataquaner.github.io/tags/MapReduce/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/tags/Hadoop/"}]},{"title":"hadoop shell命令","slug":"hadoop fs、hadoop dfs与hdfs dfs命令的区别及hadoop fs命令说明","date":"2020-06-08T10:40:00.000Z","updated":"2020-06-08T11:13:27.302Z","comments":true,"path":"2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/","link":"","permalink":"https://dataquaner.github.io/2020/06/08/hadoop-fs-hadoop-dfs-yu-hdfs-dfs-ming-ling-de-qu-bie-ji-hadoop-fs-ming-ling-shuo-ming/","excerpt":"","text":"0.前言FS Shell调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。 对HDFS文件系统，scheme是hdfs， 对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。 一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。 大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。 1. hadoop fs 命令列表FS Shell cat chgrp chmod chown copyFromLocal copyToLocal cp du dus expunge get getmerge ls lsr mkdir movefromLocal mv put rm rmr setrep stat tail test text touchz 特别说明： hadoop fs：通用的文件系统命令，针对任何系统，比如本地文件、HDFS文件、HFTP文件、S3文件系统等。 hadoop dfs：特定针对HDFS的文件系统的相关操作，但是已经不推荐使用。 hdfs dfs：与hadoop dfs类似，同样是针对HDFS文件系统的操作，替代hadoop dfs。 2. 各命令使用说明 cat使用方法：hadoop fs -cat URI [URI …] ​ 将路径指定文件的内容输出到stdout。 示例： hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2 hadoop fs -cat file:///file3 /user/hadoop/file4 返回值： 成功返回0，失败返回-1。 chgrp使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] ​ Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. ​ 改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chmod使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …] ​ 改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chown使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] ​ 改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。 copyFromLocal使用方法：hadoop fs -copyFromLocal URI ​ 除了限定源路径是一个本地文件外，和put命令相似。 copyToLocal使用方法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI ​ 除了限定目标路径是一个本地文件外，和get命令类似。 cp使用方法：hadoop fs -cp URI [URI …] ​ 将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。示例： hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir 返回值： ​ 成功返回0，失败返回-1。 du使用方法：hadoop fs -du URI [URI …] ​ 显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。示例： hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1 返回值： 成功返回0，失败返回-1。 dus使用方法：hadoop fs -dus ​ 显示文件的大小。 expunge使用方法：hadoop fs -expunge 清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。 get使用方法：hadoop fs -get [-ignorecrc] [-crc] ​ 复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。 示例： hadoop fs -get /user/hadoop/file localfile hadoop fs -get hdfs://host:port/user/hadoop/file localfile 返回值： ​ 成功返回0，失败返回-1。 getmerge使用方法：hadoop fs -getmerge [addnl] 接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。 ls使用方法：hadoop fs -ls ​ 如果是文件，则按照如下格式返回文件信息：​ 文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID​ 如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：​ 目录名 ​ 修改日期 修改时间 权限 用户ID 组ID示例： hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile 返回值： 成功返回0，失败返回-1。 mkdir使用方法：hadoop fs -mkdir ​ 接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。 示例： hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir 返回值： ​ 成功返回0，失败返回-1。 movefromLocal使用方法：dfs -moveFromLocal 输出一个”not implemented“信息。 mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1 返回值： ​ 成功返回0，失败返回-1。 put使用方法：hadoop fs -put … ​ 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile 从标准输入中读取输入。返回值： ​ 成功返回0，失败返回-1。 rm使用方法：hadoop fs -rm URI [URI …] ​ 删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir 返回值： 成功返回0，失败返回-1。 rmr使用方法：hadoop fs -rmr URI [URI …] delete的递归版本。示例： hadoop fs -rmr /user/hadoop/dir hadoop fs -rmr hdfs://host:port/user/hadoop/dir 返回值： 成功返回0，失败返回-1。 setrep使用方法：hadoop fs -setrep [-R] 改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。 示例： hadoop fs -setrep -w 3 -R /user/hadoop/dir1返回值： 成功返回0，失败返回-1。 stat使用方法：hadoop fs -stat URI [URI …] 返回指定路径的统计信息。 示例： hadoop fs -stat path返回值： 成功返回0，失败返回-1。 tail使用方法：hadoop fs -tail [-f] URI 将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。 示例： hadoop fs -tail pathname返回值： 成功返回0，失败返回-1。 test使用方法：hadoop fs -test -[ezd] URI 选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。-d 如果路径是个目录，则返回1，否则返回0。 示例： hadoop fs -test -e filenametext使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。 touchz使用方法：hadoop fs -touchz URI [URI …] 创建一个0字节的空文件。 示例： hadoop -touchz pathname返回值：成功返回0，失败返回-1。 更多详细信息访问：http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dataquaner.github.io/tags/Hadoop/"},{"name":"Shell","slug":"Shell","permalink":"https://dataquaner.github.io/tags/Shell/"}]},{"title":"【Hive日常问题】导入数据成功，查询显示NULL","slug":"【Hive日常问题】导入数据成功，查询显示NULL","date":"2020-05-06T13:40:00.000Z","updated":"2020-05-06T11:17:26.029Z","comments":true,"path":"2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/","link":"","permalink":"https://dataquaner.github.io/2020/05/06/hive-ri-chang-wen-ti-dao-ru-shu-ju-cheng-gong-cha-xun-xian-shi-null/","excerpt":"","text":"问题描述hive导入数据成功，但是查询结果为NULL： load data local inpath '/user/hive/student.txt' into table hive_test.students; Loading data to table hive_test.students OK select * from hive_test.students; OK NULL NULL NULL NULL 问题原因查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格： OKNULL NULLNULL NULL查其原因是创建表格时没有对导入的数据格式没有处理，比如每行数据以tab键隔开，以换行键结尾，就要以如下语句创建表格： CREATE TABLE students(id int, name string); CREATE TABLE students(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LINES TERMINATED BY '\\n' STORED AS TEXTFILE; OK 1 sun 2 lin document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Question","slug":"Data-Question","permalink":"https://dataquaner.github.io/categories/Data-Question/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://dataquaner.github.io/tags/Hive/"}]},{"title":"Python高级特性之切片","slug":"Python高级特性之切片","date":"2020-04-25T14:14:00.000Z","updated":"2020-04-25T10:10:47.302Z","comments":true,"path":"2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/","link":"","permalink":"https://dataquaner.github.io/2020/04/25/python-gao-ji-te-xing-zhi-qie-pian/","excerpt":"","text":"1. Python可切片对象的索引方式​ 包括：正索引和负索引两部分，如下图所示，以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例： 2. Python切片操作的一般方式​ 一个完整的切片表达式包含两个“:”，用于分隔三个参数(start_index、end_index、step)。 当只有一个“:”时，默认第三个参数step=1； 当一个“:”也没有时，start_index=end_index，表示切取start_index指定的那个元素。 ​ 切片操作基本表达式：object[start_index:end_index:step] step：正负数均可，其绝对值大小决定了切取数据时的‘‘步长”，而正负号决定了“切取方向”，正表示“从左往右”取值，负表示“从右往左”取值。当step省略时，默认为1，即从左往右以步长1取值。“切取方向非常重要！”“切取方向非常重要！”“切取方向非常重要！”，重要的事情说三遍！ start_index：表示起始索引（包含该索引对应值）；该参数省略时，表示从对象“端点”开始取值，至于是从“起点”还是从“终点”开始，则由step参数的正负决定，step为正从“起点”开始，为负从“终点”开始。 end_index：表示终止索引（不包含该索引对应值）；该参数省略时，表示一直取到数据“端点”，至于是到“起点”还是到“终点”，同样由step参数的正负决定，step为正时直到“终点”，为负时直到“起点”。 3. Python切片操作详细例子​ 以下示例均以list对象a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]为例： >>>a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1. 切取单个元素>>>a[0] >>>0 >>>a[-4] >>>6 当索引只有一个数时，表示切取某一个元素。 2. 切取完整对象>>>a[:] #从左往右 >>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>a[::]#从左往右 >>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>a[::-1]#从右往左 >>> [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] 3. start_index和end_index全为正（+）索引的情况>>>a[1:6] >>> [1, 2, 3, 4, 5] step=1，从左往右取值，start_index=1到end_index=6同样表示从左往右取值。 >>>a[1:6:-1] >>> [] 输出为空列表，说明没取到数据。 step=-1，决定了从右往左取值，而start_index=1到end_index=6决定了从左往右取值，两者矛盾，所以为空。 >>>a[6:2] >>> [] 同样输出为空列表。 step=1，决定了从左往右取值，而start_index=6到end_index=2决定了从右往左取值，两者矛盾，所以为空。 >>>a[:6] >>> [0, 1, 2, 3, 4, 5] step=1，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值0开始一直取到end_index=6（该点不包括）。 >>>a[:6:-1] >>> [9, 8, 7] step=-1，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值9开始一直取到end_index=6（该点不包括）。 >>>a[6:] >>> [6, 7, 8, 9] step=1，从左往右取值，从start_index=6开始，一直取到“终点”值9。 >>>a[6::-1] >>> [6, 5, 4, 3, 2, 1, 0] step=-1，从右往左取值，从start_index=6开始，一直取到“起点”0。 4. start_index和end_index全为负（-）索引的情况>>>a[-1:-6] >>> [] step=1，从左往右取值，而start_index=-1到end_index=-6决定了从右往左取值，两者矛盾，所以为空。 索引-1在-6的右边（如上图） >>>a[-1:-6:-1] >>> [9, 8, 7, 6, 5] step=-1，从右往左取值，start_index=-1到end_index=-6同样是从右往左取值。 索引-1在6的右边（如上图） >>>a[-6:-1] >>> [4, 5, 6, 7, 8] step=1，从左往右取值，而start_index=-6到end_index=-1同样是从左往右取值。 索引-6在-1的左边（如上图） >>>a[:-6] >>> [0, 1, 2, 3] step=1，从左往右取值，从“起点”开始一直取到end_index=-6（该点不包括）。 >>>a[:-6:-1] >>> [9, 8, 7, 6, 5] step=-1，从右往左取值，从“终点”开始一直取到end_index=-6（该点不包括）。 >>>a[-6:] >>> [4, 5, 6, 7, 8, 9] step=1，从左往右取值，从start_index=-6开始，一直取到“终点”。 >>>a[-6::-1] >>> [4, 3, 2, 1, 0] step=-1，从右往左取值，从start_index=-6开始，一直取到“起点”。 5. start_index和end_index正（+）负（-）混合索引的情况>>>a[1:-6] >>> [1, 2, 3] start_index=1在end_index=-6的左边，因此从左往右取值，而step=1同样决定了从左往右取值，因此结果正确 >>>a[1:-6:-1] >>> [] start_index=1在end_index=-6的左边，因此从左往右取值，但step=-则决定了从右往左取值，两者矛盾，因此为空。 >>>a[-1:6] >>> [] start_index=-1在end_index=6的右边，因此从右往左取值，但step=1则决定了从左往右取值，两者矛盾，因此为空。 >>>a[-1:6:-1] >>> [9, 8, 7] start_index=-1在end_index=6的右边，因此从右往左取值，而step=-1同样决定了从右往左取值，因此结果正确。 6. 多层切片操作>>>a[:8][2:5][-1:] >>> [4] 相当于： a[:8]=[0, 1, 2, 3, 4, 5, 6, 7] a[:8][2:5]= [2, 3, 4] a[:8][2:5][-1:] = [4] 理论上可无限次多层切片操作，只要上一次返回的是非空可切片对象即可。 7. 切片操作的三个参数可以用表达式>>>a[2+1:3*2:7%3] >>> [3, 4, 5] 即：a[2+1:3*2:7%3] = a[3:6:1] 8. 其他对象的切片操作​ 前面的切片操作以list对象为例进行说明，但实际上可进行切片操作的数据类型还有很多，包括元组、字符串等等。 >>> (0, 1, 2, 3, 4, 5)[:3] >>> (0, 1, 2) 元组的切片操作 >>>'ABCDEFG'[::2] >>>'ACEG' 字符串的切片操作 >>>for i in range(1,100)[2::3][-5:]: print(i) >>>87 90 93 96 99 就是利用range()函数生成1-99的整数，然后从start_index=2（即3）开始以step=3取值，直到终点，再在新序列中取最后五个数。 4. 常用切片操作1.取偶数位置>>>b = a[::2] [0, 2, 4, 6, 8] 2.取奇数位置>>>b = a[1::2] [1, 3, 5, 7, 9] 3.拷贝整个对象>>>b = a[:] # >>>print(b) #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>print(id(a)) #41946376 >>>print(id(b)) #41921864 或 >>>b = a.copy() >>>print(b) #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>>print(id(a)) #39783752 >>>print(id(b)) #39759176 需要注意的是：[:]和.copy()都属于“浅拷贝”，只拷贝最外层元素，内层嵌套元素则通过引用方式共享，而非独立分配内存，如果需要彻底拷贝则需采用“深拷贝”方式，如下例所示： >>>a = [1,2,['A','B']] >>>print('a={}'.format(a)) >>>b = a[:] >>>b[0] = 9 #修改b的最外层元素，将1变成9 >>>b[2][0] = 'D' #修改b的内嵌层元素 >>>print('a={}'.format(a)) >>>print('b={}'.format(b)) >>>print('id(a)={}'.format(id(a))) >>>print('id(b)={}'.format(id(b))) a=[1, 2, ['A', 'B']] #原始a a=[1, 2, ['D', 'B']] #b修改内部元素A为D后，a中的A也变成了D，说明共享内部嵌套元素，但外部元素1没变。 b=[9, 2, ['D', 'B']] #修改后的b id(a)=38669128 id(b)=38669192 4.修改单个元素>>>a[3] = ['A','B'] [0, 1, 2, ['A', 'B'], 4, 5, 6, 7, 8, 9] 5.在某个位置插入元素>>>a[3:3] = ['A','B','C'] [0, 1, 2, 'A', 'B', 'C', 3, 4, 5, 6, 7, 8, 9] >>>a[0:0] = ['A','B'] ['A', 'B', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 6.替换一部分元素>>>a[3:6] = ['A','B'] [0, 1, 2, 'A', 'B', 6, 7, 8, 9] 5. 总结 start_index、end_index、step三者可同为正、同为负，或正负混合。但必须遵循一个原则，即：当start_index表示的实际位置在end_index的左边时，从左往右取值，此时step必须是正数（同样表示从左往右）；当start_index表示的实际位置在end_index的右边时，表示从右往左取值，此时step必须是负数（同样表示从右往左），即两者的取值顺序必须相同。 当start_index或end_index省略时，取值的起始索引和终止索引由step的正负来决定，这种情况不会有取值方向矛盾（即不会返回空列表[]），但正和负取到的结果顺序是相反的，因为一个向左一个向右。 step的正负是必须要考虑的，尤其是当step省略时。比如a[-1:]，很容易就误认为是从“终点”开始一直取到“起点”，即a[-1:]= [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]，但实际上a[-1:]=[9]（注意不是9），原因在于step省略时step=1表示从左往右取值，而起始索引start_index=-1本身就是对象的最右边元素了，再往右已经没数据了，因此结果只含有9一个元素。 需要注意：“取单个元素（不带“:”）”时，返回的是对象的某个元素，其类型由元素本身的类型决定，而与母对象无关，如上面的a[0]=0、a[-4]=6，元素0和6都是“数值型”，而母对象a却是“list”型；“取连续切片（带“:”）”时，返回结果的类型与母对象相同，哪怕切取的连续切片只包含一个元素，如上面的a[-1:]=[9]，返回的是一个只包含元素“9”的list，而非数值型“9”。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Python","slug":"Python","permalink":"https://dataquaner.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dataquaner.github.io/tags/Python/"}]},{"title":"LeetCode数组系列之#88：合并两个有序数组","slug":"1.LeetCode刷题数组系列之#88：合并数组","date":"2020-04-25T08:16:16.000Z","updated":"2020-04-25T11:02:08.816Z","comments":true,"path":"2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/","link":"","permalink":"https://dataquaner.github.io/2020/04/25/1.leetcode-shua-ti-shu-zu-xi-lie-zhi-88-he-bing-shu-zu/","excerpt":"","text":"题目：合并两个有序数组难度：Easy题目描述： 题目​ 给你两个有序整数数组 nums1 和 nums2，请你将 nums2 合并到 nums1 中，使 nums1 成为一个有序数组。 说明: 初始化 nums1 和 nums2 的元素数量分别为 m 和 n 。 你可以假设 nums1 有足够的空间（空间大小大于或等于 m + n）来保存 nums2 中的元素。 示例:​ 输入:​ nums1 = [1,2,3,0,0,0], m = 3​ nums2 = [2,5,6], n = 3 ​ 输出: ​ [1,2,2,3,5,6] 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/merge-sorted-array著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 解题思路方法一：合并后排序直觉​ 最朴素的解法就是将两个数组合并之后再排序。 ​ 该算法只需要一行(Java是2行)，时间复杂度较差，为O((n + m)log(n + m))。这是由于这种方法没有利用两个数组本身已经有序这一点。 实现Python版class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -> None: \"\"\" Do not return anything, modify nums1 in-place instead. \"\"\" nums1[:] = sorted(nums1[:m] + nums2) Java版class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { System.arraycopy(nums2, 0, nums1, m, n); Arrays.sort(nums1); } } Scala版object Solution { def merge(nums1: Array[Int], m: Int, nums2: Array[Int], n: Int): Unit = { } } 复杂度分析 时间复杂度 : O((n+m)log(n+m)) 空间复杂度 : O(1) 方法二：双指针 / 从前往后直觉​ 一般而言，对于有序数组可以通过双指针法达到O(n+m)的时间复杂度。 最直接的算法实现是将指针p1 置为 nums1的开头， p2为 nums2的开头，在每一步将最小值放入输出数组中。 ​ 由于 nums1 是用于输出的数组，需要将nums1中的前m个元素放在其他地方，也就需要O(m) 的空间复杂度。 实现Python版class Solution(object): def merge(self, nums1, m, nums2, n): \"\"\" :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: void Do not return anything, modify nums1 in-place instead. \"\"\" # Make a copy of nums1. nums1_copy = nums1[:m] nums1[:] = [] # Two get pointers for nums1_copy and nums2. p1 = 0 p2 = 0 # Compare elements from nums1_copy and nums2 # and add the smallest one into nums1. while p1 &lt; m and p2 &lt; n: if nums1_copy[p1] &lt; nums2[p2]: nums1.append(nums1_copy[p1]) p1 += 1 else: nums1.append(nums2[p2]) p2 += 1 # if there are still elements to add if p1 &lt; m: nums1[p1 + p2:] = nums1_copy[p1:] if p2 &lt; n: nums1[p1 + p2:] = nums2[p2:] 复杂度分析 时间复杂度 : O(n + m) 空间复杂度 : O(m) 方法三 : 双指针 / 从后往前直觉​ 方法二已经取得了最优的时间复杂度O(n + m)，但需要使用额外空间。这是由于在从头改变nums1的值时，需要把nums1中的元素存放在其他位置。 ​ 如果我们从结尾开始改写 nums1 的值又会如何呢？这里没有信息，因此不需要额外空间。 这里的指针 p 用于追踪添加元素的位置。 实现Python版 class Solution(object): def merge(self, nums1, m, nums2, n): \"\"\" :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: void Do not return anything, modify nums1 in-place instead. \"\"\" # two get pointers for nums1 and nums2 p1 = m - 1 p2 = n - 1 # set pointer for nums1 p = m + n - 1 # while there are still elements to compare while p1 >= 0 and p2 >= 0: if nums1[p1] &lt; nums2[p2]: nums1[p] = nums2[p2] p2 -= 1 else: nums1[p] = nums1[p1] p1 -= 1 p -= 1 # add missing elements from nums2 nums1[:p2 + 1] = nums2[:p2 + 1] Java版 class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { // two get pointers for nums1 and nums2 int p1 = m - 1; int p2 = n - 1; // set pointer for nums1 int p = m + n - 1; // while there are still elements to compare while ((p1 >= 0) &amp;&amp; (p2 >= 0)) // compare two elements from nums1 and nums2 // and add the largest one in nums1 nums1[p--] = (nums1[p1] &lt; nums2[p2]) ? nums2[p2--] : nums1[p1--]; // add missing elements from nums2 System.arraycopy(nums2, 0, nums1, 0, p2 + 1); } } 复杂度 时间复杂度 : O(n + m) 空间复杂度 : O(1) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://dataquaner.github.io/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://dataquaner.github.io/tags/LeetCode/"}]},{"title":"数据倾斜问题总结","slug":"数据倾斜问题总结","date":"2020-04-22T14:14:00.000Z","updated":"2020-04-22T14:35:55.073Z","comments":true,"path":"2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2020/04/22/shu-ju-qing-xie-wen-ti-zong-jie/","excerpt":"","text":"0. 什么是数据倾斜 ​ 对于集群系统，一般缓存是分布式的，即不同节点负责一定范围的缓存数据。我们把缓存数据分散度不够，导致大量的缓存数据集中到了一台或者几台服务节点上，称为数据倾斜。一般来说数据倾斜是由于负载均衡实施的效果不好引起的。 来源百度百科 ​ 对于数据计算过程来说，数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 1. 数据倾斜的现象​ 多数task执行速度较快,少数task执行时间非常长，或者等待很长时间后提示你内存不足，执行失败。 2. 数据倾斜的影响1）数过多的数据在同一个task中执行，将会把executor撑爆，造成OOM，程序终止运行。,据倾斜直接会导致一种情况：Out Of Memory。 2）运行速度慢 ,spark中一个stage的执行时间受限于最后那个执行完的task，因此运行缓慢的任务会拖累整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。 一个经验结论是：一般情况下，OOM的原因都是数据倾斜\\ 3. 如何定位数据倾斜​ 数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。 原因： 查看任务-》查看Stage-》查看代码 ​ 某个task执行特别慢的情况 ​ 某个task莫名其妙内存溢出的情况 ​ 查看导致数据倾斜的key的数据分布情况 也可从以下几种情况考虑： 1、是不是有OOM情况出现，一般是少数内存溢出的问题 2、是不是应用运行时间差异很大，总体时间很长 3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题 4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断 5、看看代码里面是否有一些导致Shuffle的算子出现 4. 数据倾斜的几种典型情况（重点） 数据源中的数据分布不均匀，Spark需要频繁交互 数据集中的不同Key由于分区方式，导致数据倾斜 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要） 聚合操作中，数据集中的数据分布不均匀（主要） JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀 数据集中少数几个key数据量很大，不重要，其他数据均匀 注意： 需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题 只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以 5. 数据倾斜的处理方法​ 发现数据倾斜的时候，不要急于提高executor的资源，修改参数或是修改程序，首先要检查数据本身，是否存在异常数据。 5.1 检查数据，找出异常的key​ 如果任务长时间卡在最后1个(几个)任务，首先要对key进行抽样分析，判断是哪些key造成的。 选取key，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个 df.select(\"key\").sample(false,0.1).(k=>(k,1)).reduceBykey(_+_).map(k=>(k._2,k._1)).sortByKey(false).take(10) ​ 如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。 经过分析，倾斜的数据主要有以下三种情况: null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。 无效数据，大量重复的测试数据或是对结果影响不大的有效数据。 有效数据，业务导致的正常数据分布。 解决办法 第1，2种情况，直接对数据进行过滤即可。 第3种情况则需要进行一些特殊操作，常见的有以下几种做法。 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。 对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。 使用reduceByKey 代替 groupByKey 使用map join。 举例：如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作如下： 将原始的 key 转化为 key + 随机值(例如Random.nextInt)对数据进行 reduceByKey(func)将 key + 随机值 转成 key再对数据进行 reduceByKey(func)tip1: 如果此时依旧存在问题，建议筛选出倾斜的数据单独处理。最后将这份数据与正常的数据进行union即可。 tips2: 单独处理异常数据时，可以配合使用Map Join解决 5.1.1 数据源中的数据分布不均匀，Spark需要频繁交互解决方案1：避免数据源的数据倾斜 实现原理：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive或者Kafka中还是会发生数据倾斜。 适用情况：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 总结：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题 5.1.2 数据集中的不同Key由于分区方式，导致数据倾斜解决方案1：调整并行度 实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。 总结：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效 解决方案2： 缓解数据倾斜**（自定义Partitioner）** 适用场景：大量不同的Key被分配到了相同的Task造成该Task数据量过大。 解决方案： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。 优势： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。 劣势： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 5.2 检查Spark运行过程相关操作5.2.1 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）解决方案：Reduce side Join转变为Map side Join 方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。 5.2.2 聚合操作中，数据集中的数据分布不均匀（主要）解决方案：两阶段聚合（局部聚合+全局聚合） 适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案 实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案 将相同key的数据分拆处理 5.2.3 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀解决方案：为倾斜key增加随机前/后缀 适用场景：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。 解决方案：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。 优势：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。 劣势：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。 注意：具有倾斜Key的RDD数据集中，key的数量比较少 5.2.4 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀解决方案：随机前缀和扩容RDD进行join 适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。 实现思路：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀） 5.2.5 数据集中少数几个key数据量很大，不重要，其他数据均匀解决方案：过滤少数倾斜Key 适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Question","slug":"Data-Question","permalink":"https://dataquaner.github.io/categories/Data-Question/"}],"tags":[{"name":"数据倾斜","slug":"数据倾斜","permalink":"https://dataquaner.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"}]},{"title":"1.数据开发工程师面试题目必知必会","slug":"1.数据开发工程师面试题目必知必会","date":"2020-04-21T10:59:38.680Z","updated":"2020-04-21T10:59:38.680Z","comments":true,"path":"2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/","link":"","permalink":"https://dataquaner.github.io/2020/04/21/1.shu-ju-kai-fa-gong-cheng-shi-mian-shi-ti-mu-bi-zhi-bi-hui/","excerpt":"","text":"面试题目梳理一. 数据结构和算法 LeetCode 合并数组 二元查找树转双向链表 二叉树层次遍历 堆 最小堆 排序算法 动态规划 青蛙跳台阶 贪心算法 字符串转换成整数 链表中倒数第K个结点 二维数组中的查找 替换空格 从尾到头打印链表 重建二叉树 用两个栈实现队列 斐波那契数列及变形题 二进制中1的个数 在O(1)时间删除链表结点 调整数组顺序使奇数位于偶数前面 反转链表 合并两个排序的链表 树的子结构 二叉树的镜像 顺时针打印矩阵 栈的压入、弹出序列 二叉搜索树的后序遍历序列 二叉树中和为某一值的路径 数组中出现次数超过一半的数字 最小的k个数 连续子数组的最大和 第一个只出现一次的字符 两个链表的第一个公共结点 链表中环的入口结点 二叉树的镜像 跳台阶 变态跳台阶 矩形覆盖 从上往下打印二叉树 二叉搜索树的第K个结点 二. 计算平台1. Hadoop 数据倾斜问题 hive开窗函数 hive UDF UDAFMapreduce原理 MR的Shuffle过程 Yarn的工作机制，以及MR Job提交运行过程 MapReduce1的工作机制和过程 HDFS写入过程 Fsimage 与 EditLog定义及合并过程 HDFS读过程 HDFS简介 在向HDFS中写数据的时候，当写某一副本时出错怎么处理？ namenode的HA实现 简述联邦HDFS HDFS源码解读–create() NameNode高可用中editlog同步的过程 HDFS写入过程客户端奔溃怎么处理？（租约恢复） 2. Hive Hive内部表与外部表的区别 Hive与传统数据库的区别 Hiverc文件 Hive分区 Hive分区过多有何坏处以及分区时的注意事项 Hive中复杂数据类型的使用好处与坏处 hive分桶？ Hive元数据库是用来做什么的，存储哪些信息？ 为何不使用Derby作为元数据库？ Hive什么情况下可以避免进行mapreduce？ Hive连接？ Hive MapJoin? Hive的sort by, order by, distribute by, cluster by区别？ Hadoop计算框架特性 Hive优化常用手段 数据倾斜整理(转) 使用Hive如何进行抽样查询？ 3. Spark Spark的运行模式 RDD是如何容错的？ Spark和MapReduce的区别 说一下Spark的RDD 自己实现一个RDD，需要实现哪些函数或者部分？ MapReduce和Spark的区别 Spark的Stage是怎么划分的？如何优化？ 宽依赖与窄依赖区别 Spark性能调优 Flink、Storm与Spark Stream的区别（未） 说下spark中的transform和action RDD、DataFrame和DataSet的区别 Spark执行任务流程（standalone、yarn） Spark的数据容错机制 Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？ Spark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？以及要注意的地方 driver的功能是什么？ spark端口 RDD有哪几种创建方式 map和flatmap的区别 Spark的基本工作流程 4. Flink4.1 Flink核心概念和基础 第一部分：Flink 中的核心概念和基础篇，包含了 Flink 的整体介绍、核心概念、算子等考察点。 第二部分：Flink 进阶篇，包含了 Flink 中的数据传输、容错机制、序列化、数据热点、反压等实际生产环境中遇到的问题等考察点。 第三部分：Flink 源码篇，包含了 Flink 的核心代码实现、Job 提交流程、数据交换、分布式快照机制、Flink SQL 的原理等考察点。 5. Storm：Storm的可靠性如何实现？包括spout和bolt两部分 怎么提高Storm的并发度？ Storm如何处理反压机制？ Storm中的Stream grouping有哪几种方式？ Storm的组件介绍 Storm怎么完成对单词的计数？ 简述Strom的计算结构 6. kafka：kafka介绍 Kafka与传统消息队列的区别？ kafka的零拷贝 kafka消息持久化和顺序读写？ 7. Kylin简介Kylin Kylin的工作原理 Kylin的技术框架 Cube、Cuboid 和 Cube Segment Kylin 对维度表的的要求 Cube的构建过程 全量构建和增量构建的区别 流式构建原理 三. 数据库 1）两大引擎Innodb引擎和MyIASM引擎， 2）mysql索引原理和底层实现BTREE、B+ TREE四. 数据仓库​ 1）拉链表​ 2）星型模型和雪花模型​ 3）维度建模过程 五. 操作系统 1）线程和进程，进程间的通信方式 2）死锁 3）内存分页 4）同步异步阻塞 六. 计算机网络 简述TCP和UDP的区别 七层协议每一层的任务及作用 简述http状态码 简述http协议与https协议 简述SSL协议 解析DNS过程 三次握手，四次挥手的过程？？为什么三握？ 七. Linux1. 比较常用Linux指令 1.1、ls/ll、cd、mkdir、rm-rf、cp、mv、ps -ef | grep xxx、kill、free-m、tar -xvf file.tar、（说那么十几二十来个估计差不多了） 2. 进程相关查看进程 2.1、ps -ef | grep xxx 2.2、ps -aux | grep xxx（-aux显示所有状态） 杀掉进程 3.1、kill -9[PID] —(PID用查看进程的方式查找) 4、启动/停止服务 4.1、cd到bin目录cd/ 4.2、./startup.sh –打开（先确保有足够的权限） 4.3、./shutdown.sh —关闭 5、查看日志 5.1、cd到服务器的logs目录（里面有xx.out文件） 5.2、tail -f xx.out –此时屏幕上实时更新日志。ctr+c停止 5.3、查看最后100行日志 tail -100 xx.out 5.4、查看关键字附件的日志。如：cat filename | grep -C 5 ‘关键字’（关键字前后五行。B表示前，A表示后，C表示前后） —-使用不多**** 5.5、还有vi查询啥的。用的也不多。 6、查看端口：（如查看某个端口是否被占用） 6.1、netstat -anp | grep 端口号（状态为LISTEN表示被占用） 7、查找文件 7.1、查找大小超过xx的文件： find . -type f -size +xxk —–(find . -type f -mtime -1 -size +100k -size-400k)–查区间大小的文件 7.2、通过文件名：find / -name xxxx —整个硬盘查找 其余的基本上不常用 8、vim（vi）编辑器 有命令模式、输入模式、末行模式三种模式。 命令模式：查找内容(/abc、跳转到指定行(20gg)、跳转到尾行(G)、跳转到首行(gg)、删除行(dd)、插入行(o)、复制粘贴(yy,p) 输入模式：编辑文件内容 末行模式：保存退出(wq)、强制退出(q!)、显示文件行号(set number) 在命令模式下，输入a或i即可切换到输入模式，输入冒号(:)即可切换到末行模式；在输入模式和末行模式下，按esc键切换到命令模式 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[]},{"title":"IDEA环境下Git使用总结","slug":"IDEA下使用git操作","date":"2020-04-20T13:40:00.000Z","updated":"2020-04-20T04:13:12.538Z","comments":true,"path":"2020/04/20/idea-xia-shi-yong-git-cao-zuo/","link":"","permalink":"https://dataquaner.github.io/2020/04/20/idea-xia-shi-yong-git-cao-zuo/","excerpt":"","text":"目录工作中多人使用版本控制软件协作开发，常见的应用场景归纳如下： 假设小组中有两个人，组长小张，组员小袁 [TOC] 场景一：小张创建项目并提交到远程Git仓库创建好项目，选择VCS - &gt; Import into Version Control -&gt; Create Git Repository 接下来指定本地仓库的位置，按个人习惯指定即可，例如这里选择了项目源代码同目录 点击OK后创建完成本地仓库，注意，这里仅仅是本地的。下面把项目源码添加到本地仓库。 下图是Git与提交有关的三个命令对应的操作，Add命令是把文件从IDE的工作目录添加到本地仓库的stage区，Commit命令把stage区的暂存文件提交到当前分支的仓库，并清空stage区。Push命令把本地仓库的提交同步到远程仓库。 IDEA中对操作做了一定的简化，Commit和Push可以在一步中完成。 具体操作，在项目上点击右键，选择Git菜单 因为是第一次提交，Push前需要指定远程仓库的地址。如下图，点击Define remote后，在弹出的窗口中输入远程仓库地址。 场景二：小袁从远程Git仓库上获取项目源码即克隆项目，操作如下： 输入小张Push时填写的远程仓库地址 接下来按向导操作，即可把项目从远程仓库克隆到本地仓库和IDE工作区。 场景三：小袁修改了部分源码，提交到远程仓库这个操作和首次提交的流程基本一致，分别是 Add -&gt; Commit -&gt; Push。请参考场景一 场景四：小张从远程仓库获取小袁的提交获取更新有两个命令：Fetch和Pull，Fetch是从远程仓库下载文件到本地的origin/master，然后可以手动对比修改决定是否合并到本地的master库。Push则是直接下载并合并。如果各成员在工作中都执行修改前先更新的规范，则可以直接使用Pull方式以简化操作。 场景五：小袁接受了一个新功能的任务，创建了一个分支并在分支上开发建分支也是一个常用的操作，例如临时修改bug、开发不确定是否加入的功能等，都可以创建一个分支，再等待合适的时机合并到主干。 创建流程如下： 选择New Branch并输入一个分支的名称 创建完成后注意IDEA的右下角，如下图，Git: wangpangzi_branch表示已经自动切换到wangpangzi_branch分支，当前工作在这个分支上。 点击后弹出一个小窗口，在Local Branches中有其他可用的本地分支选项，点击后选择Checkout即可切换当前工作的分支。 如下图，点击Checkout 注意，这里创建的分支仅仅在本地仓库，如果想让组长小张获取到这个分支，还需要提交到远程仓库。 场景六：小袁把分支提交到远程Git仓库切换到新建的分支，使用Push功能 场景七：小张获取小袁提交的分支使用Pull功能打开更新窗口，点击Remote栏后面的刷新按钮，会在Branches to merge栏中刷新出新的分支。这里并不想做合并，所以不要选中任何分支，直接点击Pull按钮完成操作。 更新后，再点击右下角，可以看到在Remote Branches区已经有了新的分支，点击后在弹出的子菜单中选择Checkout as new local branch，在本地仓库中创建该分支。完成后在Local Branches区也会出现该分支的选项，可以按上面的方法，点击后选择Checkout切换。 场景八：小张把分支合并到主干新功能开发完成，体验很好，项目组决定把该功能合并到主干上。 切换到master分支，选择Merge Changes 选择要合并的分支，点击Merge完成 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Git","slug":"Git","permalink":"https://dataquaner.github.io/categories/Git/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"https://dataquaner.github.io/tags/IDEA/"},{"name":"Git","slug":"Git","permalink":"https://dataquaner.github.io/tags/Git/"}]},{"title":"Flink面试问题梳理(基础+进阶+源码)","slug":"Flink面试问题梳理基础进阶源码","date":"2020-04-18T13:40:00.000Z","updated":"2020-04-18T14:39:18.053Z","comments":true,"path":"2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/","link":"","permalink":"https://dataquaner.github.io/2020/04/18/flink-mian-shi-wen-ti-shu-li-ji-chu-jin-jie-yuan-ma/","excerpt":"","text":"第一部分：Flink 面试基础篇1. 简单介绍一下 Flink​ Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。 ​ Flink提供了诸多高抽象层的API以便用户编写分布式任务： DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。 DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。 Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。 ​ 此外，Flink 还针对特定的应用领域提供了领域库，例如：Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。 根据官网的介绍，Flink 的特性包含： 支持高吞吐、低延迟、高性能的流处理支持带有事件时间的窗口 （Window） 操作支持有状态计算的 Exactly-once 语义支持高度灵活的窗口 （Window） 操作，支持基于 time、count、session 以及 data-driven 的窗口操作支持具有 Backpressure 功能的持续流模型支持基于轻量级分布式快照（Snapshot）实现的容错一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理Flink 在 JVM 内部实现了自己的内存管理支持迭代计算支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存 2. Flink 相比传统的 Spark Streaming 有什么区别?​ 这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来： Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。 下面我们就分几个方面介绍两个框架的主要区别： [1] 架构模型​ Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。 [2] 任务调度​ Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。 ​ Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。 [3] 时间机制​ Spark Streaming 支持的时间机制有限，只支持处理时间。Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。 [4] 容错机制​ 对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。 ​ Flink 则使用两阶段提交协议来解决这个问题。 3. Flink 的组件栈有哪些？​ 根据 Flink 官网描述，Flink 是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。 图片来源于：https://flink.apache.org ​ 自下而上，每一层分别代表： Deploy 层：该层主要涉及了Flink的部署模式，在上图中我们可以看出，Flink 支持包括local、Standalone、Cluster、Cloud等多种部署模式。 Runtime 层：Runtime层提供了支持 Flink 计算的核心实现，比如：支持分布式 Stream 处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。 API层：API 层主要实现了面向流（Stream）处理和批（Batch）处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API，后续版本，Flink有计划将DataStream和DataSet API进行统一。 Libraries层：该层称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。 4. Flink 的运行必须依赖 Hadoop组件吗？​ Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadoop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点。 5. 你们的Flink集群规模多大？​ 大家注意，这个问题看起来是问你实际应用中的Flink集群规模，其实还隐藏着另一个问题：Flink可以支持多少节点的集群规模？ ​ 在回答这个问题时候，可以将自己生产环节中的集群规模、节点、内存情况说明，同时说明部署模式（一般是Flink on Yarn），除此之外，用户也可以同时在小集群（少于5个节点）和拥有 TB 级别状态的上千个节点上运行 Flink 任务。 6. Flink的基础编程模型了解吗？ ​ 上图是来自Flink官网的运行流程图。通过上图我们可以得知，Flink 程序的基本构建是数据输入来自一个 Source，Source 代表数据的输入端，经过 Transformation 进行转换，然后在一个或者多个Sink接收器中结束。数据流（stream）就是一组永远不会停止的数据记录流，而转换（transformation）是将一个或多个流作为输入，并生成一个或多个输出流的操作。执行时，Flink程序映射到 streaming dataflows，由流（streams）和转换操作（transformation operators）组成。 7. Flink集群有哪些角色？各自有什么作用？ ​ Flink 程序在运行时主要有 TaskManager，JobManager，Client三种角色。其中JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。 ​ TaskManager是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。 ​ Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。 8. 说说 Flink 资源管理中 Task Slot 的概念 ​ 在Flink架构角色中我们提到，TaskManager是实际负责执行计算的Worker，TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 Task Slot 的概念。 ​ 简单的说，TaskManager会将自己节点上管理的资源分为不同的Slot：固定大小的资源子集。这样就避免了不同Job的Task互相竞争内存资源，但是需要主要的是，Slot只会做内存的隔离。没有做CPU的隔离。 9. 说说 Flink 的常用算子？​ Flink 最常用的常用算子包括：Map：DataStream → DataStream，输入一个参数产生一个参数，map的功能是对输入的参数进行转换操作。Filter：过滤掉指定条件的数据。KeyBy：按照指定的key进行分组。Reduce：用来进行结果汇总合并。Window：窗口函数，根据某些特性将每个key的数据进行分组（例如：在5s内到达的数据） 10. 说说你知道的Flink分区策略？​ 什么要搞懂什么是分区策略。分区策略是用来决定数据如何发送至下游。目前 Flink 支持了8中分区策略的实现。 上图是整个Flink实现的分区策略继承图： GlobalPartitioner数据会被分发到下游算子的第一个实例中进行处理。 ShufflePartitioner数据会被随机分发到下游算子的每一个实例中进行处理。 RebalancePartitioner数据会被循环发送到下游的每一个实例中进行处理。 RescalePartitioner这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为2，编号为A和B。下游并行度为4，编号为1，2，3，4。那么A则把数据循环发送给1和2，B则把数据循环发送给3和4。假设上游并行度为4，编号为A，B，C，D。下游并行度为2，编号为1，2。那么A和B则把数据发送给1，C和D则把数据发送给2。 BroadcastPartitioner广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做join的场景。 ForwardPartitioner 用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。 KeyGroupStreamPartitioner Hash分区器。会将数据按 Key 的 Hash 值输出到下游算子实例中。 CustomPartitionerWrapper用户自定义分区器。需要用户自己实现Partitioner接口，来定义自己的分区逻辑。 11. Flink的并行度了解吗？Flink的并行度设置是怎样的？​ Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。这些并行实例的数量被称为并行度。 ​ 我们在实际生产环境中可以从四个不同层面设置并行度： 操作算子层面(Operator Level) 执行环境层面(Execution Environment Level) 客户端层面(Client Level) 系统层面(System Level) 需要注意的优先级：算子层面&gt;环境层面&gt;客户端层面&gt;系统层面。 12. Flink的Slot和parallelism有什么区别？官网上十分经典的图： slot是指taskmanager的并发执行能力，假设我们将 taskmanager.numberOfTaskSlots 配置为3那么每一个 taskmanager 中分配3个 TaskSlot, 3个 taskmanager 一共有9个TaskSlot。 parallelism是指taskmanager实际使用的并发能力。假设我们把 parallelism.default 设置为1，那么9个 TaskSlot 只能用1个，有8个空闲。 13. Flink有没有重启策略？说说有哪几种？Flink 实现了多种重启策略。 固定延迟重启策略（Fixed Delay Restart Strategy） 故障率重启策略（Failure Rate Restart Strategy） 没有重启策略（No Restart Strategy） Fallback重启策略（Fallback Restart Strategy） 14. 用过Flink中的分布式缓存吗？如何使用？​ Flink实现的分布式缓存和Hadoop有异曲同工之妙。目的是在本地读取文件，并把他放在 taskmanager 节点中，防止task重复拉取。 val env = ExecutionEnvironment.getExecutionEnvironment // register a file from HDFS env.registerCachedFile(\"hdfs:///path/to/your/file\", \"hdfsFile\") // register a local executable file (script, executable, ...) env.registerCachedFile(\"file:///path/to/exec/file\", \"localExecFile\", true) // define your program and execute ... val input: DataSet[String] = ... val result: DataSet[Integer] = input.map(new MyMapper()) ... env.execute() 15. 说说Flink中的广播变量，使用时需要注意什么？​ 我们知道Flink是并行的，计算过程可能不在一个 Slot 中进行，那么有一种情况即：当我们需要访问同一份数据。那么Flink中的广播变量就是为了解决这种情况。 ​ 我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。 16. 说说Flink中的窗口？​ 来一张官网经典的图： ​ Flink 支持两种划分窗口的方式，按照time和count。如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window。 ​ flink支持窗口的两个重要属性（size和interval） ​ 如果size=interval,那么就会形成tumbling-window(无重叠数据)如果size&gt;interval,那么就会形成sliding-window(有重叠数据)如果size&lt; interval, 那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。 ​ 通过组合可以得出四种基本窗口： time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5)) time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3)) count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5) count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3) 17. 说说Flink中的状态存储？​ Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和 checkpoint 交互。 ​ Flink提供了三种状态存储方式：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。 18. Flink 中的时间有哪几类​ Flink 中的时间和其他流式计算系统的时间一样分为三类：事件时间，摄入时间，处理时间三种。 如果以 EventTime 为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime。 如果以 IngesingtTime 为基准来定义时间窗口将形成 IngestingTimeWindow,以 source 的systemTime为准。 如果以 ProcessingTime 基准来定义时间窗口将形成 ProcessingTimeWindow，以 operator 的systemTime 为准。 19. Flink 中水印是什么概念，起到什么作用？​ Watermark 是 Apache Flink 为了处理 EventTime 窗口计算提出的一种机制, 本质上是一种时间戳。一般来讲Watermark经常和Window一起被用来处理乱序事件。 20. Flink Table &amp; SQL 熟悉吗？TableEnvironment这个类有什么作用​ TableEnvironment是Table API和SQL集成的核心概念。 这个类主要用来： 在内部catalog中注册表 注册外部catalog 执行SQL查询 注册用户定义（标量，表或聚合）函数 将DataStream或DataSet转换为表 持有对ExecutionEnvironment或StreamExecutionEnvironment的引用 21. Flink SQL的实现原理是什么？ 是如何实现 SQL 解析的呢？​ 首先大家要知道 Flink 的SQL解析是基于Apache Calcite这个开源框架。 基于此，一次完整的SQL解析过程如下： 用户使用对外提供Stream SQL的语法开发业务应用 用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划 采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划 对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行 第二部分：Flink 面试进阶篇1. Flink是如何支持批流一体的？ ​ 本道面试题考察的其实就是一句话：Flink的开发者认为批处理是流处理的一种特殊情况。批处理是有限的流处理。Flink 使用一个引擎支持了DataSet API 和 DataStream API。 2. Flink是如何做到高效的数据交换的？​ 在一个Flink Job中，数据需要在不同的task中进行交换，整个数据交换是有 TaskManager 负责的，TaskManager 的网络组件首先从缓冲buffer中收集records，然后再发送。Records 并不是一个一个被发送的，二是积累一个批次再发送，batch 技术可以更加高效的利用网络资源。 3. Flink是如何做容错的？​ Flink 实现容错主要靠强大的CheckPoint机制和State机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态。 4. Flink 分布式快照的原理是什么？​ Flink的分布式快照是根据Chandy-Lamport算法量身定做的。简单来说就是持续创建分布式数据流及其状态的一致快照。 ​ 核心思想是在 input source 端插入 barrier，控制 barrier 的同步来实现 snapshot 的备份和 exactly-once 语义。 5. Flink 是如何保证Exactly-once语义的？​ Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。分为以下几个步骤： 开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面 预提交（preCommit）将内存中缓存的数据写入文件并关闭 正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟 丢弃（abort）丢弃临时文件 ​ 若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。 6. Flink 的 kafka 连接器有什么特别的地方？​ Flink源码中有一个独立的connector模块，所有的其他connector都依赖于此模块，Flink 在1.9版本发布的全新kafka连接器，摒弃了之前连接不同版本的kafka集群需要依赖不同版本的connector这种做法，只需要依赖一个connector即可。 7. 说说 Flink的内存管理是如何做的?​ Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。 理论上Flink的内存管理分为三部分： Network Buffers：这个是在TaskManager启动的时候分配的，这是一组用于缓存网络数据的内存，每个块是32K，默认分配2048个，可以通过“taskmanager.network.numberOfBuffers”修改 Memory Manage pool：大量的Memory Segment块，用于运行时的算法（Sort/Join/Shuffle等），这部分启动的时候就会分配。下面这段代码，根据配置文件中的各种参数来计算内存的分配方法。（heap or off-heap，这个放到下节谈），内存的分配支持预分配和lazy load，默认懒加载的方式。 User Code，这部分是除了Memory Manager之外的内存用于User code和TaskManager本身的数据结构。 8. 说说 Flink的序列化如何做的?​ Java本身自带的序列化和反序列化的功能，但是辅助信息占用空间比较大，在序列化对象时记录了过多的类信息。 ​ Apache Flink摒弃了Java原生的序列化方法，以独特的方式处理数据类型和序列化，包含自己的类型描述符，泛型类型提取和类型序列化框架。 ​ TypeInformation 是所有类型描述符的基类。它揭示了该类型的一些基本属性，并且可以生成序列化器。TypeInformation 支持以下几种类型： BasicTypeInfo: 任意Java 基本类型或 String 类型 BasicArrayTypeInfo: 任意Java基本类型数组或 String 数组 WritableTypeInfo: 任意 Hadoop Writable 接口的实现类 TupleTypeInfo: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现 CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples) PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法 GenericTypeInfo: 任意无法匹配之前几种类型的类 ​ 针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。 9. Flink中的Window出现了数据倾斜，你有什么解决办法？​ window产生数据倾斜指的是数据在不同的窗口内堆积的数据量相差过多。本质上产生这种情况的原因是数据源头发送的数据量速度不同导致的。出现这种情况一般通过两种方式来解决： 在数据进入窗口前做预聚合 重新设计窗口聚合的key 10. Flink中在使用聚合函数 GroupBy、Distinct、KeyBy 等函数时出现数据热点该如何解决？​ 数据倾斜和数据热点是所有大数据框架绕不过去的问题。处理这类问题主要从3个方面入手： 在业务上规避这类问题 ​ 例如一个假设订单场景，北京和上海两个城市订单量增长几十倍，其余城市的数据量不变。这时候我们在进行聚合的时候，北京和上海就会出现数据堆积，我们可以单独数据北京和上海的数据。 Key的设计上 ​ 把热key进行拆分，比如上个例子中的北京和上海，可以把北京和上海按照地区进行拆分聚合。 参数设置 ​ Flink 1.9.0 SQL(Blink Planner) 性能优化中一项重要的改进就是升级了微批模型，即 MiniBatch。原理是缓存一定的数据后再触发处理，以减少对State的访问，从而提升吞吐和减少数据的输出量。 11. Flink任务延迟高，想解决这个问题，你会如何入手？​ 在Flink的后台任务管理中，我们可以看到Flink的哪个算子和task出现了反压。最主要的手段是资源调优和算子调优。资源调优即是对作业中的Operator的并发数（parallelism）、CPU（core）、堆内存（heap_memory）等参数进行调优。作业参数调优包括：并行度的设置，State的设置，checkpoint的设置。 12. Flink是如何处理反压的？​ Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。下游消费者消费变慢，上游就会受到阻塞。 13. Flink的反压和Storm有哪些不同？​ Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。 ​ Flink中的反压使用了高效有界的分布式阻塞队列，下游消费变慢会导致发送端阻塞。 ​ 二者最大的区别是Flink是逐级反压，而Storm是直接从源头降速。 14. Operator Chains（算子链）这个概念你了解吗？​ 为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。这就是我们所说的算子链。 15. Flink什么情况下才会把Operator chain在一起形成算子链？​ 两个operator chain在一起的的条件： 上下游的并行度一致 下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入） 上下游节点都在同一个 slot group 中（下面会解释 slot group） 下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS） 上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD） 两个节点间数据分区方式是 forward（参考理解数据流的分区） 用户没有禁用 chain 16. 说说Flink1.9的新特性？ 支持hive读写，支持UDF Flink SQL TopN和GroupBy等优化 Checkpoint跟savepoint针对实际业务场景做了优化 Flink state查询 17. 消费kafka数据的时候，如何处理脏数据？ 可以在处理前加一个fliter算子，将不符合规则的数据过滤出去。 第三部分：Flink 面试源码篇1. Flink Job的提交流程​ 用户提交的Flink Job会被转化成一个DAG任务运行，分别是：StreamGraph、JobGraph、ExecutionGraph，Flink中JobManager与TaskManager，JobManager与Client的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。 2. Flink所谓”三层图”结构是哪几个”图”？ 一个Flink任务的DAG生成计算图大致经历以下三个过程： StreamGraph最接近代码所表达的逻辑层面的计算拓扑结构，按照用户代码的执行顺序向StreamExecutionEnvironment添加StreamTransformation构成流式图。 JobGraph从StreamGraph生成，将可以串联合并的节点进行合并，设置节点之间的边，安排资源共享slot槽位和放置相关联的节点，上传任务所需的文件，设置检查点配置等。相当于经过部分初始化和优化处理的任务图。 ExecutionGraph由JobGraph转换而来，包含了任务具体执行所需的内容，是最贴近底层实现的执行图。 3. JobManger在集群中扮演了什么角色？​ JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。 ​ JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务管理和资源管理。 ​ JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。 ​ 同时在任务执行的过程中，Flink JobManager 会触发 Checkpoint 操作，每个 TaskManager 节点 收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Fink JobManager 中完成。 ​ 当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。 4. JobManger在集群启动过程中起到什么作用？​ JobManager的职责主要是接收Flink作业，调度Task，收集作业状态和管理TaskManager。它包含一个Actor，并且做如下操作： RegisterTaskManager: 它由想要注册到JobManager的TaskManager发送。注册成功会通过AcknowledgeRegistration消息进行Ack。 SubmitJob: 由提交作业到系统的Client发送。提交的信息是JobGraph形式的作业描述信息。 CancelJob: 请求取消指定id的作业。成功会返回CancellationSuccess，否则返回CancellationFailure。 UpdateTaskExecutionState: 由TaskManager发送，用来更新执行节点(ExecutionVertex)的状态。成功则返回true，否则返回false。 RequestNextInputSplit: TaskManager上的Task请求下一个输入split，成功则返回NextInputSplit，否则返回null。 JobStatusChanged： 它意味着作业的状态(RUNNING, CANCELING, FINISHED,等)发生变化。这个消息由ExecutionGraph发送。 5. TaskManager在集群中扮演了什么角色？​ TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。 ​ 客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。 ​ TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。 ​ 可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进行的方式有很大的区别，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。 6. TaskManager在集群启动过程中起到什么作用？​ TaskManager的启动流程较为简单：启动类：org.apache.flink.runtime.taskmanager.TaskManager核心启动方法 ： selectNetworkInterfaceAndRunTaskManager启动后直接向JobManager注册自己，注册完成后，进行部分模块的初始化。 7. Flink 计算资源的调度是如何实现的？​ TaskManager中最细粒度的资源是Task slot，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。 ​ 通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。 ​ 而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot： 8. 简述Flink的数据抽象及数据交换过程？​ Flink 为了避免JVM的固有缺陷例如java对象存储密度低，FGC影响吞吐和响应等，实现了自主管理内存。MemorySegment就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。 ​ 在MemorySegment这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是Buffer。 ​ 对接从Java对象转为Buffer的中间对象是另一个抽象StreamRecord。 9. Flink 中的分布式快照机制是如何实现的？​ Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。 ​ barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。 ​ 然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。 一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。在所有sink确认快照后，意味快照着已完成。 ​ 一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。 10. 简单说说FlinkSQL的是如何实现的？​ Flink 将 SQL 校验、SQL 解析以及 SQL 优化交给了Apache Calcite。Calcite 在其他很多开源项目里也都应用到了，譬如 Apache Hive, Apache Drill, Apache Kylin, Cascading。Calcite 在新的架构中处于核心的地位，如下图所示。 构建抽象语法树的事情交给了 Calcite 去做。SQL query 会经过 Calcite 解析器转变成 SQL 节点树，通过验证后构建成 Calcite 的抽象语法树（也就是图中的 Logical Plan）。另一边，Table API 上的调用会构建成 Table API 的抽象语法树，并通过 Calcite 提供的 RelBuilder 转变成 Calcite 的抽象语法树。然后依次被转换成逻辑执行计划和物理执行计划。 在提交任务后会分发到各个 TaskManager 中运行，在运行时会使用 Janino 编译器编译代码后运行。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/categories/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://dataquaner.github.io/tags/Flink/"},{"name":"面试必备","slug":"面试必备","permalink":"https://dataquaner.github.io/tags/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87/"}]},{"title":"机器学习系列之决策树算法（03）：决策树的剪枝","slug":"机器学习系列之决策树算法（03）：决策树的剪枝","date":"2020-04-11T11:32:09.079Z","updated":"2020-04-11T11:32:09.079Z","comments":true,"path":"2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-03-jue-ce-shu-de-jian-zhi/","excerpt":"","text":"1. 前言上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现过拟合现象。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为剪枝(pruning)。 具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss funtion)或代价函数(cost funtion)来实现。 2.剪枝算法2.1 为什么要剪枝现象 接上一次讲的生成决策树，下面给出一张图。 横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。 实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。 可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。 原因 原因1：噪声、样本冲突，即错误的样本数据。 原因2：特征即属性不能完全作为分类标准。 原因3：巧合的规律性，数据量不够大。 这个时候，就需要对生成树进行修剪，也就是剪枝。 2.2 如何进行剪枝预剪枝预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法: (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。 (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。 (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。 (4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。 优点&amp;缺点 由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【怎样精确地估计何时停止树的增长是相当困难的】。 预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。 后剪枝后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。 这里主要介绍四种： REP-错误率降低剪枝 PEP-悲观剪枝 CCP-代价复杂度剪枝 MEP-最小错误剪枝 REP(Reduced Error Pruning)方法 对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S 优点： REP 是当前最简单的事后剪枝方法之一。 它的计算复杂性是线性的。 和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。 缺点： 但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意. PEP(Pessimistic Error Pruning)方法 为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, PEP 方法对误差估计增加了连续性校正(continuity correction)。关于PEP方法的数据解释待后续开专题梳理。 优点： PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一 PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。 TIPS： 个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。 MEP(Minimum Error Pruning)方法 MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。 优点： MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集. 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： 类别平均分配的前提假设现实几率不大&amp;对K太过敏感 对此，也有改进算法，我没有深入研究。 CCP(Cost-Complexity Pruning)方法 CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤: (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。 ​ (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。 缺点： 生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的. 对比四种方法 剪枝名称 剪枝方式 计算复杂度 误差估计 REP 自底向上 0(n) 剪枝集上误差估计 PEP 自顶向下 o(n) 使用连续纠正 CCP 自底向上 o(n2) 标准误差 MEP 自底向上 o(n) 使用连续纠正 ① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。 ② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。 ③ 如果数据集多（REP&amp;CCP←复杂但树小） ④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定） 3.总结决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。 剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。 预剪枝： 预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。 简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。 这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。 后剪枝： 后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。 同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。 后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。 剪枝方法的选择 如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。 后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。 有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。 经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。 无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。 函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点 函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。 函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值 上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"机器学习系列之决策树算法（02）：决策树的生成","slug":"机器学习系列之决策树算法（02）：决策树的生成","date":"2020-04-11T11:31:53.052Z","updated":"2020-04-11T11:31:53.052Z","comments":true,"path":"2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-02-jue-ce-shu-de-sheng-cheng/","excerpt":"","text":"1. 前言上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。 2. 决策树的生成 示例数据表格 文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中 2.1 相关概念阐述2.1.1 决策树 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。 2.1.2 ID3、C4.5、CART在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【机器学习系列之决策树算法（01）：决策树特征选择】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。 信息熵 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。 ​ Entopy(t)=-Σp(i|t)log2p(i|t) 信息增益信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为： ​ Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di) 信息增益率 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7： ​ H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) 基尼系数 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下基尼系数 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低. CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分. 基尼系数的计算公式如下: ​ Gini = 1 – Σ (Pi)2 for i=1 to number of classes 2.2 完整生成过程 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例 2.2.1 根节点的选择 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为： ​ Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985 天气 其数据表格如下: 信息增益计算如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下： D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981 D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为： 3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965 其信息增益为： Gain(天气)=0.985 - 0.965 = 0.020 信息增益率计算 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为： H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556 则其信息增益率为： Gain_ratio(天气)=0.020/1.556=0.012 基尼系数计算 Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9 Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21 温度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 作为子节点的归一化信息熵为： 4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857 其信息增益为： Gain(温度)=0.985 - 0.857 = 0.128 信息增益率计算 属性熵为： H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378 则其信息增益率为： Gain_ratio(温度)=0.128/1.378=0.0928 基尼系数计算 Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 4/7 * 1.0 + 3/7 * 0.918 = 0.964 其信息增益为： Gain(湿度)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(湿度)=0.021/0.985=0.021 基尼系数计算 Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 作为子节点的归一化信息熵为： 3/7 * 1.0 + 4/7 * 0.918 = 0.964 其信息增益为： Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数计算 Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 根节点的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度 信息增益 Gain(天气)=0.985 - 0.965 = 0.020 Gain(温度)=0.985 - 0.857 = 0.128 Gain(湿度)=0.985 - 0.964 = 0.021 Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率 Gain_ratio(天气)=0.020/1.556=0.012 Gain_ratio(温度)=0.128/1.378=0.0928 Gain_ratio(湿度)=0.021/0.985=0.021 Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择: 根节点 子节点温度高:(待进一步进行选择) 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.2 子节点温度高的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： ​ Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 归一化信息熵为： 2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5 其信息增益为： Gain(天气)=1.0 - 0.5 = 0.5 信息增益率计算 对应的属性熵为： H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5 则其信息增益率为： Gain_ratio(天气)=0.5/1.5=0.33333 基尼系数计算 Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0 D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0 作为子节点的归一化信息熵为： 2/4 * 0.0 + 2/4 * 0.0 = 0.0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0 Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0 D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 1/4 * 0.0 + 3/4 * 0.918 = 0.688 其信息增益为： Gain(刮风)=1.0 - 0.688 = 0.312 信息增益率计算 属性熵为： H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918 则其信息增益率为： Gain_ratio(刮风)=0.312/0.918=0.349 基尼系数计算 Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 子节点温度高的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度 Gain(天气)=1.0 - 0.5 = 0.5 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 0.688 = 0.312 Gain_ratio(天气)=0.5/1.5=0.33333 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0.312/0.918=0.349 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.3 子节点温度中的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D (阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(天气)=1.0 - 0 = 1.0 信息增益率计算 对应的属性熵为： H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(天气)=1.0/1.0=1.0 基尼系数计算 Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 作为子节点的归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 作为子节点的归一化信息熵为： 1/1 * 1.0 = 1.0 其信息增益为： Gain(刮风)=1.0 - 1.0 = 0 信息增益率计算 属性熵为： H(刮风)=-(2/2 * log2(2/2) = 0.0 则其信息增益率为： Gain_ratio(刮风)=0/0 = 0 基尼系数计算 Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5 Gini(刮风)=2/2 * 0.5 = 0.5 子节点温度中的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧 Gain(天气)=1.0 - 0 = 1.0 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 1.0 = 0 Gain_ratio(天气)=1.0/1.0=1.0 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0/0 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(刮风)=2/2 * 0.5 = 0.5 确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.4 最终的决策树 在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树： 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 3. 思考 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？ 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"数据存储之MySQL系列（01）：MySQL体系结构","slug":"数据存储之MySQL系列（01）：MySQL体系结构","date":"2020-04-11T11:31:10.255Z","updated":"2020-04-11T11:31:10.255Z","comments":true,"path":"2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/shu-ju-cun-chu-zhi-mysql-xi-lie-01-mysql-ti-xi-jie-gou/","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"DataBase","slug":"DataBase","permalink":"https://dataquaner.github.io/categories/DataBase/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dataquaner.github.io/tags/MySQL/"}]},{"title":"xgboost算法模型输出的解释","slug":"xgboost算模型输出的解释","date":"2020-04-11T11:30:52.923Z","updated":"2020-04-11T11:30:52.923Z","comments":true,"path":"2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/xgboost-suan-mo-xing-shu-chu-de-jie-shi/","excerpt":"","text":"1. 问题描述 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。 2. 数据集 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据. from sklearn import datasets iris = datasets.load_iris() data = iris.data[:100] print data.shape #(100L, 4L) #一共有100个样本数据, 维度为4维 label = iris.target[:100] print label #正好选取label为0和1的数据 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 3. 训练集与测试集from sklearn.cross_validation import train_test_split train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=0) 4. Xgboost建模4.1 模型初始化设置import xgboost as xgb dtrain=xgb.DMatrix(train_x,label=train_y) dtest=xgb.DMatrix(test_x) params={'booster':'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'auc', 'max_depth':4, 'lambda':10, 'subsample':0.75, 'colsample_bytree':0.75, 'min_child_weight':2, 'eta': 0.025, 'seed':0, 'nthread':8, 'silent':1} watchlist = [(dtrain,'train')] 4.2 建模与预测bst=xgb.train(params,dtrain,num_boost_round=100,evals=watchlist) ypred=bst.predict(dtest) # 设置阈值, 输出一些评价指标 y_pred = (ypred >= 0.5)*1 from sklearn import metrics print 'AUC: %.4f' % metrics.roc_auc_score(test_y,ypred) print 'ACC: %.4f' % metrics.accuracy_score(test_y,y_pred) print 'Recall: %.4f' % metrics.recall_score(test_y,y_pred) print 'F1-score: %.4f' %metrics.f1_score(test_y,y_pred) print 'Precesion: %.4f' %metrics.precision_score(test_y,y_pred) metrics.confusion_matrix(test_y,y_pred) Out[23]: AUC: 1.0000 ACC: 1.0000 Recall: 1.0000 F1-score: 1.0000 Precesion: 1.0000 array([[13, 0], [ 0, 12]], dtype=int64) Yeah, 完美的模型, 完美的预测! 4.3 可视化输出#对于预测的输出有三种方式 ?bst.predict Signature: bst.predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False) pred_leaf : bool When this option is on, the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree. Note that the leaf index of a tree is unique per tree, so you may find leaf 1 in both tree 1 and tree 0. pred_contribs : bool When this option is on, the output will be a matrix of (nsample, nfeats+1) with each record indicating the feature contributions (SHAP values) for that prediction. The sum of all feature contributions is equal to the prediction. Note that the bias is added as the final column, on top of the regular features. 4.3.1 得分默认的输出就是得分, 这没什么好说的, 直接上code. ypred = bst.predict(dtest) ypred Out[32]: array([ 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.20081411], dtype=float32) 在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出. 4.3.2 所属的叶子节点当设置pred_leaf=True的时候, 这时就会输出每个样本在所有树中的叶子节点 ypred_leaf = bst.predict(dtest, pred_leaf=True) ypred_leaf Out[33]: array([[1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1]]) 输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以ypred_leaf的维度为[100*100]. 对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树). 那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型. xgb.to_graphviz(bst, num_trees=0) #可视化第一棵树的生成情况 #直接输出模型的迭代工程 bst.dump_model(\"model.txt\") booster[0]: 0:[f3&lt;0.75] yes=1,no=2,missing=1 1:leaf=-0.019697 2:leaf=0.0214286 booster[1]: 0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0212184 2:leaf=0.0212 booster[2]: 0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0197404 2:leaf=0.0197235 booster[3]: …… 通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值. 在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值. 同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值. leaf1 -1.381214 leaf2 1.410950 在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了'objective': 'binary:logistic', 因此使用函数将累加的值转换为实际的打分: f(x)=1/(1+exp(−x)) 1/float(1+np.exp(1.38121416)) Out[24]: 0.20081407112186503 1/float(1+np.exp(-1.410950)) Out[25]: 0.8039157403338895 这就与ypred = bst.predict(dtest) 的分值相对应上了. 4.3.2 特征重要性接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性. ypred_contribs = bst.predict(dtest, pred_contribs=True) ypred_contribs Out[37]: array([[ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663]], dtype=float32) 输出的ypred_contribs的维度为[100,5], 通过阅读前面的文档注释就可以知道, 最后一列是bias, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的. 通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例. score_a = sum(ypred_contribs[0]) print score_a # -1.38121373579 score_b = sum(ypred_contribs[1]) print score_b # 1.41094945744 相同的分值, 相同的处理情况. 到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"LightGBM算法基础系列之基础理论篇（1）","slug":"LightGBM算法基础系列之基础理论篇（1）","date":"2020-04-11T11:30:37.965Z","updated":"2020-04-11T11:30:37.965Z","comments":true,"path":"2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/lightgbm-suan-fa-ji-chu-xi-lie-zhi-ji-chu-li-lun-pian-1/","excerpt":"","text":"这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"零基础自学人工智能路径规划，附资源，亲身经验","slug":"零基础自学人工智能路径规划附资源亲身经验","date":"2020-04-11T01:25:00.000Z","updated":"2020-04-11T12:08:44.555Z","comments":true,"path":"2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/","link":"","permalink":"https://dataquaner.github.io/2020/04/11/ling-ji-chu-zi-xue-ren-gong-zhi-neng-lu-jing-gui-hua-fu-zi-yuan-qin-shen-jing-yan/","excerpt":"","text":"0. 前言下面的每个资源都是我亲身学过的，且是网上公开公认最优质的资源。 下面的每个学习步骤也是我一步步走过来的。希望大家以我为参考，少走弯路。 请大家不要浪费时间找非常多的资料，只看最精华的！ 综述，机器学习的自学简单来说分为三个步骤 前期：知识储备包括数学知识，机器学习经典算法知识，编程技术（python）的掌握 中期：算法的代码实现 后期：实战水平提升 机器学习路径规划图 1. 数学基础很多人看到数学知识的时候就望而却步，数学是需要的，但是作为入门水平，对数学的要求没有那么的高。假设你上过大学的数学课（忘了也没事），需要的数学知识啃一啃还是基本能理解下来的。 1.1 数学内容线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然(MLE)和最大后验估计(MAP)等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的如梯度下降、牛顿法等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等 上面的看不太懂没事，不是特别难，学习一下就能理解了。 1.2 数学资源网上有很多人会列举大量大量的课程资源，这是非常不负责任的事，学完那些我头发都得白了。实际上，我们只需要学习其中的一部分就够了。 1.2.1 吴恩达的斯坦福大学机器学习王牌课程CS229课后就有对学生数学知识的要求和补充，这些数学知识是完全符合机器学习要求的，不多也不少。墙裂推荐要看，不过只有英文版的。 链接：https://pan.baidu.com/s/1NrCAW38C9lXFqPwqTlrVRA密码：3k3m 1.2.2 深度学习的三大开山鼻祖之一Yoshua Bengio写的深度学习（包含了机器学习）领域的教科书现在以开源的形式在网上公开。这部书被誉为深度学习的圣经。在这里我们只看这本书的第一部分，也就是数学基础。囊括了机器学习所需的所有必备数学基础，而且是从最基础的说起，也不多，必读的。 链接：https://pan.baidu.com/s/1GmmbqFewyCuEA7blXNC-7g密码：6qqm 1.2.3 跟机器学习算法相结合的数学知识。上面两部分是理论层面的数学，机器学习算法中会对这些数学进行应用。链接：https://zhuanlan.zhihu.com/p/25197792，知乎专栏上的一篇好文章，囊括了所有的应用知识点。 好了，数学方面我只推荐上面三个资源，三个都是必看的。里面很多可能你现在看不太懂，没关系。先大概过一遍，知道自己的数学水平在哪。在看到算法知识的时候，不懂的再回来补就好。后期需要更多的数学资料我会再更新的。 2 编程技术编程语言：python3.5及以上，python易学，这个这期先不细讲。 3 经典算法知识算法包括机器学习和深度学习，机器学习是深度学习的基础。所以务必先学机器学习的经典算法，再学深度学习的算法。 3.1 机器学习3.1.1 课程资料首推吴恩达的CS229，经典中的经典，在网易公开课里有视频，翻译，课程讲义，笔记是非常非常完备的。墙裂推荐。这个课程对数学有一定的要求，但我觉得只要你上过大学的数学，然后补一下上面的数学，完全可以直接来看这个CS229。 假设你的数学真的很差的话，怎么办？吴恩达在coursera上也开了一门跟CS229完全匹配的课程，coursera机器学习课。这门课是CS229的翻版，唯一不同的是它对数学基本是没有要求了，如果你对数学真的不懂的话，那就先看这个的教程吧。它跟CS229的关系就是同样的广度，但是深度浅很多，不过你学完coursera还是要回过头来看CS229的。这个也是免费的。 CS229课程视频：http://open.163.com/special/opencourse/machinelearning.html 课程讲义和中文笔记：https://pan.baidu.com/s/1MC_yWjcz_m5YoZFNBcsRSQ密码：6rw6 3.1.2 配套书籍：机器学习实战，必看。用代码实现了一遍各大经典机器学习算法，必须看，对你理解算法有很大帮助，同时里面也有应用。如果大家看上面纯理论的部分太枯燥了，就可以来看看这本书来知道在现实中机器学习算法是怎么应用的，会很大程度提升你的学习兴趣，当初我可是看了好几遍。 书籍及课后代码：链接：https://pan.baidu.com/s/15XtFOH18si316076GLKYfg密码：sawb 李航《统计学习方法》，配合着看 链接：https://pan.baidu.com/s/1Mk_O71k-H8GHeaivWbzM-Q密码：adep，配合着看 周志华《机器学习》，机器学习的百科全书，配合着看。 链接：https://pan.baidu.com/s/1lJoQnWToonvBU6cYwjrRKg密码：7rzl 3.2 深度学习说到深度学习，我们不得不提斯坦福的另一门王牌课程CS231，李飞飞教授的。这门课的课程，课后习题，堪称完美。必须必须看。整个系列下来，特别是课后的习题要做，做完之后你会发现，哇哦！它的课后习题就是写代码来实现算法的。这个在网易云课堂上有。 视频地址：http://study.163.com/course/introduction.htm?courseId=1004697005 课程笔记翻译，知乎专栏：https://zhuanlan.zhihu.com/p/21930884 墙裂建议要阅读这个知乎专栏，关于怎么学这门课，这个专栏写的很清楚。 课后作业配套答案：https://blog.csdn.net/bigdatadigest/article/category/7425092 3.3 学习时间到这里了，你的机器学习和深度学习算是入门了。学完上面这些，按一天6小时，一周六天的话，起码也得三个月吧。上面是基本功一定要认真学。但是，还找不了工作。因为你还没把这些知识应用到实际当中。 3.4 实战部分3.4.1 实战基础这一个阶段，你要开始用tensorflow（谷歌的深度学习框架）、scikit-learn（python的机器学习框架），这两个框架极大程度地集成了各大算法。其实上面在学习cs231n的时候你就会用到一部分。 scikit-learn的学习：http://sklearn.apachecn.org/cn/0.19.0/ 这是scikit-learn的官方文档中文版翻译，有理论有实战，最好的库学习资源，没有之一。认真看，传统的机器学习就是用这个库来实现的。 Tensorflow的学习：https://tensorflow.google.cn/api_docs/python/?hl=zh-cn 官方文档很详尽，还有实战例子，学习tensorflow的不二之选 3.4.2 实战进阶仅仅看这两个教程是不够的，你需要更多地去应用这两个库。 接下来推荐一部神书，机器学习和深度学习的实战教学，非常非常的棒，网上有很多号称实战的书或者例子，我看了基本就是照搬官网的，只有这一本书，是完全按照工业界的流程解决方案进行实战，你不仅能学习到库的应用，还能深入了解工业界的流程解决方案，最好的实战教学书，没有之一。书名是hands-on-ml-with-sklearn-and-tf 链接：https://pan.baidu.com/s/1x318qTHGt9oZKQwHkoUvKA密码：xssj 3.4.3 实战最终阶段kaggle数据竞赛，如果你已经学到了这一步，恭喜你离梦想越来越近了：对于我们初学者来说，没有机会接触到机器学习真正的应用项目，所以一些比赛平台是我们不错的选择。参加kaggle竞赛可以给你的简历增分不少，里面有入门级别到专家级别的实战案例，满足你的各方面需求。如果你能学到这一步了，我相信也不需要再看这个了。 上述所有资料的合集：https://pan.baidu.com/s/1tPqsSmSMZa6qLyD0ng87IQ密码：ve75 补充： 学到这个水平，应该是能够实习的水平了，还有很多后面再说吧。比如深度学习和机器学习的就业方向，深度学习得看论文，找工作还得对你得编程基础进行加强，具体就是数据结构与算法，我当年在这个上面可是吃了很大的亏。 这里面关于深度学习和机器学习的就业其实是两个方向，上面的其实也没有说全。一般来说，你得选择一个方向专攻。我建议的是，自学的最好在后期侧重机器学习方向，而不是深度学习。深度学习的岗位实在是太少，要求太高。机器学习还算稍微好点。 重点：上面的学习路径是主要框架，但是不意味着仅仅学习这些就够了。根据每个人基础的不同，你有可能需要另外的学习资料补充。但是，我希望大家可以按照上面的主框架走，先按上面我推荐的资源学，有需要的再去看别的（我之后也会推荐），上面的我能列出来的都是最经典的，最有效率，而且我亲身学过的。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"LearnPath","slug":"LearnPath","permalink":"https://dataquaner.github.io/categories/LearnPath/"}],"tags":[{"name":"LearnPath","slug":"LearnPath","permalink":"https://dataquaner.github.io/tags/LearnPath/"},{"name":"AI","slug":"AI","permalink":"https://dataquaner.github.io/tags/AI/"}]},{"title":"机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化","slug":"机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化","date":"2020-01-16T06:08:00.000Z","updated":"2020-04-11T11:34:10.409Z","comments":true,"path":"2020/01/16/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-10-jue-ce-shu-mo-xing-xgboost-lightgbm-he-catboost-mo-xing-ke-shi-hua/","link":"","permalink":"https://dataquaner.github.io/2020/01/16/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-10-jue-ce-shu-mo-xing-xgboost-lightgbm-he-catboost-mo-xing-ke-shi-hua/","excerpt":"","text":"安装 graphviz 参考文档 http://graphviz.readthedocs.io/en/stable/manual.html#installation graphviz安装包下载地址 https://www.graphviz.org/download/ 将graphviz的安装位置添加到系统环境变量 使用pip install graphviz安装graphviz python包 使用pip install pydotplus安装pydotplus python包 决策树模型可视化 以iris数据为例。训练一个分类决策树，调用export_graphviz函数导出DOT格式的文件。并用pydotplus包绘制图片。 #在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from sklearn import tree from sklearn.datasets import load_iris iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) import pydotplus from IPython.display import Image dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data) Image(graph.create_png()) XGBoost模型可视化参考文档 https://xgboost.readthedocs.io/en/latest/python/python_api.htmlxgboost中，对应的可视化函数是xgboost.to_graphviz。以iris数据为例，训练一个xgb分类模型并可视化 # 在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' import xgboost as xgb from sklearn.datasets import load_iris iris = load_iris() xgb_clf = xgb.XGBClassifier() xgb_clf.fit(iris.data, iris.target) xgb.to_graphviz(xgb_clf, num_trees=1) 也可以通过Digraph对象可以将保存文件并查看 digraph = xgb.to_graphviz(xgb_clf, num_trees=1) digraph.format = 'png' digraph.view('./iris_xgb') xgboost中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。 import matplotlib.pyplot as plt fig = plt.figure(figsize=(10, 10)) ax = fig.subplots() xgb.plot_tree(xgb_clf, num_trees=1, ax=ax) plt.show() LightGBM模型可视化参考文档 https://lightgbm.readthedocs.io/en/latest/Python-API.html#plottinglgb中，对应的可视化函数是lightgbm.create_tree_digraph。以iris数据为例，训练一个lgb分类模型并可视化 # 在环境变量中加入安装的Graphviz路径 import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from sklearn.datasets import load_iris import lightgbm as lgb iris = load_iris() lgb_clf = lgb.LGBMClassifier() lgb_clf.fit(iris.data, iris.target) lgb.create_tree_digraph(lgb_clf, tree_index=1) lgb中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。 import matplotlib.pyplot as plt fig2 = plt.figure(figsize=(20, 20)) ax = fig2.subplots() lgb.plot_tree(lgb_clf, tree_index=1, ax=ax) plt.show() CatBoost模型可视化参考文档 catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)以iris数据为例，训练一个catboost模型。 参考文档 https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)以iris数据为例，训练一个catboost模型。 from sklearn.datasets import load_iris from catboost import CatBoostClassifier iris = load_iris() cat_clf = CatBoostClassifier(iterations=100) cat_clf.fit(iris.data, iris.target) 以python代码格式保存模型文件 cat_clf.save_model('catboost_model_file.py', format=\"python\", export_parameters=None) 也可以保存以C++代码格式保存模型文件 cat_clf.save_model('catboost_model_file.cpp', format=\"cpp\", export_parameters=None) 查看保存到的python代码，部分信息如下 需要自己解析出文件了树的结构，再用 graphviz 绘制图像 导出的Python文件首先第一个for循环部分 binary_feature_index = 0 binary_features = [0] * model.binary_feature_count for i in range(model.float_feature_count): for j in range(model.border_counts[i]): binary_features[binary_feature_index] = 1 if (float_features[i] > model.borders[binary_feature_index]) else 0 binary_feature_index += 1 输入的参数float_features存储输入的数值型特征值。model.binary_feature_count表示booster中所有树的节点总数。model.border_counts存储每个feature对应的节点数量，model.borders存储所有节点的判断边界。显然，CatBoost并没有按照二叉树结构从左到右，从上到下的存储结构。此段代码的功能，生成所有节点的判断结果。如果特征值大于判断边界，表示为1，否则为0。存储在binary_features中。 第二个for循环部分 # Extract and sum values from trees result = 0.0 tree_splits_index = 0 current_tree_leaf_values_index = 0 for tree_id in range(model.tree_count): current_tree_depth = model.tree_depth[tree_id] index = 0 for depth in range(current_tree_depth): index |= (binary_features[model.tree_splits[tree_splits_index + depth]] &lt;&lt; depth) result += model.leaf_values[current_tree_leaf_values_index + index] tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) return result 这段点代码功能是生成模型的预测结果result。model.tree_count表示决策树的数量，遍历每棵决策树。model.tree_depth存储每棵决策树的深度，取当前树的深度，存储在current_tree_depth。model.tree_splits存储决策树当前深度的节点在binary_features中的索引，每棵树有current_tree_depth个节点。看似CatBoost模型存储了都是完全二叉树，而且每一层的节点以及该节点的判断边界一致。如一棵6层的决策，可以从binary_features中得到一个长度为6，值为0和1组成的list。model.leaf_values存储所有叶子节点的值，每棵树的叶子节点有(1 &lt;&lt; current_tree_depth)个。将之前得到的list，倒序之后，看出一个2进制表示的数index，加上current_tree_leaf_values_index后，即是值在model.leaf_values的索引。将所有树得到的值相加，得到CatBoost模型的结果。 还原CatBoost模型树先从第二个for循环开始，打印每棵树序号，树的深度，当前树节点索引在tree_splits的便宜了，已经每个节点对应在tree_splits中的值。这个值对应的是在第一个for循环中生成的binary_features的索引。 tree_splits_index = 0 current_tree_leaf_values_index = 0 for tree_id in range(tree_count): current_tree_depth = tree_depth[tree_id] tree_splits_list = [] for depth in range(current_tree_depth): tree_splits_list.append(tree_splits[tree_splits_index + depth]) print tree_id, current_tree_depth, tree_splits_index, tree_splits_list tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) 0 6 0 [96, 61, 104, 2, 52, 81] 1 6 6 [95, 99, 106, 44, 91, 14] 2 6 12 [96, 31, 81, 102, 16, 34] 3 6 18 [95, 105, 15, 106, 57, 111] 4 6 24 [95, 51, 30, 8, 75, 57] 5 6 30 [94, 96, 103, 104, 25, 33] 6 6 36 [60, 8, 25, 39, 15, 99] 7 6 42 [96, 27, 48, 50, 69, 111] 8 6 48 [61, 80, 71, 3, 45, 2] 9 4 54 [61, 21, 90, 37] 从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。 12345678910从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。 split_list = [0] for i in range(len(border_counts)): split_list.append(split_list[-1] + border_counts[i]) print border_counts print zip(split_list[:-1], split_list[1:]) [32, 21, 39, 20] [(0, 32), (32, 53), (53, 92), (92, 112)] 在拿到一个binary_features的索引后即可知道该索引对应的节点使用的特征序号（float_features的索引）。 def find_feature(tree_splits_index): for i in range(len(split_list) - 1): if split_list[i] &lt;= tree_splits_index &lt; split_list[i+1]: return i 有了节点在binary_features中的索引，该索引也对应特征的判断边界数值索引，也知道了如何根据索引获取特征序号。决策树索引信息都的得到了，现在可以绘制树了。 绘制单棵决策树首先修改一下代码，便于获取单棵树的节点 class CatBoostTree(object): def __init__(self, CatboostModel): self.model = CatboostModel self.split_list = [0] for i in range(self.model.float_feature_count): self.split_list.append(self.split_list[-1] + self.model.border_counts[i]) def find_feature(self, splits_index): # 可优化成二分查找 for i in range(self.model.float_feature_count): if self.split_list[i] &lt;= splits_index &lt; self.split_list[i + 1]: return i def get_split_index(self, tree_id): tree_splits_index = 0 current_tree_leaf_values_index = 0 for index in range(tree_id): current_tree_depth = self.model.tree_depth[index] tree_splits_index += current_tree_depth current_tree_leaf_values_index += (1 &lt;&lt; current_tree_depth) return tree_splits_index, current_tree_leaf_values_index def get_tree_info(self, tree_id): tree_splits_index, current_tree_leaf_values_index = self.get_split_index(tree_id) current_tree_depth = self.model.tree_depth[tree_id] tree_splits_list = [] for depth in range(current_tree_depth): tree_splits_list.append(self.model.tree_splits[tree_splits_index + depth]) node_feature_list = [self.find_feature(index) for index in tree_splits_list] node_feature_borders = [self.model.borders[index] for index in tree_splits_list] end_tree_leaf_values_index = current_tree_leaf_values_index + (1 &lt;&lt; current_tree_depth) tree_leaf_values = self.model.leaf_values[current_tree_leaf_values_index: end_tree_leaf_values_index] return current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values 下面是绘制一棵决策树的函数，CatBoost导出的python代码文件通过model_file参数传入。 import imp import os os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' from graphviz import Digraph def draw_tree(model_file, tree_id): fp, pathname, description = imp.find_module(model_file) CatboostModel = imp.load_module('CatboostModel', fp, pathname, description) catboost_tree = CatBoostTree(CatboostModel.CatboostModel) current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values = catboost_tree.get_tree_info(tree_id) dot = Digraph(name='tree_'+str(tree_id)) for depth in range(current_tree_depth): node_name = str(node_feature_list[current_tree_depth - 1 - depth]) node_border = str(node_feature_borders[current_tree_depth - 1 - depth]) label = 'column_' + node_name + '>' + node_border if depth == 0: dot.node(str(depth) + '_0', label) else: for j in range(1 &lt;&lt; depth): dot.node(str(depth) + '_' + str(j), label) dot.edge(str(depth-1) + '_' + str(j//2), str(depth) + '_' + str(j), label='No' if j%2 == 0 else 'Yes') depth = current_tree_depth for j in range(1 &lt;&lt; depth): dot.node(str(depth) + '_' + str(j), str(tree_leaf_values[j])) dot.edge(str(depth-1) + '_' + str(j//2), str(depth) + '_' + str(j), label='No' if j%2 == 0 else 'Yes') # dot.format = 'png' path = dot.render('./' + str(tree_id), cleanup=True) print path 例如绘制第11棵树（序数从0开始）。draw_tree(‘catboost_model_file’, 11)。 为了验证这个对不对，需要对一个测试特征生成每棵树的路径和结果，抽查一两个测试用例以及其中的一两颗树，观察结果是否相同。 def test_tree(model_file, float_features): fp, pathname, description = imp.find_module(model_file) CatboostModel = imp.load_module('CatboostModel', fp, pathname, description) model = CatboostModel.CatboostModel catboost_tree = CatBoostTree(CatboostModel.CatboostModel) result = 0 for tree_id in range(model.tree_count): current_tree_depth, node_feature_list, node_feature_borders, tree_leaf_values = catboost_tree.get_tree_info(tree_id) route = [] for depth in range(current_tree_depth): route.append(1 if float_features[node_feature_list[depth]] > node_feature_borders[depth] else 0) index = 0 for depth in range(current_tree_depth): index |= route[depth] &lt;&lt; depth tree_value = tree_leaf_values[index] print route, index, tree_value result += tree_value return result 如我们生成了第11棵树的图像，根据测试测试特征，手动在图像上查找可以得到一个值A。test_tree函数会打印一系列值，其中第11行对应的结果为值B。值A与值B相同，则测试为问题。其次还需要测试所有树的结果和导出文件中apply_catboost_model函数得到的结果相同。这个可以写个脚本，拿训练数据集跑一边。 from catboost_model_file import apply_catboost_model from CatBoostModelInfo import test_tree from sklearn.datasets import load_iris def main(): iris = load_iris() # print iris.data # print iris.target for feature in iris.data: if apply_catboost_model(feature) != test_tree('catboost_model_file', feature): print False print 'End.' if name == 'main': main() 至此，CatBoost模型的可视化完成了。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"},{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"},{"name":"CatBoost","slug":"CatBoost","permalink":"https://dataquaner.github.io/tags/CatBoost/"}]},{"title":"DBSCAN算法python实现（附完整数据集和代码）","slug":"DBSCAN算法python实现（附完整数据集和代码）","date":"2020-01-07T08:05:00.000Z","updated":"2020-04-11T11:30:29.639Z","comments":true,"path":"2020/01/07/dbscan-suan-fa-python-shi-xian-fu-wan-zheng-shu-ju-ji-he-dai-ma/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/dbscan-suan-fa-python-shi-xian-fu-wan-zheng-shu-ju-ji-he-dai-ma/","excerpt":"","text":"目录[TOC] 1. 算法思路DBSCAN算法的核心是“延伸”。先找到一个未访问的点p，若该点是核心点，则创建一个新的簇C，将其邻域中的点放入该簇，并遍历其邻域中的点，若其邻域中有点q为核心点，则将q的邻域内的点也划入簇C，直到C不再扩展。直到最后所有的点都标记为已访问。 点p通过密度可达来扩大自己的“地盘”，实际上就是簇在“延伸”。 图示网站：https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ 可以看一下簇是如何延伸的。 2. 算法实现2.1 计算两点之间的距离# 计算两个点之间的欧式距离，参数为两个元组 def dist(t1, t2): dis = math.sqrt((np.power((t1[0]-t2[0]),2) + np.power((t1[1]-t2[1]),2))) # print(\"两点之间的距离为：\"+str(dis)) return dis 2.2 读取文件，加载数据集def loadDataSet(fileName, splitChar='\\t'): dataSet = [] with open(fileName) as fr: for line in fr.readlines(): curline = line.strip().split(splitChar) fltline = list(map(float, curline)) dataSet.append(fltline) return dataSet 2.3 DBSCAN算法实现1、标记点是否被访问：我设置了两个列表，一个存放未访问的点unvisited，一个存放已访问的点visited。每次访问一个点，unvisited列表remove该点，visited列表append该点，以此来实现点的标记改变。 2、C作为输出结果，初始时是一个长度为所有点的个数的值全为-1的列表。之后修改点对应的索引的值来设置点属于哪个簇 # DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值 def dbscan(Data, Eps, MinPts): num = len(Data) # 点的个数 # print(\"点的个数：\"+str(num)) unvisited = [i for i in range(num)] # 没有访问到的点的列表 # print(unvisited) visited = [] # 已经访问的点的列表 C = [-1 for i in range(num)] # C为输出结果，默认是一个长度为num的值全为-1的列表 # 用k来标记不同的簇，k = -1表示噪声点 k = -1 # 如果还有没访问的点 while len(unvisited) > 0: # 随机选择一个unvisited对象 p = random.choice(unvisited) unvisited.remove(p) visited.append(p) # N为p的epsilon邻域中的对象的集合 N = [] for i in range(num): if (dist(Data[i], Data[p]) &lt;= Eps):# and (i!=p): N.append(i) # 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象 if len(N) >= MinPts: k = k+1 # print(k) C[p] = k # 对于p的epsilon邻域中的每个对象pi for pi in N: if pi in unvisited: unvisited.remove(pi) visited.append(pi) # 找到pi的邻域中的核心对象，将这些对象放入N中 # M是位于pi的邻域中的点的列表 M = [] for j in range(num): if (dist(Data[j], Data[pi])&lt;=Eps): #and (j!=pi): M.append(j) if len(M)>=MinPts: for t in M: if t not in N: N.append(t) # 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动 if C[pi] == -1: C[pi] = k # 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点 else: C[p] = -1 return C 3. 问题记录代码思路非常简单，让我以为实现起来也很简单。结果拖拖拉拉半个多月才终于将算法改好。 算法实现过程中遇到的问题其实是小问题，但是导致的结果非常严重。因为不起眼所以才难以察觉。 这是刚开始我运行算法得到的结果（Eps为10，MinPts为10）： Eps为2，MinPts为10（我改了点的大小）： 可以看出图中颜色特别多，实际上就是聚成的簇太多，可实际上目测应该只有七八个簇。这是为什么呢？ 原来是变量k的重复使用问题。 前面我用k来标识不同的簇，后面（如下图）我又将k变成了循环变量，注意M列表中都是整数，代表点在数据集中的索引，所以实际上是k在整数列表中遍历，覆盖掉了前面用来标识不同簇的k值，导致每次运行出来k取值特别多（如下下图）。 4. 运行结果 5. 完整代码5.1 源数据附数据集 链接：数据集788个点 提取码：rv06 5.2 源代码# encoding:utf-8 import matplotlib.pyplot as plt import random import numpy as np import math from sklearn import datasets list_1 = [] list_2 = [] # 数据集一：随机生成散点图,参数为点的个数 # def scatter(num): # for i in range(num): # x = random.randint(0, 100) # list_1.append(x) # y = random.randint(0, 100) # list_2.append(y) # print(list_1) # print(list_2) # data = list(zip(list_1, list_2)) # print(data) # #plt.scatter(list_1, list_2) # #plt.show() # return data #scatter(50) def loadDataSet(fileName, splitChar='\\t'): dataSet = [] with open(fileName) as fr: for line in fr.readlines(): curline = line.strip().split(splitChar) fltline = list(map(float, curline)) dataSet.append(fltline) return dataSet # 计算两个点之间的欧式距离，参数为两个元组 def dist(t1, t2): dis = math.sqrt((np.power((t1[0]-t2[0]),2) + np.power((t1[1]-t2[1]),2))) # print(\"两点之间的距离为：\"+str(dis)) return dis # dis = dist((1,1),(3,4)) # print(dis) # DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值 def dbscan(Data, Eps, MinPts): num = len(Data) # 点的个数 # print(\"点的个数：\"+str(num)) unvisited = [i for i in range(num)] # 没有访问到的点的列表 # print(unvisited) visited = [] # 已经访问的点的列表 C = [-1 for i in range(num)] # C为输出结果，默认是一个长度为num的值全为-1的列表 # 用k来标记不同的簇，k = -1表示噪声点 k = -1 # 如果还有没访问的点 while len(unvisited) > 0: # 随机选择一个unvisited对象 p = random.choice(unvisited) unvisited.remove(p) visited.append(p) # N为p的epsilon邻域中的对象的集合 N = [] for i in range(num): if (dist(Data[i], Data[p]) &lt;= Eps):# and (i!=p): N.append(i) # 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象 if len(N) >= MinPts: k = k+1 # print(k) C[p] = k # 对于p的epsilon邻域中的每个对象pi for pi in N: if pi in unvisited: unvisited.remove(pi) visited.append(pi) # 找到pi的邻域中的核心对象，将这些对象放入N中 # M是位于pi的邻域中的点的列表 M = [] for j in range(num): if (dist(Data[j], Data[pi])&lt;=Eps): #and (j!=pi): M.append(j) if len(M)>=MinPts: for t in M: if t not in N: N.append(t) # 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动 if C[pi] == -1: C[pi] = k # 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点 else: C[p] = -1 return C # 数据集二：788个点 dataSet = loadDataSet('788points.txt', splitChar=',') C = dbscan(dataSet, 2, 14) print(C) x = [] y = [] for data in dataSet: x.append(data[0]) y.append(data[1]) plt.figure(figsize=(8, 6), dpi=80) plt.scatter(x,y, c=C, marker='o') plt.show() # print(x) # print(y) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://dataquaner.github.io/tags/DBSCAN/"}]},{"title":"短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示","slug":"短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示","date":"2020-01-07T08:05:00.000Z","updated":"2020-04-11T11:34:53.017Z","comments":true,"path":"2020/01/07/duan-wen-ben-ju-lei-dbscan-suan-fa-yuan-li-python-dai-ma-shi-xian-ju-lei-jie-guo-zhan-shi/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/duan-wen-ben-ju-lei-dbscan-suan-fa-yuan-li-python-dai-ma-shi-xian-ju-lei-jie-guo-zhan-shi/","excerpt":"","text":"目录[TOC] 1. 算法原理1.1 常见的聚类算法聚类算法属于常见的无监督分类算法，在很多场景下都有应用，如用户聚类，文本聚类等。常见的聚类算法可以分成两类： 以 k-means 为代表的基于分区的算法 以层次聚类为代表的基于层次划分的算法 对于第一类方法，有以下几个缺点： 1）需要事先确定聚类的个数，当数据集比较大时，很难事先给出一个合适的值； 2）只适用于具有凸形状的簇，不适用于具有任意形状的簇； 3）对内存的占用资源比较大，难以推广至大规模数据集； 对于第二类方法，有以下缺点： 1）需要确定停止分裂的条件 2）计算速度慢 1.2 DBSCAN聚类 A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise （Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu） DBSCAN是一类基于密度的算法，能有效解决上述两类算法的问题。 DBSCAN的基本假设是一个集群的密度要显著高于噪声点的密度。因此，其基本思想是对于集群中的每一个点，在给定的半径范围内，相邻点的数量必须超过预先设定的某一个阈值。 因此，DBSCAN算法中包含两个重要的参数： eps：聚类类别中样本的相似度衡量，与类别内样本相似度成反比。可以理解为同一个类别当中，对两个样本之间距离的最大值限定。 min_samples：每个聚类类别中的最小样本数，会对未分类样本数量造成影响，与未分类样本数量成正比。当相似样本数量少于该参数时，不会聚到一起。 在实际应用过程中，根据样本的大小，以及样本的大致分布，了解聚类结果会随着这两个参数如何变化之后，可以根据自己的经验对两个参数进行调整。只有两个模型参数需要调整，因此调参过程也不会太麻烦。 2. 代码实现2.1 import需要的包# === import packages === # import jieba.posseg as pseg from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import CountVectorizer import numpy as np from sklearn.cluster import DBSCAN 2.2 载入数据根据数据文件的不同存在不同的数据载入方法，我当时使用的是两种类型的数据，分别是直接包含目标短文本的txt，以json格式存储的txt。如果有用到这两种类型的文件可以参考这部分的数据载入代码，其他的请根据文件类型和数据样式自行载入。首先是载入以json格式存储的txt文件，可以用正则表达式，也可以根据数据存储的方式提取出对应的字段。先展示一下数据的存储格式： { \"code\": \"200\", \"data\": { \"result\": [ { \"updateDate\": 1551923786433, \"ensureIntentName\": \"新意图\", \"corpus\": \"怎么查询之前的小微提醒\", \"recommendResult\": 0, \"remark\": \"\", \"source\": 2, \"result\": 2, \"eventName\": \"\", \"id\": \"b07328fc-8383-44b7-b466-15b063b8544a\", \"state\": 0, \"tag\": \"\", \"isHandle\": 1, \"createDate\": 1551669751334, \"eventId\": \"\", \"corpusTagId\": \"3335d2d8-a16e-46a2-9ed7-76739108d684\", \"intentName\": \"\", \"ensureIntent\": \"newIntent\", \"recommendIntent\": [ \"setmsgnotifications\" ], \"uploadTime\": 1551669751333, \"w3account\": \"x00286769\", \"createBy\": \"x00286769\", \"intentCode\": \"\", \"isBotSupport\": 0, \"userRole\": \"0\", \"welinkVersion\": \"3.9.13\" } ], \"pagination\": { \"pageCount\": 1, \"pageSizes\": 50, \"pageNumber\": 1, \"offset\": 0, \"pageTotal\": 1, \"pageNumbers\": 1, \"pageSize\": 50 } }, \"error\": \"\", \"stack\": \"\", \"message\": \"ok\" } 我的目标是对上述数据当中，字典中key “data” 对应的字典中的 “result” 中每一个item 的 “corpus” 进行提取，于是就有了下列代码。 # === Data loading === # data = [] corpus = [] for line in open(\"新意图语料.txt\", 'r+', encoding='UTF-8'): data.append(eval(line)) for i in range(len(data)): tmp = data[i]['data']['result'] for j in range(len(tmp)): corpus.append(tmp[j]['corpus']) 然后是载入包含目标短文本的txt，也就是说该txt直接存储了上面的 “corpus” 对应的内容，但是每一行的内容都加上了双引号和逗号，就通过strip把这些不需要的部分去掉了，最后得到所有 “corpus” 组成的list。 for line in open(\"未识别语料.txt\", 'r+'): line = line.strip('\\n') line = line.strip('\\t') line = line.rstrip(',') line = line.lstrip('\"') line = line.rstrip('\"') corpus.append(line) 2.3 对文本进行分词，并记录词性调用结巴词库对语料进行分词，并记录分词结果中每个词的词性。我的数据集在处理之后得到了5316条短文本，分词得到20640个不重复的词汇及其对应的词性，并建立了两者之间的字典联系。 # === Record the text cut and POS === # part_of_speech = [] word_after_cut = [] cut_corpus_iter = corpus.copy() cut_corpus = corpus.copy() for i in range(len(corpus)): cut_corpus_iter[i] = pseg.cut(corpus[i]) # 5316 cut_corpus[i] = \"\" for every in cut_corpus_iter[i]: cut_corpus[i] = (cut_corpus[i] + \" \" + str(every.word)).strip() part_of_speech.append(every.flag) # 20640 word_after_cut.append(every.word) # 20640 word_pos_dict = {word_after_cut[i]: part_of_speech[i] for i in range(len(word_after_cut))} 2.4 文本向量化–TF-IDF权重使用TF-IDF对文本进行向量化，得到文本的TF-IDF权重。 # === Get the TF-IDF weights === # Count_vectorizer = CountVectorizer() transformer = TfidfTransformer() # 用于统计每个词语的tf-idf权值 tf_idf = transformer.fit_transform(Count_vectorizer.fit_transform(cut_corpus)) # （5316，2039）第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵 word = Count_vectorizer.get_feature_names() # 2039，获取词袋模型中的所有词语 weight = tf_idf.toarray() # （5316，2039）将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重 2.5 基于词性的新权重前面得到了分词的结果，并对词性进行了记录，接下来可以针对不同词汇的词性码，给与其TF-IDF权重以不同的乘数，这样可以突出某些类型的词汇的重要性，在一定程度上有助于聚类的效果。 具体的乘数构造规则可以根据需求自行调整。 # === Get new weight with POS considered === # word_weight = [1 for i in range(len(word))] for i in range(len(word)): if word[i] not in word_pos_dict.keys(): continue if word_pos_dict[word[i]] == 'n': word_weight[i] = 1.2 elif word_pos_dict[word[i]] == \"vn\": word_weight[i] = 1.1 elif word_pos_dict[word[i]] == \"m\": word_weight[i] = 0 else: # 权重调整可以根据实际情况进行更改 continue word_weight = np.array(word_weight) new_weight = weight.copy() for i in range(len(weight)): for j in range(len(word)): new_weight[i][j] = weight[i][j] * word_weight[j] 2.6 DBSCAN模型得到了文本的向量化表示之后就可以将其投喂到模型当中了，eps和min_samples都是可以调整的参数。 # === Fit the DBSCAN model and get the classify labels === # DBS_clf = DBSCAN(eps=1, min_samples=4) DBS_clf.fit(new_weight) print(DBS_clf.labels_) 3. 聚类结果DBSCAN模型实现聚类之后，聚类的结果会存储在 labels_ 中，将 labels_ 与原来的文本一一对应，可以得到最终的聚类结果： # === Define the function of classify the original corpus according to the labels === # def labels_to_original(labels, original_corpus): assert len(labels) == len(original_corpus) max_label = max(labels) number_label = [i for i in range(0, max_label + 1, 1)] number_label.append(-1) result = [[] for i in range(len(number_label))] for i in range(len(labels)): index = number_label.index(labels[i]) result[index].append(original_corpus[i]) return result labels_original = labels_to_original(DBS_clf.labels_, corpus) for i in range(5): print(labels_original[i]) # 聚类结果展示（部分） ['社保卡', '社保卡', '社保卡。', '社保卡办理', '社保卡', '社保卡', '社保卡挂失', '社保卡。', '社保卡', '领取社保卡。'] ['五险一金', '五险一金。', '五险一金。', '五险一金介绍', '看看二月份五险一金情况'] ['打开汇钱。', '打开汇钱。', '我要汇钱', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。'] ['车辆通行证。', '车辆通行证。', '我要办车辆通行证。', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证。', '车辆通行证', '车辆通行证。', '车辆通行证。', '车辆通行证'] ['邮件附件权限', '等等邮件附件权限。', '邮件附件权限', '邮件附件权限', '邮件附件权限', '邮件附件权限', '您好，请问怎样申请图片查看权限和邮件附件查看权限？'] 4 附件：完整代码# === import packages === # import jieba.posseg as pseg from sklearn.feature_extraction.text import TfidfTransformer from sklearn.feature_extraction.text import CountVectorizer import numpy as np from sklearn.cluster import DBSCAN # === Data loading === # data = [] corpus = [] for line in open(\"新意图语料.txt\", 'r+', encoding='UTF-8'): data.append(eval(line)) for i in range(len(data)): tmp = data[i]['data']['result'] for j in range(len(tmp)): corpus.append(tmp[j]['corpus']) for line in open(\"未识别语料.txt\", 'r+'): line = line.strip('\\n') line = line.strip('\\t') line = line.rstrip(',') line = line.lstrip('\"') line = line.rstrip('\"') corpus.append(line) # === Record the text cut and POS === # part_of_speech = [] word_after_cut = [] cut_corpus_iter = corpus.copy() cut_corpus = corpus.copy() for i in range(len(corpus)): cut_corpus_iter[i] = pseg.cut(corpus[i]) # 5316 cut_corpus[i] = \"\" for every in cut_corpus_iter[i]: cut_corpus[i] = (cut_corpus[i] + \" \" + str(every.word)).strip() part_of_speech.append(every.flag) # 20640 word_after_cut.append(every.word) # 20640 word_pos_dict = {word_after_cut[i]: part_of_speech[i] for i in range(len(word_after_cut))} # === Get new weight with POS considered === # word_weight = [1 for i in range(len(word))] for i in range(len(word)): if word[i] not in word_pos_dict.keys(): continue if word_pos_dict[word[i]] == 'n': word_weight[i] = 1.2 elif word_pos_dict[word[i]] == \"vn\": word_weight[i] = 1.1 elif word_pos_dict[word[i]] == \"m\": word_weight[i] = 0 else: # 权重调整可以根据实际情况进行更改 continue word_weight = np.array(word_weight) new_weight = weight.copy() for i in range(len(weight)): for j in range(len(word)): new_weight[i][j] = weight[i][j] * word_weight[j] # === Fit the DBSCAN model and get the classify labels === # DBS_clf = DBSCAN(eps=1, min_samples=4) DBS_clf.fit(new_weight) print(DBS_clf.labels_) # === Define the function of classify the original corpus according to the labels === # def labels_to_original(labels, original_corpus): assert len(labels) == len(original_corpus) max_label = max(labels) number_label = [i for i in range(0, max_label + 1, 1)] number_label.append(-1) result = [[] for i in range(len(number_label))] for i in range(len(labels)): index = number_label.index(labels[i]) result[index].append(original_corpus[i]) return result labels_original = labels_to_original(DBS_clf.labels_, corpus) for i in range(5): print(labels_original[i]) # 聚类结果展示（部分） ['社保卡', '社保卡', '社保卡。', '社保卡办理', '社保卡', '社保卡', '社保卡挂失', '社保卡。', '社保卡', '领取社保卡。'] ['五险一金', '五险一金。', '五险一金。', '五险一金介绍', '看看二月份五险一金情况'] ['打开汇钱。', '打开汇钱。', '我要汇钱', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。', '我要汇钱。'] ['车辆通行证。', '车辆通行证。', '我要办车辆通行证。', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证', '车辆通行证。', '车辆通行证', '车辆通行证。', '车辆通行证。', '车辆通行证'] ['邮件附件权限', '等等邮件附件权限。', '邮件附件权限', '邮件附件权限', '邮件附件权限', '邮件附件权限', '您好，请问怎样申请图片查看权限和邮件附件查看权限？'] document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://dataquaner.github.io/tags/DBSCAN/"}]},{"title":"机器学习系列之决策树算法(08):梯度提升树算法LightGBM","slug":"机器学习系列之决策树算法（08）：梯度提升树算法LightGBM","date":"2020-01-07T02:30:00.000Z","updated":"2020-04-11T11:33:39.995Z","comments":true,"path":"2020/01/07/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-08-ti-du-ti-sheng-shu-suan-fa-lightgbm/","link":"","permalink":"https://dataquaner.github.io/2020/01/07/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-08-ti-du-ti-sheng-shu-suan-fa-lightgbm/","excerpt":"","text":"1. LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。而LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。 1.1 LightGBM提出的动机常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。 LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。 1.2 XGBoost的缺点及LightGBM的优化（1）XGBoost的缺点在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是： 首先，对所有特征都按照特征的数值进行预排序。 其次，在遍历分割点的时候用的代价找到一个特征上的最好分割点。 最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。 这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显： 首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。 最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。 （2）LightGBM的优化为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化： 基于Histogram的决策树算法。 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 直接支持类别特征(Categorical Feature) 支持高效并行 Cache命中率优化 下面我们就详细介绍以上提到的lightGBM优化算法。 2. LightGBM的基本原理2.1 基于Histogram的决策树算法（1）直方图算法Histogram algorithm应该翻译为直方图算法，直方图算法的基本思想是： 先把连续的浮点特征值离散化成 k个整数，同时构造一个宽度为 k 的 直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。 图：直方图算法 直方图算法简单理解为： 首先确定对于每一个特征需要多少个箱子（bin）并为每一个箱子分配一个整数； 然后将浮点数的范围均分成若干区间，区间个数与箱子个数相等，将属于该箱子的样本数据更新为箱子的值； 最后用直方图（#bins）表示。看起来很高大上，其实就是直方图统计，将大规模的数据放在了直方图中。 我们知道特征离散化具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等。对于直方图算法来说最直接的有以下两个优点： 内存占用更小： 直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8 。也就是说XGBoost需要用32位的浮点数去存储特征值，并用32位的整形去存储索引，而 LightGBM只需要用8位去存储直方图，内存相当于减少为 ； 图：内存占用优化为预排序算法的1/8 计算代价更小： 预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算 k次（ 可以认为是常数），直接将时间复杂度从O(#data * #feature )降低到 O(k * #feature )，而我们知道#data &gt;&gt;k。 当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 （2）直方图做差加速LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。 注意： XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。 2.2 带深度限制的 Leaf-wise 算法在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。 LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。 2.3 单边梯度采样算法Gradient-based One-Side Sampling 应该被翻译为单边梯度采样（GOSS）。GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 AdaBoost中，样本权重是数据重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的是，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用。即梯度小的样本，训练误差也比较小，说明数据已经被模型学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练模型的精确度，为了避免此问题，提出了GOSS算法。 GOSS是一个样本的采样算法，目的是丢弃一些对计算信息增益没有帮助的样本留下有帮助的。根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。因此，GOSS在进行数据采样的时候只保留了梯度较大的数据，但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。所以，GOSS首先将要进行分裂的特征的所有取值按照绝对值大小降序排序（XGBoost一样也进行了排序，但是LightGBM不用保存排序后的结果），选取绝对值最大的 个数据。然后在剩下的较小梯度数据中随机选择 个数据。接着将这 个数据乘以一个常数 ，这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这 个数据来计算信息增益。下图是GOSS的具体算法。 2.4 互斥特征捆绑算法高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从 变为 ，这里 指特征融合绑定后特征包的个数，且 远小于 。 针对这种想法，我们会遇到两个问题： 怎么判定哪些特征应该绑在一起（build bundled）？ 怎么把特征绑为一个（merge feature）？ （1）解决哪些特征应该绑在一起将相互独立的特征进行绑定是一个 NP-Hard 问题，LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。此外，我们注意到通常有很多特征，尽管不是％相互排斥，但也很少同时取非零值。如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。经过简单的计算，随机污染小部分特征值将影响精度最多 ， 是每个绑定中的最大冲突比率，当其相对较小时，能够完成精度和效率之间的平衡。具体步骤可以总结如下： 构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关； 根据节点的度进行降序排序，度越大，与其它特征的冲突越大； 遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。 算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 来平衡算法的精度和效率。EFB 算法的伪代码如下所示： 算法3的时间复杂度是 ，训练之前只处理一次，其时间复杂度在特征不是特别多的情况下是可以接受的，但难以应对百万维度的特征。为了继续提高效率，LightGBM提出了一种更加高效的无图的排序策略：将特征按照非零值个数排序，这和使用图节点的度排序相似，因为更多的非零值通常会导致冲突，新算法在算法3基础上改变了排序策略。 （2）解决怎么把特征绑为一捆特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间 ，B特征的原始取值为区间，我们可以在B特征的取值上加一个偏置常量，将其取值范围变为，绑定后的特征取值范围为 ，这样就可以放心的融合特征A和B了。具体的特征合并算法如下所示： 3. LightGBM的工程优化我们将论文《Lightgbm: A highly efficient gradient boosting decision tree》中没有提到的优化方案，而在其相关论文《A communication-efficient parallel algorithm for decision tree》中提到的优化方案，放到本节作为LightGBM的工程优化来向大家介绍。 3.1 直接支持类别特征实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的特征，降低了空间和时间的效率。但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题： 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为1 ，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别。 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，如下图左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差。但如果使用下图右边的切分方法，数据会被切分到两个比较大的空间，进一步的学习也会更好。下图右边叶子节点的含义是或者放到左孩子，其余放到右孩子。 图：左图为基于 one-hot 编码进行分裂，右图为 LightGBM 基于 many-vs-many 进行分裂 而类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 个类别，则有 种可能，时间复杂度为 ，LightGBM 基于 Fisher的《On Grouping For Maximum Homogeneity》论文实现了 的时间复杂度。 算法流程如下图所示，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。从下图可以看到， 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。 图：LightGBM求解类别特征的最优切分算法 在Expo数据集上的实验结果表明，相比展开的方法，使用LightGBM支持的类别特征可以使训练速度加速倍，并且精度一致。更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。 3.2 支持高效并行（1）特征并行特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法。这种特征并行方法有个很大的缺点：就是对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度。 LightGBM 则不进行数据垂直划分，而是在每台机器上保存全部训练数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。具体过程如下图所示。 （2）数据并行传统的数据并行策略主要为水平划分数据，让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 ；如果使用集成的通信，则通讯开销为 。 LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。具体过程如下图所示。 图：数据并行 （3）投票并行基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行的方式只合并部分特征的直方图从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。 大致步骤为两步： 本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征； 合并时只合并每个机器选出来的特征。 图：投票并行 3.3 Cache命中率优化XGBoost对cache优化不友好，如下图所示。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问算法进行改进。 图：随机访问会造成cache miss 而 LightGBM 所使用直方图算法对 Cache 天生友好： 首先，所有的特征都采用相同的方式获得梯度（区别于XGBoost的不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率； 其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss的问题。 图：LightGBM增加缓存命中率 4. LightGBM的优缺点4.1 优点这部分主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。 （1）速度更快 LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度； LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算； LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量； LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略； LightGBM 对缓存也进行了优化，增加了缓存命中率； （2）内存更小 XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 降低为 ，极大的减少了内存消耗； LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗； LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。 4.2 缺点 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合； Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感； 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去； 5. LightGBM实例本篇文章所有数据集和代码均在我的GitHub中，地址：https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/LightGBM 5.1 安装LightGBM依赖包pip install lightgbm 5.2 LightGBM分类和回归LightGBM有两大类接口：LightGBM原生接口 和 scikit-learn接口 ，并且LightGBM能够实现分类和回归两种任务。 （1）基于LightGBM原生接口的分类import lightgbm as lgb from sklearn import datasets from sklearn.model_selection import train_test_split import numpy as np from sklearn.metrics import roc_auc_score, accuracy_score # 加载数据 iris = datasets.load_iris() # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3) # 转换为Dataset数据格式 train_data = lgb.Dataset(X_train, label=y_train) validation_data = lgb.Dataset(X_test, label=y_test) # 参数 params = { 'learning_rate': 0.1, 'lambda_l1': 0.1, 'lambda_l2': 0.2, 'max_depth': 4, 'objective': 'multiclass', # 目标函数 'num_class': 3, } # 模型训练 gbm = lgb.train(params, train_data, valid_sets=[validation_data]) # 模型预测 y_pred = gbm.predict(X_test) y_pred = [list(x).index(max(x)) for x in y_pred] print(y_pred) # 模型评估 print(accuracy_score(y_test, y_pred)) （2）基于Scikit-learn接口的分类 from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.externals import joblib # 加载数据 iris = load_iris() data = iris.data target = iris.target # 划分训练数据和测试数据 X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2) # 模型训练 gbm = LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=20) gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5) # 模型存储 joblib.dump(gbm, 'loan_model.pkl') # 模型加载 gbm = joblib.load('loan_model.pkl') # 模型预测 y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_) # 模型评估 print('The accuracy of prediction is:', accuracy_score(y_test, y_pred)) # 特征重要度 print('Feature importances:', list(gbm.feature_importances_)) # 网格搜索，参数优化 estimator = LGBMClassifier(num_leaves=31) param_grid = { 'learning_rate': [0.01, 0.1, 1], 'n_estimators': [20, 40] } gbm = GridSearchCV(estimator, param_grid) gbm.fit(X_train, y_train) print('Best parameters found by grid search are:', gbm.best_params_) （3）基于LightGBM原生接口的回归对于LightGBM解决回归问题，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.转换为Dataset数据格式 lgb_train = lgb.Dataset(train_X, train_y) lgb_eval = lgb.Dataset(test_X, test_y, reference=lgb_train) # 6.参数 params = { 'task': 'train', 'boosting_type': 'gbdt', # 设置提升类型 'objective': 'regression', # 目标函数 'metric': {'l2', 'auc'}, # 评估函数 'num_leaves': 31, # 叶子节点数 'learning_rate': 0.05, # 学习速率 'feature_fraction': 0.9, # 建树的特征选择比例 'bagging_fraction': 0.8, # 建树的样本采样比例 'bagging_freq': 5, # k 意味着每 k 次迭代执行bagging 'verbose': 1 # &lt;0 显示致命的, =0 显示错误 (警告), >0 显示信息 } # 7.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5) # 8.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X, num_iteration=my_model.best_iteration) # 9.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) （4）基于Scikit-learn接口的回归import pandas as pd from sklearn.model_selection import train_test_split import lightgbm as lgb from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import Imputer # 1.读文件 data = pd.read_csv('./dataset/train.csv') # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.调用LightGBM模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=20, verbosity=2) my_model.fit(train_X, train_y, verbose=False) # 6.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X) # 7.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 5.3 LightGBM调参在上一部分中，LightGBM模型的参数有一部分进行了简单的设置，但大都使用了模型的默认参数，但默认参数并不是最好的。要想让LightGBM表现的更好，需要对LightGBM模型进行参数微调。下图展示的是回归模型需要调节的参数，分类模型需要调节的参数与此类似。 6. 关于LightGBM若干问题的思考6.1 LightGBM与XGBoost的联系和区别有哪些？（1）LightGBM使用了基于histogram的决策树算法，这一点不同于XGBoost中的贪心算法和近似算法，histogram算法在内存和计算代价上都有不小优势。1）内存上优势：很明显，直方图算法的内存消耗为 (因为对特征分桶后只需保存特征离散化之后的值)，而XGBoost的贪心算法内存消耗为： ，因为XGBoost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要位的浮点数来保存。2）计算上的优势：预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为，而直方图算法只需要遍历桶就行了，时间为。 （2）XGBoost采用的是level-wise的分裂策略，而LightGBM采用了leaf-wise的策略，区别是XGBoost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGBoost也进行了分裂，带来了不必要的开销。leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。 （3）XGBoost在每一层都动态构建直方图，因为XGBoost的直方图算法不是针对某个特定的特征，而是所有特征共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而LightGBM中对每个特征都有一个直方图，所以构建一次直方图就够了。 （4）LightGBM使用直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。 （5）LightGBM支持类别特征，不需要进行独热编码处理。 （6）LightGBM优化了特征并行和数据并行算法，除此之外还添加了投票并行方案。 （7）LightGBM采用基于梯度的单边采样来减少训练样本并保持数据分布不变，减少模型因数据分布发生变化而造成的模型精度下降。 （8）特征捆绑转化为图着色问题，减少特征数量。 7. Reference由于参考的文献较多，我把每篇参考文献按照自己的学习思路，进行了详细的归类和标注。 LightGBM论文解读： 【1】Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]//Advances in Neural Information Processing Systems. 2017: 3146-3154. 【2】Taifeng Wang分享LightGBM的视频，地址：https://v.qq.com/x/page/k0362z6lqix.html 【3】开源|LightGBM：三天内收获GitHub 1000+ 星，地址：https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog 【4】Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/anshuai_aw1/article/details/83048709 【5】快的不要不要的lightGBM - 王乐的文章 - 知乎 https://zhuanlan.zhihu.com/p/31986189 【6】『 论文阅读』LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/shine19930820/article/details/79123216 LightGBM算法讲解： 【7】【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/87885678 【8】入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同，地址：https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw 【9】CatBoost vs. Light GBM vs. XGBoost，地址：https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db 【10】机器学习算法之LightGBM，地址：https://www.biaodianfu.com/lightgbm.html LightGBM工程优化： 【11】Meng Q, Ke G, Wang T, et al. A communication-efficient parallel algorithm for decision tree[C]//Advances in Neural Information Processing Systems. 2016: 1279-1287. 【12】Zhang H, Si S, Hsieh C J. GPU-acceleration for Large-scale Tree Boosting[J]. arXiv preprint arXiv:1706.08359, 2017. 【13】LightGBM的官方GitHub代码库，地址：https://github.com/microsoft/LightGBM 【14】关于sklearn中的决策树是否应该用one-hot编码？- 柯国霖的回答 - 知乎 https://www.zhihu.com/question/266195966/answer/306104444 LightGBM实例： 【15】LightGBM使用，地址：https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/ 【16】LightGBM两种使用方式 ，地址：https://www.cnblogs.com/chenxiangzhen/p/10894306.html LightGBM若干问题的思考： 【17】GBDT、XGBoost、LightGBM的区别和联系，地址：https://www.jianshu.com/p/765efe2b951a 【18】xgboost和lightgbm的区别和适用场景，地址：https://www.nowcoder.com/ta/review-ml/review?page=101 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战：原生接口和sklearn接口区别","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST实战：原生接口和sklearn接口的区别","date":"2019-12-26T16:00:00.000Z","updated":"2020-04-11T11:33:22.949Z","comments":true,"path":"2019/12/27/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan-yuan-sheng-jie-kou-he-sklearn-jie-kou-de-qu-bie/","link":"","permalink":"https://dataquaner.github.io/2019/12/27/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan-yuan-sheng-jie-kou-he-sklearn-jie-kou-de-qu-bie/","excerpt":"","text":"1 前言2 官方文档英文官方文档 中文文档 3 sklearn接口from xgboost.sklearn import XGBClassifier xgbc = XGBClassifier(n_jobs=-1) # 新建xgboost sklearn的分类class # xgboost的sklearn接口默认只使用cpu单线程，设置n_jobs=-1使用所有线程 print(\"开始xgboost classifier训练\") xgbc.fit(train_vector,np.array(train_label)) # 喂给分类器训练numpy形式的训练特征向量和标签向量 print(\"完成xgboost classifier训练，开始预测\") pre_train_Classifier = xgbc.predict(test_vector) # 喂给分类器numpy形式的测试特征向量 np.save(os.path.join(model_path,\"pre_train_Classifier.npy\"),pre_train_Classifier) # 保存结果 xgboost的sklearn接口，可以不经过标签标准化(即将标签编码为0~n_class-1)，直接喂给分类器特征向量和标签向量，使用fit训练后调用predict就能得到预测向量的预测标签，它会在内部调用sklearn.preprocessing.LabelEncoder()将标签在分类器使用时transform，在输出结果时inverse_transform。 优点：使用简单，无需对标签进行标准化处理，直接得到预测标签； 缺点：在模型保存后重新载入，丢失LabelEncoder，不能增量训练只能用一次. 4 xgboost的原生接口vector_matrix,label_single_new = get_data(data_path) # 获取得到特征矩阵、标签向量 print(\"标签总数为：%d；数据量总数为：%d\"%(len(list(set(label_single_new))),len(vector_matrix))) # 将标签标准化为0~class number-1,则xgboost概率最大的下标即为该位置数对应的标签 from sklearn import preprocessing label_coder = preprocessing.LabelEncoder() label_single_code = label_coder.fit_transform(label_single_new) # 切割训练集、测试集 from sklearn.model_selection import train_test_split train_matrix,test_matrix,train_label,test_label = train_test_split( vector_matrix,label_single_code,test_size=0.1,random_state=0) import xgboost as xgb # 参数设置见 http://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/xgboost/chapters/xgboost_usage.html params = { 'booster': 'gbtree', 'silent':0, # 如果为 0（默认值），则表示打印运行时的信息；如果为 1，则表示不打印这些信息 'objective': 'multi:softprob', # 基于softmax 的多分类模型，但是它的输出是一个矩阵：ndata*nclass，给出了每个样本属于每个类别的概率。 'num_class':len(set(label_single_new)),#指定类别数量 } dtrain = xgb.DMatrix(train_matrix, label=train_label, nthread=-1) # xgboost原生接口需要使用DMatrix格式的数据，这里与sklearn接口不同 print(\"开始xgboost训练\") xgbc = xgb.train(params,dtrain) # 初始化xgboost分类器，原生接口默认启用全部线程 xgbc.save_model(model_path+save_name+'xgbc_0.9.model') # 保存模型 # ============================================================================= # xgbc = xgb.Booster() # 重新载入模型 # xgbc.load_model(fname=model_path+save_name+'xgbc_0.9.model') # ============================================================================= print(\"xgboost训练完成，得到概率矩阵\") pre_train = xgbc.predict(xgb.DMatrix(train_matrix, nthread=-1)) # 训练数据的预测概率矩阵，启用全部线程 pre_test = xgbc.predict(xgb.DMatrix(test_matrix, nthread=-1)) # 测试数据的预测概率矩阵，启用全部线程 # 概率矩阵各行的数据为各条数据的预测概率，各行数据之和为1； # 概率矩阵各行的下标即为标准化后的label标签(0~class number-1) # 数据保存 np.save(model_path+save_name+'pre_train.npy',pre_train) np.save(model_path+save_name+'train_label.npy',train_label) np.save(model_path+save_name+'pre_test.npy',pre_test) np.save(model_path+save_name+'test_label.npy',test_label) # 数据载入 # ============================================================================= # pre_train = np.load(model_path+save_name+'pre_train.npy') # train_label = np.load(model_path+save_name+'train_label.npy') # pre_test = np.load(model_path+save_name+'pre_test.npy') # test_label = np.load(model_path+save_name+'test_label.npy') # ============================================================================= # narray_target.argsort(axis=1)，获得按行(排序对象为各行数值)升序后的下标矩阵，axis=0为按列升序; # np.fliplr(narray_target)获取矩阵的左右翻转，narray_target[::-1]获取矩阵的上下翻转 # narray_target[:,-5:]获取矩阵的后5列; top_k = 5 # 获取预测概率最大的5个标签 # 获取概率矩阵排序信息，得到按行升序的下标矩阵,切割得到各行的后5个下标, # 将其左右翻转后，得到各行降序的前5个下标，即标准化后的标签 pre_test_index = np.fliplr(pre_test.argsort(axis=1)[:,-1*top_k:]) pre_test_label = label_coder.inverse_transform(pre_test_index) # 调用label标准化工具inverse_transform将下标转化为真实标签 pre_train_index = np.fliplr(pre_train.argsort(axis=1)[:,-1*top_k:]) pre_train_label = label_coder.inverse_transform(pre_train_index) xgboost原生接口，数据需要经过标签标准化(LabelEncoder().fit_transform)、输入数据标准化(xgboost.DMatrix)和输出结果反标签标准化(LabelEncoder().inverse_transform)，训练调用train预测调用predict. 需要注意的是，xgboost原生接口输出的预测标签概率矩阵各行的下标即为标准化后的label标签(0~class number-1). 5 结论优先考虑使用原生接口形式，便于模型保存后的复用。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST实战","date":"2019-12-25T16:00:00.000Z","updated":"2020-04-11T11:33:03.342Z","comments":true,"path":"2019/12/26/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan/","link":"","permalink":"https://dataquaner.github.io/2019/12/26/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost-shi-zhan/","excerpt":"","text":"1 前言上一篇从数据原理角度深入介绍了XGBoost的实现原理及优化，参考《梯度提升树算法XGBoost》。本篇主要介绍XGBoost的工程实战，参数调优等内容。 学习一个算法实战，一般按照以下几步，第一步能够基于某个平台、某种语言构建一个模型，第二步是能够优化一个模型 。我们将学习以下内容 如果使用xgboost构建分类器 xgboost 的参数含义，以及如何调参 xgboost 的如何做cv xgboost的可视化 2 XGBoost模型构建回归模型准备数据我们使用房价数据 ，做的是一个回归任务，预测房价，分类任务类似。 导入包 import pandas as pd from xgboost import XGBRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import Imputer from sklearn.metrics import mean_absolute_error 读入和展示数据 data = pd.read_csv('../input/train.csv') data.dropna(axis=0, subset=['SalePrice'], inplace=True) y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25) my_imputer = Imputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) print(train_X.shape) print(test_X.shape) print(train_y.shape) print(test_y.shape) --- ##执行结果 (1095, 37) (365, 37) (1095,) (365,) --- 创建并训练XGBoost模型随机选取默认参数进行初始化建模 my_model = XGBRegressor() # Add silent=True to avoid printing out updates with each cycle my_model.fit(train_X, train_y, verbose=False) 评估并预测模型# make predictions predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 模型调优XGBoost有一些参数可以显著影响模型的准确性和训练速度。 n_estimatorsn_estimators 指定训练循环次数。在 欠拟合 vs 过拟合 图表, n_estimators让训练沿着图表向右移动。 值太低会导致欠拟合，这对训练数据和新数据的预测都是不准确的。 太大的值会导致过度拟合，这是对训练数据的准确预测，但对新数据的预测不准确（这是我们关心的）。 通过实际实验来找到理想的n_estimators。 典型值范围为100-1000，但这很大程度上取决于下面讨论的 early_stopping_roundsearly_stopping_rounds 提供了一种自动查找理想值的方法。 early_stopping_rounds会导致模型在validation score停止改善时停止迭代，即使迭代次数还没有到n_estimators。为n_estimators设置一个高值然后使用early_stopping_rounds来找到停止迭代的最佳时间是明智的。 存在随机的情况有时会导致validation score无法改善，因此需要指定一个数字，以确定在停止前允许多少轮退化。early_stopping_rounds = 5是一个合理的值。 因此，在五轮validation score无法改善之后训练将停止。 以下是early_stopping的代码： my_model = XGBRegressor(n_estimators=1000) my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 当使用early_stopping_rounds时，需要留出一些数据来检查要使用的轮数。 如果以后想要使所有数据拟合模型，请将n_estimators设置为在早期停止运行时发现的最佳值。 learning_rate对于更好的XGBoost模型，这是一个微妙但重要的技巧： XGBoost模型不是通过简单地将每个组件模型中的预测相加来获得预测，而是在将它们添加之前将每个模型的预测乘以一个小数字。这意味着我们添加到集合中的每个树都不会对最后结果有决定性的影响。在实践中，这降低了模型过度拟合的倾向。 因此，使用一个较大的n_estimators值并不会造成过拟合。如果使用early_stopping_rounds，树的数量会被设置成一个合适的值。 通常，较小的learning rate（以及大量的estimators）将产生更准确的XGBoost模型，但是由于它在整个循环中进行更多迭代，因此也将使模型更长时间进行训练。 包含学习率的代码如下： my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model.predict(test_X) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) 小结XGBoost目前是用于在传统数据（也称为表格或结构数据）上构建精确模型的主要算法 from xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model1 = XGBRegressor() my_model1.fit(train_X, train_y, verbose=False) predictions = my_model1.predict(test_X) print(\"Mean Absolute Error 1: \" + str(mean_absolute_error(predictions, test_y))) my_model2 = XGBRegressor(n_estimators=1000) my_model2.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model2.predict(test_X) print(\"Mean Absolute Error 2: \" + str(mean_absolute_error(predictions, test_y))) my_model3 = XGBRegressor(n_estimators=1000, learning_rate=0.05) my_model3.fit(train_X, train_y, eval_set=[(test_X, test_y)], verbose=False) predictions = my_model3.predict(test_X) print(\"Mean Absolute Error 3: \" + str(mean_absolute_error(predictions, test_y))) 分类模型以天池竞赛中的《快来一起挖掘幸福感！》中的数据为例，开始一个多分类模型的的实例 导入包import pandas as pd from matplotlib import pyplot as plt import xgboost as xgb from sklearn.model_selection import learning_curve, train_test_split,GridSearchCV from sklearn.metrics import accuracy_score from sklearn.metrics import mean_absolute_error 导入数据''' ## 准备训练集和测试集 ''' data = pd.read_csv('happiness_train_abbr.csv') y=data['happiness'] data.drop('happiness',axis=1,inplace=True) data.drop('survey_time',axis=1,inplace=True)#survey_time格式不能直接识别 X=data 数据集划分train_x, test_x, train_y, test_y = train_test_split (X, y, test_size =0.30, early_stopping_rounds=10,random_state = 33) XGBoost模型训练''' ## xgboost训练 ''' params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1 } #第一次设置300次的迭代，评测的指标是\"merror\",\"mlogloss\"，这是一个多分类问题。 model = xgb.XGBClassifier(params) eval_set = [(train_x, train_y), (test_x, test_y)] model.fit(train_x, train_y, eval_set=eval_set, eval_metric=[\"merror\", \"mlogloss\"],verbose=True) predictions = model.predict(test_x) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) accuracy = accuracy_score(test_y, predictions) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) 模型可视化''' ## 可视化训练过程 ''' results = model.evals_result() epochs = len(results['validation_0']['merror']) x_axis = range(0, epochs) from matplotlib import pyplot fig, ax = pyplot.subplots(1,2,figsize=(10,5)) ax[0].plot(x_axis, results['validation_0']['mlogloss'], label='Train') ax[0].plot(x_axis, results['validation_1']['mlogloss'], label='Test') ax[0].legend() ax[0].set_title('XGBoost Log Loss') ax[0].set_ylabel('Log Loss') ax[0].set_xlabel('epochs') ax[1].plot(x_axis, results['validation_0']['merror'], label='Train') ax[1].plot(x_axis, results['validation_1']['merror'], label='Test') ax[1].legend() ax[1].set_title('XGBoost Classification Error') ax[1].set_ylabel('Classification Error') ax[1].set_xlabel('epochs') pyplot.show() 实际训练效果，在第146次迭代就停止了，说明最好的效果实在136次左右。根据许多大牛的实践经验，选择early_stopping_rounds = 10% * n_estimators。 最终输出模型最佳状态下的结果： print (\"best iteration:\",model.best_iteration) limit = model.best_iteration predictions = model.predict(test_x,ntree_limit=limit) print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) accuracy = accuracy_score(test_y, predictions) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) 3 参考资料https://www.kaggle.com/dansbecker/xgboost https://blog.csdn.net/lujiandong1/article/details/52777168 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（07）：梯度提升树算法XGBoost","slug":"机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST","date":"2019-12-24T16:00:00.000Z","updated":"2020-04-11T11:32:44.329Z","comments":true,"path":"2019/12/25/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost/","link":"","permalink":"https://dataquaner.github.io/2019/12/25/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-07-ti-du-ti-sheng-shu-suan-fa-xgboost/","excerpt":"","text":"前言XGBoost的全称是eXtreme Gradient Boosting，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。XGBoost是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用XGBoost进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，XGBoost的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从XGBoost的数学原理和工程实现上进行介绍，然后介绍XGBoost的优缺点。 数学原理生成一棵树Boosting Tree回顾XGBoost模型是大规模并行boosting tree的工具，它是目前较好的开源boosting tree工具包。因此，在了解XGBoost算法基本原理之前，需要首先了解Boosting Tree算法基本原理。Boosting方法是一类应用广泛且非常有效的统计学习方法。它是基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比任何一个专家单独的判断要好。这种思想整体上可以分为两种： 强可学习：如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称为强可学习，直接单个模型就搞定常规问题。就好比专家给出的意见都很接近且都是正确率很高的结果，那么一个专家的结论就可以用了，这种情况非常少见。 弱可学习：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。这种情况是比较常见的。 boosting算法主要是针对弱可学习的分类器来开展优化工作。其关心的问题包括两方面内容： （1）在每一轮如何改变训练数据的权值和概率分布； （2）如何将弱分类器组合成一个强分类器，这种思路较好的就是AdaBoost算法，以前在遥感图像地物识别中得到过应用。 Boosting Tree模型采用加法模型与前向分步算法，而且基模型都是决策树模型。前向分步算法（Forward stage wise additive model）是指在叠加新的基模型的基础上同步进行优化，具体而言，就是每一次叠加的模型都去拟合上一次模型拟合后产生的残差（Residual）。从算法模型解释上来说，Boosting Tree是决策树的加法模型： （1） 上式中M为决策树的数量； 为某个决策树； 为对应决策树的参数。 Boosting Tree模型采用前向分步算法，其中假设 ，则第m步的模型是： （2） 为求解对应的参数 ，需要最小化相应损失函数来确定，具体公式如下： （3） 由前向分步算法得到M棵决策树 后，再进行加和，就得到了提升树模型 。在xgboost论文中提到的一个明显的boosting tree的加和应用案例如图3所示。 图2 boosting tree的累加效果示意图 相关树模型的参数值求解主要依据于损失函数的定义。 一般来言对于分类问题，选择指数损失函数作为损失函数时，将形成AdaBoost模型； 对于回归问题，损失函数常利用平方损失函数。为了扩展Boosting Tree的应用范围，需要构建一种可以广泛适用的残差描述方式来满足于任意损失函数的形式，为解决分类问题的Gradient Boosting Decision Tree算法应运而生。 带正则项的Boosting Tree模型和带梯度的Boosting Tree推导过程 目标函数我们知道 XGBoost 是由 个基模型组成的一个加法运算式： 其中 为第 个基模型， 为第 个样本的预测值。 损失函数可由预测值 与真实值 进行表示： 其中 为样本数量。 我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的损失函数 与抑制模型复杂度的正则项 组成，所以我们有： 为模型的正则项，由于 XGBoost 支持决策树也支持线性模型，所以这里再不展开描述。 我们知道 boosting 模型是前向加法，以第 步的模型为例，模型对第 个样本 的预测为： 其中 由第 步的模型给出的预测值，是已知常数， 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成： 求此时最优化目标函数，就相当于求解 。 泰勒公式是将一个在 处具有 阶导数的函数 利用关于 的 次多项式来逼近函数的方法，若函数 在包含 的某个闭区间 上具有 阶导数，且在开区间 上具有 阶导数，则对闭区间 上任意一点 有 ，其中的多项式称为函数在 处的泰勒展开式， 是泰勒公式的余项且是 的高阶无穷小。 根据泰勒公式我们把函数 在点 处进行泰勒的二阶展开，可得到如下等式： 我们把 视为 ， 视为 ，故可以将目标函数写为： 其中 为损失函数的一阶导， 为损失函数的二阶导，注意这里的导是对 求导。 我们以平方损失函数为例： 则： 由于在第 步时 其实是一个已知的值，所以 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成： 所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 ，最后根据加法模型得到一个整体模型。 基于决策树的目标函数损失函数可由预测值 与真实值 进行表示： 其中， 为样本的数量。 我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要在目标函数中添加正则项，用于防止过拟合。所以目标函数由模型的损失函数 与抑制模型复杂度的正则项 组成，目标函数的定义如下： 其中， 是将全部 棵树的复杂度进行求和，添加到目标函数中作为正则化项，用于防止模型过度拟合。 由于XGBoost是boosting族中的算法，所以遵从前向分步加法，以第 步的模型为例，模型对第 个样本 的预测值为： 其中， 是由第 步的模型给出的预测值，是已知常数， 是这次需要加入的新模型的预测值。此时，目标函数就可以写成： 注意上式中，只有一个变量，那就是第 棵树 ，其余都是已知量或可通过已知量可以计算出来的。细心的同学可能会问，上式中的第二行到第三行是如何得到的呢？这里我们将正则化项进行拆分，由于前 棵树的结构已经确定，因此前 棵树的复杂度之和可以用一个常量表示，如下所示： 泰勒公式展开泰勒公式是将一个在 处具有 阶导数的函数 利用关于 的 次多项式来逼近函数的方法。若函数 在包含 的某个闭区间 上具有 阶导数，且在开区间 上具有 阶导数，则对闭区间 上任意一点 有： 其中的多项式称为函数在 处的泰勒展开式， 是泰勒公式的余项且是 的高阶无穷小。 根据泰勒公式，把函数 在点 处进行泰勒的二阶展开，可得如下等式： 回到XGBoost的目标函数上来， 对应损失函数 ， 对应前 棵树的预测值 ， 对应于我们正在训练的第 棵树 ，则可以将损失函数写为： 其中， 为损失函数的一阶导， 为损失函数的二阶导，注意这里的求导是对 求导。 我们以平方损失函数为例： 则： 将上述的二阶展开式，带入到XGBoost的目标函数中，可以得到目标函数的近似值： 由于在第 步时 其实是一个已知的值，所以 是一个常数，其对函数的优化不会产生影响。因此，去掉全部的常数项，得到目标函数为： 所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 ，最后根据加法模型得到一个整体模型。 一棵树的生长细节分裂结点在实际训练过程中，当建立第 t 棵树时，XGBoost采用贪心法进行树结点的分裂： 从树深为0时开始： 对树中的每个叶子结点尝试进行分裂； 每次分裂后，原来的一个叶子结点继续分裂为左右两个子叶子结点，原叶子结点中的样本集将根据该结点的判断规则分散到左右两个叶子结点中； 新分裂一个结点后，我们需要检测这次分裂是否会给损失函数带来增益，增益的定义如下： 如果增益Gain&gt;0，即分裂为两个叶子节点后，目标函数下降了，那么我们会考虑此次分裂的结果。 但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？接下来会讲到。 寻找最佳分裂点 在实际训练过程中，当建立第 棵树时，一个非常关键的问题是如何找到叶子节点的最优切分点，XGBoost支持两种分裂节点的方法——贪心算法和近似算法。 贪心算法 从树的深度为0开始： 对每个叶节点枚举所有的可用特征； 针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益； 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集； 回到第1步，递归执行直到满足特定条件为止； 那么如何计算每个特征的分裂收益呢？ 假设我们在某一节点完成特征分裂，则分裂前的目标函数可以写为： 分裂后的目标函数为： 则对于目标函数来说，分裂后的收益为： 注意：该特征收益也可作为特征重要性输出的重要依据。 对于每次分裂，我们都需要枚举所有特征可能的分割方案，如何高效地枚举所有的分割呢？ 假设我们要枚举某个特征所有 这样条件的样本，对于某个特定的分割点 我们要计算 左边和右边的导数和。 我们可以发现对于所有的分裂点 ，只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 、 。然后用上面的公式计算每个分割方案的收益就可以了。 观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入新叶子的惩罚项，也就是说引入的分割带来的增益如果小于一个阀值的时候，我们可以剪掉这个分割。 上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。 但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。 基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案： 特征预排序+缓存：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。 分位点近似法：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。 并行查找：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。 近似算法 贪心算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪心算法这一缺点给出了近似最优解。 对于每个特征，只考察分位点可以减少计算复杂度。 该算法首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。 在提出候选切分点时有两种策略： Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割； Local：每次分裂前将重新提出候选切分点。 直观上来看，Local策略需要更多的计算步骤，而Global策略因为节点已有划分所以需要更多的候选点。 下图给出不同种分裂策略的AUC变化曲线，横坐标为迭代次数，纵坐标为测试集AUC，eps为近似算法的精度，其倒数为桶的数量。 从上图我们可以看到， Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在eps取值合理的情况下，分位数策略可以获得与贪心算法相同的精度。 近似算法简单来说，就是根据特征 的分布来确定 个候选切分点 ，然后根据这些候选切分点把相应的样本放入对应的桶中，对每个桶的 进行累加。最后在候选切分点集合上贪心查找。该算法描述如下： 算法讲解： 第一个for循环：对特征k根据该特征分布的分位数找到切割点的候选集合 。这样做的目的是提取出部分的切分点不用遍历所有的切分点。其中获取某个特征k的候选切割点的方式叫proposal(策略)。XGBoost 支持 Global 策略和 Local 策略。 第二个for循环：将每个特征的取值映射到由该特征对应的候选点集划分的分桶区间，即 。对每个桶区间内的样本统计值 G,H并进行累加，最后在这些累计的统计量上寻找最佳分裂点。这样做的目的是获取每个特征的候选分割点的 G,H值。 下图给出近似算法的具体例子，以三分位为例： 根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 G,H 值，最终求解节点划分的增益。 停止生长一棵树不会一直生长下去，下面是一些常见的限制条件。 (1) 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 (2) 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 (3) 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。 每个叶子结点的样本权值和计算方式如下： 总结推导过程： 算法工程优化对内存的优化：列块并行学习在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而 XGBoost 在训练之前会根据特征对数据进行排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。 作者提出通过按特征进行分块并排序，在块里面保存排序后的特征值及对应样本的引用，以便于获取样本的一阶、二阶导数值。具体流程为： 整体训练数据可以看做一个 的超大规模稀疏矩阵 按照mini-batch的方式横向分割，可以切成很多个“Block” 每一个“Block”内部采用一种Compress Sparse Column的稀疏短阵格式，每一列特征分别做好升序排列，便于搜索切分点，整体的时间复杂度有效降低。 通过Block的设置，可以采用并行计算，从而提升模型训练速度。 具体方式如图： 通过顺序访问排序后的块遍历样本特征的特征值，方便进行切分点的查找。此外分块存储后多个特征之间互不干涉，可以使用多线程同时对不同的特征进行切分点查找，即特征的并行化处理。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 XGBoost 能够实现分布式或者多线程计算的原因。 对CPU Cache的优化：缓存优化针对一个具体的块(block)，其中存储了排序好的特征值，以及指向特征值所属样本的索引指针，算法需要间接地利用索引指针来获得样本的梯度值。列块并行学习的设计可以减少节点分裂时的计算量，在顺序访问特征值时，访问的是一块连续的内存空间，但通过特征值持有的索引（样本索引）访问样本获取一阶、二阶导数时，这个访问操作访问的内存空间并不连续，这样可能造成cpu缓存命中率低，影响算法效率。由于块中数据是按特征值来排序的，当索引指针指向内存中不连续的样本时，无法充分利用CPU缓存来提速。 为了解决缓存命中率低的问题，XGBoost 提出了两种优化思路。 （1）提前取数（Prefetching） 对于精确搜索，利用多线程的方式，给每个线程划分一个连续的缓存空间，当training线程在按特征值的顺序计算梯度的累加时，prefetching线程可以提前将接下来的一批特征值对应的梯度加载到CPU缓存中。为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就实现了非连续空间到连续空间的转换，提高了算法效率。 （2）合理设置分块大小 对于近似分桶搜索，按行分块时需要准确地选择块的大小。块太小会导致每个线程的工作量太少，切换线程的成本过高，不利于并行计算；块太大导致缓存命中率低，需要花费更多时间在读取数据上。经过反复实验，作者找到一个合理的block_size为 。 对IO的优化：核外块计算当数据量非常大时，我们不能把所有的数据都加载到内存中。那么就必须将一部分需要加载进内存的数据先存放在硬盘中，当需要时再加载进内存。这样操作具有很明显的瓶颈，即硬盘的IO操作速度远远低于内存的处理速度，肯定会存在大量等待硬盘IO操作的情况。针对这个问题作者提出了“核外”计算的优化方法。具体操作为，将数据集分成多个块存放在硬盘中，使用一个独立的线程专门从硬盘读取数据，加载到内存中，这样算法在内存中处理数据就可以和从硬盘读取数据同时进行。此外，XGBoost 还用了两种方法来降低硬盘读写的开销： 块压缩（Block Compression）。论文使用的是按列进行压缩，读取的时候用另外的线程解压。对于行索引，只保存第一个索引值，然后用16位的整数保存与该block第一个索引的差值。作者通过测试在block设置为 个样本大小时，压缩比率几乎达到26% 29%。 块分区（Block Sharding ）。块分区是将特征block分区存放在不同的硬盘上，以此来增加硬盘IO的吞吐量。 优缺点优点 精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数； 灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导； 正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是XGBoost优于传统GBDT的一个特性。 Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统GBDT的实现也有学习速率； 列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是XGBoost异于传统GBDT的一个特性； 缺失值处理：对于特征的值有缺失的样本，XGBoost 采用的稀疏感知算法可以自动学习出它的分裂方向； XGBoost工具支持并行：boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 可并行的近似算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，用于高效地生成候选的分割点。 缺点 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集； 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。 XGBoost与GBDT的差异在分析XGBooting优缺点的时候，通过比较该算法与GBDT的差异，即可有较清楚的描述，具体表现在如下方面。 （1）基分类器的差异 GBDT算法只能利用CART树作为基学习器，满足分类应用； XGBoost算法除了回归树之外还支持线性的基学习器，因此其一方面可以解决带L1与L2正则化项的逻辑回归分类问题，也可以解决线性回问题。 （2）节点分类方法的差异 GBDT算法主要是利用Gini impurity针对特征进行节点划分； XGBoost经过公式推导，提出的weighted quantile sketch（加权分位数缩略图）划分方法，依据影响Loss的程度来确定连续特征的切分值。 （3）模型损失函数的差异 传统GBDT在优化时只用到一阶导数信息； xgboost则对代价函数进行了二阶泰勒展开，二阶导数有利于梯度下降的更快更准。 （4）模型防止过拟合的差异 GBDT算法无正则项，可能出现过拟合； Xgboost在代价函数里加入了正则项，用于控制模型的复杂度，降低了过拟合的可能性。 （5）模型实现上的差异 决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。其能够实现在特征粒度的并行。 XGBoost代码实现安装XGBoost依赖包pip install xgboost XGBoost分类和回归XGBoost有两大类接口：XGBoost原生接口 和 scikit-learn接口 ，并且XGBoost能够实现分类和回归两种任务。 （1）基于XGBoost原生接口的分类 from sklearn.datasets import load_iris import xgboost as xgb from xgboost import plot_importance from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split # read in the iris data iris = load_iris() X = iris.data y = iris.target # split train data and test data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234565) # set XGBoost's parameters params = { 'booster': 'gbtree', 'objective': 'multi:softmax', # 回归任务设置为：'objective': 'reg:gamma', 'num_class': 3, # 回归任务没有这个参数 'gamma': 0.1, 'max_depth': 6, 'lambda': 2, 'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 3, 'silent': 1, 'eta': 0.1, 'seed': 1000, 'nthread': 4, } plst = params.items() dtrain = xgb.DMatrix(X_train, y_train) num_rounds = 500 model = xgb.train(plst, dtrain, num_rounds) # 对测试集进行预测 dtest = xgb.DMatrix(X_test) ans = model.predict(dtest) # 计算准确率 cnt1 = 0 cnt2 = 0 for i in range(len(y_test)): if ans[i] == y_test[i]: cnt1 += 1 else: cnt2 += 1 print(\"Accuracy: %.2f %% \" % (100 * cnt1 / (cnt1 + cnt2))) # 显示重要特征 plot_importance(model) plt.show() （2）基于Scikit-learn接口的回归 这里，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 来进行实例讲解。 该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。 import pandas as pd from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer import xgboost as xgb from sklearn.metrics import mean_absolute_error # 1.读文件 data = pd.read_csv('./dataset/train.csv') data.dropna(axis=0, subset=['SalePrice'], inplace=True) # 2.切分数据输入：特征 输出：预测目标变量 y = data.SalePrice X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object']) # 3.切分训练集、测试集,切分比例7.5 : 2.5 train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25) # 4.空值处理，默认方法：使用特征列的平均值进行填充 my_imputer = SimpleImputer() train_X = my_imputer.fit_transform(train_X) test_X = my_imputer.transform(test_X) # 5.调用XGBoost模型，使用训练集数据进行训练（拟合） # Add verbosity=2 to print messages while running boosting my_model = xgb.XGBRegressor(objective='reg:squarederror', verbosity=2) # xgb.XGBClassifier() XGBoost分类模型 my_model.fit(train_X, train_y, verbose=False) # 6.使用模型对测试集数据进行预测 predictions = my_model.predict(test_X) # 7.对模型的预测结果进行评判（平均绝对误差） print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y))) XGBoost调参在上一部分中，XGBoot模型的参数都使用了模型的默认参数，但默认参数并不是最好的。要想让XGBoost表现的更好，需要对XGBoost模型进行参数微调。XGBoost需要调的参数不算多，他们可以分成三个部分： 1、General Parameters，即与整个模型属基调相关的参数； 2、Booster Parameters，即与单颗树生成有关的参数； 3、Learning Task Parameters，与模型调优相关的参数； General Parameters1、booster [default=gbtree] 即xgboost中基学习器类型，有两种选择，分别是树模型（gbtree）和线性模型（linear models） 2、silent [default=0] 即控制迭代日志的是否输出，默认输出； 3、nthread [default to maximum number of threads available if not set] 即控制模型训练调用机器的核心数，与sklearn中n_jobs的含义相似； Booster parameters因为booster有两种类型，常用的一般是树模型，这里只列树模型相关的参数： 1、eta [default=0.3] ：学习率 学习率，这个相当于sklearn中的learning_rate，常见的设置范围在0.01-0.2之间 2、min_child_weight [default=1]：叶节点的最小权重值 这个参数与GBM（sklearn）中的“min_samples_leaf”很相似，只不过这里不是样本数，而是权重值，如果样本的权重都是1，这两个参数是等同的；这个值设置较大时，通常树不会太深，可以控制过拟合，但太大时，容易造成欠拟合的现象，具体调参需要cv； 3、max_depth：树的最大深度 树的最大深度，含义很直白，控制树的复杂性；通常取值范围在3-10； 4、max_leaf_nodes：最大叶节点数 一般这个参数与max_depth二选一控制即可； 5、gamma [default=0]：分裂收益阈值 即用来比较每次节点分裂带来的收益，有效控制节点的过度分裂； 这个参数的变化范围受损失函数的选取影响； 6、max_delta_step [default=0] 这个参数暂时不是很理解它的作用范围，一般可以忽略它； 7、subsample [default=1]：采样比例 与sklearn中的参数一样，即每颗树的生成可以不去全部样本，这样可以控制模型的过拟合；通常取值范围0.5-1； 8、colsample_bytree [default=1]：特征采样的比例（每棵树） 即每棵树不使用全部的特征，控制模型的过拟合； 通常取值范围0.5-1； 9、colsample_bylevel [default=1] 特征采样的比例（每次分裂）； 这个与随机森林的思想很相似，即每次分裂都不取全部变量； 当7、8的参数设置较好时，该参数可以不用在意； 10、lambda [default=1] L2范数的惩罚系数，叶子结点的分数？； 11、alpha [default=0] L1范数的惩罚系数，叶子结点数？； 12、scale_pos_weight [default=1] 这个参数也不是很理解，貌似与类别不平衡的问题相关； Learning Task Parameters1、objective [default=reg:linear]：目标函数 通常的选项分别是：binary:logistic，用于二分类，产生每类的概率值；multi:softmax，用于多分类，但不产生概率值，直接产生类别结果；multi:softprob，类似softmax，但产生多分类的概率值； 2、eval_metric [ default according to objective ]：评价指标 当你给模型一个验证集时，会输出对应的评价指标值； 一般有：rmse ，均方误差；mae ，绝对平均误差；logloss ，对数似然值；error ，二分类错误率；merror ，多分类错误率；mlogloss ；auc 3、seed：即随机种子 关于XGBoost若干问题的思考XGBoost与GBDT的联系和区别有哪些？（1）GBDT是机器学习算法，XGBoost是该算法的工程实现。 （2）正则项：在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。 （3）导数信息：GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。 （4）基分类器：传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。 （5）子采样：传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。 （6）缺失值处理：传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。 （7）并行化：传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 为什么XGBoost泰勒二阶展开后效果就比较好呢？（1）从为什么会想到引入泰勒二阶的角度来说（可扩展性）：XGBoost官网上有说，当目标函数是MSE时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如logistic loss的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把MSE推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与MSE统一？是因为MSE是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与MSE统一了，那就只用推导MSE就好了。 （2）从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。 XGBoost对缺失值是怎么处理的？在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。 XGBoost为什么可以并行训练？（1）XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。 （2）XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 20道XGBoost面试题简单介绍一下XGBoost首先需要说一说GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。 XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。 XGBoost与GBDT有什么不同 基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。 导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。 正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。 列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。 并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 XGBoost为什么使用泰勒二阶展开 精准性：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导。 XGBoost为什么可以并行训练 XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。 XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 XGBoost为什么快 分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点 候选分位点：每个特征采用常数个分位点作为候选分割点 CPU cache 命中优化： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。 Block 处理优化：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐 XGBoost防止过拟合的方法XGBoost在设计时，为了防止过拟合做了很多优化，具体如下： 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化 列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可） 子采样：每轮计算可以不使用全部样本，使算法更加保守 shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间 XGBoost如何处理缺失值XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下： 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。 find_split时，缺失值处理的伪代码 XGBoost中叶子结点的权重如何计算出来XGBoost目标函数最终推导形式如下： 利用一元二次函数求最值的知识，当目标函数达到最小值Obj时，每个叶子结点的权重为wj。 具体公式如下： XGBoost中的一棵树的停止生长条件 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。 RF和GBDT的区别相同点： 都是由多棵树组成，最终的结果都是由多棵树一起决定。 不同点： 集成学习：RF属于bagging思想，而GBDT是boosting思想 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成) 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感 泛化能力：RF不易过拟合，而GBDT容易过拟合 XGBoost如何处理不平衡数据对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决： 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10； 第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。 原话是这么说的： For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of xgboost model, and there are two ways to improve it. If you care only about the ranking order (AUC) of your prediction Balance the positive and negative weights, via scale_pos_weight Use AUC for evaluation If you care about predicting the right probability In such a case, you cannot re-balance the dataset In such a case, set parameter max_delta_step to a finite number (say 1) will help convergence 那么，源码到底是怎么利用scale_pos_weight来平衡样本的呢，是调节权重还是过采样呢？请看源码： if (info.labels[i] == 1.0f) w *= param_.scale_pos_weight 可以看出，应该是增大了少数样本的权重。 除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。 比较LR和GBDT，说说什么情景下GBDT不如LR先说说LR和GBDT的区别： LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程 GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合； 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下： 先看一个例子： 假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。 那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1f1 + Wifi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？ 仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。 XGBoost中如何对树进行剪枝 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。 XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。 XGBoost如何选择最佳分裂点？XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。 因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。 如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。 XGBoost的Scalable性如何体现 基分类器的scalability：弱分类器可以支持CART决策树，也可以支持LR和Linear。 目标函数的scalability：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。 学习方法的scalability：Block结构支持并行化，支持 Out-of-core计算。 XGBoost如何评价特征的重要性我们采用三种方法来评判XGBoost模型中特征的重要程度： 官方文档：（1）weight - the number of times a feature is used to split the data across all trees. （2）gain - the average gain of the feature when it is used in trees. （3）cover - the average coverage of the feature when it is used in trees. weight ：该特征在所有树中被用作分割样本的特征的总次数。 gain ：该特征在其出现过的所有树中产生的平均增益。 cover ：该特征在其出现过的所有树中的平均覆盖范围。 注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。 XGBooost参数调优的一般步骤首先需要初始化一些基本变量，例如： max_depth = 5 min_child_weight = 1 gamma = 0 subsample, colsample_bytree = 0.8 scale_pos_weight = 1 (1) 确定learning rate和estimator的数量 learning rate可以先用0.1，用cv来寻找最优的estimators (2) max_depth和 min_child_weight 我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。 max_depth，每棵子树的最大深度，check from range(3,10,2)。 min_child_weight，子节点的权重阈值，check from range(1,6,2)。 如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。 (3) gamma 也称作最小划分损失min_split_loss，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。 如果大于该阈值，则该叶子节点值得继续划分 如果小于该阈值，则该叶子节点不值得继续划分 (4) subsample, colsample_bytree subsample是对训练的采样比例 colsample_bytree是对特征的采样比例 both check from 0.6 to 0.9 (5) 正则化参数 alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100 lambda 是L2正则化系数 (6) 降低学习率 降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1 XGBoost模型如果过拟合了怎么解决当出现过拟合时，有两类参数可以缓解： 第一类参数：用于直接控制模型的复杂度。包括max_depth,min_child_weight,gamma 等参数 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample,colsample_bytree 还有就是直接减小learning rate，但需要同时增加estimator 参数。 为什么XGBoost相比某些模型对缺失值不敏感对存在缺失值的特征，一般的解决方法是： 离散型变量：用出现次数最多的特征值填充； 连续型变量：用中位数或均值填充； 一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。 而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。 XGBoost对缺失数据有特定的处理方法，详情参考上篇文章第7题。 因此，对于有缺失值的数据在经过缺失处理后： 当数据量很小时，优先用朴素贝叶斯 数据量适中或者较大，用树模型，优先XGBoost 数据量较大，也可以用神经网络 避免使用距离度量相关的模型，如KNN和SVM XGBoost和LightGBM的区别 （1）树生长策略：XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 （2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下： 减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。 计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑏𝑖𝑛𝑠)。 LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算 但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？ xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。 （3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。 （4）缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。 （5）LightGBM 与 XGboost 的并行策略不同： 特征并行 ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。 数据并行 ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。 投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。 参考资料XGBoost论文解读： 【1】Chen T , Guestrin C . XGBoost: A Scalable Tree Boosting System[J]. 2016. 【2】Tianqi Chen的XGBoost的Slides 【3】对xgboost的理解 - 金贵涛的文章 - 知乎 【4】CTR预估 论文精读(一)–XGBoost 【5】XGBoost论文阅读及其原理 - Salon sai的文章 - 知乎 【6】XGBoost 论文翻译+个人注释 XGBoost算法讲解： 【7】XGBoost超详细推导，终于有人讲明白了！ 【8】终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！ 【9】机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - wepon的回答 - 知乎 【10】GBDT算法原理与系统设计简介，wepon XGBoost实例： 【11】Kaggle 神器 xgboost 【12】干货 | XGBoost在携程搜索排序中的应用 【13】史上最详细的XGBoost实战 - 章华燕的文章 - 知乎 【14】XGBoost模型构建流程及模型参数微调（房价预测附代码讲解） - 人工智能学术前沿的文章 - 知乎 XGBoost面试题： 【15】珍藏版 | 20道XGBoost面试题，你会几个？(上篇) 【16】珍藏版 | 20道XGBoost面试题，你会几个？(下篇) 【17】推荐收藏 | 10道XGBoost面试题送给你 【18】面试题：xgboost怎么给特征评分？ 【19】[校招-基础算法]GBDT/XGBoost常见问题 - Jack Stark的文章 - 知乎 【20】《百面机器学习》诸葛越主编、葫芦娃著，P295-P297。 【21】灵魂拷问，你看过Xgboost原文吗？ - 小雨姑娘的文章 - 知乎 【22】为什么xgboost泰勒二阶展开后效果就比较好了呢？ - Zsank的回答 - 知乎 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","slug":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","date":"2019-12-24T06:08:00.000Z","updated":"2020-04-11T11:32:24.599Z","comments":true,"path":"2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-05-ti-du-ti-sheng-shu-suan-fa-gbdt/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-05-ti-du-ti-sheng-shu-suan-fa-gbdt/","excerpt":"","text":"1 前言前面讲述了《决策树的特征选择》、《决策树的生成》、《决策树的剪枝》，熟悉了单棵决策树的的实现细节，在实际应用时，往往采用多棵决策树组合的形式完成目标任务。那么如何组合单棵决策树可以使得模型效果更优呢？目前主要有两种思想：bagging和boosting，分别对应的典型算法随机森林和Adaboost、GBDT等。 Bagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。典型实现算法随机森林 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。典型实现算法是GBDT boosting的思想如下图： 基于boosting思想的经典算法是Adaboost和GBDT。关于Adaboost的介绍可以参考《Adaboost算法》，本文重点介绍GBDT。 2 什么是GBDT GBDT(Gradient Boosting Decision Tree) 是一种迭代的决策树算法，是回归树，而不是分类树。该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性。 GBDT用来做回归预测，调整后也可以用于分类。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。具体训练过程如下图示意： 3 GBDT算法原理GBDT算法的核心思想 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。即所有弱分类器相加等于预测值，下一个弱分类器去拟合误差函数对预测值的梯度。 GBDT加入了简单的数值优化思想。 Xgboost更加有效应用了数值优化。相比于gbdt，最重要是对损失函数变得更复杂。目标函数依然是所有树想加等于预测值。损失函数引入了一阶导数，二阶导数。 不同于随机森林所有树的预测求均值，GBDT所有的树的预测值加起来是最终的预测值，可以不断接近真实值。 GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想的通俗解释 假如有个人30岁， 第一棵树，我们首先用20岁去拟合，发现损失有10岁， 第二颗，这时我们用6岁去拟合剩下的损失，发现差距还有4岁， 第三颗，我们用3岁拟合剩下的差距，差距就只有一岁了。 三棵树加起来为29岁，距离30最近。 从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？ 4 负梯度拟合在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为 利用(xi,rti)(i=1,2,..m),我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域Rtj,j=1,2,…,J。其中J为叶子节点的个数。 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj如下： 这样就得到了本轮的决策树拟合函数如下： 从而本轮最终得到的强学习器的表达式如下： 通过损失函数的负梯度来拟合，找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 传统模型中，我们定义一个固定结构的函数，然后通过样本训练拟合更新该函数的参数，获得最后的最优函数。 GBDT提升树并非如此。它是加法模型，是不定结构的函数，通过不断加入新的子函数来使得模型能更加拟合训练数据，直到最优。函数更新的迭代方式可以写作：)。所以如果要更快逼近最优的函数，我们就需要在正确的方向上添加子函数，这个“正确的方向”当然就是损失减少最快的方向。所以我们需要用损失函数)对函数)求导（注意不是对x求导），求得的导数，就是接下来)需要弥补的方向。在上式中就是表示导数的拟合。 导数值跟损失函数的选择有关系。如果选择平方损失误差，那么它的导数就是： 令人惊喜的是这正是真实值和估计值之间的残差！ 这就是为什么谈到GBDT的时候，很多文章都提到“残差”的拟合，却没有说“梯度”的拟合。其实它们在平方损失误差条件下是一个意思！BTW，上面之所以用了是为了计算方便，常数项并不会影响平方损失误差，以及残差的比较。 现在让我们重新理解这个式子： 1）先求取一个拟合函数Fm-1(x) 2）用Fm-1(x)进行预测，计算预测值和实际值的残差 3）为了弥补上面的残差，用一个函数△F(x)来拟合这个残差 4）这样最终的函数就变成了)，其中Fm-1(x)用来拟合原数据，△F(x)用来拟合残差 5）如果目前还有较大的残差，则循环2)~4)，更新函数到Fm+1(x) , Fm+2(x), …..直到残差满足条件。 针对以上流程，我们用实例来说明 5 提升树的生成过程有以下数据需要用回归，并要求平方损失误差小于0.2（这0.2就是我们人为设置的最优条件，否则训练可能会无休止地进行下去）时，可以停止建树： 第一棵树 1） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 比如s=1.5,分割成了两个子集： 通过公式求平方损失误差 而其中)为各自子集的平均值时，可以使得每个子集的平方损失误差最小。 求平均值为：)，进而求得平方损失误差为 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=6.5时,为所有切分点里平方损失误差最小的 2) 选择切分点s=6.5构建第一颗回归树，各分支数值使用 ： 第一轮过后，我们提升树为: 3) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果： 提升树拟合数据的平方损失误差计算： 大于0.2，则还需要继续建树。 第二棵树 4) 确定需要拟合的训练数据为上一棵树的残差： 5） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=3.5时,为所有切分点里平方损失误差最小的 6) 选择切分点s=3.5构建第二颗回归树，各分支数值使用 ： 第二轮过后，我们提升树为: 7) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果，同时对比初始值和上一颗树的残差： 可以看见，随着树的增多，残差一直在减少。 到目前为止，提升树拟合数据的平方损失误差计算： 多说一句，这里是从全局提升树的角度去计算损失，其实和上面第5）步中从最后一颗树的角度去计算损失，结果是一样的 目前损失大于0.2的阈值，还需要继续建树 … … 第六棵树 到第六颗树的时候，我们已经累计获得了： 此时提升树为： 此时用拟合训练数据的平方损失误差为： 平方损失误差小于0.2的阈值，停止建树。 为我们最终所求的提升树。 6 回归算法输入： 最大迭代次数T, 损失函数L，训练样本集 输出： 强学习器f(x) 1） 初始化弱学习器 2）对迭代轮数t=1,2,…T有： a) 对样本i=1,2，…m，计算负梯度 b) 利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,…,J。其中J为回归树t的叶子节点的个数。 c) 对叶子区域j =1,2,..J,计算最佳拟合值 (d) 更新强学习器 3） 得到强学习器f(x)的表达式 7 分类算法GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。 为了解决这个问题，主要有两个方法， 1）一个是用指数损失函数，此时GBDT退化为Adaboost算法。 2）另一种方法是用类似于逻辑回归的对数似然损失函数的方法。 也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。 7.1 二元分类算法对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为： 其中y∈{−1,+1}。则此时的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。 7.2 多元分类算法多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为： 集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。 8 正则化和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。 GBDT的正则化主要有三种方式。 第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代 如果我们加上了正则化项，则有 ν的取值范围为0&lt;ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了 9 总结GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。 优点 1) 可以灵活处理各种类型的数据，包括连续值和离散值。 2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 缺点 1) 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"}]},{"title":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","slug":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","date":"2019-12-24T06:08:00.000Z","updated":"2020-04-11T11:33:49.104Z","comments":true,"path":"2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-09-id3-c4.5-cart-sui-ji-sen-lin-bagging-boosting-adaboost-gbdt-xgboost-suan-fa-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-09-id3-c4.5-cart-sui-ji-sen-lin-bagging-boosting-adaboost-gbdt-xgboost-suan-fa-zong-jie/","excerpt":"","text":"最近心血来潮，整理了一下和树有关的方法和模型，请多担待！ 决策树首先，决策树是一个有监督的分类模型，其本质是选择一个能带来最大信息增益的特征值进行树的分割，直到到达结束条件或者叶子结点纯度到达一定阈值。下图是决策树的一个简单例子 按照分割指标和分割方法，决策树的经典模型可以分为ID3、C4.5以及CART ID3：以信息增益为准则来选择最优划分属性信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 C4.5：基于信息增益率准则选择最优分割属性信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 CART：以基尼系数为准则选择最优划分属性CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 随机森林在讲随机森林之前，我们需要补充一点组合分类器的概念，将多个分类器的结果进行多票表决或者是取平均值，以此作为最终的结果。 1、构建组合分类器的好处： （1）、提升模型精度：整合各个模型的分类结果，得到更合理的决策边界，减少整体错误，实现更好的分类效果； （2）、处理过大或过小的数据集：数据集较大时，可以将数据集划分成多个子集，对子集构建分类器；数据集较小时，可通过多种抽样方式（bootstrap）从原始数据集抽样产生多组不同的数据集，构建分类器。 （3）、若决策边界过于复杂，则线性模型不能很好地描述真实情况。因此先对于特定区域的数据集，训练多个线性分类器，再将它们集成。 （4）、比较适合处理多源异构数据（存储方式不同（关系型、非关系型），类别不同（时序型、离散型、连续型、网络结构数据）） 随机森林是一个典型的多个决策树的组合分类器。主要包括两个方面：数据的随机性选取，以及待选特征的随机选取。 （1）、数据的随机选取：第一，从原始的数据集中采取有放回的抽样（bootstrap），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。 （2）、待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。 组合树示例图 GBDT和xgboostbagging和boostingBagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。 boosting算法结构图 拿著名的Adaboost算法举例： 我们有一个数据集，样本大小为N，每一个样本对应一个原始标签起初，我们初始化样本的权重为1/N em计算的是当前数据下，模型的分类误差率，模型的系数值是基于分类误差率的 根据模型的分类结果，更新原始数据中数据的分布，增加被错分的数据被抽中的概率，以便下一次迭代的时候能被模型重新训练 最终的分类器是各个基分类器的组合 GBDTGBDT是以决策树（CART）为基学习器的GB算法，是迭代树，而不是分类树。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。有了前面Adaboost的铺垫，大家应该能很容易理解大体思想。 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。 xgboostXgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。 损失函数如下，引入了一阶导数，二阶导数。： 好的模型需要具备两个基本要素：一是要有好的精度（即好的拟合程度），二是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项） 常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。 每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差 目标函数如上图，最后一行画圈部分实际上就是预测值和真实值之间的残差 先对训练误差进行展开： xgboost则对代价函数进行了二阶泰勒展开，同时用到了残差平方和的一阶和二阶导数 再研究目标函数中的正则项： 树的复杂度可以用树的分支数目来衡量，树的分支我们可以用叶子结点的数量来表示 那么树的复杂度式子：右边第一项是叶子结点的数量T，第二项是树的叶子结点权重w的l2正则化，正则化是为了防止叶子结点过多 此时，每一次迭代，相当于在原有模型中增加一棵树，目标函数中，我们用wq（x）表示一棵树，包括了树的结构以及叶子结点的权重，w表示权重（反映预测的概率），q表示样本所在的索引号（反映树的结构） 将最终得到的目标函数对参数w求导，带回目标函数，可知目标函数值由红色方框部分决定： 因此，xgboost的迭代是以下图中gain式子定义的指标选择最优分割点的： 那么如何得到优秀的组合树呢？ 一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。 另一种方法：XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照特征值的密度分布，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将bucket边界上的特征值作为splitpoint的候选，遍历所有的候选分裂点来找到最佳分裂点。 上图近似算法公式的解释：将特征k的特征值进行排序，计算特征值分布，rk（z）表示的是对于特征k而言，其特征值小于z的权重之和占总权重的比例，代表了这些特征值的重要程度，我们按照这个比例计算公式，将特征值分成若干个bucket，每个bucket的比例相同，选取这几类特征值的边界作为划分候选点，构成候选集；选择候选集的条件是要使得相邻的两个候选分裂节点差值小于某个阈值。 综合以上的解说，我们可以得到xgboost相比于GBDT的创新之处： 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象； 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销 指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。 并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"极客时间《数据分析45讲总结》","slug":"极客时间《数据分析45讲总结》","date":"2019-12-17T08:05:00.000Z","updated":"2020-04-11T11:34:35.238Z","comments":true,"path":"2019/12/17/ji-ke-shi-jian-shu-ju-fen-xi-45-jiang-zong-jie/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/ji-ke-shi-jian-shu-ju-fen-xi-45-jiang-zong-jie/","excerpt":"","text":"1.前言该讲主要引导读者从全局去了解什么是数据分析？为什么做数据分析？怎么去做数据分析？答案就是：掌握数据，就是掌握规律。当你了解了市场数据，对它进行分析，就可以得到市场规律。当你掌握了产品自身的数据，对它进行分析，就可以了解产品的用户来源、用户画像等等。所以说数据是个全新的视角。数据分析如此重要，它不仅是新时代的“数据结构 + 算法”，也更是企业争夺人才的高地。 谈到数据分析，我们一般都会从3个方面入手： 数据采集 – 数据源，我们要用的原材料 数据挖掘 – 它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值（所谓的商业智能BI） 数据的可视化 – 数据领域中的万金油，直观了解数据分析结构 数据分析的三驾马车的关系如下： 下面来大致认识下这三驾马车： 2.数据采集：数据的采集，主要是和数据打交道，用工具对数据进行采集，常用的数据源，如何获取它们。在专栏里，后续会将介绍如何掌握“八爪鱼”这个自动抓取的神器，它可以帮你抓取 99% 的页面源。也会教读者如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如“王祖贤”的海报，还能自动给微博加粉丝，让你掌握自动化的快感。 3.数据挖掘：数据挖掘，它可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。 掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的。 4.数据可视化 为什么说数据要可视化，因为数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。 数据可视化的两种方法： Python ：在 Python 对数据进行清洗、挖掘的过程中，很多的库可以使用，像 Matplotlib、Seaborn 等第三方库进行呈现。 第三方工具：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。 数据分析包括数据采集、数据挖掘、数据可视化这三个部分。乍看你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点“高深莫测”，掌握起来是不是会吃力。其实这些都是不必要的烦恼。个人觉得只要内心笃定，认为自己一定能做成，学成，其他一切都是“纸老虎”哈。 再说下，陈博在文章中提到的如何来快速掌握数据分析，核心就是认知。我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程就是认知升级的过程。 我本人也是很赞同这种说法，简单一句就是“知行合一” 总结 记录下你每天的认知 这些认知对应工具的哪些操作 做更多练习来巩固你的认知 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/categories/Data-Analysis/"}],"tags":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/tags/Data-Analysis/"}]},{"title":"机器学习系列之决策树算法（01）：决策树特征选择","slug":"机器学习系列之决策树算法（01）：决策树特征选择","date":"2019-12-17T07:07:00.000Z","updated":"2020-04-11T11:31:34.243Z","comments":true,"path":"2019/12/17/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-01-jue-ce-shu-te-zheng-xuan-ze/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/ji-qi-xue-xi-xi-lie-zhi-jue-ce-shu-suan-fa-01-jue-ce-shu-te-zheng-xuan-ze/","excerpt":"","text":"1.什么是特征选择【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。 【那么选择的是什么样的特征呢？】这里我们选择的特征需要是对训练数据有分类能力的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征没有分类能力，舍去这个特征对学习的精度不会有特别大的影响。 特征选择是决定用哪个特征来划分特征空间。 比如女生找男朋友，可能这个女生首先会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了 【那怎么判断哪个特征有更好的分类能力呢？】这时候【信息增益】就要出场了。 2.信息增益为了解释什么是信息增益，我们首先要讲解一下什么是【熵（entropy）】 熵（Entropy） 在热力学与化学中： 熵是一种测量在动力学方面【不能做功的能量的总数】，当总体熵增加，其做功能力也下降，熵的度量是能量退化的指标。 1948 年，香农把热力学中的熵引入到信息论中，称为香农熵。根据维基百科的描述： 在信息论中，熵是接收的每条消息中包含的信息的平均量。 更一般的，【熵表示随机变量的不确定性】。假设一个有限取值的离散随机变量 X 的概率分布如下： 那么它的熵定义为： 上式中的 b 通常取 2 或者自然对数 e，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。 从上式中，我们可以看到，熵与 X 的取值是没有关系的，它只与 X 的分布有关，所以 H 也可以写作 p 的函数： 我们现在来看两个随机变量的情况。 假设随机变量 (X, Y) 的联合概率分布如下： 我们使用条件熵（conditional entropy）H(Y|X)来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。 条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。 是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法： 有了上面的公式以后，条件熵的定义就非常容易理解了。 那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？ 信息增益的定义与信息增益算法既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？ 没错，由于熵表示不确定性，严格来说，信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」。 我们给出信息增益的最终定义： 特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。 这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。 假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。 很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要 信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大 条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定 信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类 下面我们给出信息增益的算法。 首先对数据做一些介绍： 假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 ， 为属于这个类的样本的个数，很显然 。 再假设我们有一个特征叫 A，A 有 n 个不同的取值 ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 ， 是这个子集的样本个数，很显然 。 我们把 中属于类别 的集合称作 ， 是其样本个数。 信息增益的计算就分为如下几个步骤： 计算 D 的经验熵 H(D)： \\2. 计算 A 对 D 的经验条件熵 H(D|A)： \\3. 计算信息增益 g(D, A)： 3.信息增益比看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？ 按照经验来看，【以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差】。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。类别越多代表特征越不确定，即熵越多，类别的信息增益越小。 为了校正这一误差，我们引入了【信息增益比（information gain ratio）】，又叫做信息增益率，它的定义如下： 特征 A 对训练数据集 D 的信息增益比 定义为其信息增益 与训练数据集 D 关于特征 A 的值的熵 之比。 其中， ，n 是 A 取值的个数。 两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。 4. ID3 ： 最大信息增益 ID3以信息增益为准则来选择最优划分属性 信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 5. C4.5 ：最大信息增益率 C4.5基于信息增益率准则选择最优分割属性的算法 信息增益比率通过引入一个被称作【分裂信息(Split information)】的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 6.CART ：最小基尼指数 CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归 CART是一棵二叉树，采用【二元切分法】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 7.分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]}]}