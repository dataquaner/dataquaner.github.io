{"meta":{"title":"DataQuaner","subtitle":"","description":"Data & Algorithm & Business & Value","author":"Leon","url":"https://dataquaner.github.io","root":"/"},"pages":[{"title":"关于","date":"2019-11-14T14:27:21.000Z","updated":"2019-11-14T14:28:04.822Z","comments":true,"path":"about/index.html","permalink":"https://dataquaner.github.io/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-11-10T09:22:35.000Z","updated":"2019-11-10T09:23:03.650Z","comments":true,"path":"categories/index.html","permalink":"https://dataquaner.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-11-10T09:17:30.000Z","updated":"2019-11-10T09:20:41.842Z","comments":true,"path":"link/index.html","permalink":"https://dataquaner.github.io/link/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-10T09:21:38.000Z","updated":"2019-11-10T09:22:17.868Z","comments":true,"path":"tags/index.html","permalink":"https://dataquaner.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"xgboost算法模型输出的解释","slug":"xgboost算模型输出的解释","date":"2019-12-25T01:45:30.134Z","updated":"2019-12-25T01:45:30.134Z","comments":true,"path":"2019/12/25/xgboost算模型输出的解释/","link":"","permalink":"https://dataquaner.github.io/2019/12/25/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/","excerpt":"","text":"1. 问题描述 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。 2. 数据集 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据. 1234567891011121314from sklearn import datasets iris = datasets.load_iris()data = iris.data[:100]print data.shape#(100L, 4L)#一共有100个样本数据, 维度为4维 label = iris.target[:100]print label#正好选取label为0和1的数据[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 3. 训练集与测试集123from sklearn.cross_validation import train_test_split train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=0) 4. Xgboost建模4.1 模型初始化设置123456789101112131415161718import xgboost as xgbdtrain=xgb.DMatrix(train_x,label=train_y)dtest=xgb.DMatrix(test_x) params=&#123;'booster':'gbtree', 'objective': 'binary:logistic', 'eval_metric': 'auc', 'max_depth':4, 'lambda':10, 'subsample':0.75, 'colsample_bytree':0.75, 'min_child_weight':2, 'eta': 0.025, 'seed':0, 'nthread':8, 'silent':1&#125; watchlist = [(dtrain,'train')] 4.2 建模与预测1234567891011121314bst=xgb.train(params,dtrain,num_boost_round=100,evals=watchlist) ypred=bst.predict(dtest) # 设置阈值, 输出一些评价指标y_pred = (ypred &gt;= 0.5)*1 from sklearn import metricsprint 'AUC: %.4f' % metrics.roc_auc_score(test_y,ypred)print 'ACC: %.4f' % metrics.accuracy_score(test_y,y_pred)print 'Recall: %.4f' % metrics.recall_score(test_y,y_pred)print 'F1-score: %.4f' %metrics.f1_score(test_y,y_pred)print 'Precesion: %.4f' %metrics.precision_score(test_y,y_pred)metrics.confusion_matrix(test_y,y_pred) Out[23]: 1234567AUC: 1.0000ACC: 1.0000Recall: 1.0000F1-score: 1.0000Precesion: 1.0000array([[13, 0], [ 0, 12]], dtype=int64) Yeah, 完美的模型, 完美的预测! 4.3 可视化输出123456789101112131415#对于预测的输出有三种方式?bst.predictSignature: bst.predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False) pred_leaf : bool When this option is on, the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree. Note that the leaf index of a tree is unique per tree, so you may find leaf 1 in both tree 1 and tree 0. pred_contribs : bool When this option is on, the output will be a matrix of (nsample, nfeats+1) with each record indicating the feature contributions (SHAP values) for that prediction. The sum of all feature contributions is equal to the prediction. Note that the bias is added as the final column, on top of the regular features. 4.3.1 得分默认的输出就是得分, 这没什么好说的, 直接上code. 12ypred = bst.predict(dtest)ypred Out[32]: 12345array([ 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.80391562, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.20081411, 0.80391562, 0.20081411, 0.80391562, 0.20081411], dtype=float32) 在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出. 4.3.2 所属的叶子节点当设置pred_leaf=True的时候, 这时就会输出每个样本在所有树中的叶子节点 12ypred_leaf = bst.predict(dtest, pred_leaf=True)ypred_leaf Out[33]: 1234567array([[1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [2, 2, 2, ..., 2, 2, 2], [1, 1, 1, ..., 1, 1, 1]]) 输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以ypred_leaf的维度为[100*100]. 对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树). 那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型. 12xgb.to_graphviz(bst, num_trees=0)#可视化第一棵树的生成情况 12#直接输出模型的迭代工程bst.dump_model(\"model.txt\") 12345678910111213booster[0]:0:[f3&lt;0.75] yes=1,no=2,missing=1 1:leaf=-0.019697 2:leaf=0.0214286booster[1]:0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0212184 2:leaf=0.0212booster[2]:0:[f2&lt;2.35] yes=1,no=2,missing=1 1:leaf=-0.0197404 2:leaf=0.0197235booster[3]: …… 通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值. 在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值. 同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值. 12leaf1 -1.381214leaf2 1.410950 在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了&#39;objective&#39;: &#39;binary:logistic&#39;, 因此使用函数将累加的值转换为实际的打分: f(x)=1/(1+exp(−x)) 12341/float(1+np.exp(1.38121416))Out[24]: 0.200814071121865031/float(1+np.exp(-1.410950))Out[25]: 0.8039157403338895 这就与ypred = bst.predict(dtest) 的分值相对应上了. 4.3.2 特征重要性接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性. 12ypred_contribs = bst.predict(dtest, pred_contribs=True)ypred_contribs Out[37]: 12345678910111213141516171819202122232425array([[ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663], [ 0. , 0. , 0.96967536, 0.39522746, 0.04604663], [ 0. , 0. , -1.01448286, -0.41277751, 0.04604663]], dtype=float32) 输出的ypred_contribs的维度为[100,5], 通过阅读前面的文档注释就可以知道, 最后一列是bias, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的. 通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例. 123456score_a = sum(ypred_contribs[0])print score_a# -1.38121373579score_b = sum(ypred_contribs[1])print score_b# 1.41094945744 相同的分值, 相同的处理情况. 到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"XGBoost","slug":"Machine-Learning/XGBoost","permalink":"https://dataquaner.github.io/categories/Machine-Learning/XGBoost/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"https://dataquaner.github.io/tags/XGBoost/"}]},{"title":"机器学习系列之决策树算法（03）：决策树的剪枝","slug":"机器学习系列之决策树算法（03）：决策树的剪枝","date":"2019-12-25T01:15:39.234Z","updated":"2019-12-25T01:15:39.234Z","comments":true,"path":"2019/12/25/机器学习系列之决策树算法（03）：决策树的剪枝/","link":"","permalink":"https://dataquaner.github.io/2019/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8803%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D/","excerpt":"","text":"1. 前言上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现过拟合现象。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为剪枝(pruning)。 具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss funtion)或代价函数(cost funtion)来实现。 2.剪枝算法2.1 为什么要剪枝现象 接上一次讲的生成决策树，下面给出一张图。 横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。 实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。 可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。 原因 原因1：噪声、样本冲突，即错误的样本数据。 原因2：特征即属性不能完全作为分类标准。 原因3：巧合的规律性，数据量不够大。 这个时候，就需要对生成树进行修剪，也就是剪枝。 2.2 如何进行剪枝预剪枝预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法: (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。 (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。 (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。 (4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。 优点&amp;缺点 由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【怎样精确地估计何时停止树的增长是相当困难的】。 预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。 后剪枝后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。 这里主要介绍四种： REP-错误率降低剪枝 PEP-悲观剪枝 CCP-代价复杂度剪枝 MEP-最小错误剪枝 REP(Reduced Error Pruning)方法 对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S 优点： REP 是当前最简单的事后剪枝方法之一。 它的计算复杂性是线性的。 和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。 缺点： 但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意. PEP(Pessimistic Error Pruning)方法 为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, PEP 方法对误差估计增加了连续性校正(continuity correction)。关于PEP方法的数据解释待后续开专题梳理。 优点： PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一 PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。 TIPS： 个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。 MEP(Minimum Error Pruning)方法 MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。 优点： MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集. 它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 . 缺点： 类别平均分配的前提假设现实几率不大&amp;对K太过敏感 对此，也有改进算法，我没有深入研究。 CCP(Cost-Complexity Pruning)方法 CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤: (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。 ​ (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。 缺点： 生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的. 对比四种方法 剪枝名称 剪枝方式 计算复杂度 误差估计 REP 自底向上 0(n) 剪枝集上误差估计 PEP 自顶向下 o(n) 使用连续纠正 CCP 自底向上 o(n2) 标准误差 MEP 自底向上 o(n) 使用连续纠正 ① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。 ② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。 ③ 如果数据集多（REP&amp;CCP←复杂但树小） ④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定） 3.总结决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。 剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。 预剪枝： 预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。 简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。 这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。 后剪枝： 后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。 同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。 后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。 剪枝方法的选择 如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。 后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。 有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。 经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。 无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。 函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点 函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。 函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值 上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"Decision Tree","slug":"Machine-Learning/Decision-Tree","permalink":"https://dataquaner.github.io/categories/Machine-Learning/Decision-Tree/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","slug":"机器学习系列之决策树算法（05）：梯度提升树算法GBDT","date":"2019-12-24T06:08:00.000Z","updated":"2019-12-24T10:27:15.357Z","comments":true,"path":"2019/12/24/机器学习系列之决策树算法（05）：梯度提升树算法GBDT/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8805%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95GBDT/","excerpt":"","text":"1 前言前面讲述了《决策树的特征选择》、《决策树的生成》、《决策树的剪枝》，熟悉了单棵决策树的的实现细节，在实际应用时，往往采用多棵决策树组合的形式完成目标任务。那么如何组合单棵决策树可以使得模型效果更优呢？目前主要有两种思想：bagging和boosting，分别对应的典型算法随机森林和Adaboost、GBDT等。 Bagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。典型实现算法随机森林 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。典型实现算法是GBDT boosting的思想如下图： 基于boosting思想的经典算法是Adaboost和GBDT。关于Adaboost的介绍可以参考《Adaboost算法》，本文重点介绍GBDT。 2 什么是GBDT GBDT(Gradient Boosting Decision Tree) 是一种迭代的决策树算法，是回归树，而不是分类树。该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性。 GBDT用来做回归预测，调整后也可以用于分类。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。具体训练过程如下图示意： 3 GBDT算法原理GBDT算法的核心思想 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。即所有弱分类器相加等于预测值，下一个弱分类器去拟合误差函数对预测值的梯度。 GBDT加入了简单的数值优化思想。 Xgboost更加有效应用了数值优化。相比于gbdt，最重要是对损失函数变得更复杂。目标函数依然是所有树想加等于预测值。损失函数引入了一阶导数，二阶导数。 不同于随机森林所有树的预测求均值，GBDT所有的树的预测值加起来是最终的预测值，可以不断接近真实值。 GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想的通俗解释 假如有个人30岁， 第一棵树，我们首先用20岁去拟合，发现损失有10岁， 第二颗，这时我们用6岁去拟合剩下的损失，发现差距还有4岁， 第三颗，我们用3岁拟合剩下的差距，差距就只有一岁了。 三棵树加起来为29岁，距离30最近。 从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？ 4 负梯度拟合在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为 利用(xi,rti)(i=1,2,..m),我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域Rtj,j=1,2,…,J。其中J为叶子节点的个数。 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj如下： 这样就得到了本轮的决策树拟合函数如下： 从而本轮最终得到的强学习器的表达式如下： 通过损失函数的负梯度来拟合，找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 传统模型中，我们定义一个固定结构的函数，然后通过样本训练拟合更新该函数的参数，获得最后的最优函数。 GBDT提升树并非如此。它是加法模型，是不定结构的函数，通过不断加入新的子函数来使得模型能更加拟合训练数据，直到最优。函数更新的迭代方式可以写作：)。所以如果要更快逼近最优的函数，我们就需要在正确的方向上添加子函数，这个“正确的方向”当然就是损失减少最快的方向。所以我们需要用损失函数)对函数)求导（注意不是对x求导），求得的导数，就是接下来)需要弥补的方向。在上式中就是表示导数的拟合。 导数值跟损失函数的选择有关系。如果选择平方损失误差，那么它的导数就是： 令人惊喜的是这正是真实值和估计值之间的残差！ 这就是为什么谈到GBDT的时候，很多文章都提到“残差”的拟合，却没有说“梯度”的拟合。其实它们在平方损失误差条件下是一个意思！BTW，上面之所以用了是为了计算方便，常数项并不会影响平方损失误差，以及残差的比较。 现在让我们重新理解这个式子： 1）先求取一个拟合函数Fm-1(x) 2）用Fm-1(x)进行预测，计算预测值和实际值的残差 3）为了弥补上面的残差，用一个函数△F(x)来拟合这个残差 4）这样最终的函数就变成了)，其中Fm-1(x)用来拟合原数据，△F(x)用来拟合残差 5）如果目前还有较大的残差，则循环2)~4)，更新函数到Fm+1(x) , Fm+2(x), …..直到残差满足条件。 针对以上流程，我们用实例来说明 5 提升树的生成过程有以下数据需要用回归，并要求平方损失误差小于0.2（这0.2就是我们人为设置的最优条件，否则训练可能会无休止地进行下去）时，可以停止建树： 第一棵树 1） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 比如s=1.5,分割成了两个子集： 通过公式求平方损失误差 而其中)为各自子集的平均值时，可以使得每个子集的平方损失误差最小。 求平均值为：)，进而求得平方损失误差为 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=6.5时,为所有切分点里平方损失误差最小的 2) 选择切分点s=6.5构建第一颗回归树，各分支数值使用 ： 第一轮过后，我们提升树为: 3) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果： 提升树拟合数据的平方损失误差计算： 大于0.2，则还需要继续建树。 第二棵树 4) 确定需要拟合的训练数据为上一棵树的残差： 5） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点： 同样的方法求得其它切分点的平方损失误差，列表入下： 可见，当s=3.5时,为所有切分点里平方损失误差最小的 6) 选择切分点s=3.5构建第二颗回归树，各分支数值使用 ： 第二轮过后，我们提升树为: 7) 求提升树拟合数据的残差和平方损失误差： 提升树拟合数据的残差计算： 各个点的计算结果，同时对比初始值和上一颗树的残差： 可以看见，随着树的增多，残差一直在减少。 到目前为止，提升树拟合数据的平方损失误差计算： 多说一句，这里是从全局提升树的角度去计算损失，其实和上面第5）步中从最后一颗树的角度去计算损失，结果是一样的 目前损失大于0.2的阈值，还需要继续建树 … … 第六棵树 到第六颗树的时候，我们已经累计获得了： 此时提升树为： 此时用拟合训练数据的平方损失误差为： 平方损失误差小于0.2的阈值，停止建树。 为我们最终所求的提升树。 6 回归算法输入： 最大迭代次数T, 损失函数L，训练样本集 输出： 强学习器f(x) 1） 初始化弱学习器 2）对迭代轮数t=1,2,…T有： a) 对样本i=1,2，…m，计算负梯度 b) 利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,…,J。其中J为回归树t的叶子节点的个数。 c) 对叶子区域j =1,2,..J,计算最佳拟合值 (d) 更新强学习器 3） 得到强学习器f(x)的表达式 7 分类算法GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。 为了解决这个问题，主要有两个方法， 1）一个是用指数损失函数，此时GBDT退化为Adaboost算法。 2）另一种方法是用类似于逻辑回归的对数似然损失函数的方法。 也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。 7.1 二元分类算法对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为： 其中y∈{−1,+1}。则此时的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。 7.2 多元分类算法多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为： 集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为 对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 由于上式比较难优化，我们一般使用近似值代替 除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。 8 正则化和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。 GBDT的正则化主要有三种方式。 第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν,对于前面的弱学习器的迭代 如果我们加上了正则化项，则有 ν的取值范围为0&lt;ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了 9 总结GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。 优点 1) 可以灵活处理各种类型的数据，包括连续值和离散值。 2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 缺点 1) 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"GBDT","slug":"Machine-Learning/GBDT","permalink":"https://dataquaner.github.io/categories/Machine-Learning/GBDT/"}],"tags":[{"name":"GBDT","slug":"GBDT","permalink":"https://dataquaner.github.io/tags/GBDT/"}]},{"title":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","slug":"机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结","date":"2019-12-24T06:08:00.000Z","updated":"2019-12-25T01:18:36.398Z","comments":true,"path":"2019/12/24/机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结/","link":"","permalink":"https://dataquaner.github.io/2019/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8809%EF%BC%89%EF%BC%9AID3%E3%80%81C4.5%E3%80%81CART%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E3%80%81bagging%E3%80%81boosting%E3%80%81Adaboost%E3%80%81GBDT%E3%80%81xgboost%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/","excerpt":"","text":"最近心血来潮，整理了一下和树有关的方法和模型，请多担待！ 决策树首先，决策树是一个有监督的分类模型，其本质是选择一个能带来最大信息增益的特征值进行树的分割，直到到达结束条件或者叶子结点纯度到达一定阈值。下图是决策树的一个简单例子 按照分割指标和分割方法，决策树的经典模型可以分为ID3、C4.5以及CART ID3：以信息增益为准则来选择最优划分属性信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 C4.5：基于信息增益率准则选择最优分割属性信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 CART：以基尼系数为准则选择最优划分属性CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 随机森林在讲随机森林之前，我们需要补充一点组合分类器的概念，将多个分类器的结果进行多票表决或者是取平均值，以此作为最终的结果。 1、构建组合分类器的好处： （1）、提升模型精度：整合各个模型的分类结果，得到更合理的决策边界，减少整体错误，实现更好的分类效果； （2）、处理过大或过小的数据集：数据集较大时，可以将数据集划分成多个子集，对子集构建分类器；数据集较小时，可通过多种抽样方式（bootstrap）从原始数据集抽样产生多组不同的数据集，构建分类器。 （3）、若决策边界过于复杂，则线性模型不能很好地描述真实情况。因此先对于特定区域的数据集，训练多个线性分类器，再将它们集成。 （4）、比较适合处理多源异构数据（存储方式不同（关系型、非关系型），类别不同（时序型、离散型、连续型、网络结构数据）） 随机森林是一个典型的多个决策树的组合分类器。主要包括两个方面：数据的随机性选取，以及待选特征的随机选取。 （1）、数据的随机选取：第一，从原始的数据集中采取有放回的抽样（bootstrap），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。 （2）、待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。 组合树示例图 GBDT和xgboostbagging和boostingBagging的思想比较简单，即每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。 boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。 boosting算法结构图 拿著名的Adaboost算法举例： 我们有一个数据集，样本大小为N，每一个样本对应一个原始标签起初，我们初始化样本的权重为1/N em计算的是当前数据下，模型的分类误差率，模型的系数值是基于分类误差率的 根据模型的分类结果，更新原始数据中数据的分布，增加被错分的数据被抽中的概率，以便下一次迭代的时候能被模型重新训练 最终的分类器是各个基分类器的组合 GBDTGBDT是以决策树（CART）为基学习器的GB算法，是迭代树，而不是分类树。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。有了前面Adaboost的铺垫，大家应该能很容易理解大体思想。 GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。 xgboostXgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。 损失函数如下，引入了一阶导数，二阶导数。： 好的模型需要具备两个基本要素：一是要有好的精度（即好的拟合程度），二是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项） 常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。 每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差 目标函数如上图，最后一行画圈部分实际上就是预测值和真实值之间的残差 先对训练误差进行展开： xgboost则对代价函数进行了二阶泰勒展开，同时用到了残差平方和的一阶和二阶导数 再研究目标函数中的正则项： 树的复杂度可以用树的分支数目来衡量，树的分支我们可以用叶子结点的数量来表示 那么树的复杂度式子：右边第一项是叶子结点的数量T，第二项是树的叶子结点权重w的l2正则化，正则化是为了防止叶子结点过多 此时，每一次迭代，相当于在原有模型中增加一棵树，目标函数中，我们用wq（x）表示一棵树，包括了树的结构以及叶子结点的权重，w表示权重（反映预测的概率），q表示样本所在的索引号（反映树的结构） 将最终得到的目标函数对参数w求导，带回目标函数，可知目标函数值由红色方框部分决定： 因此，xgboost的迭代是以下图中gain式子定义的指标选择最优分割点的： 那么如何得到优秀的组合树呢？ 一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。 另一种方法：XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照特征值的密度分布，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将bucket边界上的特征值作为splitpoint的候选，遍历所有的候选分裂点来找到最佳分裂点。 上图近似算法公式的解释：将特征k的特征值进行排序，计算特征值分布，rk（z）表示的是对于特征k而言，其特征值小于z的权重之和占总权重的比例，代表了这些特征值的重要程度，我们按照这个比例计算公式，将特征值分成若干个bucket，每个bucket的比例相同，选取这几类特征值的边界作为划分候选点，构成候选集；选择候选集的条件是要使得相邻的两个候选分裂节点差值小于某个阈值。 综合以上的解说，我们可以得到xgboost相比于GBDT的创新之处： 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象； 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销 指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。 并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"XGBoost","slug":"Machine-Learning/XGBoost","permalink":"https://dataquaner.github.io/categories/Machine-Learning/XGBoost/"}],"tags":[{"name":"XGBoost - GBDT","slug":"XGBoost-GBDT","permalink":"https://dataquaner.github.io/tags/XGBoost-GBDT/"},{"name":"Adaboost","slug":"Adaboost","permalink":"https://dataquaner.github.io/tags/Adaboost/"}]},{"title":"LightGBM算法基础系列之基础理论篇（1）","slug":"LightGBM算法基础系列之基础理论篇（1）","date":"2019-12-19T10:39:35.388Z","updated":"2019-12-19T10:39:35.388Z","comments":true,"path":"2019/12/19/LightGBM算法基础系列之基础理论篇（1）/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/","excerpt":"","text":"这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"LightGBM","slug":"Machine-Learning/LightGBM","permalink":"https://dataquaner.github.io/categories/Machine-Learning/LightGBM/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://dataquaner.github.io/tags/LightGBM/"}]},{"title":"数据存储之MySQL系列（01）：MySQL体系结构","slug":"数据存储之MySQL系列（01）：MySQL体系结构","date":"2019-12-19T10:39:12.063Z","updated":"2019-12-19T10:39:12.063Z","comments":true,"path":"2019/12/19/数据存储之MySQL系列（01）：MySQL体系结构/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/","excerpt":"","text":"","categories":[{"name":"DataBase","slug":"DataBase","permalink":"https://dataquaner.github.io/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"https://dataquaner.github.io/categories/DataBase/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dataquaner.github.io/tags/MySQL/"}]},{"title":"机器学习系列之决策树算法（02）：决策树的生成","slug":"机器学习系列之决策树算法（02）：决策树的生成","date":"2019-12-19T10:28:04.764Z","updated":"2019-12-19T10:27:49.949Z","comments":true,"path":"2019/12/19/机器学习系列之决策树算法（02）：决策树的生成/","link":"","permalink":"https://dataquaner.github.io/2019/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8802%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/","excerpt":"","text":"1. 前言上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。 2. 决策树的生成 示例数据表格 文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中 2.1 相关概念阐述2.1.1 决策树 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。 2.1.2 ID3、C4.5、CART在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【机器学习系列之决策树算法（01）：决策树特征选择】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。 信息熵 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。 ​ Entopy(t)=-Σp(i|t)log2p(i|t) 信息增益信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为： ​ Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di) 信息增益率 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7： ​ H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) 基尼系数 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下基尼系数 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低. CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分. 基尼系数的计算公式如下: ​ Gini = 1 – Σ (Pi)2 for i=1 to number of classes 2.2 完整生成过程 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例 2.2.1 根节点的选择 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为： ​ Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985 天气 其数据表格如下: 信息增益计算如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下： D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981 D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为： 3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965 其信息增益为： Gain(天气)=0.985 - 0.965 = 0.020 信息增益率计算 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为： H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556 则其信息增益率为： Gain_ratio(天气)=0.020/1.556=0.012 基尼系数计算 Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9 Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21 温度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 作为子节点的归一化信息熵为： 4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857 其信息增益为： Gain(温度)=0.985 - 0.857 = 0.128 信息增益率计算 属性熵为： H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378 则其信息增益率为： Gain_ratio(温度)=0.128/1.378=0.0928 基尼系数计算 Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 4/7 * 1.0 + 3/7 * 0.918 = 0.964 其信息增益为： Gain(湿度)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(湿度)=0.021/0.985=0.021 基尼系数计算 Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 作为子节点的归一化信息熵为： 3/7 * 1.0 + 4/7 * 0.918 = 0.964 其信息增益为： Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率计算 属性熵为： H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985 则其信息增益率为： Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数计算 Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 根节点的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度 信息增益 Gain(天气)=0.985 - 0.965 = 0.020 Gain(温度)=0.985 - 0.857 = 0.128 Gain(湿度)=0.985 - 0.964 = 0.021 Gain(刮风)=0.985 - 0.964 = 0.021 信息增益率 Gain_ratio(天气)=0.020/1.556=0.012 Gain_ratio(温度)=0.128/1.378=0.0928 Gain_ratio(湿度)=0.021/0.985=0.021 Gain_ratio(刮风)=0.021/0.985=0.021 基尼系数 Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619 Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857 Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619 确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择: 根节点 子节点温度高:(待进一步进行选择) 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.2 子节点温度高的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： ​ Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 归一化信息熵为： 2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5 其信息增益为： Gain(天气)=1.0 - 0.5 = 0.5 信息增益率计算 对应的属性熵为： H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5 则其信息增益率为： Gain_ratio(天气)=0.5/1.5=0.33333 基尼系数计算 Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5 Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0 D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0 作为子节点的归一化信息熵为： 2/4 * 0.0 + 2/4 * 0.0 = 0.0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0 Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0 D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918 作为子节点的归一化信息熵为： 1/4 * 0.0 + 3/4 * 0.918 = 0.688 其信息增益为： Gain(刮风)=1.0 - 0.688 = 0.312 信息增益率计算 属性熵为： H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918 则其信息增益率为： Gain_ratio(刮风)=0.312/0.918=0.349 基尼系数计算 Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 子节点温度高的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度 Gain(天气)=1.0 - 0.5 = 0.5 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 0.688 = 0.312 Gain_ratio(天气)=0.5/1.5=0.33333 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0.312/0.918=0.349 Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25 Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0 Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333 确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中:(待进一步进行选择) 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.3 子节点温度中的选择 其剩下的数据集如下,温度不再进行下面的节点选择参与: 根据信息熵的计算公式可以得到子节点温度高的信息熵为： Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 天气 其数据表格如下: 信息增益计算 相应的信息熵如下： D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D (阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(天气)=1.0 - 0 = 1.0 信息增益率计算 对应的属性熵为： H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(天气)=1.0/1.0=1.0 基尼系数计算 Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 湿度 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0 D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 作为子节点的归一化信息熵为： 1/2 * 0.0 + 1/2 * 0.0 = 0 其信息增益为： Gain(湿度)=1.0 - 0.0 = 1.0 信息增益率计算 属性熵为： H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 则其信息增益率为： Gain_ratio(湿度)=1.0/1.0=1.0 基尼系数计算 Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0 Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 刮风 其数据表格如下: 信息增益计算 各情况的信息熵如下： D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0 作为子节点的归一化信息熵为： 1/1 * 1.0 = 1.0 其信息增益为： Gain(刮风)=1.0 - 1.0 = 0 信息增益率计算 属性熵为： H(刮风)=-(2/2 * log2(2/2) = 0.0 则其信息增益率为： Gain_ratio(刮风)=0/0 = 0 基尼系数计算 Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5 Gini(刮风)=2/2 * 0.5 = 0.5 子节点温度中的选择 如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧 Gain(天气)=1.0 - 0 = 1.0 Gain(湿度)=1.0 - 0.0 = 1.0 Gain(刮风)=1.0 - 1.0 = 0 Gain_ratio(天气)=1.0/1.0=1.0 Gain_ratio(湿度)=1.0/1.0=1.0 Gain_ratio(刮风)=0/0 = 0 Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0 Gini(刮风)=2/2 * 0.5 = 0.5 确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果: 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 2.2.4 最终的决策树 在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树： 根节点 子节点温度高 叶节点湿度高：打篮球 叶节点湿度中：不打篮球 子节点温度中 叶节点天气晴：打篮球 叶节点天气阴：不打篮球 叶节点温度低:不打篮球(能直接确定为不打篮球) 3. 思考 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？ 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n)","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"Decision Tree","slug":"Machine-Learning/Decision-Tree","permalink":"https://dataquaner.github.io/categories/Machine-Learning/Decision-Tree/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]},{"title":"极客时间《数据分析45讲总结》","slug":"极客时间《数据分析45讲总结》","date":"2019-12-17T08:05:00.000Z","updated":"2019-12-17T08:08:35.390Z","comments":true,"path":"2019/12/17/极客时间《数据分析45讲总结》/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4%E3%80%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%9045%E8%AE%B2%E6%80%BB%E7%BB%93%E3%80%8B/","excerpt":"","text":"1.前言该讲主要引导读者从全局去了解什么是数据分析？为什么做数据分析？怎么去做数据分析？答案就是：掌握数据，就是掌握规律。当你了解了市场数据，对它进行分析，就可以得到市场规律。当你掌握了产品自身的数据，对它进行分析，就可以了解产品的用户来源、用户画像等等。所以说数据是个全新的视角。数据分析如此重要，它不仅是新时代的“数据结构 + 算法”，也更是企业争夺人才的高地。 谈到数据分析，我们一般都会从3个方面入手： 数据采集 – 数据源，我们要用的原材料 数据挖掘 – 它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值（所谓的商业智能BI） 数据的可视化 – 数据领域中的万金油，直观了解数据分析结构 数据分析的三驾马车的关系如下： 下面来大致认识下这三驾马车： 2.数据采集：数据的采集，主要是和数据打交道，用工具对数据进行采集，常用的数据源，如何获取它们。在专栏里，后续会将介绍如何掌握“八爪鱼”这个自动抓取的神器，它可以帮你抓取 99% 的页面源。也会教读者如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如“王祖贤”的海报，还能自动给微博加粉丝，让你掌握自动化的快感。 3.数据挖掘：数据挖掘，它可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。 掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的。 4.数据可视化 为什么说数据要可视化，因为数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。 数据可视化的两种方法： Python ：在 Python 对数据进行清洗、挖掘的过程中，很多的库可以使用，像 Matplotlib、Seaborn 等第三方库进行呈现。 第三方工具：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。 数据分析包括数据采集、数据挖掘、数据可视化这三个部分。乍看你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点“高深莫测”，掌握起来是不是会吃力。其实这些都是不必要的烦恼。个人觉得只要内心笃定，认为自己一定能做成，学成，其他一切都是“纸老虎”哈。 再说下，陈博在文章中提到的如何来快速掌握数据分析，核心就是认知。我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程就是认知升级的过程。 我本人也是很赞同这种说法，简单一句就是“知行合一” 总结 记录下你每天的认知 这些认知对应工具的哪些操作 做更多练习来巩固你的认知","categories":[{"name":"Learning Path","slug":"Learning-Path","permalink":"https://dataquaner.github.io/categories/Learning-Path/"},{"name":"Data Analysis","slug":"Learning-Path/Data-Analysis","permalink":"https://dataquaner.github.io/categories/Learning-Path/Data-Analysis/"}],"tags":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dataquaner.github.io/tags/Data-Analysis/"}]},{"title":"机器学习系列之决策树算法（01）：决策树特征选择","slug":"机器学习系列之决策树算法（01）：决策树特征选择","date":"2019-12-17T07:07:00.000Z","updated":"2019-12-19T10:36:47.474Z","comments":true,"path":"2019/12/17/机器学习系列之决策树算法（01）：决策树特征选择/","link":"","permalink":"https://dataquaner.github.io/2019/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8801%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"","text":"1.什么是特征选择【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。 【那么选择的是什么样的特征呢？】这里我们选择的特征需要是对训练数据有分类能力的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征没有分类能力，舍去这个特征对学习的精度不会有特别大的影响。 特征选择是决定用哪个特征来划分特征空间。 比如女生找男朋友，可能这个女生首先会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了 【那怎么判断哪个特征有更好的分类能力呢？】这时候【信息增益】就要出场了。 2.信息增益为了解释什么是信息增益，我们首先要讲解一下什么是【熵（entropy）】 熵（Entropy） 在热力学与化学中： 熵是一种测量在动力学方面【不能做功的能量的总数】，当总体熵增加，其做功能力也下降，熵的度量是能量退化的指标。 1948 年，香农把热力学中的熵引入到信息论中，称为香农熵。根据维基百科的描述： 在信息论中，熵是接收的每条消息中包含的信息的平均量。 更一般的，【熵表示随机变量的不确定性】。假设一个有限取值的离散随机变量 X 的概率分布如下： 那么它的熵定义为： 上式中的 b 通常取 2 或者自然对数 e，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。 从上式中，我们可以看到，熵与 X 的取值是没有关系的，它只与 X 的分布有关，所以 H 也可以写作 p 的函数： 我们现在来看两个随机变量的情况。 假设随机变量 (X, Y) 的联合概率分布如下： 我们使用条件熵（conditional entropy）H(Y|X)来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。 条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。 是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法： 有了上面的公式以后，条件熵的定义就非常容易理解了。 那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？ 信息增益的定义与信息增益算法既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？ 没错，由于熵表示不确定性，严格来说，信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」。 我们给出信息增益的最终定义： 特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。 这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。 假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。 很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要 信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大 条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定 信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类 下面我们给出信息增益的算法。 首先对数据做一些介绍： 假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 ， 为属于这个类的样本的个数，很显然 。 再假设我们有一个特征叫 A，A 有 n 个不同的取值 ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 ， 是这个子集的样本个数，很显然 。 我们把 中属于类别 的集合称作 ， 是其样本个数。 信息增益的计算就分为如下几个步骤： 计算 D 的经验熵 H(D)： \\2. 计算 A 对 D 的经验条件熵 H(D|A)： \\3. 计算信息增益 g(D, A)： 3.信息增益比看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？ 按照经验来看，【以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差】。 ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。类别越多代表特征越不确定，即熵越多，类别的信息增益越小。 为了校正这一误差，我们引入了【信息增益比（information gain ratio）】，又叫做信息增益率，它的定义如下： 特征 A 对训练数据集 D 的信息增益比 定义为其信息增益 与训练数据集 D 关于特征 A 的值的熵 之比。 其中， ，n 是 A 取值的个数。 两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。 4. ID3 ： 最大信息增益 ID3以信息增益为准则来选择最优划分属性 信息增益的计算要基于信息熵（度量样本集合纯度的指标） 信息熵越小，数据集X的纯度越大 因此，假设于数据集D上建立决策树，数据有K个类别： 公式（1）中： 表示第k类样本的数据占数据集D样本总数的比例 公式（2）表示的是以特征A作为分割的属性，得到的信息熵： Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合 因此，该公式求的是以属性A为划分，n个分支的信息熵总和 公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好 但是这种分割算法存在一定的缺陷： 假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。 5. C4.5 ：最大信息增益率 C4.5基于信息增益率准则选择最优分割属性的算法 信息增益比率通过引入一个被称作【分裂信息(Split information)】的项来惩罚取值较多的属性。 上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。 6.CART ：最小基尼指数 CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归 CART是一棵二叉树，采用【二元切分法】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。 Di表示以A是属性值划分成n个分支里的数目 Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。 7.分类树和回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dataquaner.github.io/categories/Machine-Learning/"},{"name":"Decision Tree","slug":"Machine-Learning/Decision-Tree","permalink":"https://dataquaner.github.io/categories/Machine-Learning/Decision-Tree/"}],"tags":[{"name":"Decision Tree","slug":"Decision-Tree","permalink":"https://dataquaner.github.io/tags/Decision-Tree/"}]}]}