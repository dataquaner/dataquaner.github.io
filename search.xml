<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习系列之决策树算法（03）：决策树的剪枝</title>
      <link href="/2020/04/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8803%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D/"/>
      <url>/2020/04/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8803%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D/</url>
      
        <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>上一篇文章介绍了决策树的生成详细过程，由于决策树生成算法过多地考虑如何提高对训练数据的正确分类，从而构建过于复杂的决策树，这样产生的决策树往往对训练数据的分类很准确，却对未知的测试数据的分类没有那么准确，即出现<strong>过拟合现象</strong>。我们需要对已生成的决策树进行简化，这个简化的过程我们称之为<strong>剪枝(pruning)。</strong></p><p>具体就是剪掉一些不重要的子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，得到最优的决策树模型。保证模型对预测数据的泛化能力。</p><blockquote><p>决策树的剪枝往往通过<strong>极小化</strong>决策树整体的<strong>损失函数(loss funtion)</strong>或<strong>代价函数(cost funtion)</strong>来实现。</p></blockquote><h1 id="2-剪枝算法"><a href="#2-剪枝算法" class="headerlink" title="2.剪枝算法"></a>2.剪枝算法</h1><h2 id="2-1-为什么要剪枝"><a href="#2-1-为什么要剪枝" class="headerlink" title="2.1 为什么要剪枝"></a>2.1 为什么要剪枝</h2><p><strong>现象</strong></p><p>接上一次讲的生成决策树，下面给出一张图。</p><img src="https://pic2.zhimg.com/80/v2-d6588457cc144c1bad2f87ec77081af1_hd.jpg" alt="决策树学习中的过渡拟合" style="zoom:50%;"><ul><li>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。</li><li>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</li></ul><p><strong>可以看出随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</strong></p><p><strong>原因</strong></p><p><img src="https://pic2.zhimg.com/80/v2-677a5e08d5b55b3b4cb14f7ad6f8eb31_hd.jpg" alt="原因"></p><ul><li>原因1：噪声、样本冲突，即错误的样本数据。</li><li>原因2：特征即属性不能完全作为分类标准。</li><li>原因3：巧合的规律性，数据量不够大。</li></ul><p>这个时候，就需要对生成树进行修剪，也就是<strong>剪枝</strong>。</p><h2 id="2-2-如何进行剪枝"><a href="#2-2-如何进行剪枝" class="headerlink" title="2.2 如何进行剪枝"></a>2.2 如何进行剪枝</h2><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><strong>预剪枝</strong></h3><p>预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法:<br>        (1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。<br>        (2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。<br>        (3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。</p><p>(4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。</p><p><strong>优点&amp;缺点</strong></p><ul><li><p>由于预剪枝不必生成整棵决策树，且算法相对简单， 效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接， 但是【<strong>怎样精确地估计何时停止树的增长是相当困难的</strong>】。</p></li><li><p>预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。</p></li></ul><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a><strong>后剪枝</strong></h3><p>后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。</p><p>这里主要介绍四种：</p><ul><li>REP-错误率降低剪枝</li><li>PEP-悲观剪枝</li><li>CCP-代价复杂度剪枝</li><li>MEP-最小错误剪枝</li></ul><h4 id="REP-Reduced-Error-Pruning-方法"><a href="#REP-Reduced-Error-Pruning-方法" class="headerlink" title="REP(Reduced Error Pruning)方法"></a><strong>REP(Reduced Error Pruning)方法</strong></h4><blockquote><p>对于决策树T 的每棵非叶子树S , 用叶子替代这棵子树. 如果 S 被叶子替代后形成的新树关于D 的误差等于或小于S 关于 D 所产生的误差, 则用叶子替代子树S</p></blockquote><p><img src="https://pic2.zhimg.com/80/v2-ff11945f2e5a8319d82ab53c363ef441_hd.jpg" alt="img"></p><p><strong>优点：</strong></p><ul><li>REP 是当前最简单的事后剪枝方法之一。</li><li>它的计算复杂性是线性的。</li><li>和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。</li></ul><p><strong>缺点：</strong></p><ul><li>但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意.</li></ul><h4 id="PEP-Pessimistic-Error-Pruning-方法"><a href="#PEP-Pessimistic-Error-Pruning-方法" class="headerlink" title="PEP(Pessimistic Error Pruning)方法"></a><strong>PEP(Pessimistic Error Pruning)方法</strong></h4><blockquote><p>为了克服 R EP 方法需要独立剪枝数据集的缺点而提出的, 它不需要分离的剪枝数据集，为了提高对未来事例的预测可靠性, <strong>PEP 方法对误差估计增加了连续性校正(continuity correction)</strong>。关于PEP方法的数据解释待后续开专题梳理。</p></blockquote><p><strong>优点：</strong></p><ul><li>PEP方法被认为是当前决策树事后剪枝方法中精度较高的算法之一</li><li>PEP 方法不需要分离的剪枝数据集, 这对于事例较少的问题非常有利</li><li>它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .</li></ul><p><strong>缺点：</strong></p><p>PEP是唯一使用自顶向下剪枝策略的事后剪枝方法, 这种策略会带来与事前剪枝方法出现的同样问题, 那就是树的某个节点会在该节点的子孙根据同样准则不需要剪裁时也会被剪裁。</p><p><strong>TIPS：</strong></p><p>个人认为，其实以时间复杂度和空间复杂度为代价，PEP是可以自下而上的，这并不是必然的。</p><h4 id="MEP-Minimum-Error-Pruning-方法"><a href="#MEP-Minimum-Error-Pruning-方法" class="headerlink" title="MEP(Minimum Error Pruning)方法"></a><strong>MEP(Minimum Error Pruning)方法</strong></h4><blockquote><p>MEP 方法的基本思路是采用自底向上的方式, 对于树中每个非叶节点, 首先计算该节点的误差 Er(t) . 然后, 计算该节点每个分枝的误差Er(Tt) , 并且加权相加, 权为每个分枝拥有的训练样本比例. 如果 Er(t) 大于 Er(Tt) , 则保留该子树; 否则, 剪裁它。</p></blockquote><p><strong>优点：</strong></p><ul><li>MEP方法不需要独立的剪枝数据集, 无论是初始版本, 还是改进版本, 在剪枝过程中, 使用的信息都来自于训练样本集.</li><li>它的计算时间复杂性也只和未剪枝树的非叶节点数目成线性关系 .</li></ul><p><strong>缺点：</strong></p><p>类别平均分配的前提假设现实几率不大&amp;对K太过敏感</p><p><img src="https://pic3.zhimg.com/80/v2-5e7deee0ee978be2eec60328192affc6_hd.jpg" alt="img"></p><p>对此，也有改进算法，我没有深入研究。</p><p><img src="https://pic1.zhimg.com/80/v2-3ffc529242dbb52dcb4946e37fed92f0_hd.jpg" alt="img"></p><h4 id="CCP-Cost-Complexity-Pruning-方法"><a href="#CCP-Cost-Complexity-Pruning-方法" class="headerlink" title="CCP(Cost-Complexity Pruning)方法"></a><strong>CCP(Cost-Complexity Pruning)方法</strong></h4><blockquote><p>CCP 方法就是著名的CART(Classificationand Regression Trees)剪枝算法，它包含两个步骤:<br>                (1) 自底向上，通过对原始决策树中的修剪得到一系列的树 {T0,T1,T2,…,Tt}， 其中Tia 是由Ti中的一个或多个子树被替换所得到的，T0为未经任何修剪的原始树，几为只有一个结点的树。</p><p>​        (2) 评价这些树，根据真实误差率来选择一个最优秀的树作为最后被剪枝的树。</p></blockquote><p><strong>缺点：</strong></p><p>生成子树序列 T ( α) 所需要的时间和原决策树非叶节点的关系是二次的, 这就意味着如果非叶节点的数目随着训练例子记录数目线性增加, 则CCP方法的运行时间和训练数据记录数的关系也是二次的 . 这就比本文中将要介绍的其它剪枝方法所需要的时间长得多, 因为其它剪枝方法的运行时间和非叶节点的关系是线性的.</p><p><strong>对比四种方法</strong></p><table><thead><tr><th><strong>剪枝名称</strong></th><th><strong>剪枝方式</strong></th><th><strong>计算复杂度</strong></th><th><strong>误差估计</strong></th></tr></thead><tbody><tr><td>REP</td><td>自底向上</td><td>0(n)</td><td>剪枝集上误差估计</td></tr><tr><td>PEP</td><td>自顶向下</td><td>o(n)</td><td>使用连续纠正</td></tr><tr><td>CCP</td><td>自底向上</td><td>o(n2)</td><td>标准误差</td></tr><tr><td>MEP</td><td>自底向上</td><td>o(n)</td><td>使用连续纠正</td></tr></tbody></table><p>① MEP比PEP不准确，且树大。两者都不需要额外数据集，故当数据集小的时候可以用。对比公式，如果类（Label）多，则用MEP；PEP在数据集uncertain时错误多，不使用。</p><p>② REP最简单且精度高，但需要额外数据集；CCP精度和REP差不多，但树小。</p><p>③ 如果数据集多（REP&amp;CCP←复杂但树小）</p><p>④ 如果数据集小（MEP←不准确树大&amp;PEP←不稳定）</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h1><p>决策树是机器学习算法中比较容易受影响的，从而导致过拟合，有效的剪枝能够减少过拟合发生的概率。</p><p>剪枝主要分为两种：预剪枝(early stopping)，后剪枝，一般说剪枝都是指后剪枝，预剪枝一般叫做early stopping，后剪枝决策树在数学上更加严谨，得到的树至少是和early stopping得到的一样好。</p><p><strong>预剪枝：</strong></p><p>预剪枝的核心思想是在对每一个节点划分之前先进行计算，如果当前节点的划分并不能够带来模型泛化能力的提升就不再进行划分，对于未能够区分的样本种类（此时可能存在不同的样本类别同时存在于节点中），按照投票（少数服从多数）的原则进行判断。</p><p>简单一点的方法可以通过测试集判断划分过后的测试集准确度能否得到提升进行确定，如果准确率不提升变不再进行节点划分。</p><p>这样做的好处是在降低过拟合风险的同时减少了训练时间的开销，但是可能会出现欠拟合的风险：虽然一次划分可能会导致准确率的降低，但是再进行几次划分后，可能会使得准确率显著提升。</p><p><strong>后剪枝：</strong></p><p>后剪枝的核心思想是让算法生成一个完全决策树，然后从最低层向上计算决定是否剪枝。</p><p>同样的，方法可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升，则进行剪枝。</p><p>后剪枝的泛化能力往往高于预剪枝，但是时间花销相对较大。</p><p><strong>剪枝方法的选择</strong></p><p>如果不在乎计算量的问题，后剪枝策略一般更加常用，更加有效。</p><p>后剪枝中REP和CCP通常需要训练集和额外的验证集，计算量更大。</p><p>有研究表明，通常reduced error pruning是效果最好的，但是也不会比其他的好太多。</p><p>经验表明，限制节点的最小样本个数对防止过拟合很重要，输的最大depth的设置往往要依赖于问题的复杂度，另外树的叶节点总个数和最大depth是相关的，所以有些设置只会要求指定其中一个参数。</p><p>无论是预剪枝还是后剪枝都是为了减少决策树过拟合的情况，在实际运用中，我使用了python中的sklearn库中的函数。</p><p>函数中的max_depth参数可以控制树的最大深度，即最多产生几层节点</p><p>函数中的min_samples_split参数可以控制最小划分样本，即当节点样本数大于阈值时才进行下一步划分。</p><p>函数中min_samples_leaf参数可以控制最后的叶子中最小的样本数量，即最后的分类中的样本需要高于阈值</p><p>上述几个参数的设置均可以从控制过拟合的方面进行理解，通过控制树的层数、节点划分样本数量以及每一个分类的样本数可以在一定程度上减少对于样本个性的关注。具体设置需要根据实际情况进行设置</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decision Tree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（02）：决策树的生成</title>
      <link href="/2020/04/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8802%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/"/>
      <url>/2020/04/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8802%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/</url>
      
        <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>上文讲到决策树的特征选择会根据不同的算法选择不同的分裂参考指标，例如信息增益、信息增益比和基尼指数，本文完整分析记录决策树的详细生成过程和剪枝处理。</p><h2 id="2-决策树的生成"><a href="#2-决策树的生成" class="headerlink" title="2. 决策树的生成"></a>2. 决策树的生成</h2><p> <strong>示例数据表格</strong></p><p>    文章所使用的数据集如下，来源于《数据分析实战45讲》17讲中</p><p><img src="https://pic3.zhimg.com/80/v2-393024075528471f43f52891d29320be_hd.jpg" alt="img"></p><h3 id="2-1-相关概念阐述"><a href="#2-1-相关概念阐述" class="headerlink" title="2.1 相关概念阐述"></a><strong>2.1 相关概念阐述</strong></h3><h4 id="2-1-1-决策树"><a href="#2-1-1-决策树" class="headerlink" title="2.1.1 决策树"></a><strong>2.1.1 决策树</strong></h4><p> 以上面的表格数据为例，比如我们考虑要不要去打篮球，先看天气是不是阴天，是阴天的话，外面刮风没，没刮风我们就去，刮风就不去。决策树就是把上面我们判断背后的逻辑整理成一个结构图，也就是一个树状结构。</p><h4 id="2-1-2-ID3、C4-5、CART"><a href="#2-1-2-ID3、C4-5、CART" class="headerlink" title="2.1.2 ID3、C4.5、CART"></a><strong>2.1.2 ID3、C4.5、CART</strong></h4><p>在决策树构造中有三个著名算法：ID3、C4.5、CART，ID3算法计算的是信息增益，C4.5计算使用的是增益率、CART计算使用的是基尼系数，关于这部分内容可以参考上文【<a href="https://dataquaner.github.io/2019/12/17/机器学习系列之决策树算法（01）：决策树特征选择/">机器学习系列之决策树算法（01）：决策树特征选择</a>】下面简单介绍下其算法，这里也不要求完全看懂，扫一眼有个印象就行，在后面的例子中有计算示例，回过头结合看应该就懂了。</p><h5 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a><strong>信息熵</strong></h5><p> 在信息论中，随机离散事件的出现的概率存在不确定性，为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。</p><p>​                                                            Entopy(t)=-Σp(i|t)log2p(i|t)</p><h5 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a><strong>信息增益</strong></h5><p>信息增益指的是划分可以带来纯度的提高，信息熵的下降。特征的信息熵越大代表特征的不确定性越大，代表得知了该特征后，数据集的信息熵会下降更多，即信息增益越大。它的计算公式是父亲节点的信息熵减去所有子节点的信息熵。信息增益的公式可以表示为：</p><p>​                                        Gain(D,a)=Entropy(D)- Σ|Di|/|D|Entropy(Di)</p><h5 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a><strong>信息增益率</strong></h5><p> 信息增益率 = 信息增益 / 属性熵。属性熵，就是每种属性的信息熵，比如天气的属性熵的计算如下,天气有晴阴雨,各占3/7,2/7,2/7：</p><p>​                H(天气)= -(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7))</p><h5 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a><strong>基尼系数</strong></h5><p> 基尼系数在经济学中用来衡量一个国家收入差距的常用指标.当基尼指数大于0.4的时候,说明财富差异悬殊.基尼系数在0.2-0.4之间说明分配合理,财富差距不大.扩展阅读下<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0">基尼系数</a></p><p> 基尼系数本身反应了样本的不确定度.当基尼系数越小的时候,说明样本之间的差异性小,不确定程度低.</p><p> CART算法在构造分类树的时候,会选择基尼系数最小的属性作为属性的划分.</p><p> 基尼系数的计算公式如下:</p><p>​                            Gini = 1 – Σ (Pi)2 for i=1 to number of classes</p><h3 id="2-2-完整生成过程"><a href="#2-2-完整生成过程" class="headerlink" title="2.2 完整生成过程"></a><strong>2.2 完整生成过程</strong></h3><p> 下面是一个完整的决策树的构造生成过程，已完整开头所给的数据为例</p><h4 id="2-2-1-根节点的选择"><a href="#2-2-1-根节点的选择" class="headerlink" title="2.2.1 根节点的选择"></a><strong>2.2.1 根节点的选择</strong></h4><p> 在上面的列表中有四个属性:天气,温度,湿度,刮风.需要先计算出这四个属性的信息增益、信息增益率、基尼系数</p><p> 数据集中有7条数据，3个打篮球，4个不打篮球，不打篮球的概率为4/7,打篮球的概率为3/7,则根据信息熵的计算公式可以得到根节点的信息熵为：</p><p>​                        Ent(D)=-(4/7 * log2(4/7) + 3/7 * log2(3/7))=0.985</p><h5 id="天气"><a href="#天气" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-4977ea2a875ea67cb75588eb04b6aee5_hd.jpg" alt="img"></p><h6 id="信息增益计算"><a href="#信息增益计算" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>如果将天气作为属性划分，分别会有三个叶节点：晴天、阴天、小雨，其中晴天2个不打篮球，1个打篮球；阴天1个打篮球，1个不打篮球；小雨1个打篮球，1个不打篮球，其对应相应的信息熵如下：</p><p>D(晴天)=-(1/3 * log2(1/3) + 2/3 * log2(2/3)) = 0.981</p><p>D(阴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(雨天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>在数据集中晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，那么作为子节点的归一化信息熵为：</p><p>3/7 * 0.918 + 2/7 * 1.0 * 2/7 * 1.0 = 0.965</p><p>其信息增益为：</p><p>Gain(天气)=0.985 - 0.965 = 0.020</p><h6 id="信息增益率计算"><a href="#信息增益率计算" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p> 天气有三个选择，晴天有3条数据，阴天有2条数据，雨天有2条数据，对应的概率为3/7、2/7、2/7，其对应的属性熵为：</p><p>H(天气)=-(3/7 * log2(3/7) + 2/7 * log2(2/7) + 2/7 * log2(2/7)) = 1.556</p><p> 则其信息增益率为：</p><p>Gain_ratio(天气)=0.020/1.556=0.012</p><h6 id="基尼系数计算"><a href="#基尼系数计算" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/3)^2 - (2/3)^2 = 1 - 1/9 - 4/9 = 4/9</li><li>Gini(天气=阴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气=小雨)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 4/21 + 1/7 + 1/7 = 10/21</li></ul><h5 id="温度"><a href="#温度" class="headerlink" title="温度"></a><strong>温度</strong></h5><p>  其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-74ad946b56a27f3bc480ba07f31552de_hd.jpg" alt="img"></p><h6 id="信息增益计算-1"><a href="#信息增益计算-1" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>D(中)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(低)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>4/7 * 1.0 + 2/7 * 1.0 * 1/7 * 0.0 = 0.857</p><p>    其信息增益为：</p><p>Gain(温度)=0.985 - 0.857 = 0.128</p><h6 id="信息增益率计算-1"><a href="#信息增益率计算-1" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(温度)=-(4/7 * log2(4/7) + 2/7 * log2(2/7) + 1/7 * log2(1/7)) = 1.378</p><p>    则其信息增益率为：</p><p>Gain_ratio(温度)=0.128/1.378=0.0928</p><h6 id="基尼系数计算-1"><a href="#基尼系数计算-1" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(温度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(温度=中)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(温度=低)=1 - (0/1)^2 - (1/1)^2 = 1 - 0 - 1 = 0</li><li>Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 3/7</li></ul><h5 id="湿度"><a href="#湿度" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic1.zhimg.com/80/v2-423f80f054a8d86eed652afff6a6c914_hd.jpg" alt="img"></p><h6 id="信息增益计算-2"><a href="#信息增益计算-2" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>D(中)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>    作为子节点的归一化信息熵为：</p><p>4/7 * 1.0 + 3/7 * 0.918 = 0.964</p><p>    其信息增益为：</p><p>Gain(湿度)=0.985 - 0.964 = 0.021</p><h6 id="信息增益率计算-2"><a href="#信息增益率计算-2" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=0.021/0.985=0.021</p><h6 id="基尼系数计算-2"><a href="#基尼系数计算-2" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(湿度=中)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</li><li>Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><h6 id="刮风"><a href="#刮风" class="headerlink" title="刮风"></a><strong>刮风</strong></h6><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-edb75f5790519fcee2dd6317f4d5557d_hd.jpg" alt="img"></p><h6 id="信息增益计算-3"><a href="#信息增益计算-3" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>D(否)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><p>    作为子节点的归一化信息熵为：</p><p>3/7 * 1.0 + 4/7 * 0.918 = 0.964</p><p>    其信息增益为：</p><p>Gain(刮风)=0.985 - 0.964 = 0.021</p><h6 id="信息增益率计算-3"><a href="#信息增益率计算-3" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(4/7 * log2(4/7) + 3/7 * log2(3/7) = 0.985</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0.021/0.985=0.021</p><h6 id="基尼系数计算-3"><a href="#基尼系数计算-3" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(刮风=是)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</li><li>Gini(刮风=否)=1 - (2/4)^2 - (2/4)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><h5 id="根节点的选择"><a href="#根节点的选择" class="headerlink" title="根节点的选择"></a><strong>根节点的选择</strong></h5><p>  如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：温度</p><p><strong>信息增益</strong></p><ul><li><p>Gain(天气)=0.985 - 0.965 = 0.020</p></li><li><p>Gain(温度)=0.985 - 0.857 = 0.128</p></li><li><p>Gain(湿度)=0.985 - 0.964 = 0.021</p></li><li><p>Gain(刮风)=0.985 - 0.964 = 0.021</p></li></ul><p><strong>信息增益率</strong></p><ul><li>Gain_ratio(天气)=0.020/1.556=0.012</li><li>Gain_ratio(温度)=0.128/1.378=0.0928</li><li>Gain_ratio(湿度)=0.021/0.985=0.021</li><li>Gain_ratio(刮风)=0.021/0.985=0.021</li></ul><p><strong>基尼系数</strong></p><ul><li>Gini(天气)=(3/7) * 4/9 + (2/7) * 0.5 + (2/7) * 0.5 = 0.47619</li><li>Gini(温度)=4/7 * 0.5 + 2/7 * 0.5 + 1/7 * 0 = 0.42857</li><li>Gini(湿度)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li><li>Gini(刮风)=(4/7) * 0.5 + (3/7) * 4/9 = 2/7 + 4/21 = 10/21 ~ 0.47619</li></ul><p>确定根节点以后,大致的树结构如下，温度低能确定结果，高和中需要进一步的进行分裂，从剩下的数据中再次进行属性选择:</p><ul><li>根节点<ul><li>子节点温度高:(待进一步进行选择)</li><li>子节点温度中:(待进一步进行选择)</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul><h4 id="2-2-2-子节点温度高的选择"><a href="#2-2-2-子节点温度高的选择" class="headerlink" title="2.2.2 子节点温度高的选择"></a><strong>2.2.2 子节点温度高的选择</strong></h4><p>    其剩下的数据集如下,温度不再进行下面的节点选择参与:</p><p><img src="https://pic1.zhimg.com/80/v2-86c2e574fc2309e9dff98e8205c0dff4_hd.jpg" alt="img"></p><p>    根据信息熵的计算公式可以得到子节点温度高的信息熵为：</p><p>​                                                Ent(D)=-(2/4 * log2(2/4) + 2/4 * log2(2/4)) = 1.0</p><h5 id="天气-1"><a href="#天气-1" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-72771013ffea869ed0ce76c7f8998f79_hd.jpg" alt="img"></p><h6 id="信息增益计算-4"><a href="#信息增益计算-4" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    相应的信息熵如下：</p><p>D(晴天)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>D(阴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>D(雨天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>    归一化信息熵为：</p><p>2/4 * 1.0 + 1/4 * 0.0 * 1/4 * 0.0 = 0.5</p><p>    其信息增益为：</p><p>Gain(天气)=1.0 - 0.5 = 0.5</p><h6 id="信息增益率计算-4"><a href="#信息增益率计算-4" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    对应的属性熵为：</p><p>H(天气)=-(2/4 * log2(2/4) + 1/4 * log2(1/4) + 1/4 * log2(1/4)) = 1.5</p><p>    则其信息增益率为：</p><p>Gain_ratio(天气)=0.5/1.5=0.33333</p><h6 id="基尼系数计算-4"><a href="#基尼系数计算-4" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/2)^2 - (1/2)^2 = 1 - 1/4 - 1/4 = 0.5</li><li>Gini(天气=阴)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气=小雨)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25</li></ul><h5 id="湿度-1"><a href="#湿度-1" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-ea98c22052cf1246618f266c52be998e_hd.jpg" alt="img"></p><h6 id="信息增益计算-5"><a href="#信息增益计算-5" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(2/2 * log2(2/2) + 0/2 * log2(0/2)) = 0.0</p><p>D(中)=-(0/2 * log2(0/2) + 2/2 * log2(2/2)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>2/4 * 0.0 + 2/4 * 0.0 = 0.0</p><p>    其信息增益为：</p><p>Gain(湿度)=1.0 - 0.0 = 1.0</p><h6 id="信息增益率计算-5"><a href="#信息增益率计算-5" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(2/4 * log2(2/4) + 2/4 * log2(2/4) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=1.0/1.0=1.0</p><h6 id="基尼系数计算-5"><a href="#基尼系数计算-5" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (2/2)^2 - (0/2)^2 = 0</li><li>Gini(湿度=中)=1 - (0/2)^2 - (2/2)^2 = 0</li><li>Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0</li></ul><h5 id="刮风-1"><a href="#刮风-1" class="headerlink" title="刮风"></a><strong>刮风</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-ba94b90f801904bae450796e8a5db0be_hd.jpg" alt="img"></p><h6 id="信息增益计算-6"><a href="#信息增益计算-6" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0</p><p>D(否)=-(2/3 * log2(2/3) + 1/3 * log2(1/3)) = 0.918</p><p>    作为子节点的归一化信息熵为：</p><p>1/4 * 0.0 + 3/4 * 0.918 = 0.688</p><p>    其信息增益为：</p><p>Gain(刮风)=1.0 - 0.688 = 0.312</p><h6 id="信息增益率计算-6"><a href="#信息增益率计算-6" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(1/3 * log2(1/3) + 2/3 * log2(2/3) = 0.918</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0.312/0.918=0.349</p><h6 id="基尼系数计算-6"><a href="#基尼系数计算-6" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li><p>Gini(刮风=是)=1 - (0/1)^2 - (1/1)^2 = 0</p></li><li><p>Gini(刮风=否)=1 - (2/3)^2 - (1/3)^2 = 1 - 4/9 - 1/9 = 4/9</p></li><li><p>Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333</p></li></ul><p><strong>子节点温度高的选择</strong></p><p>    如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到选择为：湿度</p><ul><li>Gain(天气)=1.0 - 0.5 = 0.5</li><li>Gain(湿度)=1.0 - 0.0 = 1.0</li><li>Gain(刮风)=1.0 - 0.688 = 0.312</li><li>Gain_ratio(天气)=0.5/1.5=0.33333</li><li>Gain_ratio(湿度)=1.0/1.0=1.0</li><li>Gain_ratio(刮风)=0.312/0.918=0.349</li><li>Gini(天气)=2/4 * 0.5 + 1/4 * 0 + 1/4 * 0 = 0.25</li><li>Gini(湿度)=(2/4) * 0 + (2/4) * 0 = 0</li><li>Gini(刮风)=(1/4) * 0 + (3/4) * 4/9 = 1/3 = 0.333333</li></ul><p>    确定跟节点以后,大致的树结构如下，选择湿度作为分裂属性后能直接确定结果:</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中:(待进一步进行选择)<ul><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h4 id="2-2-3-子节点温度中的选择"><a href="#2-2-3-子节点温度中的选择" class="headerlink" title="2.2.3 子节点温度中的选择"></a><strong>2.2.3 子节点温度中的选择</strong></h4><p>    其剩下的数据集如下,温度不再进行下面的节点选择参与:</p><p><img src="https://pic1.zhimg.com/80/v2-3f1b0c5e060288599f90c428441aaabc_hd.jpg" alt="img"></p><p>    根据信息熵的计算公式可以得到子节点温度高的信息熵为：</p><p>Ent(D)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><h5 id="天气-2"><a href="#天气-2" class="headerlink" title="天气"></a><strong>天气</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic4.zhimg.com/80/v2-7ac9cd99fc8388c6c3f34fdb2e132b57_hd.jpg" alt="img"></p><h6 id="信息增益计算-7"><a href="#信息增益计算-7" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    相应的信息熵如下：</p><p>D(晴天)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0 D</p><p>(阴天)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>    归一化信息熵为：</p><p>1/2 * 0.0 + 1/2 * 0.0 = 0</p><p>    其信息增益为：</p><p>Gain(天气)=1.0 - 0 = 1.0</p><h6 id="信息增益率计算-7"><a href="#信息增益率计算-7" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    对应的属性熵为：</p><p>H(天气)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(天气)=1.0/1.0=1.0</p><h6 id="基尼系数计算-7"><a href="#基尼系数计算-7" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(天气=晴)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(天气=阴)=1 - (0/1)^2 - (1/1)^2 = 0</li><li>Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0</li></ul><h5 id="湿度-2"><a href="#湿度-2" class="headerlink" title="湿度"></a><strong>湿度</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic3.zhimg.com/80/v2-a27cfcdbbde0c07ab39cbdac34b8e6da_hd.jpg" alt="img"></p><h6 id="信息增益计算-8"><a href="#信息增益计算-8" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(高)=-(0/1 * log2(0/1) + 1/1 * log2(1/1)) = 0.0</p><p>D(中)=-(1/1 * log2(1/1) + 0/1 * log2(0/1)) = 0.0</p><p>    作为子节点的归一化信息熵为：</p><p>1/2 * 0.0 + 1/2 * 0.0 = 0</p><p>    其信息增益为：</p><p>Gain(湿度)=1.0 - 0.0 = 1.0</p><h6 id="信息增益率计算-8"><a href="#信息增益率计算-8" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(湿度)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(湿度)=1.0/1.0=1.0</p><h6 id="基尼系数计算-8"><a href="#基尼系数计算-8" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(湿度=高)=1 - (0/1)^2 - (1/1)^2 = 0</li><li>Gini(湿度=中)=1 - (1/1)^2 - (0/1)^2 = 0</li><li>Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0</li></ul><h5 id="刮风-2"><a href="#刮风-2" class="headerlink" title="刮风"></a><strong>刮风</strong></h5><p>    其数据表格如下:</p><p><img src="https://pic2.zhimg.com/80/v2-887ebcf7eecd07ee13ca02d024a3b321_hd.jpg" alt="img"></p><h6 id="信息增益计算-9"><a href="#信息增益计算-9" class="headerlink" title="信息增益计算"></a><strong>信息增益计算</strong></h6><p>    各情况的信息熵如下：</p><p>D(是)=-(1/2 * log2(1/2) + 1/2 * log2(1/2)) = 1.0</p><p>    作为子节点的归一化信息熵为：</p><p>1/1 * 1.0 = 1.0</p><p>    其信息增益为：</p><p>Gain(刮风)=1.0 - 1.0 = 0</p><h6 id="信息增益率计算-9"><a href="#信息增益率计算-9" class="headerlink" title="信息增益率计算"></a><strong>信息增益率计算</strong></h6><p>    属性熵为：</p><p>H(刮风)=-(2/2 * log2(2/2) = 0.0</p><p>    则其信息增益率为：</p><p>Gain_ratio(刮风)=0/0 = 0</p><h6 id="基尼系数计算-9"><a href="#基尼系数计算-9" class="headerlink" title="基尼系数计算"></a><strong>基尼系数计算</strong></h6><ul><li>Gini(刮风=是)=1 - (1/2)^2 - (1/2)^2 = 0.5</li><li>Gini(刮风)=2/2 * 0.5 = 0.5</li></ul><p><strong>子节点温度中的选择</strong></p><p>如下汇总所有接口,第一个为信息增益的，第二个为信息增益率的，第三个为基尼系数的。其中信息增益和信息增益率选择最大的，基尼系数选择最小的。从下面的结果可以得到天气和湿度是一样好的，我们随机选天气吧</p><ul><li>Gain(天气)=1.0 - 0 = 1.0</li><li>Gain(湿度)=1.0 - 0.0 = 1.0</li><li>Gain(刮风)=1.0 - 1.0 = 0</li><li>Gain_ratio(天气)=1.0/1.0=1.0</li><li>Gain_ratio(湿度)=1.0/1.0=1.0</li><li>Gain_ratio(刮风)=0/0 = 0</li><li>Gini(天气)=1/2 * 0.0 + 1/2 * 0.0 = 0</li><li>Gini(湿度)=1/2 * 0.0 + 1/2 * 0.0 = 0</li><li>Gini(刮风)=2/2 * 0.5 = 0.5</li></ul><p>    确定跟节点以后,大致的树结构如下，选择天气作为分裂属性后能直接确定结果:</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中<ul><li>叶节点天气晴：打篮球</li><li>叶节点天气阴：不打篮球</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h4 id="2-2-4-最终的决策树"><a href="#2-2-4-最终的决策树" class="headerlink" title="2.2.4 最终的决策树"></a><strong>2.2.4 最终的决策树</strong></h4><p>    在上面的步骤已经进行完整的演示，得到当前数据一个完整的决策树：</p><ul><li>根节点<ul><li>子节点温度高<ul><li>叶节点湿度高：打篮球</li><li>叶节点湿度中：不打篮球</li></ul></li><li>子节点温度中<ul><li>叶节点天气晴：打篮球</li><li>叶节点天气阴：不打篮球</li><li>叶节点温度低:不打篮球(能直接确定为不打篮球)</li></ul></li></ul></li></ul><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a><strong>3. 思考</strong></h2><p> 在构造的过程中我们可以发现，有可能同一个属性在同一级会被选中两次，比如上面的决策树中子节点温度高中都能选中温度作为分裂属性，这样是否合理？</p><p> 完整的构造整个决策树后，发现整个决策树的高度大于等于属性数量，感觉决策树应该是构造时间较长，但用于决策的时候很快，时间复杂度也就是O(n)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decision Tree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据存储之MySQL系列（01）：MySQL体系结构</title>
      <link href="/2020/04/11/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
      <url>/2020/04/11/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> DataBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xgboost算法模型输出的解释</title>
      <link href="/2020/04/11/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/"/>
      <url>/2020/04/11/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p> 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。</p><h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2. 数据集"></a>2. 数据集</h2><p> 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasetsiris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>data <span class="token operator">=</span> iris<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span> data<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true">#(100L, 4L)</span><span class="token comment" spellcheck="true">#一共有100个样本数据, 维度为4维</span>label <span class="token operator">=</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span> label<span class="token comment" spellcheck="true">#正好选取label为0和1的数据</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-训练集与测试集"><a href="#3-训练集与测试集" class="headerlink" title="3. 训练集与测试集"></a>3. 训练集与测试集</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cross_validation <span class="token keyword">import</span> train_test_splittrain_x<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="4-Xgboost建模"><a href="#4-Xgboost建模" class="headerlink" title="4. Xgboost建模"></a>4. Xgboost建模</h2><h3 id="4-1-模型初始化设置"><a href="#4-1-模型初始化设置" class="headerlink" title="4.1 模型初始化设置"></a>4.1 模型初始化设置</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgbdtrain<span class="token operator">=</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>train_x<span class="token punctuation">,</span>label<span class="token operator">=</span>train_y<span class="token punctuation">)</span>dtest<span class="token operator">=</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>test_x<span class="token punctuation">)</span>params<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'booster'</span><span class="token punctuation">:</span><span class="token string">'gbtree'</span><span class="token punctuation">,</span>    <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'binary:logistic'</span><span class="token punctuation">,</span>    <span class="token string">'eval_metric'</span><span class="token punctuation">:</span> <span class="token string">'auc'</span><span class="token punctuation">,</span>    <span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token string">'lambda'</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">,</span>    <span class="token string">'subsample'</span><span class="token punctuation">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>    <span class="token string">'colsample_bytree'</span><span class="token punctuation">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>    <span class="token string">'min_child_weight'</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span>    <span class="token string">'eta'</span><span class="token punctuation">:</span> <span class="token number">0.025</span><span class="token punctuation">,</span>    <span class="token string">'seed'</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token string">'nthread'</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">,</span>     <span class="token string">'silent'</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">}</span>watchlist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>dtrain<span class="token punctuation">,</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-建模与预测"><a href="#4-2-建模与预测" class="headerlink" title="4.2 建模与预测"></a>4.2 建模与预测</h3><pre class="line-numbers language-python"><code class="language-python">bst<span class="token operator">=</span>xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>params<span class="token punctuation">,</span>dtrain<span class="token punctuation">,</span>num_boost_round<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>evals<span class="token operator">=</span>watchlist<span class="token punctuation">)</span>ypred<span class="token operator">=</span>bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 设置阈值, 输出一些评价指标</span>y_pred <span class="token operator">=</span> <span class="token punctuation">(</span>ypred <span class="token operator">>=</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1</span><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token keyword">print</span> <span class="token string">'AUC: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>roc_auc_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>ypred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'ACC: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'Recall: %.4f'</span> <span class="token operator">%</span> metrics<span class="token punctuation">.</span>recall_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'F1-score: %.4f'</span> <span class="token operator">%</span>metrics<span class="token punctuation">.</span>f1_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token string">'Precesion: %.4f'</span> <span class="token operator">%</span>metrics<span class="token punctuation">.</span>precision_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>metrics<span class="token punctuation">.</span>confusion_matrix<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Out[23]:</p><pre class="line-numbers language-python"><code class="language-python">AUC<span class="token punctuation">:</span> <span class="token number">1.0000</span>ACC<span class="token punctuation">:</span> <span class="token number">1.0000</span>Recall<span class="token punctuation">:</span> <span class="token number">1.0000</span>F1<span class="token operator">-</span>score<span class="token punctuation">:</span> <span class="token number">1.0000</span>Precesion<span class="token punctuation">:</span> <span class="token number">1.0000</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>int64<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Yeah, 完美的模型, 完美的预测!</p><h3 id="4-3-可视化输出"><a href="#4-3-可视化输出" class="headerlink" title="4.3 可视化输出"></a>4.3 可视化输出</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#对于预测的输出有三种方式</span>?bst<span class="token punctuation">.</span>predictSignature<span class="token punctuation">:</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>data<span class="token punctuation">,</span> output_margin<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> ntree_limit<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> pred_leaf<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> pred_contribs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> approx_contribs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>pred_leaf <span class="token punctuation">:</span> bool    When this option <span class="token keyword">is</span> on<span class="token punctuation">,</span> the output will be a matrix of <span class="token punctuation">(</span>nsample<span class="token punctuation">,</span> ntrees<span class="token punctuation">)</span>    <span class="token keyword">with</span> each record indicating the predicted leaf index of each sample <span class="token keyword">in</span> each tree<span class="token punctuation">.</span>    Note that the leaf index of a tree <span class="token keyword">is</span> unique per tree<span class="token punctuation">,</span> so you may find leaf <span class="token number">1</span>    <span class="token keyword">in</span> both tree <span class="token number">1</span> <span class="token operator">and</span> tree <span class="token number">0</span><span class="token punctuation">.</span>pred_contribs <span class="token punctuation">:</span> bool    When this option <span class="token keyword">is</span> on<span class="token punctuation">,</span> the output will be a matrix of <span class="token punctuation">(</span>nsample<span class="token punctuation">,</span> nfeats<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> each record indicating the feature contributions <span class="token punctuation">(</span>SHAP values<span class="token punctuation">)</span> <span class="token keyword">for</span> that    prediction<span class="token punctuation">.</span> The sum of all feature contributions <span class="token keyword">is</span> equal to the prediction<span class="token punctuation">.</span>    Note that the bias <span class="token keyword">is</span> added <span class="token keyword">as</span> the final column<span class="token punctuation">,</span> on top of the regular features<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-3-1-得分"><a href="#4-3-1-得分" class="headerlink" title="4.3.1 得分"></a>4.3.1 得分</h4><p>默认的输出就是得分, 这没什么好说的, 直接上code.</p><pre class="line-numbers language-python"><code class="language-python">ypred <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span>ypred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[32]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>        <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>        <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>        <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>        <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">,</span>  <span class="token number">0.80391562</span><span class="token punctuation">,</span>  <span class="token number">0.20081411</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出.</p><h4 id="4-3-2-所属的叶子节点"><a href="#4-3-2-所属的叶子节点" class="headerlink" title="4.3.2 所属的叶子节点"></a>4.3.2 所属的叶子节点</h4><p>当设置<code>pred_leaf=True</code>的时候, 这时就会输出每个样本在所有树中的叶子节点</p><pre class="line-numbers language-python"><code class="language-python">ypred_leaf <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">,</span> pred_leaf<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>ypred_leaf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[33]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以<code>ypred_leaf</code>的维度为<code>[100*100]</code>.</p><p>对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树).</p><p>那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型.</p><pre class="line-numbers language-python"><code class="language-python">xgb<span class="token punctuation">.</span>to_graphviz<span class="token punctuation">(</span>bst<span class="token punctuation">,</span> num_trees<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可视化第一棵树的生成情况</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://images2017.cnblogs.com/blog/957413/201710/957413-20171017204407818-1932629185.png" alt="img"></p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#直接输出模型的迭代工程</span>bst<span class="token punctuation">.</span>dump_model<span class="token punctuation">(</span><span class="token string">"model.txt"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">booster<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f3<span class="token operator">&lt;</span><span class="token number">0.75</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.019697</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0214286</span>booster<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f2<span class="token operator">&lt;</span><span class="token number">2.35</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.0212184</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0212</span>booster<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">[</span>f2<span class="token operator">&lt;</span><span class="token number">2.35</span><span class="token punctuation">]</span> yes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>no<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>missing<span class="token operator">=</span><span class="token number">1</span>    <span class="token number">1</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token operator">-</span><span class="token number">0.0197404</span>    <span class="token number">2</span><span class="token punctuation">:</span>leaf<span class="token operator">=</span><span class="token number">0.0197235</span>booster<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">:</span> ……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值.</p><p>在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值.</p><p>同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值.</p><pre class="line-numbers language-python"><code class="language-python">leaf1   <span class="token operator">-</span><span class="token number">1.381214</span>leaf2    <span class="token number">1.410950</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了<code>'objective': 'binary:logistic'</code>, 因此使用函数将累加的值转换为实际的打分:</p><p>f(x)=1/(1+exp(−x))</p><pre class="line-numbers language-python"><code class="language-python"><span class="token number">1</span><span class="token operator">/</span>float<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1.38121416</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">0.20081407112186503</span><span class="token number">1</span><span class="token operator">/</span>float<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.410950</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">0.8039157403338895</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这就与<code>ypred = bst.predict(dtest)</code> 的分值相对应上了.</p><h4 id="4-3-2-特征重要性"><a href="#4-3-2-特征重要性" class="headerlink" title="4.3.2 特征重要性"></a>4.3.2 特征重要性</h4><p>接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性.</p><pre class="line-numbers language-python"><code class="language-python">ypred_contribs <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">,</span> pred_contribs<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>ypred_contribs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Out[37]:</p><pre class="line-numbers language-python"><code class="language-python">array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0.96967536</span><span class="token punctuation">,</span>  <span class="token number">0.39522746</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.01448286</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.41277751</span><span class="token punctuation">,</span>  <span class="token number">0.04604663</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的<code>ypred_contribs</code>的维度为<code>[100,5]</code>, 通过阅读前面的文档注释就可以知道, 最后一列是<code>bias</code>, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的.</p><p>通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例.</p><pre class="line-numbers language-python"><code class="language-python">score_a <span class="token operator">=</span> sum<span class="token punctuation">(</span>ypred_contribs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> score_a<span class="token comment" spellcheck="true"># -1.38121373579</span>score_b <span class="token operator">=</span> sum<span class="token punctuation">(</span>ypred_contribs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> score_b<span class="token comment" spellcheck="true"># 1.41094945744</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相同的分值, 相同的处理情况.</p><p>到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LightGBM算法基础系列之基础理论篇（1）</title>
      <link href="/2020/04/11/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/"/>
      <url>/2020/04/11/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LightGBM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（10）：决策树模型，XGBoost，LightGBM和CatBoost模型可视化</title>
      <link href="/2020/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8810%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%EF%BC%8CXGBoost%EF%BC%8CLightGBM%E5%92%8CCatBoost%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
      <url>/2020/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8810%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%EF%BC%8CXGBoost%EF%BC%8CLightGBM%E5%92%8CCatBoost%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="安装-graphviz"><a href="#安装-graphviz" class="headerlink" title="安装 graphviz"></a>安装 graphviz</h2><ol><li><p>参考文档 <a href="http://graphviz.readthedocs.io/en/stable/manual.html#installation" target="_blank" rel="noopener">http://graphviz.readthedocs.io/en/stable/manual.html#installation</a></p></li><li><p>graphviz安装包下载地址 <a href="https://www.graphviz.org/download/" target="_blank" rel="noopener">https://www.graphviz.org/download/</a></p></li><li><p>将graphviz的安装位置添加到系统环境变量</p></li><li><p>使用pip install graphviz安装graphviz python包</p></li><li><p>使用pip install pydotplus安装pydotplus python包</p></li></ol><h2 id="决策树模型可视化"><a href="#决策树模型可视化" class="headerlink" title="决策树模型可视化"></a>决策树模型可视化</h2><p>   以iris数据为例。训练一个分类决策树，调用export_graphviz函数导出DOT格式的文件。并用pydotplus包绘制图片。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#在环境变量中加入安装的Graphviz路径</span><span class="token keyword">import</span> osos<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"PATH"</span><span class="token punctuation">]</span> <span class="token operator">+=</span> os<span class="token punctuation">.</span>pathsep <span class="token operator">+</span> <span class="token string">'E:/Program Files (x86)/Graphviz2.38/bin'</span><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> tree<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_irisiris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span class="token keyword">import</span> pydotplus<span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> Imagedot_data <span class="token operator">=</span> tree<span class="token punctuation">.</span>export_graphviz<span class="token punctuation">(</span>clf<span class="token punctuation">,</span> out_file<span class="token operator">=</span>None<span class="token punctuation">,</span>                          feature_names<span class="token operator">=</span>iris<span class="token punctuation">.</span>feature_names<span class="token punctuation">,</span>                           class_names<span class="token operator">=</span>iris<span class="token punctuation">.</span>target_names<span class="token punctuation">,</span>                           filled<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> rounded<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> special_characters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  graph <span class="token operator">=</span> pydotplus<span class="token punctuation">.</span>graph_from_dot_data<span class="token punctuation">(</span>dot_data<span class="token punctuation">)</span>Image<span class="token punctuation">(</span>graph<span class="token punctuation">.</span>create_png<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdn.net/20180808124429851?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xfeHpt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="XGBoost模型可视化"><a href="#XGBoost模型可视化" class="headerlink" title="XGBoost模型可视化"></a>XGBoost模型可视化</h2><p>参考文档 <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" target="_blank" rel="noopener">https://xgboost.readthedocs.io/en/latest/python/python_api.html</a><br>xgboost中，对应的可视化函数是xgboost.to_graphviz。以iris数据为例，训练一个xgb分类模型并可视化</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在环境变量中加入安装的Graphviz路径</span><span class="token keyword">import</span> osos<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"PATH"</span><span class="token punctuation">]</span> <span class="token operator">+=</span> os<span class="token punctuation">.</span>pathsep <span class="token operator">+</span> <span class="token string">'E:/Program Files (x86)/Graphviz2.38/bin'</span><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_irisiris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>xgb_clf <span class="token operator">=</span> xgb<span class="token punctuation">.</span>XGBClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>xgb_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>xgb<span class="token punctuation">.</span>to_graphviz<span class="token punctuation">(</span>xgb_clf<span class="token punctuation">,</span> num_trees<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdn.net/20180808130159227?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xfeHpt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>也可以通过Digraph对象可以将保存文件并查看</p><pre class="line-numbers language-python"><code class="language-python">digraph <span class="token operator">=</span> xgb<span class="token punctuation">.</span>to_graphviz<span class="token punctuation">(</span>xgb_clf<span class="token punctuation">,</span> num_trees<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>digraph<span class="token punctuation">.</span>format <span class="token operator">=</span> <span class="token string">'png'</span>digraph<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token string">'./iris_xgb'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>xgboost中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltfig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>xgb<span class="token punctuation">.</span>plot_tree<span class="token punctuation">(</span>xgb_clf<span class="token punctuation">,</span> num_trees<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ax<span class="token operator">=</span>ax<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdn.net/20180808131546220?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xfeHpt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h2 id="LightGBM模型可视化"><a href="#LightGBM模型可视化" class="headerlink" title="LightGBM模型可视化"></a>LightGBM模型可视化</h2><p>参考文档 <a href="https://lightgbm.readthedocs.io/en/latest/Python-API.html#plotting" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/Python-API.html#plotting</a><br>lgb中，对应的可视化函数是lightgbm.create_tree_digraph。以iris数据为例，训练一个lgb分类模型并可视化</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在环境变量中加入安装的Graphviz路径</span><span class="token keyword">import</span> osos<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"PATH"</span><span class="token punctuation">]</span> <span class="token operator">+=</span> os<span class="token punctuation">.</span>pathsep <span class="token operator">+</span> <span class="token string">'E:/Program Files (x86)/Graphviz2.38/bin'</span><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris<span class="token keyword">import</span> lightgbm <span class="token keyword">as</span> lgbiris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>lgb_clf <span class="token operator">=</span> lgb<span class="token punctuation">.</span>LGBMClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>lgb_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>lgb<span class="token punctuation">.</span>create_tree_digraph<span class="token punctuation">(</span>lgb_clf<span class="token punctuation">,</span> tree_index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>lgb中提供了另一个api plot_tree，使用matplotlib可视化树模型。效果上没有graphviz清楚。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltfig2 <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax <span class="token operator">=</span> fig2<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>lgb<span class="token punctuation">.</span>plot_tree<span class="token punctuation">(</span>lgb_clf<span class="token punctuation">,</span> tree_index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ax<span class="token operator">=</span>ax<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CatBoost模型可视化"><a href="#CatBoost模型可视化" class="headerlink" title="CatBoost模型可视化"></a>CatBoost模型可视化</h2><p><a href="https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/" target="_blank" rel="noopener">参考文档</a><br>        catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)<br>以iris数据为例，训练一个catboost模型。</p><p>参考文档 <a href="https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/" target="_blank" rel="noopener">https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/</a><br>catboost并没有提供模型可视化的api。唯一可以导出模型结构的api是save_model(fname, format=”cbm”, export_parameters=None)<br>以iris数据为例，训练一个catboost模型。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris<span class="token keyword">from</span> catboost <span class="token keyword">import</span> CatBoostClassifieriris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>cat_clf <span class="token operator">=</span> CatBoostClassifier<span class="token punctuation">(</span>iterations<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>cat_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以python代码格式保存模型文件</p><pre class="line-numbers language-python"><code class="language-python">cat_clf<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token string">'catboost_model_file.py'</span><span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">"python"</span><span class="token punctuation">,</span> export_parameters<span class="token operator">=</span>None<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也可以保存以C++代码格式保存模型文件</p><pre class="line-numbers language-python"><code class="language-python">cat_clf<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token string">'catboost_model_file.cpp'</span><span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">"cpp"</span><span class="token punctuation">,</span> export_parameters<span class="token operator">=</span>None<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看保存到的python代码，部分信息如下 </p><p><img src="https://img-blog.csdn.net/20180808143016336?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xfeHpt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180808142319424?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xfeHpt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>需要自己解析出文件了树的结构，再用 graphviz 绘制图像</p><h3 id="导出的Python文件"><a href="#导出的Python文件" class="headerlink" title="导出的Python文件"></a>导出的Python文件</h3><p>首先第一个for循环部分</p><pre class="line-numbers language-python"><code class="language-python">binary_feature_index <span class="token operator">=</span> <span class="token number">0</span>binary_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> model<span class="token punctuation">.</span>binary_feature_count<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>model<span class="token punctuation">.</span>float_feature_count<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>model<span class="token punctuation">.</span>border_counts<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        binary_features<span class="token punctuation">[</span>binary_feature_index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>float_features<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">></span> model<span class="token punctuation">.</span>borders<span class="token punctuation">[</span>binary_feature_index<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">0</span>        binary_feature_index <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输入的参数float_features存储输入的数值型特征值。model.binary_feature_count表示booster中所有树的节点总数。model.border_counts存储每个feature对应的节点数量，model.borders存储所有节点的判断边界。显然，CatBoost并没有按照二叉树结构从左到右，从上到下的存储结构。此段代码的功能，生成所有节点的判断结果。如果特征值大于判断边界，表示为1，否则为0。存储在binary_features中。</p><p>第二个for循环部分</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Extract and sum values from trees</span>result <span class="token operator">=</span> <span class="token number">0.0</span>tree_splits_index <span class="token operator">=</span> <span class="token number">0</span>current_tree_leaf_values_index <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> tree_id <span class="token keyword">in</span> range<span class="token punctuation">(</span>model<span class="token punctuation">.</span>tree_count<span class="token punctuation">)</span><span class="token punctuation">:</span>    current_tree_depth <span class="token operator">=</span> model<span class="token punctuation">.</span>tree_depth<span class="token punctuation">[</span>tree_id<span class="token punctuation">]</span>    index <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> depth <span class="token keyword">in</span> range<span class="token punctuation">(</span>current_tree_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>        index <span class="token operator">|</span><span class="token operator">=</span> <span class="token punctuation">(</span>binary_features<span class="token punctuation">[</span>model<span class="token punctuation">.</span>tree_splits<span class="token punctuation">[</span>tree_splits_index <span class="token operator">+</span> depth<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> depth<span class="token punctuation">)</span>    result <span class="token operator">+=</span> model<span class="token punctuation">.</span>leaf_values<span class="token punctuation">[</span>current_tree_leaf_values_index <span class="token operator">+</span> index<span class="token punctuation">]</span>    tree_splits_index <span class="token operator">+=</span> current_tree_depth    current_tree_leaf_values_index <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> current_tree_depth<span class="token punctuation">)</span><span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段点代码功能是生成模型的预测结果result。model.tree_count表示决策树的数量，遍历每棵决策树。model.tree_depth存储每棵决策树的深度，取当前树的深度，存储在current_tree_depth。model.tree_splits存储决策树当前深度的节点在binary_features中的索引，每棵树有current_tree_depth个节点。看似CatBoost模型存储了都是完全二叉树，而且每一层的节点以及该节点的判断边界一致。如一棵6层的决策，可以从binary_features中得到一个长度为6，值为0和1组成的list。model.leaf_values存储所有叶子节点的值，每棵树的叶子节点有(1 &lt;&lt; current_tree_depth)个。将之前得到的list，倒序之后，看出一个2进制表示的数index，加上current_tree_leaf_values_index后，即是值在model.leaf_values的索引。将所有树得到的值相加，得到CatBoost模型的结果。</p><h3 id="还原CatBoost模型树"><a href="#还原CatBoost模型树" class="headerlink" title="还原CatBoost模型树"></a>还原CatBoost模型树</h3><p>先从第二个for循环开始，打印每棵树序号，树的深度，当前树节点索引在tree_splits的便宜了，已经每个节点对应在tree_splits中的值。这个值对应的是在第一个for循环中生成的binary_features的索引。</p><pre class="line-numbers language-python"><code class="language-python">tree_splits_index <span class="token operator">=</span> <span class="token number">0</span>current_tree_leaf_values_index <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> tree_id <span class="token keyword">in</span> range<span class="token punctuation">(</span>tree_count<span class="token punctuation">)</span><span class="token punctuation">:</span>    current_tree_depth <span class="token operator">=</span> tree_depth<span class="token punctuation">[</span>tree_id<span class="token punctuation">]</span>    tree_splits_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> depth <span class="token keyword">in</span> range<span class="token punctuation">(</span>current_tree_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>        tree_splits_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tree_splits<span class="token punctuation">[</span>tree_splits_index <span class="token operator">+</span> depth<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span> tree_id<span class="token punctuation">,</span> current_tree_depth<span class="token punctuation">,</span> tree_splits_index<span class="token punctuation">,</span> tree_splits_list    tree_splits_index <span class="token operator">+=</span> current_tree_depth    current_tree_leaf_values_index <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> current_tree_depth<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token number">0</span> <span class="token number">6</span> <span class="token number">0</span> <span class="token punctuation">[</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">104</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">]</span><span class="token number">1</span> <span class="token number">6</span> <span class="token number">6</span> <span class="token punctuation">[</span><span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">,</span> <span class="token number">106</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token number">2</span> <span class="token number">6</span> <span class="token number">12</span> <span class="token punctuation">[</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">34</span><span class="token punctuation">]</span><span class="token number">3</span> <span class="token number">6</span> <span class="token number">18</span> <span class="token punctuation">[</span><span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">105</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">106</span><span class="token punctuation">,</span> <span class="token number">57</span><span class="token punctuation">,</span> <span class="token number">111</span><span class="token punctuation">]</span><span class="token number">4</span> <span class="token number">6</span> <span class="token number">24</span> <span class="token punctuation">[</span><span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">75</span><span class="token punctuation">,</span> <span class="token number">57</span><span class="token punctuation">]</span><span class="token number">5</span> <span class="token number">6</span> <span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">103</span><span class="token punctuation">,</span> <span class="token number">104</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">]</span><span class="token number">6</span> <span class="token number">6</span> <span class="token number">36</span> <span class="token punctuation">[</span><span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">]</span><span class="token number">7</span> <span class="token number">6</span> <span class="token number">42</span> <span class="token punctuation">[</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">69</span><span class="token punctuation">,</span> <span class="token number">111</span><span class="token punctuation">]</span><span class="token number">8</span> <span class="token number">6</span> <span class="token number">48</span> <span class="token punctuation">[</span><span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">71</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token number">9</span> <span class="token number">4</span> <span class="token number">54</span> <span class="token punctuation">[</span><span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。</p><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>从第一个for循环可以看出，每个feature对应的节点都在一起，且每个feature的数量保存在model.border_counts。即可生成每个feature在binary_features的索引区间。</p><pre class="line-numbers language-python"><code class="language-python">split_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>border_counts<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    split_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>split_list<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> border_counts<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> border_counts<span class="token keyword">print</span> zip<span class="token punctuation">(</span>split_list<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> split_list<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">53</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">53</span><span class="token punctuation">,</span> <span class="token number">92</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">92</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在拿到一个binary_features的索引后即可知道该索引对应的节点使用的特征序号（float_features的索引）。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">find_feature</span><span class="token punctuation">(</span>tree_splits_index<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>split_list<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> split_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> tree_splits_index <span class="token operator">&lt;</span> split_list<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> i<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>有了节点在binary_features中的索引，该索引也对应特征的判断边界数值索引，也知道了如何根据索引获取特征序号。决策树索引信息都的得到了，现在可以绘制树了。</p><h2 id="绘制单棵决策树"><a href="#绘制单棵决策树" class="headerlink" title="绘制单棵决策树"></a>绘制单棵决策树</h2><p>首先修改一下代码，便于获取单棵树的节点</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">CatBoostTree</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> CatboostModel<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> CatboostModel        self<span class="token punctuation">.</span>split_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>float_feature_count<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>split_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>split_list<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>border_counts<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">find_feature</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> splits_index<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 可优化成二分查找</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>float_feature_count<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>split_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> splits_index <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>split_list<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> i<span class="token keyword">def</span> <span class="token function">get_split_index</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tree_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    tree_splits_index <span class="token operator">=</span> <span class="token number">0</span>    current_tree_leaf_values_index <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> index <span class="token keyword">in</span> range<span class="token punctuation">(</span>tree_id<span class="token punctuation">)</span><span class="token punctuation">:</span>        current_tree_depth <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>tree_depth<span class="token punctuation">[</span>index<span class="token punctuation">]</span>        tree_splits_index <span class="token operator">+=</span> current_tree_depth        current_tree_leaf_values_index <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> current_tree_depth<span class="token punctuation">)</span>    <span class="token keyword">return</span> tree_splits_index<span class="token punctuation">,</span> current_tree_leaf_values_index<span class="token keyword">def</span> <span class="token function">get_tree_info</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tree_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    tree_splits_index<span class="token punctuation">,</span> current_tree_leaf_values_index <span class="token operator">=</span> self<span class="token punctuation">.</span>get_split_index<span class="token punctuation">(</span>tree_id<span class="token punctuation">)</span>    current_tree_depth <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>tree_depth<span class="token punctuation">[</span>tree_id<span class="token punctuation">]</span>    tree_splits_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> depth <span class="token keyword">in</span> range<span class="token punctuation">(</span>current_tree_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>        tree_splits_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>tree_splits<span class="token punctuation">[</span>tree_splits_index <span class="token operator">+</span> depth<span class="token punctuation">]</span><span class="token punctuation">)</span>    node_feature_list <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>find_feature<span class="token punctuation">(</span>index<span class="token punctuation">)</span> <span class="token keyword">for</span> index <span class="token keyword">in</span> tree_splits_list<span class="token punctuation">]</span>    node_feature_borders <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>borders<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token keyword">for</span> index <span class="token keyword">in</span> tree_splits_list<span class="token punctuation">]</span>    end_tree_leaf_values_index <span class="token operator">=</span> current_tree_leaf_values_index <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> current_tree_depth<span class="token punctuation">)</span>    tree_leaf_values <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>leaf_values<span class="token punctuation">[</span>current_tree_leaf_values_index<span class="token punctuation">:</span> end_tree_leaf_values_index<span class="token punctuation">]</span>    <span class="token keyword">return</span> current_tree_depth<span class="token punctuation">,</span> node_feature_list<span class="token punctuation">,</span> node_feature_borders<span class="token punctuation">,</span> tree_leaf_values<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面是绘制一棵决策树的函数，CatBoost导出的python代码文件通过model_file参数传入。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> imp<span class="token keyword">import</span> osos<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"PATH"</span><span class="token punctuation">]</span> <span class="token operator">+=</span> os<span class="token punctuation">.</span>pathsep <span class="token operator">+</span> <span class="token string">'E:/Program Files (x86)/Graphviz2.38/bin'</span><span class="token keyword">from</span> graphviz <span class="token keyword">import</span> Digraph<span class="token keyword">def</span> <span class="token function">draw_tree</span><span class="token punctuation">(</span>model_file<span class="token punctuation">,</span> tree_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    fp<span class="token punctuation">,</span> pathname<span class="token punctuation">,</span> description <span class="token operator">=</span> imp<span class="token punctuation">.</span>find_module<span class="token punctuation">(</span>model_file<span class="token punctuation">)</span>    CatboostModel <span class="token operator">=</span> imp<span class="token punctuation">.</span>load_module<span class="token punctuation">(</span><span class="token string">'CatboostModel'</span><span class="token punctuation">,</span> fp<span class="token punctuation">,</span> pathname<span class="token punctuation">,</span> description<span class="token punctuation">)</span>    catboost_tree <span class="token operator">=</span> CatBoostTree<span class="token punctuation">(</span>CatboostModel<span class="token punctuation">.</span>CatboostModel<span class="token punctuation">)</span>    current_tree_depth<span class="token punctuation">,</span> node_feature_list<span class="token punctuation">,</span> node_feature_borders<span class="token punctuation">,</span> tree_leaf_values <span class="token operator">=</span> catboost_tree<span class="token punctuation">.</span>get_tree_info<span class="token punctuation">(</span>tree_id<span class="token punctuation">)</span>    dot <span class="token operator">=</span> Digraph<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'tree_'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>tree_id<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> depth <span class="token keyword">in</span> range<span class="token punctuation">(</span>current_tree_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>    node_name <span class="token operator">=</span> str<span class="token punctuation">(</span>node_feature_list<span class="token punctuation">[</span>current_tree_depth <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> depth<span class="token punctuation">]</span><span class="token punctuation">)</span>    node_border <span class="token operator">=</span> str<span class="token punctuation">(</span>node_feature_borders<span class="token punctuation">[</span>current_tree_depth <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> depth<span class="token punctuation">]</span><span class="token punctuation">)</span>    label <span class="token operator">=</span> <span class="token string">'column_'</span> <span class="token operator">+</span> node_name <span class="token operator">+</span> <span class="token string">'>'</span> <span class="token operator">+</span> node_border    <span class="token keyword">if</span> depth <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        dot<span class="token punctuation">.</span>node<span class="token punctuation">(</span>str<span class="token punctuation">(</span>depth<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_0'</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> depth<span class="token punctuation">)</span><span class="token punctuation">:</span>            dot<span class="token punctuation">.</span>node<span class="token punctuation">(</span>str<span class="token punctuation">(</span>depth<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>            dot<span class="token punctuation">.</span>edge<span class="token punctuation">(</span>str<span class="token punctuation">(</span>depth<span class="token number">-1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>j<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> str<span class="token punctuation">(</span>depth<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'No'</span> <span class="token keyword">if</span> j<span class="token operator">%</span><span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">'Yes'</span><span class="token punctuation">)</span>depth <span class="token operator">=</span> current_tree_depth<span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> depth<span class="token punctuation">)</span><span class="token punctuation">:</span>    dot<span class="token punctuation">.</span>node<span class="token punctuation">(</span>str<span class="token punctuation">(</span>depth<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">,</span> str<span class="token punctuation">(</span>tree_leaf_values<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    dot<span class="token punctuation">.</span>edge<span class="token punctuation">(</span>str<span class="token punctuation">(</span>depth<span class="token number">-1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>j<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> str<span class="token punctuation">(</span>depth<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'_'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'No'</span> <span class="token keyword">if</span> j<span class="token operator">%</span><span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">'Yes'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># dot.format = 'png'</span>path <span class="token operator">=</span> dot<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token string">'./'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>tree_id<span class="token punctuation">)</span><span class="token punctuation">,</span> cleanup<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span> path<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>例如绘制第11棵树（序数从0开始）。draw_tree(‘catboost_model_file’, 11)。 </p><p><img src="https://img-blog.csdn.net/20180809104012816?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xfeHpt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>为了验证这个对不对，需要对一个测试特征生成每棵树的路径和结果，抽查一两个测试用例以及其中的一两颗树，观察结果是否相同。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_tree</span><span class="token punctuation">(</span>model_file<span class="token punctuation">,</span> float_features<span class="token punctuation">)</span><span class="token punctuation">:</span>    fp<span class="token punctuation">,</span> pathname<span class="token punctuation">,</span> description <span class="token operator">=</span> imp<span class="token punctuation">.</span>find_module<span class="token punctuation">(</span>model_file<span class="token punctuation">)</span>    CatboostModel <span class="token operator">=</span> imp<span class="token punctuation">.</span>load_module<span class="token punctuation">(</span><span class="token string">'CatboostModel'</span><span class="token punctuation">,</span> fp<span class="token punctuation">,</span> pathname<span class="token punctuation">,</span> description<span class="token punctuation">)</span>    model <span class="token operator">=</span> CatboostModel<span class="token punctuation">.</span>CatboostModel    catboost_tree <span class="token operator">=</span> CatBoostTree<span class="token punctuation">(</span>CatboostModel<span class="token punctuation">.</span>CatboostModel<span class="token punctuation">)</span>    result <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> tree_id <span class="token keyword">in</span> range<span class="token punctuation">(</span>model<span class="token punctuation">.</span>tree_count<span class="token punctuation">)</span><span class="token punctuation">:</span>        current_tree_depth<span class="token punctuation">,</span> node_feature_list<span class="token punctuation">,</span> node_feature_borders<span class="token punctuation">,</span> tree_leaf_values <span class="token operator">=</span> catboost_tree<span class="token punctuation">.</span>get_tree_info<span class="token punctuation">(</span>tree_id<span class="token punctuation">)</span>        route <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> depth <span class="token keyword">in</span> range<span class="token punctuation">(</span>current_tree_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>            route<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">1</span> <span class="token keyword">if</span> float_features<span class="token punctuation">[</span>node_feature_list<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">></span> node_feature_borders<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token keyword">else</span> <span class="token number">0</span><span class="token punctuation">)</span>        index <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> depth <span class="token keyword">in</span> range<span class="token punctuation">(</span>current_tree_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>            index <span class="token operator">|</span><span class="token operator">=</span> route<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> depth        tree_value <span class="token operator">=</span> tree_leaf_values<span class="token punctuation">[</span>index<span class="token punctuation">]</span>        <span class="token keyword">print</span> route<span class="token punctuation">,</span> index<span class="token punctuation">,</span> tree_value        result <span class="token operator">+=</span> tree_value    <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如我们生成了第11棵树的图像，根据测试测试特征，手动在图像上查找可以得到一个值A。test_tree函数会打印一系列值，其中第11行对应的结果为值B。值A与值B相同，则测试为问题。<br>其次还需要测试所有树的结果和导出文件中apply_catboost_model函数得到的结果相同。这个可以写个脚本，拿训练数据集跑一边。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> catboost_model_file <span class="token keyword">import</span> apply_catboost_model<span class="token keyword">from</span> CatBoostModelInfo <span class="token keyword">import</span> test_tree<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># print iris.data</span>    <span class="token comment" spellcheck="true"># print iris.target</span>    <span class="token keyword">for</span> feature <span class="token keyword">in</span> iris<span class="token punctuation">.</span>data<span class="token punctuation">:</span>    <span class="token keyword">if</span> apply_catboost_model<span class="token punctuation">(</span>feature<span class="token punctuation">)</span> <span class="token operator">!=</span> test_tree<span class="token punctuation">(</span><span class="token string">'catboost_model_file'</span><span class="token punctuation">,</span> feature<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> <span class="token boolean">False</span> <span class="token keyword">print</span> <span class="token string">'End.'</span><span class="token keyword">if</span> name <span class="token operator">==</span> <span class="token string">'main'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>至此，CatBoost模型的可视化完成了。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LightGBM </tag>
            
            <tag> XGBoost </tag>
            
            <tag> GBDT </tag>
            
            <tag> CatBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DBSCAN算法python实现（附完整数据集和代码）</title>
      <link href="/2020/01/07/DBSCAN%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0%EF%BC%88%E9%99%84%E5%AE%8C%E6%95%B4%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
      <url>/2020/01/07/DBSCAN%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0%EF%BC%88%E9%99%84%E5%AE%8C%E6%95%B4%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E4%BB%A3%E7%A0%81%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>[TOC]</p><h1 id="1-算法思路"><a href="#1-算法思路" class="headerlink" title="1. 算法思路"></a>1. 算法思路</h1><p>DBSCAN算法的核心是“延伸”。先找到一个未访问的点p，若该点是核心点，则创建一个新的簇C，将其邻域中的点放入该簇，并遍历其邻域中的点，若其邻域中有点q为核心点，则将q的邻域内的点也划入簇C，直到C不再扩展。直到最后所有的点都标记为已访问。</p><p>点p通过密度可达来扩大自己的“地盘”，实际上就是簇在“延伸”。 </p><p>图示网站：<a href="https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/" target="_blank" rel="noopener">https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/</a> 可以看一下簇是如何延伸的。</p><h1 id="2-算法实现"><a href="#2-算法实现" class="headerlink" title="2. 算法实现"></a>2. 算法实现</h1><h2 id="2-1-计算两点之间的距离"><a href="#2-1-计算两点之间的距离" class="headerlink" title="2.1 计算两点之间的距离"></a>2.1 计算两点之间的距离</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算两个点之间的欧式距离，参数为两个元组</span><span class="token keyword">def</span> <span class="token function">dist</span><span class="token punctuation">(</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span><span class="token punctuation">:</span>    dis <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-</span>t2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">-</span>t2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># print("两点之间的距离为："+str(dis))</span>    <span class="token keyword">return</span> dis<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-2-读取文件，加载数据集"><a href="#2-2-读取文件，加载数据集" class="headerlink" title="2.2 读取文件，加载数据集"></a><strong>2.2 读取文件，加载数据集</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loadDataSet</span><span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> splitChar<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    dataSet <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span>fileName<span class="token punctuation">)</span> <span class="token keyword">as</span> fr<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> fr<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            curline <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span>splitChar<span class="token punctuation">)</span>            fltline <span class="token operator">=</span> list<span class="token punctuation">(</span>map<span class="token punctuation">(</span>float<span class="token punctuation">,</span> curline<span class="token punctuation">)</span><span class="token punctuation">)</span>            dataSet<span class="token punctuation">.</span>append<span class="token punctuation">(</span>fltline<span class="token punctuation">)</span>    <span class="token keyword">return</span> dataSet<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-3-DBSCAN算法实现"><a href="#2-3-DBSCAN算法实现" class="headerlink" title="2.3 DBSCAN算法实现"></a>2.3 DBSCAN算法实现</h2><p>1、标记点是否被访问：我设置了两个列表，一个存放未访问的点unvisited，一个存放已访问的点visited。每次访问一个点，unvisited列表remove该点，visited列表append该点，以此来实现点的标记改变。</p><p>2、C作为输出结果，初始时是一个长度为所有点的个数的值全为-1的列表。之后修改点对应的索引的值来设置点属于哪个簇</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值</span><span class="token keyword">def</span> <span class="token function">dbscan</span><span class="token punctuation">(</span>Data<span class="token punctuation">,</span> Eps<span class="token punctuation">,</span> MinPts<span class="token punctuation">)</span><span class="token punctuation">:</span>    num <span class="token operator">=</span> len<span class="token punctuation">(</span>Data<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 点的个数</span>    <span class="token comment" spellcheck="true"># print("点的个数："+str(num))</span>    unvisited <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 没有访问到的点的列表</span>    <span class="token comment" spellcheck="true"># print(unvisited)</span>    visited <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 已经访问的点的列表</span>    C <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># C为输出结果，默认是一个长度为num的值全为-1的列表</span>    <span class="token comment" spellcheck="true"># 用k来标记不同的簇，k = -1表示噪声点</span>    k <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    <span class="token comment" spellcheck="true"># 如果还有没访问的点</span>    <span class="token keyword">while</span> len<span class="token punctuation">(</span>unvisited<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 随机选择一个unvisited对象</span>        p <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>unvisited<span class="token punctuation">)</span>        unvisited<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        visited<span class="token punctuation">.</span>append<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># N为p的epsilon邻域中的对象的集合</span>        N <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>dist<span class="token punctuation">(</span>Data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> Data<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> Eps<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># and (i!=p):</span>                N<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token operator">>=</span> MinPts<span class="token punctuation">:</span>            k <span class="token operator">=</span> k<span class="token operator">+</span><span class="token number">1</span>            <span class="token comment" spellcheck="true"># print(k)</span>            C<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> k            <span class="token comment" spellcheck="true"># 对于p的epsilon邻域中的每个对象pi</span>            <span class="token keyword">for</span> pi <span class="token keyword">in</span> N<span class="token punctuation">:</span>                <span class="token keyword">if</span> pi <span class="token keyword">in</span> unvisited<span class="token punctuation">:</span>                    unvisited<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>pi<span class="token punctuation">)</span>                    visited<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pi<span class="token punctuation">)</span>                    <span class="token comment" spellcheck="true"># 找到pi的邻域中的核心对象，将这些对象放入N中</span>                    <span class="token comment" spellcheck="true"># M是位于pi的邻域中的点的列表</span>                    M <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token keyword">if</span> <span class="token punctuation">(</span>dist<span class="token punctuation">(</span>Data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> Data<span class="token punctuation">[</span>pi<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&lt;=</span>Eps<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">#and (j!=pi):</span>                            M<span class="token punctuation">.</span>append<span class="token punctuation">(</span>j<span class="token punctuation">)</span>                    <span class="token keyword">if</span> len<span class="token punctuation">(</span>M<span class="token punctuation">)</span><span class="token operator">>=</span>MinPts<span class="token punctuation">:</span>                        <span class="token keyword">for</span> t <span class="token keyword">in</span> M<span class="token punctuation">:</span>                            <span class="token keyword">if</span> t <span class="token operator">not</span> <span class="token keyword">in</span> N<span class="token punctuation">:</span>                                N<span class="token punctuation">.</span>append<span class="token punctuation">(</span>t<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动</span>                <span class="token keyword">if</span> C<span class="token punctuation">[</span>pi<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                    C<span class="token punctuation">[</span>pi<span class="token punctuation">]</span> <span class="token operator">=</span> k        <span class="token comment" spellcheck="true"># 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            C<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    <span class="token keyword">return</span> C<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-问题记录"><a href="#3-问题记录" class="headerlink" title="3. 问题记录"></a>3. 问题记录</h1><p>代码思路非常简单，让我以为实现起来也很简单。结果拖拖拉拉半个多月才终于将算法改好。</p><p>算法实现过程中遇到的问题其实是小问题，但是导致的结果非常严重。因为不起眼所以才难以察觉。</p><p>这是刚开始我运行算法得到的结果（Eps为10，MinPts为10）：</p><p><img src="https://img-blog.csdnimg.cn/20190614104513322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pveWNlX0Zm,size_16,color_FFFFFF,t_70" alt="img"></p><p>Eps为2，MinPts为10（我改了点的大小）：</p><p><img src="https://img-blog.csdnimg.cn/20190614104535578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pveWNlX0Zm,size_16,color_FFFFFF,t_70" alt="img"></p><p>可以看出图中颜色特别多，实际上就是聚成的簇太多，可实际上目测应该只有七八个簇。这是为什么呢？</p><p>原来是变量k的重复使用问题。</p><p>前面我用k来标识不同的簇，后面（如下图）我又将k变成了循环变量，注意M列表中都是整数，代表点在数据集中的索引，所以实际上是k在整数列表中遍历，覆盖掉了前面用来标识不同簇的k值，导致每次运行出来k取值特别多（如下下图）。</p><p><img src="https://img-blog.csdnimg.cn/20190614104625815.png" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/2019061410463417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pveWNlX0Zm,size_16,color_FFFFFF,t_70" alt="img"></p><h1 id="4-运行结果"><a href="#4-运行结果" class="headerlink" title="4. 运行结果"></a>4. 运行结果</h1><p><img src="https://img-blog.csdnimg.cn/20190614104713963.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pveWNlX0Zm,size_16,color_FFFFFF,t_70" alt="img"></p><h1 id="5-完整代码"><a href="#5-完整代码" class="headerlink" title="5. 完整代码"></a>5. 完整代码</h1><h2 id="5-1-源数据"><a href="#5-1-源数据" class="headerlink" title="5.1 源数据"></a>5.1 源数据</h2><p>附数据集</p><p>链接：<a href="https://pan.baidu.com/s/1dI1Eu7etWvx8IZX-ikqBig" target="_blank" rel="noopener">数据集788个点</a><br>        提取码：rv06</p><h2 id="5-2-源代码"><a href="#5-2-源代码" class="headerlink" title="5.2 源代码"></a>5.2 源代码</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># encoding:utf-8</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> math<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasetslist_1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>list_2 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 数据集一：随机生成散点图,参数为点的个数</span><span class="token comment" spellcheck="true"># def scatter(num):</span><span class="token comment" spellcheck="true">#     for i in range(num):</span><span class="token comment" spellcheck="true">#         x = random.randint(0, 100)</span><span class="token comment" spellcheck="true">#         list_1.append(x)</span><span class="token comment" spellcheck="true">#         y = random.randint(0, 100)</span><span class="token comment" spellcheck="true">#         list_2.append(y)</span><span class="token comment" spellcheck="true">#     print(list_1)</span><span class="token comment" spellcheck="true">#     print(list_2)</span><span class="token comment" spellcheck="true">#     data = list(zip(list_1, list_2))</span><span class="token comment" spellcheck="true">#     print(data)</span><span class="token comment" spellcheck="true">#     #plt.scatter(list_1, list_2)</span><span class="token comment" spellcheck="true">#     #plt.show()</span><span class="token comment" spellcheck="true">#     return data</span><span class="token comment" spellcheck="true">#scatter(50)</span><span class="token keyword">def</span> <span class="token function">loadDataSet</span><span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> splitChar<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    dataSet <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span>fileName<span class="token punctuation">)</span> <span class="token keyword">as</span> fr<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> fr<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            curline <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span>splitChar<span class="token punctuation">)</span>            fltline <span class="token operator">=</span> list<span class="token punctuation">(</span>map<span class="token punctuation">(</span>float<span class="token punctuation">,</span> curline<span class="token punctuation">)</span><span class="token punctuation">)</span>            dataSet<span class="token punctuation">.</span>append<span class="token punctuation">(</span>fltline<span class="token punctuation">)</span>    <span class="token keyword">return</span> dataSet<span class="token comment" spellcheck="true"># 计算两个点之间的欧式距离，参数为两个元组</span><span class="token keyword">def</span> <span class="token function">dist</span><span class="token punctuation">(</span>t1<span class="token punctuation">,</span> t2<span class="token punctuation">)</span><span class="token punctuation">:</span>    dis <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-</span>t2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">-</span>t2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># print("两点之间的距离为："+str(dis))</span>    <span class="token keyword">return</span> dis<span class="token comment" spellcheck="true"># dis = dist((1,1),(3,4))</span><span class="token comment" spellcheck="true"># print(dis)</span><span class="token comment" spellcheck="true"># DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值</span><span class="token keyword">def</span> <span class="token function">dbscan</span><span class="token punctuation">(</span>Data<span class="token punctuation">,</span> Eps<span class="token punctuation">,</span> MinPts<span class="token punctuation">)</span><span class="token punctuation">:</span>    num <span class="token operator">=</span> len<span class="token punctuation">(</span>Data<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 点的个数</span>    <span class="token comment" spellcheck="true"># print("点的个数："+str(num))</span>    unvisited <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 没有访问到的点的列表</span>    <span class="token comment" spellcheck="true"># print(unvisited)</span>    visited <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 已经访问的点的列表</span>    C <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># C为输出结果，默认是一个长度为num的值全为-1的列表</span>    <span class="token comment" spellcheck="true"># 用k来标记不同的簇，k = -1表示噪声点</span>    k <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    <span class="token comment" spellcheck="true"># 如果还有没访问的点</span>    <span class="token keyword">while</span> len<span class="token punctuation">(</span>unvisited<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 随机选择一个unvisited对象</span>        p <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>unvisited<span class="token punctuation">)</span>        unvisited<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        visited<span class="token punctuation">.</span>append<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># N为p的epsilon邻域中的对象的集合</span>        N <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>dist<span class="token punctuation">(</span>Data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> Data<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> Eps<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># and (i!=p):</span>                N<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token operator">>=</span> MinPts<span class="token punctuation">:</span>            k <span class="token operator">=</span> k<span class="token operator">+</span><span class="token number">1</span>            <span class="token comment" spellcheck="true"># print(k)</span>            C<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> k            <span class="token comment" spellcheck="true"># 对于p的epsilon邻域中的每个对象pi</span>            <span class="token keyword">for</span> pi <span class="token keyword">in</span> N<span class="token punctuation">:</span>                <span class="token keyword">if</span> pi <span class="token keyword">in</span> unvisited<span class="token punctuation">:</span>                    unvisited<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>pi<span class="token punctuation">)</span>                    visited<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pi<span class="token punctuation">)</span>                    <span class="token comment" spellcheck="true"># 找到pi的邻域中的核心对象，将这些对象放入N中</span>                    <span class="token comment" spellcheck="true"># M是位于pi的邻域中的点的列表</span>                    M <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token keyword">if</span> <span class="token punctuation">(</span>dist<span class="token punctuation">(</span>Data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> Data<span class="token punctuation">[</span>pi<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&lt;=</span>Eps<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">#and (j!=pi):</span>                            M<span class="token punctuation">.</span>append<span class="token punctuation">(</span>j<span class="token punctuation">)</span>                    <span class="token keyword">if</span> len<span class="token punctuation">(</span>M<span class="token punctuation">)</span><span class="token operator">>=</span>MinPts<span class="token punctuation">:</span>                        <span class="token keyword">for</span> t <span class="token keyword">in</span> M<span class="token punctuation">:</span>                            <span class="token keyword">if</span> t <span class="token operator">not</span> <span class="token keyword">in</span> N<span class="token punctuation">:</span>                                N<span class="token punctuation">.</span>append<span class="token punctuation">(</span>t<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动</span>                <span class="token keyword">if</span> C<span class="token punctuation">[</span>pi<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                    C<span class="token punctuation">[</span>pi<span class="token punctuation">]</span> <span class="token operator">=</span> k        <span class="token comment" spellcheck="true"># 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            C<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    <span class="token keyword">return</span> C<span class="token comment" spellcheck="true"># 数据集二：788个点</span>dataSet <span class="token operator">=</span> loadDataSet<span class="token punctuation">(</span><span class="token string">'788points.txt'</span><span class="token punctuation">,</span> splitChar<span class="token operator">=</span><span class="token string">','</span><span class="token punctuation">)</span>C <span class="token operator">=</span> dbscan<span class="token punctuation">(</span>dataSet<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>C<span class="token punctuation">)</span>x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> data <span class="token keyword">in</span> dataSet<span class="token punctuation">:</span>    x<span class="token punctuation">.</span>append<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dpi<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span> c<span class="token operator">=</span>C<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'o'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># print(x)</span><span class="token comment" spellcheck="true"># print(y)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DBSCAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>短文本聚类【DBSCAN】算法原理+Python代码实现+聚类结果展示</title>
      <link href="/2020/01/07/%E7%9F%AD%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E3%80%90DBSCAN%E3%80%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86+Python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0+%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA/"/>
      <url>/2020/01/07/%E7%9F%AD%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E3%80%90DBSCAN%E3%80%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86+Python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0+%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>[TOC]</p><h1 id="1-算法原理"><a href="#1-算法原理" class="headerlink" title="1. 算法原理"></a>1. 算法原理</h1><h2 id="1-1-常见的聚类算法"><a href="#1-1-常见的聚类算法" class="headerlink" title="1.1 常见的聚类算法"></a>1.1 常见的聚类算法</h2><p>聚类算法属于常见的无监督分类算法，在很多场景下都有应用，如用户聚类，文本聚类等。常见的聚类算法可以分成两类：</p><ul><li>以 k-means 为代表的基于分区的算法</li><li>以层次聚类为代表的基于层次划分的算法</li></ul><p>对于第一类方法，有以下几个缺点：</p><blockquote><p>1）需要事先确定聚类的个数，当数据集比较大时，很难事先给出一个合适的值；</p><p>2）只适用于具有凸形状的簇，不适用于具有任意形状的簇；</p><p>3）对内存的占用资源比较大，难以推广至大规模数据集；        </p></blockquote><p>对于第二类方法，有以下缺点：</p><blockquote><p>1）需要确定停止分裂的条件</p><p>2）计算速度慢</p></blockquote><h2 id="1-2-DBSCAN聚类"><a href="#1-2-DBSCAN聚类" class="headerlink" title="1.2 DBSCAN聚类"></a>1.2 DBSCAN聚类</h2><blockquote><p><a href="http://www.philippe-fournier-viger.com/spmf/DBScan.pdf" target="_blank" rel="noopener">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise （Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu）</a></p></blockquote><p>DBSCAN是一类基于密度的算法，能有效解决上述两类算法的问题。</p><blockquote><p>DBSCAN的基本假设是一个集群的密度要显著高于噪声点的密度。因此，其基本思想是对于集群中的每一个点，在给定的半径范围内，相邻点的数量必须超过预先设定的某一个阈值。</p></blockquote><p>因此，DBSCAN算法中包含两个重要的参数：</p><blockquote><p><strong>eps：</strong>聚类类别中样本的相似度衡量，与类别内样本相似度成反比。可以理解为同一个类别当中，对两个样本之间距离的最大值限定。<br>    <strong>min_samples：</strong>每个聚类类别中的最小样本数，会对未分类样本数量造成影响，与未分类样本数量成正比。当相似样本数量少于该参数时，不会聚到一起。</p></blockquote><p>在实际应用过程中，根据样本的大小，以及样本的大致分布，了解聚类结果会随着这两个参数如何变化之后，可以根据自己的经验对两个参数进行调整。只有两个模型参数需要调整，因此调参过程也不会太麻烦。</p><h1 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2. 代码实现"></a>2. 代码实现</h1><h2 id="2-1-import需要的包"><a href="#2-1-import需要的包" class="headerlink" title="2.1 import需要的包"></a>2.1 import需要的包</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === import packages === #</span><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>posseg <span class="token keyword">as</span> pseg<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfTransformer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> DBSCAN<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-2-载入数据"><a href="#2-2-载入数据" class="headerlink" title="2.2 载入数据"></a>2.2 载入数据</h2><p>根据数据文件的不同存在不同的数据载入方法，我当时使用的是两种类型的数据，分别是直接包含目标短文本的txt，以json格式存储的txt。如果有用到这两种类型的文件可以参考这部分的数据载入代码，其他的请根据文件类型和数据样式自行载入。首先是载入以json格式存储的txt文件，可以用正则表达式，也可以根据数据存储的方式提取出对应的字段。先展示一下数据的存储格式：</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span><span class="token property">"code"</span><span class="token operator">:</span> <span class="token string">"200"</span><span class="token punctuation">,</span><span class="token property">"data"</span><span class="token operator">:</span> <span class="token punctuation">{</span><span class="token property">"result"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token property">"updateDate"</span><span class="token operator">:</span> <span class="token number">1551923786433</span><span class="token punctuation">,</span><span class="token property">"ensureIntentName"</span><span class="token operator">:</span> <span class="token string">"新意图"</span><span class="token punctuation">,</span><span class="token property">"corpus"</span><span class="token operator">:</span> <span class="token string">"怎么查询之前的小微提醒"</span><span class="token punctuation">,</span><span class="token property">"recommendResult"</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token property">"remark"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"source"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token property">"result"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token property">"eventName"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"id"</span><span class="token operator">:</span> <span class="token string">"b07328fc-8383-44b7-b466-15b063b8544a"</span><span class="token punctuation">,</span><span class="token property">"state"</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token property">"tag"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"isHandle"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token property">"createDate"</span><span class="token operator">:</span> <span class="token number">1551669751334</span><span class="token punctuation">,</span><span class="token property">"eventId"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"corpusTagId"</span><span class="token operator">:</span> <span class="token string">"3335d2d8-a16e-46a2-9ed7-76739108d684"</span><span class="token punctuation">,</span><span class="token property">"intentName"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"ensureIntent"</span><span class="token operator">:</span> <span class="token string">"newIntent"</span><span class="token punctuation">,</span><span class="token property">"recommendIntent"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"setmsgnotifications"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"uploadTime"</span><span class="token operator">:</span> <span class="token number">1551669751333</span><span class="token punctuation">,</span><span class="token property">"w3account"</span><span class="token operator">:</span> <span class="token string">"x00286769"</span><span class="token punctuation">,</span><span class="token property">"createBy"</span><span class="token operator">:</span> <span class="token string">"x00286769"</span><span class="token punctuation">,</span><span class="token property">"intentCode"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"isBotSupport"</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token property">"userRole"</span><span class="token operator">:</span> <span class="token string">"0"</span><span class="token punctuation">,</span><span class="token property">"welinkVersion"</span><span class="token operator">:</span> <span class="token string">"3.9.13"</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"pagination"</span><span class="token operator">:</span> <span class="token punctuation">{</span><span class="token property">"pageCount"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token property">"pageSizes"</span><span class="token operator">:</span> <span class="token number">50</span><span class="token punctuation">,</span><span class="token property">"pageNumber"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token property">"offset"</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token property">"pageTotal"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token property">"pageNumbers"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token property">"pageSize"</span><span class="token operator">:</span> <span class="token number">50</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token property">"error"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"stack"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><span class="token property">"message"</span><span class="token operator">:</span> <span class="token string">"ok"</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我的目标是对上述数据当中，字典中key “data” 对应的字典中的 “result” 中每一个item 的 “corpus” 进行提取，于是就有了下列代码。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === Data loading === #</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span><span class="token string">"新意图语料.txt"</span><span class="token punctuation">,</span> <span class="token string">'r+'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'UTF-8'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>eval<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tmp <span class="token operator">=</span> data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'result'</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        corpus<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tmp<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'corpus'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后是载入包含目标短文本的txt，也就是说该txt直接存储了上面的 “corpus” 对应的内容，但是每一行的内容都加上了双引号和逗号，就通过strip把这些不需要的部分去掉了，最后得到所有 “corpus” 组成的list。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span><span class="token string">"未识别语料.txt"</span><span class="token punctuation">,</span> <span class="token string">'r+'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token string">','</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>lstrip<span class="token punctuation">(</span><span class="token string">'"'</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token string">'"'</span><span class="token punctuation">)</span>    corpus<span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-3-对文本进行分词，并记录词性"><a href="#2-3-对文本进行分词，并记录词性" class="headerlink" title="2.3 对文本进行分词，并记录词性"></a>2.3 对文本进行分词，并记录词性</h2><p>调用结巴词库对语料进行分词，并记录分词结果中每个词的词性。我的数据集在处理之后得到了5316条短文本，分词得到20640个不重复的词汇及其对应的词性，并建立了两者之间的字典联系。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === Record the text cut and POS === #</span>part_of_speech <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>word_after_cut <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>cut_corpus_iter <span class="token operator">=</span> corpus<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>cut_corpus <span class="token operator">=</span> corpus<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    cut_corpus_iter<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> pseg<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 5316</span>    cut_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">""</span>    <span class="token keyword">for</span> every <span class="token keyword">in</span> cut_corpus_iter<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>        cut_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>cut_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>every<span class="token punctuation">.</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        part_of_speech<span class="token punctuation">.</span>append<span class="token punctuation">(</span>every<span class="token punctuation">.</span>flag<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 20640</span>        word_after_cut<span class="token punctuation">.</span>append<span class="token punctuation">(</span>every<span class="token punctuation">.</span>word<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 20640</span>word_pos_dict <span class="token operator">=</span> <span class="token punctuation">{</span>word_after_cut<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span> part_of_speech<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word_after_cut<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-4-文本向量化–TF-IDF权重"><a href="#2-4-文本向量化–TF-IDF权重" class="headerlink" title="2.4 文本向量化–TF-IDF权重"></a>2.4 文本向量化–TF-IDF权重</h2><p>使用TF-IDF对文本进行向量化，得到文本的TF-IDF权重。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === Get the TF-IDF weights === #</span>Count_vectorizer <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>transformer <span class="token operator">=</span> TfidfTransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 用于统计每个词语的tf-idf权值</span>tf_idf <span class="token operator">=</span> transformer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>Count_vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>cut_corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># （5316，2039）第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵</span>word <span class="token operator">=</span> Count_vectorizer<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 2039，获取词袋模型中的所有词语</span>weight <span class="token operator">=</span> tf_idf<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># （5316，2039）将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-5-基于词性的新权重"><a href="#2-5-基于词性的新权重" class="headerlink" title="2.5 基于词性的新权重"></a>2.5 基于词性的新权重</h2><p>前面得到了分词的结果，并对词性进行了记录，接下来可以针对不同词汇的词性码，给与其TF-IDF权重以不同的乘数，这样可以突出某些类型的词汇的重要性，在一定程度上有助于聚类的效果。</p><p>具体的乘数构造规则可以根据需求自行调整。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === Get new weight with POS considered === #</span>word_weight <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> word<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">not</span> <span class="token keyword">in</span> word_pos_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">continue</span>    <span class="token keyword">if</span> word_pos_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'n'</span><span class="token punctuation">:</span>        word_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.2</span>    <span class="token keyword">elif</span> word_pos_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"vn"</span><span class="token punctuation">:</span>        word_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.1</span>    <span class="token keyword">elif</span> word_pos_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"m"</span><span class="token punctuation">:</span>        word_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 权重调整可以根据实际情况进行更改</span>        <span class="token keyword">continue</span>word_weight <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>word_weight<span class="token punctuation">)</span>new_weight <span class="token operator">=</span> weight<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>weight<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        new_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*</span> word_weight<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-6-DBSCAN模型"><a href="#2-6-DBSCAN模型" class="headerlink" title="2.6 DBSCAN模型"></a>2.6 DBSCAN模型</h2><p>得到了文本的向量化表示之后就可以将其投喂到模型当中了，eps和min_samples都是可以调整的参数。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === Fit the DBSCAN model and get the classify labels === #</span>DBS_clf <span class="token operator">=</span> DBSCAN<span class="token punctuation">(</span>eps<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_samples<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>DBS_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>new_weight<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>DBS_clf<span class="token punctuation">.</span>labels_<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-聚类结果"><a href="#3-聚类结果" class="headerlink" title="3. 聚类结果"></a>3. 聚类结果</h1><p>DBSCAN模型实现聚类之后，聚类的结果会存储在 <code>labels_</code> 中，将 <code>labels_</code> 与原来的文本一一对应，可以得到最终的聚类结果：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === Define the function of classify the original corpus according to the labels === #</span><span class="token keyword">def</span> <span class="token function">labels_to_original</span><span class="token punctuation">(</span>labels<span class="token punctuation">,</span> original_corpus<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> len<span class="token punctuation">(</span>labels<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>original_corpus<span class="token punctuation">)</span>    max_label <span class="token operator">=</span> max<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>    number_label <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_label <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    number_label<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>number_label<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        index <span class="token operator">=</span> number_label<span class="token punctuation">.</span>index<span class="token punctuation">(</span>labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        result<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>original_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">labels_original <span class="token operator">=</span> labels_to_original<span class="token punctuation">(</span>DBS_clf<span class="token punctuation">.</span>labels_<span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>labels_original<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 聚类结果展示（部分）    </span><span class="token punctuation">[</span><span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡。'</span><span class="token punctuation">,</span> <span class="token string">'社保卡办理'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡挂失'</span><span class="token punctuation">,</span> <span class="token string">'社保卡。'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'领取社保卡。'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'五险一金'</span><span class="token punctuation">,</span> <span class="token string">'五险一金。'</span><span class="token punctuation">,</span> <span class="token string">'五险一金。'</span><span class="token punctuation">,</span> <span class="token string">'五险一金介绍'</span><span class="token punctuation">,</span> <span class="token string">'看看二月份五险一金情况'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'打开汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'打开汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'我要办车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'等等邮件附件权限。'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'您好，请问怎样申请图片查看权限和邮件附件查看权限？'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="4-附件：完整代码"><a href="#4-附件：完整代码" class="headerlink" title="4 附件：完整代码"></a>4 附件：完整代码</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># === import packages === #</span><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>posseg <span class="token keyword">as</span> pseg<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfTransformer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> DBSCAN<span class="token comment" spellcheck="true"># === Data loading === #</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span><span class="token string">"新意图语料.txt"</span><span class="token punctuation">,</span> <span class="token string">'r+'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'UTF-8'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>eval<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    tmp <span class="token operator">=</span> data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'result'</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        corpus<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tmp<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'corpus'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span><span class="token string">"未识别语料.txt"</span><span class="token punctuation">,</span> <span class="token string">'r+'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token string">','</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>lstrip<span class="token punctuation">(</span><span class="token string">'"'</span><span class="token punctuation">)</span>    line <span class="token operator">=</span> line<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token string">'"'</span><span class="token punctuation">)</span>    corpus<span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># === Record the text cut and POS === #</span>part_of_speech <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>word_after_cut <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>cut_corpus_iter <span class="token operator">=</span> corpus<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>cut_corpus <span class="token operator">=</span> corpus<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    cut_corpus_iter<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> pseg<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 5316</span>    cut_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">""</span>    <span class="token keyword">for</span> every <span class="token keyword">in</span> cut_corpus_iter<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>        cut_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>cut_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>every<span class="token punctuation">.</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        part_of_speech<span class="token punctuation">.</span>append<span class="token punctuation">(</span>every<span class="token punctuation">.</span>flag<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 20640</span>        word_after_cut<span class="token punctuation">.</span>append<span class="token punctuation">(</span>every<span class="token punctuation">.</span>word<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 20640</span>word_pos_dict <span class="token operator">=</span> <span class="token punctuation">{</span>word_after_cut<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span> part_of_speech<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word_after_cut<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># === Get new weight with POS considered === #</span>word_weight <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> word<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">not</span> <span class="token keyword">in</span> word_pos_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">continue</span>    <span class="token keyword">if</span> word_pos_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'n'</span><span class="token punctuation">:</span>        word_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.2</span>    <span class="token keyword">elif</span> word_pos_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"vn"</span><span class="token punctuation">:</span>        word_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.1</span>    <span class="token keyword">elif</span> word_pos_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"m"</span><span class="token punctuation">:</span>        word_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 权重调整可以根据实际情况进行更改</span>        <span class="token keyword">continue</span>word_weight <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>word_weight<span class="token punctuation">)</span>new_weight <span class="token operator">=</span> weight<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>weight<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        new_weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> weight<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*</span> word_weight<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># === Fit the DBSCAN model and get the classify labels === #</span>DBS_clf <span class="token operator">=</span> DBSCAN<span class="token punctuation">(</span>eps<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_samples<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>DBS_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>new_weight<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>DBS_clf<span class="token punctuation">.</span>labels_<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># === Define the function of classify the original corpus according to the labels === #</span><span class="token keyword">def</span> <span class="token function">labels_to_original</span><span class="token punctuation">(</span>labels<span class="token punctuation">,</span> original_corpus<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> len<span class="token punctuation">(</span>labels<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>original_corpus<span class="token punctuation">)</span>    max_label <span class="token operator">=</span> max<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>    number_label <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_label <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    number_label<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>number_label<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        index <span class="token operator">=</span> number_label<span class="token punctuation">.</span>index<span class="token punctuation">(</span>labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        result<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>original_corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> resultlabels_original <span class="token operator">=</span> labels_to_original<span class="token punctuation">(</span>DBS_clf<span class="token punctuation">.</span>labels_<span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>labels_original<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 聚类结果展示（部分）    </span><span class="token punctuation">[</span><span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡。'</span><span class="token punctuation">,</span> <span class="token string">'社保卡办理'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'社保卡挂失'</span><span class="token punctuation">,</span> <span class="token string">'社保卡。'</span><span class="token punctuation">,</span> <span class="token string">'社保卡'</span><span class="token punctuation">,</span> <span class="token string">'领取社保卡。'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'五险一金'</span><span class="token punctuation">,</span> <span class="token string">'五险一金。'</span><span class="token punctuation">,</span> <span class="token string">'五险一金。'</span><span class="token punctuation">,</span> <span class="token string">'五险一金介绍'</span><span class="token punctuation">,</span> <span class="token string">'看看二月份五险一金情况'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'打开汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'打开汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">,</span> <span class="token string">'我要汇钱。'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'我要办车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证。'</span><span class="token punctuation">,</span> <span class="token string">'车辆通行证'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'等等邮件附件权限。'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'邮件附件权限'</span><span class="token punctuation">,</span> <span class="token string">'您好，请问怎样申请图片查看权限和邮件附件查看权限？'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DBSCAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法(08):梯度提升树算法LightGBM</title>
      <link href="/2020/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8808%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95LightGBM/"/>
      <url>/2020/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8808%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95LightGBM/</url>
      
        <content type="html"><![CDATA[<h1 id="1-LightGBM简介"><a href="#1-LightGBM简介" class="headerlink" title="1. LightGBM简介"></a>1. LightGBM简介</h1><p>GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。而LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。</p><h2 id="1-1-LightGBM提出的动机"><a href="#1-1-LightGBM提出的动机" class="headerlink" title="1.1 LightGBM提出的动机"></a>1.1 LightGBM提出的动机</h2><p>常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。</p><p>LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。</p><h2 id="1-2-XGBoost的缺点及LightGBM的优化"><a href="#1-2-XGBoost的缺点及LightGBM的优化" class="headerlink" title="1.2 XGBoost的缺点及LightGBM的优化"></a>1.2 XGBoost的缺点及LightGBM的优化</h2><h3 id="（1）XGBoost的缺点"><a href="#（1）XGBoost的缺点" class="headerlink" title="（1）XGBoost的缺点"></a>（1）XGBoost的缺点</h3><p>在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：</p><blockquote><p>首先，对所有特征都按照特征的数值进行<strong>预排序</strong>。</p><p>其次，在遍历分割点的时候用的代价找到一个特征上的<strong>最好分割点</strong>。</p><p>最后，在找到一个特征的最好分割点后，将数据分裂成<strong>左右子节点</strong>。</p></blockquote><p>这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显：</p><blockquote><p>首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。</p><p>其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</p><p>最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</p></blockquote><h3 id="（2）LightGBM的优化"><a href="#（2）LightGBM的优化" class="headerlink" title="（2）LightGBM的优化"></a>（2）LightGBM的优化</h3><p>为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化：</p><ul><li>基于Histogram的决策树算法。</li><li>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。</li><li>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</li><li>带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。</li><li>直接支持类别特征(Categorical Feature)</li><li>支持高效并行</li><li>Cache命中率优化</li></ul><p>下面我们就详细介绍以上提到的lightGBM优化算法。</p><h1 id="2-LightGBM的基本原理"><a href="#2-LightGBM的基本原理" class="headerlink" title="2. LightGBM的基本原理"></a>2. LightGBM的基本原理</h1><h2 id="2-1-基于Histogram的决策树算法"><a href="#2-1-基于Histogram的决策树算法" class="headerlink" title="2.1 基于Histogram的决策树算法"></a>2.1 基于Histogram的决策树算法</h2><h3 id="（1）直方图算法"><a href="#（1）直方图算法" class="headerlink" title="（1）直方图算法"></a>（1）直方图算法</h3><p>Histogram algorithm应该翻译为直方图算法，直方图算法的基本思想是：</p><blockquote><p>先把连续的浮点特征值离散化成 k个整数，同时构造一个宽度为 k 的 直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p></blockquote><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnwCicKXgHhvyufUDOaWKtOMvGHTnOp7qty1WdDzoCibZ6613YRCngqLjg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="直方图算法"></p><p>图：直方图算法</p><p>直方图算法简单理解为：</p><blockquote><p>首先确定对于每一个特征需要多少个箱子（bin）并为每一个箱子分配一个整数；</p><p>然后将浮点数的范围均分成若干区间，区间个数与箱子个数相等，将属于该箱子的样本数据更新为箱子的值；</p><p>最后用直方图（#bins）表示。看起来很高大上，其实就是直方图统计，将大规模的数据放在了直方图中。</p></blockquote><p>我们知道特征离散化具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等。对于直方图算法来说最直接的有以下两个优点：</p><ul><li><strong>内存占用更小：</strong> 直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8 。也就是说XGBoost需要用32位的浮点数去存储特征值，并用32位的整形去存储索引，而 LightGBM只需要用8位去存储直方图，内存相当于减少为 ；</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnXTiclyL79CUh8dTMCllo4QEbHTHSqDRxaia9ke6UZicdticGPMpBfOJoIQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>图：内存占用优化为预排序算法的1/8</p><ul><li><strong>计算代价更小：</strong> 预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算 k次（ 可以认为是常数），直接将时间复杂度从O(#data * #feature )降低到 O(k * #feature )，而我们知道#data &gt;&gt;k。</li></ul><p>当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。</p><h3 id="（2）直方图做差加速"><a href="#（2）直方图做差加速" class="headerlink" title="（2）直方图做差加速"></a>（2）直方图做差加速</h3><p>LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnJftGtGmKibGV4OycNSiaE6YcAjONvEh9aglgOzCNAl75kia3QzF6Nc4Og/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：直方图做差"></p><p><strong>注意：</strong> XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。</p><h2 id="2-2-带深度限制的-Leaf-wise-算法"><a href="#2-2-带深度限制的-Leaf-wise-算法" class="headerlink" title="2.2 带深度限制的 Leaf-wise 算法"></a>2.2 带深度限制的 Leaf-wise 算法</h2><p>在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。</p><p>XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnZKttaTX2iajSgicfL5jMIsgFEiad6yk28rJClIwtH9abX9gmMQj0l5fJg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：按层生长的决策树"></p><p>LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnwT3h60X31B6gLFlw9Dhh5z81OicEBrmbFkqrcQuL0soSogOazr882bg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：按叶子生长的决策树"></p><h2 id="2-3-单边梯度采样算法"><a href="#2-3-单边梯度采样算法" class="headerlink" title="2.3 单边梯度采样算法"></a>2.3 单边梯度采样算法</h2><p>Gradient-based One-Side Sampling 应该被翻译为单边梯度采样（GOSS）。GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。</p><p>AdaBoost中，样本权重是数据重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的是，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用。即梯度小的样本，训练误差也比较小，说明数据已经被模型学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练模型的精确度，为了避免此问题，提出了GOSS算法。</p><p>GOSS是一个样本的采样算法，目的是丢弃一些对计算信息增益没有帮助的样本留下有帮助的。根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。因此，GOSS在进行数据采样的时候只保留了梯度较大的数据，但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。所以，GOSS首先将要进行分裂的特征的所有取值按照绝对值大小降序排序（XGBoost一样也进行了排序，但是LightGBM不用保存排序后的结果），选取绝对值最大的 个数据。然后在剩下的较小梯度数据中随机选择 个数据。接着将这 个数据乘以一个常数 ，这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这 个数据来计算信息增益。下图是GOSS的具体算法。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnxhBz107Qv6GDvoWZVdFk3VJCy9Iq5nzwsZPkCANyvC9cNdySWpDhWQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：单边梯度采样算法"></p><h2 id="2-4-互斥特征捆绑算法"><a href="#2-4-互斥特征捆绑算法" class="headerlink" title="2.4 互斥特征捆绑算法"></a>2.4 互斥特征捆绑算法</h2><p>高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从 变为 ，这里 指特征融合绑定后特征包的个数，且 远小于 。</p><p>针对这种想法，我们会遇到两个问题：</p><ul><li>怎么判定哪些特征应该绑在一起（build bundled）？</li><li>怎么把特征绑为一个（merge feature）？</li></ul><h3 id="（1）解决哪些特征应该绑在一起"><a href="#（1）解决哪些特征应该绑在一起" class="headerlink" title="（1）解决哪些特征应该绑在一起"></a>（1）解决哪些特征应该绑在一起</h3><p>将相互独立的特征进行绑定是一个 NP-Hard 问题，LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。此外，我们注意到通常有很多特征，尽管不是％相互排斥，但也很少同时取非零值。如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。经过简单的计算，随机污染小部分特征值将影响精度最多 ， 是每个绑定中的最大冲突比率，当其相对较小时，能够完成精度和效率之间的平衡。具体步骤可以总结如下：</p><ol><li>构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关；</li><li>根据节点的度进行降序排序，度越大，与其它特征的冲突越大；</li><li>遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。</li></ol><p>算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 来平衡算法的精度和效率。EFB 算法的伪代码如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnibV09sDI2NasyH6cbofAZ26FmibiaXxnDo78qHfeoFc9X1waibNYLhAo7A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：贪心绑定算法"></p><p>算法3的时间复杂度是 ，训练之前只处理一次，其时间复杂度在特征不是特别多的情况下是可以接受的，但难以应对百万维度的特征。为了继续提高效率，LightGBM提出了一种更加高效的无图的排序策略：将特征按照非零值个数排序，这和使用图节点的度排序相似，因为更多的非零值通常会导致冲突，新算法在算法3基础上改变了排序策略。</p><h3 id="（2）解决怎么把特征绑为一捆"><a href="#（2）解决怎么把特征绑为一捆" class="headerlink" title="（2）解决怎么把特征绑为一捆"></a>（2）解决怎么把特征绑为一捆</h3><p>特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间 ，B特征的原始取值为区间，我们可以在B特征的取值上加一个偏置常量，将其取值范围变为，绑定后的特征取值范围为 ，这样就可以放心的融合特征A和B了。具体的特征合并算法如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnutkSNWGiaNTSXibPu6tY7Os5LeTnycfzXyd1BXX9oib6sVia4nHZxj4uSw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：特征合并算法"></p><h1 id="3-LightGBM的工程优化"><a href="#3-LightGBM的工程优化" class="headerlink" title="3. LightGBM的工程优化"></a>3. LightGBM的工程优化</h1><p>我们将论文《Lightgbm: A highly efficient gradient boosting decision tree》中没有提到的优化方案，而在其相关论文《A communication-efficient parallel algorithm for decision tree》中提到的优化方案，放到本节作为LightGBM的工程优化来向大家介绍。</p><h2 id="3-1-直接支持类别特征"><a href="#3-1-直接支持类别特征" class="headerlink" title="3.1 直接支持类别特征"></a>3.1 直接支持类别特征</h2><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的特征，降低了空间和时间的效率。但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题：</p><ul><li>会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为1 ，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别。</li><li>会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，如下图左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差。但如果使用下图右边的切分方法，数据会被切分到两个比较大的空间，进一步的学习也会更好。下图右边叶子节点的含义是或者放到左孩子，其余放到右孩子。</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnIjV8dicRWyoeRWTiayd7Y7ZDUibN47IuzOOaFV8hfjpmDjpSeLSxCyLLw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>图：左图为基于 one-hot 编码进行分裂，右图为 LightGBM 基于 many-vs-many 进行分裂</p><p>而类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 个类别，则有 种可能，时间复杂度为 ，LightGBM 基于 Fisher的《On Grouping For Maximum Homogeneity》论文实现了 的时间复杂度。</p><p>算法流程如下图所示，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。从下图可以看到， 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnyEyRiaYAe6iaMpstC2KzEMyP5G3akoJNTAZ7t69fBokqOeZwznhPGq7Q/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>图：LightGBM求解类别特征的最优切分算法</p><p>在Expo数据集上的实验结果表明，相比展开的方法，使用LightGBM支持的类别特征可以使训练速度加速倍，并且精度一致。更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。</p><h2 id="3-2-支持高效并行"><a href="#3-2-支持高效并行" class="headerlink" title="3.2 支持高效并行"></a>3.2 支持高效并行</h2><h3 id="（1）特征并行"><a href="#（1）特征并行" class="headerlink" title="（1）特征并行"></a>（1）特征并行</h3><p>特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法。这种特征并行方法有个很大的缺点：就是对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度。</p><p>LightGBM 则不进行数据垂直划分，而是在每台机器上保存全部训练数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。具体过程如下图所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnOoy04GLlKxTI9EqajtYApyiaUwnSaSrQIGBSQtGH2sUEjCMibz7msiapw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：特征并行"></p><h3 id="（2）数据并行"><a href="#（2）数据并行" class="headerlink" title="（2）数据并行"></a>（2）数据并行</h3><p>传统的数据并行策略主要为水平划分数据，让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 ；如果使用集成的通信，则通讯开销为 。</p><p>LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。具体过程如下图所示。</p><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="img"></p><p>图：数据并行</p><h3 id="（3）投票并行"><a href="#（3）投票并行" class="headerlink" title="（3）投票并行"></a>（3）投票并行</h3><p>基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行的方式只合并部分特征的直方图从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。</p><p>大致步骤为两步：</p><ol><li>本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征；</li><li>合并时只合并每个机器选出来的特征。</li></ol><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="img"></p><p>图：投票并行</p><h2 id="3-3-Cache命中率优化"><a href="#3-3-Cache命中率优化" class="headerlink" title="3.3 Cache命中率优化"></a>3.3 Cache命中率优化</h2><p>XGBoost对cache优化不友好，如下图所示。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问算法进行改进。</p><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="img"></p><p>图：随机访问会造成cache miss</p><p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p><ul><li>首先，所有的特征都采用相同的方式获得梯度（区别于XGBoost的不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率；</li><li>其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss的问题。</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnrdNfRmcoo5BTWT9mrzpepqrD3znib63F2aBy8icHcMsx5DCLQUdia7jdg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>图：LightGBM增加缓存命中率</p><h1 id="4-LightGBM的优缺点"><a href="#4-LightGBM的优缺点" class="headerlink" title="4. LightGBM的优缺点"></a>4. LightGBM的优缺点</h1><h2 id="4-1-优点"><a href="#4-1-优点" class="headerlink" title="4.1 优点"></a>4.1 优点</h2><p>这部分主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。</p><h3 id="（1）速度更快"><a href="#（1）速度更快" class="headerlink" title="（1）速度更快"></a>（1）速度更快</h3><ul><li>LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；</li><li>LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；</li><li>LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；</li><li>LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；</li><li>LightGBM 对缓存也进行了优化，增加了缓存命中率；</li></ul><h3 id="（2）内存更小"><a href="#（2）内存更小" class="headerlink" title="（2）内存更小"></a>（2）内存更小</h3><ul><li>XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 降低为 ，极大的减少了内存消耗；</li><li>LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；</li><li>LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。</li></ul><h2 id="4-2-缺点"><a href="#4-2-缺点" class="headerlink" title="4.2 缺点"></a>4.2 缺点</h2><ul><li>可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合；</li><li>Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感；</li><li>在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去；</li></ul><h1 id="5-LightGBM实例"><a href="#5-LightGBM实例" class="headerlink" title="5. LightGBM实例"></a>5. LightGBM实例</h1><p>本篇文章所有数据集和代码均在我的GitHub中，地址：<a href="https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/LightGBM" target="_blank" rel="noopener">https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/LightGBM</a></p><h2 id="5-1-安装LightGBM依赖包"><a href="#5-1-安装LightGBM依赖包" class="headerlink" title="5.1 安装LightGBM依赖包"></a>5.1 安装LightGBM依赖包</h2><pre class="line-numbers language-shell"><code class="language-shell">pip install lightgbm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-2-LightGBM分类和回归"><a href="#5-2-LightGBM分类和回归" class="headerlink" title="5.2 LightGBM分类和回归"></a>5.2 LightGBM分类和回归</h2><p>LightGBM有两大类接口：LightGBM原生接口 和 scikit-learn接口 ，并且LightGBM能够实现分类和回归两种任务。</p><h3 id="（1）基于LightGBM原生接口的分类"><a href="#（1）基于LightGBM原生接口的分类" class="headerlink" title="（1）基于LightGBM原生接口的分类"></a>（1）基于LightGBM原生接口的分类</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> lightgbm <span class="token keyword">as</span> lgb<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score<span class="token punctuation">,</span> accuracy_score<span class="token comment" spellcheck="true"># 加载数据</span>iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 划分训练集和测试集</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转换为Dataset数据格式</span>train_data <span class="token operator">=</span> lgb<span class="token punctuation">.</span>Dataset<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> label<span class="token operator">=</span>y_train<span class="token punctuation">)</span>validation_data <span class="token operator">=</span> lgb<span class="token punctuation">.</span>Dataset<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> label<span class="token operator">=</span>y_test<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 参数</span>params <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'learning_rate'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>    <span class="token string">'lambda_l1'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>    <span class="token string">'lambda_l2'</span><span class="token punctuation">:</span> <span class="token number">0.2</span><span class="token punctuation">,</span>    <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span>    <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'multiclass'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 目标函数</span>    <span class="token string">'num_class'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># 模型训练</span>gbm <span class="token operator">=</span> lgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>params<span class="token punctuation">,</span> train_data<span class="token punctuation">,</span> valid_sets<span class="token operator">=</span><span class="token punctuation">[</span>validation_data<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型预测</span>y_pred <span class="token operator">=</span> gbm<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>y_pred <span class="token operator">=</span> <span class="token punctuation">[</span>list<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>index<span class="token punctuation">(</span>max<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> y_pred<span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型评估</span><span class="token keyword">print</span><span class="token punctuation">(</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（2）基于Scikit-learn接口的分类"><a href="#（2）基于Scikit-learn接口的分类" class="headerlink" title="（2）基于Scikit-learn接口的分类"></a>（2）基于Scikit-learn接口的分类</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> lightgbm <span class="token keyword">import</span> LGBMClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblib<span class="token comment" spellcheck="true"># 加载数据</span>iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>data <span class="token operator">=</span> iris<span class="token punctuation">.</span>datatarget <span class="token operator">=</span> iris<span class="token punctuation">.</span>target<span class="token comment" spellcheck="true"># 划分训练数据和测试数据</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型训练</span>gbm <span class="token operator">=</span> LGBMClassifier<span class="token punctuation">(</span>num_leaves<span class="token operator">=</span><span class="token number">31</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> n_estimators<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>gbm<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> eval_set<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> early_stopping_rounds<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型存储</span>joblib<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>gbm<span class="token punctuation">,</span> <span class="token string">'loan_model.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型加载</span>gbm <span class="token operator">=</span> joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'loan_model.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型预测</span>y_pred <span class="token operator">=</span> gbm<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> num_iteration<span class="token operator">=</span>gbm<span class="token punctuation">.</span>best_iteration_<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型评估</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'The accuracy of prediction is:'</span><span class="token punctuation">,</span> accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 特征重要度</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Feature importances:'</span><span class="token punctuation">,</span> list<span class="token punctuation">(</span>gbm<span class="token punctuation">.</span>feature_importances_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 网格搜索，参数优化</span>estimator <span class="token operator">=</span> LGBMClassifier<span class="token punctuation">(</span>num_leaves<span class="token operator">=</span><span class="token number">31</span><span class="token punctuation">)</span>param_grid <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'learning_rate'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">}</span>gbm <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>estimator<span class="token punctuation">,</span> param_grid<span class="token punctuation">)</span>gbm<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Best parameters found by grid search are:'</span><span class="token punctuation">,</span> gbm<span class="token punctuation">.</span>best_params_<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（3）基于LightGBM原生接口的回归"><a href="#（3）基于LightGBM原生接口的回归" class="headerlink" title="（3）基于LightGBM原生接口的回归"></a>（3）基于LightGBM原生接口的回归</h3><p>对于LightGBM解决回归问题，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a> 来进行实例讲解。</p><p>该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">import</span> lightgbm <span class="token keyword">as</span> lgb<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_error<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> Imputer<span class="token comment" spellcheck="true"># 1.读文件</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'./dataset/train.csv'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.切分数据输入：特征 输出：预测目标变量</span>y <span class="token operator">=</span> data<span class="token punctuation">.</span>SalePriceX <span class="token operator">=</span> data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select_dtypes<span class="token punctuation">(</span>exclude<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 3.切分训练集、测试集,切分比例7.5 : 2.5</span>train_X<span class="token punctuation">,</span> test_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">.</span>values<span class="token punctuation">,</span> y<span class="token punctuation">.</span>values<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 4.空值处理，默认方法：使用特征列的平均值进行填充</span>my_imputer <span class="token operator">=</span> Imputer<span class="token punctuation">(</span><span class="token punctuation">)</span>train_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>train_X<span class="token punctuation">)</span>test_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 5.转换为Dataset数据格式</span>lgb_train <span class="token operator">=</span> lgb<span class="token punctuation">.</span>Dataset<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span>lgb_eval <span class="token operator">=</span> lgb<span class="token punctuation">.</span>Dataset<span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_y<span class="token punctuation">,</span> reference<span class="token operator">=</span>lgb_train<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 6.参数</span>params <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'task'</span><span class="token punctuation">:</span> <span class="token string">'train'</span><span class="token punctuation">,</span>    <span class="token string">'boosting_type'</span><span class="token punctuation">:</span> <span class="token string">'gbdt'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 设置提升类型</span>    <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'regression'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 目标函数</span>    <span class="token string">'metric'</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token string">'l2'</span><span class="token punctuation">,</span> <span class="token string">'auc'</span><span class="token punctuation">}</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 评估函数</span>    <span class="token string">'num_leaves'</span><span class="token punctuation">:</span> <span class="token number">31</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 叶子节点数</span>    <span class="token string">'learning_rate'</span><span class="token punctuation">:</span> <span class="token number">0.05</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 学习速率</span>    <span class="token string">'feature_fraction'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 建树的特征选择比例</span>    <span class="token string">'bagging_fraction'</span><span class="token punctuation">:</span> <span class="token number">0.8</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 建树的样本采样比例</span>    <span class="token string">'bagging_freq'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># k 意味着每 k 次迭代执行bagging</span>    <span class="token string">'verbose'</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token comment" spellcheck="true"># &lt;0 显示致命的, =0 显示错误 (警告), >0 显示信息</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># 7.调用LightGBM模型，使用训练集数据进行训练（拟合）</span><span class="token comment" spellcheck="true"># Add verbosity=2 to print messages while running boosting</span>my_model <span class="token operator">=</span> lgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lgb_train<span class="token punctuation">,</span> num_boost_round<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> valid_sets<span class="token operator">=</span>lgb_eval<span class="token punctuation">,</span> early_stopping_rounds<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 8.使用模型对测试集数据进行预测</span>predictions <span class="token operator">=</span> my_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> num_iteration<span class="token operator">=</span>my_model<span class="token punctuation">.</span>best_iteration<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 9.对模型的预测结果进行评判（平均绝对误差）</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（4）基于Scikit-learn接口的回归"><a href="#（4）基于Scikit-learn接口的回归" class="headerlink" title="（4）基于Scikit-learn接口的回归"></a>（4）基于Scikit-learn接口的回归</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">import</span> lightgbm <span class="token keyword">as</span> lgb<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_error<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> Imputer<span class="token comment" spellcheck="true"># 1.读文件</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'./dataset/train.csv'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.切分数据输入：特征 输出：预测目标变量</span>y <span class="token operator">=</span> data<span class="token punctuation">.</span>SalePriceX <span class="token operator">=</span> data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select_dtypes<span class="token punctuation">(</span>exclude<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 3.切分训练集、测试集,切分比例7.5 : 2.5</span>train_X<span class="token punctuation">,</span> test_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">.</span>values<span class="token punctuation">,</span> y<span class="token punctuation">.</span>values<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 4.空值处理，默认方法：使用特征列的平均值进行填充</span>my_imputer <span class="token operator">=</span> Imputer<span class="token punctuation">(</span><span class="token punctuation">)</span>train_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>train_X<span class="token punctuation">)</span>test_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 5.调用LightGBM模型，使用训练集数据进行训练（拟合）</span><span class="token comment" spellcheck="true"># Add verbosity=2 to print messages while running boosting</span>my_model <span class="token operator">=</span> lgb<span class="token punctuation">.</span>LGBMRegressor<span class="token punctuation">(</span>objective<span class="token operator">=</span><span class="token string">'regression'</span><span class="token punctuation">,</span> num_leaves<span class="token operator">=</span><span class="token number">31</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> n_estimators<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>                             verbosity<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>my_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 6.使用模型对测试集数据进行预测</span>predictions <span class="token operator">=</span> my_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 7.对模型的预测结果进行评判（平均绝对误差）</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-3-LightGBM调参"><a href="#5-3-LightGBM调参" class="headerlink" title="5.3 LightGBM调参"></a>5.3 LightGBM调参</h2><p>在上一部分中，LightGBM模型的参数有一部分进行了简单的设置，但大都使用了模型的默认参数，但默认参数并不是最好的。要想让LightGBM表现的更好，需要对LightGBM模型进行参数微调。下图展示的是回归模型需要调节的参数，分类模型需要调节的参数与此类似。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuy3ml2bDMRhOk4LzRVUVMnnXqMSrXo3fOt7NhvJnOKYEe0EXg7yOPGLr27tQdJUaPNoKAAF9P6frQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图：LightGBM回归模型调参"></p><h1 id="6-关于LightGBM若干问题的思考"><a href="#6-关于LightGBM若干问题的思考" class="headerlink" title="6. 关于LightGBM若干问题的思考"></a>6. 关于LightGBM若干问题的思考</h1><h2 id="6-1-LightGBM与XGBoost的联系和区别有哪些？"><a href="#6-1-LightGBM与XGBoost的联系和区别有哪些？" class="headerlink" title="6.1 LightGBM与XGBoost的联系和区别有哪些？"></a>6.1 LightGBM与XGBoost的联系和区别有哪些？</h2><p>（1）LightGBM使用了基于histogram的决策树算法，这一点不同于XGBoost中的贪心算法和近似算法，histogram算法在内存和计算代价上都有不小优势。1）内存上优势：很明显，直方图算法的内存消耗为 (因为对特征分桶后只需保存特征离散化之后的值)，而XGBoost的贪心算法内存消耗为： ，因为XGBoost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要位的浮点数来保存。2）计算上的优势：预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为，而直方图算法只需要遍历桶就行了，时间为。</p><p>（2）XGBoost采用的是level-wise的分裂策略，而LightGBM采用了leaf-wise的策略，区别是XGBoost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGBoost也进行了分裂，带来了不必要的开销。leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。</p><p>（3）XGBoost在每一层都动态构建直方图，因为XGBoost的直方图算法不是针对某个特定的特征，而是所有特征共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而LightGBM中对每个特征都有一个直方图，所以构建一次直方图就够了。</p><p>（4）LightGBM使用直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。</p><p>（5）LightGBM支持类别特征，不需要进行独热编码处理。</p><p>（6）LightGBM优化了特征并行和数据并行算法，除此之外还添加了投票并行方案。</p><p>（7）LightGBM采用基于梯度的单边采样来减少训练样本并保持数据分布不变，减少模型因数据分布发生变化而造成的模型精度下降。</p><p>（8）特征捆绑转化为图着色问题，减少特征数量。</p><h1 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h1><p>由于参考的文献较多，我把每篇参考文献按照自己的学习思路，进行了详细的归类和标注。</p><p><strong>LightGBM论文解读：</strong></p><p>【1】Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]//Advances in Neural Information Processing Systems. 2017: 3146-3154.</p><p>【2】Taifeng Wang分享LightGBM的视频，地址：<a href="https://v.qq.com/x/page/k0362z6lqix.html" target="_blank" rel="noopener">https://v.qq.com/x/page/k0362z6lqix.html</a></p><p>【3】开源|LightGBM：三天内收获GitHub 1000+ 星，地址：<a href="https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog</a></p><p>【4】Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：<a href="https://blog.csdn.net/anshuai_aw1/article/details/83048709" target="_blank" rel="noopener">https://blog.csdn.net/anshuai_aw1/article/details/83048709</a></p><p>【5】快的不要不要的lightGBM - 王乐的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/31986189" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31986189</a></p><p>【6】『 论文阅读』LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：<a href="https://blog.csdn.net/shine19930820/article/details/79123216" target="_blank" rel="noopener">https://blog.csdn.net/shine19930820/article/details/79123216</a></p><p><strong>LightGBM算法讲解：</strong></p><p>【7】【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/87885678" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/87885678</a></p><p>【8】入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同，地址：<a href="https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw</a></p><p>【9】CatBoost vs. Light GBM vs. XGBoost，地址：<a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db" target="_blank" rel="noopener">https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db</a></p><p>【10】机器学习算法之LightGBM，地址：<a href="https://www.biaodianfu.com/lightgbm.html" target="_blank" rel="noopener">https://www.biaodianfu.com/lightgbm.html</a></p><p><strong>LightGBM工程优化：</strong></p><p>【11】Meng Q, Ke G, Wang T, et al. A communication-efficient parallel algorithm for decision tree[C]//Advances in Neural Information Processing Systems. 2016: 1279-1287.</p><p>【12】Zhang H, Si S, Hsieh C J. GPU-acceleration for Large-scale Tree Boosting[J]. arXiv preprint arXiv:1706.08359, 2017.</p><p>【13】LightGBM的官方GitHub代码库，地址：<a href="https://github.com/microsoft/LightGBM" target="_blank" rel="noopener">https://github.com/microsoft/LightGBM</a></p><p>【14】关于sklearn中的决策树是否应该用one-hot编码？- 柯国霖的回答 - 知乎 <a href="https://www.zhihu.com/question/266195966/answer/306104444" target="_blank" rel="noopener">https://www.zhihu.com/question/266195966/answer/306104444</a></p><p><strong>LightGBM实例：</strong></p><p>【15】LightGBM使用，地址：<a href="https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/</a></p><p>【16】LightGBM两种使用方式 ，地址：<a href="https://www.cnblogs.com/chenxiangzhen/p/10894306.html" target="_blank" rel="noopener">https://www.cnblogs.com/chenxiangzhen/p/10894306.html</a></p><p><strong>LightGBM若干问题的思考：</strong></p><p>【17】GBDT、XGBoost、LightGBM的区别和联系，地址：<a href="https://www.jianshu.com/p/765efe2b951a" target="_blank" rel="noopener">https://www.jianshu.com/p/765efe2b951a</a></p><p>【18】xgboost和lightgbm的区别和适用场景，地址：<a href="https://www.nowcoder.com/ta/review-ml/review?page=101" target="_blank" rel="noopener">https://www.nowcoder.com/ta/review-ml/review?page=101</a> </p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LightGBM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战：原生接口和sklearn接口区别</title>
      <link href="/2019/12/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8807%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95XGBOOST%E5%AE%9E%E6%88%98%EF%BC%9A%E5%8E%9F%E7%94%9F%E6%8E%A5%E5%8F%A3%E5%92%8Csklearn%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/12/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8807%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95XGBOOST%E5%AE%9E%E6%88%98%EF%BC%9A%E5%8E%9F%E7%94%9F%E6%8E%A5%E5%8F%A3%E5%92%8Csklearn%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><h1 id="2-官方文档"><a href="#2-官方文档" class="headerlink" title="2 官方文档"></a>2 官方文档</h1><p><strong><a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="noopener">英文官方文档</a></strong></p><p><strong><a href="https://xgboost.apachecn.org/#/xgboost.apachecn.org" target="_blank" rel="noopener">中文文档</a></strong></p><h1 id="3-sklearn接口"><a href="#3-sklearn接口" class="headerlink" title="3 sklearn接口"></a>3 sklearn接口</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> xgboost<span class="token punctuation">.</span>sklearn <span class="token keyword">import</span> XGBClassifierxgbc <span class="token operator">=</span> XGBClassifier<span class="token punctuation">(</span>n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 新建xgboost sklearn的分类class</span><span class="token comment" spellcheck="true"># xgboost的sklearn接口默认只使用cpu单线程，设置n_jobs=-1使用所有线程</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"开始xgboost classifier训练"</span><span class="token punctuation">)</span>xgbc<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_vector<span class="token punctuation">,</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>train_label<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 喂给分类器训练numpy形式的训练特征向量和标签向量</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"完成xgboost classifier训练，开始预测"</span><span class="token punctuation">)</span>pre_train_Classifier <span class="token operator">=</span> xgbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_vector<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 喂给分类器numpy形式的测试特征向量</span>np<span class="token punctuation">.</span>save<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span><span class="token string">"pre_train_Classifier.npy"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>pre_train_Classifier<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 保存结果</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>xgboost的sklearn接口，可以不经过标签标准化(即将标签编码为0~n_class-1)，直接喂给分类器特征向量和标签向量，使用fit训练后调用predict就能得到预测向量的预测标签，它会在内部调用sklearn.preprocessing.LabelEncoder()将标签在分类器使用时transform，在输出结果时inverse_transform。</p><p><strong>优点：使用简单，无需对标签进行标准化处理，直接得到预测标签；</strong></p><p><strong>缺点：在模型保存后重新载入，丢失LabelEncoder，不能增量训练只能用一次.</strong></p><h1 id="4-xgboost的原生接口"><a href="#4-xgboost的原生接口" class="headerlink" title="4 xgboost的原生接口"></a>4 xgboost的原生接口</h1><pre class="line-numbers language-python"><code class="language-python">vector_matrix<span class="token punctuation">,</span>label_single_new <span class="token operator">=</span> get_data<span class="token punctuation">(</span>data_path<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 获取得到特征矩阵、标签向量</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"标签总数为：%d；数据量总数为：%d"</span><span class="token operator">%</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>list<span class="token punctuation">(</span>set<span class="token punctuation">(</span>label_single_new<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>len<span class="token punctuation">(</span>vector_matrix<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将标签标准化为0~class number-1,则xgboost概率最大的下标即为该位置数对应的标签</span><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessinglabel_coder <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>label_single_code <span class="token operator">=</span> label_coder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>label_single_new<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 切割训练集、测试集</span><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splittrain_matrix<span class="token punctuation">,</span>test_matrix<span class="token punctuation">,</span>train_label<span class="token punctuation">,</span>test_label <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>        vector_matrix<span class="token punctuation">,</span>label_single_code<span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token comment" spellcheck="true"># 参数设置见 http://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/xgboost/chapters/xgboost_usage.html</span>params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'booster'</span><span class="token punctuation">:</span> <span class="token string">'gbtree'</span><span class="token punctuation">,</span><span class="token string">'silent'</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">,</span>                    <span class="token comment" spellcheck="true"># 如果为 0（默认值），则表示打印运行时的信息；如果为 1，则表示不打印这些信息</span><span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'multi:softprob'</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 基于softmax 的多分类模型，但是它的输出是一个矩阵：ndata*nclass，给出了每个样本属于每个类别的概率。</span><span class="token string">'num_class'</span><span class="token punctuation">:</span>len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>label_single_new<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指定类别数量</span><span class="token punctuation">}</span>dtrain <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>train_matrix<span class="token punctuation">,</span> label<span class="token operator">=</span>train_label<span class="token punctuation">,</span> nthread<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># xgboost原生接口需要使用DMatrix格式的数据，这里与sklearn接口不同</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"开始xgboost训练"</span><span class="token punctuation">)</span>xgbc <span class="token operator">=</span> xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>params<span class="token punctuation">,</span>dtrain<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 初始化xgboost分类器，原生接口默认启用全部线程</span>xgbc<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span>model_path<span class="token operator">+</span>save_name<span class="token operator">+</span><span class="token string">'xgbc_0.9.model'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 保存模型 </span><span class="token comment" spellcheck="true"># =============================================================================</span><span class="token comment" spellcheck="true">#     xgbc = xgb.Booster()  # 重新载入模型</span><span class="token comment" spellcheck="true">#     xgbc.load_model(fname=model_path+save_name+'xgbc_0.9.model')</span><span class="token comment" spellcheck="true"># =============================================================================</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"xgboost训练完成，得到概率矩阵"</span><span class="token punctuation">)</span>pre_train <span class="token operator">=</span> xgbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>train_matrix<span class="token punctuation">,</span> nthread<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 训练数据的预测概率矩阵，启用全部线程</span>pre_test <span class="token operator">=</span> xgbc<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>test_matrix<span class="token punctuation">,</span> nthread<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 测试数据的预测概率矩阵，启用全部线程</span><span class="token comment" spellcheck="true"># 概率矩阵各行的数据为各条数据的预测概率，各行数据之和为1；</span><span class="token comment" spellcheck="true"># 概率矩阵各行的下标即为标准化后的label标签(0~class number-1)</span><span class="token comment" spellcheck="true"># 数据保存</span>np<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model_path<span class="token operator">+</span>save_name<span class="token operator">+</span><span class="token string">'pre_train.npy'</span><span class="token punctuation">,</span>pre_train<span class="token punctuation">)</span>np<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model_path<span class="token operator">+</span>save_name<span class="token operator">+</span><span class="token string">'train_label.npy'</span><span class="token punctuation">,</span>train_label<span class="token punctuation">)</span>  np<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model_path<span class="token operator">+</span>save_name<span class="token operator">+</span><span class="token string">'pre_test.npy'</span><span class="token punctuation">,</span>pre_test<span class="token punctuation">)</span>np<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model_path<span class="token operator">+</span>save_name<span class="token operator">+</span><span class="token string">'test_label.npy'</span><span class="token punctuation">,</span>test_label<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 数据载入</span><span class="token comment" spellcheck="true"># =============================================================================</span><span class="token comment" spellcheck="true"># pre_train = np.load(model_path+save_name+'pre_train.npy') </span><span class="token comment" spellcheck="true"># train_label = np.load(model_path+save_name+'train_label.npy') </span><span class="token comment" spellcheck="true"># pre_test = np.load(model_path+save_name+'pre_test.npy') </span><span class="token comment" spellcheck="true"># test_label = np.load(model_path+save_name+'test_label.npy') </span><span class="token comment" spellcheck="true"># =============================================================================</span><span class="token comment" spellcheck="true"># narray_target.argsort(axis=1)，获得按行(排序对象为各行数值)升序后的下标矩阵，axis=0为按列升序;</span><span class="token comment" spellcheck="true"># np.fliplr(narray_target)获取矩阵的左右翻转，narray_target[::-1]获取矩阵的上下翻转</span><span class="token comment" spellcheck="true"># narray_target[:,-5:]获取矩阵的后5列;</span>top_k <span class="token operator">=</span> <span class="token number">5</span>  <span class="token comment" spellcheck="true"># 获取预测概率最大的5个标签</span><span class="token comment" spellcheck="true"># 获取概率矩阵排序信息，得到按行升序的下标矩阵,切割得到各行的后5个下标,</span><span class="token comment" spellcheck="true"># 将其左右翻转后，得到各行降序的前5个下标，即标准化后的标签</span>pre_test_index <span class="token operator">=</span> np<span class="token punctuation">.</span>fliplr<span class="token punctuation">(</span>pre_test<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token operator">*</span>top_k<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pre_test_label <span class="token operator">=</span> label_coder<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>pre_test_index<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 调用label标准化工具inverse_transform将下标转化为真实标签</span>pre_train_index <span class="token operator">=</span> np<span class="token punctuation">.</span>fliplr<span class="token punctuation">(</span>pre_train<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token operator">*</span>top_k<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pre_train_label <span class="token operator">=</span> label_coder<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>pre_train_index<span class="token punctuation">)</span>        <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>xgboost原生接口，数据需要经过标签标准化(LabelEncoder().fit_transform)、输入数据标准化(xgboost.DMatrix)和输出结果反标签标准化(LabelEncoder().inverse_transform)，训练调用train预测调用predict.</p><p>需要注意的是，<strong>xgboost原生接口输出的预测标签概率矩阵各行的下标即为标准化后的label标签(0~class number-1).</strong></p><h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h1><p>优先考虑使用原生接口形式，便于模型保存后的复用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（07）：梯度提升树算法XGBoost实战</title>
      <link href="/2019/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8807%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95XGBOOST%E5%AE%9E%E6%88%98/"/>
      <url>/2019/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8807%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95XGBOOST%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>上一篇从数据原理角度深入介绍了XGBoost的实现原理及优化，参考《<a href="https://dataquaner.github.io/2019/12/25/机器学习系列之决策树算法（07）：梯度提升树算法XGBOOST/">梯度提升树算法XGBoost</a>》。本篇主要介绍XGBoost的工程实战，参数调优等内容。</p><blockquote><p>学习一个算法实战，一般按照以下几步，第一步能够基于某个平台、某种语言构建一个模型，第二步是能够优化一个模型 。我们将学习以下内容</p><ol><li>如果使用xgboost构建分类器</li><li>xgboost 的参数含义，以及如何调参</li><li>xgboost 的如何做cv</li><li>xgboost的可视化</li></ol></blockquote><h1 id="2-XGBoost模型构建"><a href="#2-XGBoost模型构建" class="headerlink" title="2 XGBoost模型构建"></a>2 XGBoost模型构建</h1><h2 id="回归模型"><a href="#回归模型" class="headerlink" title="回归模型"></a>回归模型</h2><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>我们使用<strong><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" target="_blank" rel="noopener">房价数据</a></strong> ，做的是一个回归任务，预测房价，分类任务类似。</p><p>导入包</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> xgboost <span class="token keyword">import</span> XGBRegressor<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> Imputer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>读入和展示数据</p><pre class="line-numbers language-python"><code class="language-python">data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'../input/train.csv'</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> subset<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> data<span class="token punctuation">.</span>SalePriceX <span class="token operator">=</span> data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select_dtypes<span class="token punctuation">(</span>exclude<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>train_X<span class="token punctuation">,</span> test_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">.</span>as_matrix<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>as_matrix<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span>my_imputer <span class="token operator">=</span> Imputer<span class="token punctuation">(</span><span class="token punctuation">)</span>train_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>train_X<span class="token punctuation">)</span>test_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>train_X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>test_X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>train_y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>test_y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token comment" spellcheck="true">##执行结果</span><span class="token punctuation">(</span><span class="token number">1095</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">365</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1095</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">365</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="创建并训练XGBoost模型"><a href="#创建并训练XGBoost模型" class="headerlink" title="创建并训练XGBoost模型"></a>创建并训练XGBoost模型</h3><p>随机选取默认参数进行初始化建模</p><pre class="line-numbers language-python"><code class="language-python">my_model <span class="token operator">=</span> XGBRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Add silent=True to avoid printing out updates with each cycle</span>my_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="评估并预测模型"><a href="#评估并预测模型" class="headerlink" title="评估并预测模型"></a>评估并预测模型</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># make predictions</span>predictions <span class="token operator">=</span> my_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="模型调优"><a href="#模型调优" class="headerlink" title="模型调优"></a>模型调优</h3><p>XGBoost有一些参数可以显著影响模型的准确性和训练速度。</p><h4 id="n-estimators"><a href="#n-estimators" class="headerlink" title="n_estimators"></a><strong>n_estimators</strong></h4><p><strong>n_estimators</strong> 指定训练循环次数。在 <a href="https://link.zhihu.com/?target=http%3A//i.imgur.com/2q85n9s.png">欠拟合 vs 过拟合 图表</a>, n_estimators让训练沿着图表向右移动。 值太低会导致欠拟合，这对训练数据和新数据的预测都是不准确的。 太大的值会导致过度拟合，这是对训练数据的准确预测，但对新数据的预测不准确（这是我们关心的）。 通过实际实验来找到理想的n_estimators。 典型值范围为100-1000，但这很大程度上取决于下面讨论的</p><h4 id="early-stopping-rounds"><a href="#early-stopping-rounds" class="headerlink" title="early_stopping_rounds"></a><strong>early_stopping_rounds</strong></h4><p><strong>early_stopping_rounds</strong> 提供了一种自动查找理想值的方法。 early_stopping_rounds会导致模型在validation score停止改善时停止迭代，即使迭代次数还没有到n_estimators。为<strong>n_estimators</strong>设置一个高值然后使用<strong>early_stopping_rounds</strong>来找到停止迭代的最佳时间是明智的。</p><p>存在随机的情况有时会导致validation score无法改善，因此需要指定一个数字，以确定在停止前允许多少轮退化。<strong>early_stopping_rounds = 5</strong>是一个合理的值。 因此，在五轮validation score无法改善之后训练将停止。 以下是early_stopping的代码：</p><pre class="line-numbers language-python"><code class="language-python">my_model <span class="token operator">=</span> XGBRegressor<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>my_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> early_stopping_rounds<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>              eval_set<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> my_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当使用<strong>early_stopping_rounds</strong>时，需要留出一些数据来检查要使用的轮数。 如果以后想要使所有数据拟合模型，请将<strong>n_estimators</strong>设置为在早期停止运行时发现的最佳值。</p><h4 id="learning-rate"><a href="#learning-rate" class="headerlink" title="learning_rate"></a>learning_rate</h4><p>对于更好的XGBoost模型，这是一个微妙但重要的技巧：</p><p>XGBoost模型不是通过简单地将每个组件模型中的预测相加来获得预测，而是在将它们添加之前将每个模型的预测乘以一个小数字。这意味着我们添加到集合中的每个树都不会对最后结果有决定性的影响。在实践中，这降低了模型过度拟合的倾向。</p><p>因此，使用一个较大的<strong>n_estimators</strong>值并不会造成过拟合。如果使用early_stopping_rounds，树的数量会被设置成一个合适的值。</p><p>通常，较小的learning rate（以及大量的estimators）将产生更准确的XGBoost模型，但是由于它在整个循环中进行更多迭代，因此也将使模型更长时间进行训练。 包含学习率的代码如下：</p><pre class="line-numbers language-python"><code class="language-python">my_model <span class="token operator">=</span> XGBRegressor<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span>my_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> early_stopping_rounds<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>              eval_set<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> my_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>XGBoost目前是用于在传统数据（也称为表格或结构数据）上构建精确模型的主要算法</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> xgboost <span class="token keyword">import</span> XGBRegressor<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_errormy_model1 <span class="token operator">=</span> XGBRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span>my_model1<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> my_model1<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error 1: "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>my_model2 <span class="token operator">=</span> XGBRegressor<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>my_model2<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> early_stopping_rounds<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>              eval_set<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> my_model2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error 2: "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>my_model3 <span class="token operator">=</span> XGBRegressor<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span>my_model3<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span>               eval_set<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> my_model3<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error 3: "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h2><p>以天池竞赛中的<a href="https://tianchi.aliyun.com/competition/entrance/231702/introduction?spm=5176.12281973.1005.1.3dd52448pr3509" target="_blank" rel="noopener">《<strong>快来一起挖掘幸福感！</strong>》</a>中的数据为例，开始一个多分类模型的的实例</p><h4 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> learning_curve<span class="token punctuation">,</span> train_test_split<span class="token punctuation">,</span>GridSearchCV<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">'''  ##         准备训练集和测试集'''</span>  data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'happiness_train_abbr.csv'</span><span class="token punctuation">)</span>y<span class="token operator">=</span>data<span class="token punctuation">[</span><span class="token string">'happiness'</span><span class="token punctuation">]</span>data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">'happiness'</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token string">'survey_time'</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#survey_time格式不能直接识别</span>X<span class="token operator">=</span>data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h4><pre class="line-numbers language-python"><code class="language-python">train_x<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size <span class="token operator">=</span><span class="token number">0.30</span><span class="token punctuation">,</span> early_stopping_rounds<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>random_state <span class="token operator">=</span> <span class="token number">33</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="XGBoost模型训练"><a href="#XGBoost模型训练" class="headerlink" title="XGBoost模型训练"></a>XGBoost模型训练</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">'''  ##         xgboost训练'''</span> params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'learning_rate'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>           <span class="token string">'n_estimators'</span><span class="token punctuation">:</span> <span class="token number">500</span><span class="token punctuation">,</span>           <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span>           <span class="token string">'min_child_weight'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>           <span class="token string">'seed'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>           <span class="token string">'subsample'</span><span class="token punctuation">:</span> <span class="token number">0.8</span><span class="token punctuation">,</span>           <span class="token string">'colsample_bytree'</span><span class="token punctuation">:</span> <span class="token number">0.8</span><span class="token punctuation">,</span>          <span class="token string">'gamma'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>           <span class="token string">'reg_alpha'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>           <span class="token string">'reg_lambda'</span><span class="token punctuation">:</span> <span class="token number">1</span>         <span class="token punctuation">}</span><span class="token comment" spellcheck="true">#第一次设置300次的迭代，评测的指标是"merror","mlogloss"，这是一个多分类问题。</span>model <span class="token operator">=</span> xgb<span class="token punctuation">.</span>XGBClassifier<span class="token punctuation">(</span>params<span class="token punctuation">)</span>eval_set <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>train_x<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>test_x<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">]</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> eval_set<span class="token operator">=</span>eval_set<span class="token punctuation">,</span> eval_metric<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"merror"</span><span class="token punctuation">,</span> <span class="token string">"mlogloss"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    accuracy <span class="token operator">=</span> accuracy_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span> predictions<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: %.2f%%"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>accuracy <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="模型可视化"><a href="#模型可视化" class="headerlink" title="模型可视化"></a>模型可视化</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">'''  ##         可视化训练过程'''</span> results <span class="token operator">=</span> model<span class="token punctuation">.</span>evals_result<span class="token punctuation">(</span><span class="token punctuation">)</span>epochs <span class="token operator">=</span> len<span class="token punctuation">(</span>results<span class="token punctuation">[</span><span class="token string">'validation_0'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'merror'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>x_axis <span class="token operator">=</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> epochs<span class="token punctuation">)</span><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplotfig<span class="token punctuation">,</span> ax <span class="token operator">=</span> pyplot<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_axis<span class="token punctuation">,</span> results<span class="token punctuation">[</span><span class="token string">'validation_0'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'mlogloss'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Train'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_axis<span class="token punctuation">,</span> results<span class="token punctuation">[</span><span class="token string">'validation_1'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'mlogloss'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Test'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'XGBoost Log Loss'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Log Loss'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'epochs'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_axis<span class="token punctuation">,</span> results<span class="token punctuation">[</span><span class="token string">'validation_0'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'merror'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Train'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_axis<span class="token punctuation">,</span> results<span class="token punctuation">[</span><span class="token string">'validation_1'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'merror'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Test'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'XGBoost Classification Error'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Classification Error'</span><span class="token punctuation">)</span>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'epochs'</span><span class="token punctuation">)</span>pyplot<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="C:\Users\liyu25\AppData\Roaming\Typora\typora-user-images\image-20191226184624206.png" alt="模型迭代结果" style="zoom: 80%;"><p>实际训练效果，在第146次迭代就停止了，说明最好的效果实在136次左右。根据许多大牛的实践经验，选择<strong>early_stopping_rounds = 10% * n_estimators</strong>。</p><p>最终输出模型最佳状态下的结果：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">"best iteration:"</span><span class="token punctuation">,</span>model<span class="token punctuation">.</span>best_iteration<span class="token punctuation">)</span>limit <span class="token operator">=</span> model<span class="token punctuation">.</span>best_iterationpredictions <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_x<span class="token punctuation">,</span>ntree_limit<span class="token operator">=</span>limit<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    accuracy <span class="token operator">=</span> accuracy_score<span class="token punctuation">(</span>test_y<span class="token punctuation">,</span> predictions<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: %.2f%%"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>accuracy <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3 参考资料"></a>3 参考资料</h2><p><a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/dansbecker/xgboost">https://www.kaggle.com/dansbecker/xgboost</a></p><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/lujiandong1/article/details/52777168">https://blog.csdn.net/lujiandong1/article/details/52777168</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（07）：梯度提升树算法XGBoost</title>
      <link href="/2019/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8807%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95XGBOOST/"/>
      <url>/2019/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8807%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95XGBOOST/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>XGBoost的全称是eXtreme Gradient Boosting，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。XGBoost是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用XGBoost进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，XGBoost的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从XGBoost的数学原理和工程实现上进行介绍，然后介绍XGBoost的优缺点。</p><h2 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h2><h3 id="生成一棵树"><a href="#生成一棵树" class="headerlink" title="生成一棵树"></a>生成一棵树</h3><h4 id="Boosting-Tree回顾"><a href="#Boosting-Tree回顾" class="headerlink" title="Boosting Tree回顾"></a><strong>Boosting Tree回顾</strong></h4><p>XGBoost模型是大规模并行boosting tree的工具，它是目前较好的开源boosting tree工具包。因此，在了解XGBoost算法基本原理之前，需要首先了解Boosting Tree算法基本原理。Boosting方法是一类应用广泛且非常有效的统计学习方法。它是基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比任何一个专家单独的判断要好。这种思想整体上可以分为两种：</p><ul><li><strong>强可学习</strong>：如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称为强可学习，直接单个模型就搞定常规问题。就好比专家给出的意见都很接近且都是正确率很高的结果，那么一个专家的结论就可以用了，这种情况非常少见。</li><li><strong>弱可学习</strong>：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。这种情况是比较常见的。</li></ul><p>boosting算法主要是针对弱可学习的分类器来开展优化工作。其关心的问题包括两方面内容：</p><p>（1）在每一轮如何改变训练数据的权值和概率分布；</p><p>（2）如何将弱分类器组合成一个强分类器，这种思路较好的就是AdaBoost算法，以前在遥感图像地物识别中得到过应用。</p><p><img src="https://pic2.zhimg.com/80/v2-35e3fd2bd53b5cbfc2f9596eb2479591_hd.jpg" alt="Boosting模型基本流程"></p><p>Boosting Tree模型采用<strong>加法模型</strong>与<strong>前向分步算法</strong>，而且基模型都是决策树模型。前向分步算法（Forward stage wise additive model）是指在叠加新的基模型的基础上同步进行优化，具体而言，就是每一次叠加的模型都去拟合上一次模型拟合后产生的残差（Residual）。从算法模型解释上来说，Boosting Tree是决策树的加法模型：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+f_%7BM%7D%28x%29+%3D+%5Csum_%7Bm%3D1%7D%5E%7BM%7DT%28x%2C%5Ctheta_%7Bm%7D%29+%5Cend%7Bequation%7D" alt="[公式]"> （1）</p><p>上式中M为决策树的数量； <img src="https://www.zhihu.com/equation?tex=T%28x%2C+%5Ctheta_%7Bm%7D%29" alt="[公式]"> 为某个决策树； <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bm%7D" alt="[公式]"> 为对应决策树的参数。</p><p>Boosting Tree模型采用前向分步算法，其中假设 <img src="https://www.zhihu.com/equation?tex=f_%7B0%7D%28x%29+%3D+0" alt="[公式]"> ，则第m步的模型是：</p><p><img src="https://www.zhihu.com/equation?tex=f_%7Bm%7D%28x%29+%3D+f_%7Bm-1%7D%28x%29%2BT%28x%2C+%5Ctheta_%7Bm%7D%29" alt="[公式]"> （2）</p><p>为求解对应的参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bm%7D" alt="[公式]"> ，需要最小化相应损失函数来确定，具体公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bm%7D%5E%7B%27%7D+%3D+arg+%5Cmin_%7B%5Ctheta_%7Bm%7D%7D%5Csum_%7Bi%3D1%7D%5E%7BM%7DL%28y_%7Bi%7D%2C+f_%7Bm-1%7D%28x_%7Bi%7D%29+%2B+T%28x_%7Bi%7D%3B%5Ctheta_%7Bm%7D%29%29" alt="[公式]"> （3）</p><p>由前向分步算法得到M棵决策树<img src="https://www.zhihu.com/equation?tex=T%28x%2C+%5Ctheta_%7Bm%7D%29" alt="[公式]"> 后，再进行加和，就得到了提升树模型 <img src="https://www.zhihu.com/equation?tex=f_%7BM%7D%28x%29" alt="[公式]"> 。在xgboost论文中提到的一个明显的boosting tree的加和应用案例如图3所示。</p><p><img src="https://pic2.zhimg.com/80/v2-fee9ec17376a633196bebbf56c18c2f5_hd.jpg" alt="img">图2 boosting tree的累加效果示意图</p><blockquote><p>相关树模型的参数值求解主要依据于<strong>损失函数</strong>的定义。</p><p>一般来言对于<strong>分类问题</strong>，选择<strong>指数损失函数</strong>作为损失函数时，将形成<strong>AdaBoost模型</strong>；</p><p>对于<strong>回归问题</strong>，损失函数常利用<strong>平方损失函数</strong>。为了扩展Boosting Tree的应用范围，需要构建一种可以广泛适用的残差描述方式来满足于任意损失函数的形式，为解决分类问题的Gradient Boosting Decision Tree算法应运而生。</p></blockquote><p><strong><a href="https://zhuanlan.zhihu.com/p/90520307" target="_blank" rel="noopener">带正则项的Boosting Tree模型和带梯度的Boosting Tree推导过程</a></strong></p><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>我们知道 XGBoost 是由 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个基模型组成的一个加法运算式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%3D%5Csum_%7Bt%3D1%7D%5E%7Bk%7D%5C+f_t%28x_i%29+%5C%5C" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=f_k" alt="[公式]"> 为第 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个基模型， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i" alt="[公式]"> 为第 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 个样本的预测值。</p><p>损失函数可由预测值 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i" alt="[公式]"> 与真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 进行表示：</p><p><img src="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bi%3D1%7D%5En+l%28+y_i%2C+%5Chat%7By%7D_i%29+%5C%5C" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 为样本数量。</p><p>我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的<strong>损失函数 <img src="https://www.zhihu.com/equation?tex=L" alt="[公式]"></strong> 与<strong>抑制模型复杂度的正则项 <img src="https://www.zhihu.com/equation?tex=%5COmega" alt="[公式]"></strong> 组成，所以我们有：</p><p><img src="https://www.zhihu.com/equation?tex=Obj+%3D%5Csum_%7Bi%3D1%7D%5En+l%28%5Chat%7By%7D_i%2C+y_i%29+%2B+%5Csum_%7Bt%3D1%7D%5Ek+%5COmega%28f_t%29+%5C%5C+" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%5COmega" alt="[公式]"> 为模型的正则项，由于 XGBoost 支持决策树也支持线性模型，所以这里再不展开描述。</p><p>我们知道 boosting 模型是前向加法，以第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 步的模型为例，模型对第 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 个样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 的预测为：</p><p><img src="https://www.zhihu.com/equation?tex=++%5Chat%7By%7D_i%5Et%3D+%5Chat%7By%7D_i%5E%7Bt-1%7D+%2B+f_t%28x_i%29++%5C%5C" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%5E%7Bt-1%7D" alt="[公式]"> 由第 <img src="https://www.zhihu.com/equation?tex=t-1" alt="[公式]"> 步的模型给出的预测值，是已知常数，<img src="https://www.zhihu.com/equation?tex=f_t%28x_i%29" alt="[公式]"> 是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+Obj%5E%7B%28t%29%7D+%26%3D+%5Csum_%7Bi%3D1%7D%5Enl%28y_i%2C+%5Chat%7By%7D_i%5Et%29+%2B+%5Csum_%7Bi%3D1%7D%5Et%5COmega%28f_i%29+%5C%5C++++%26%3D+%5Csum_%7Bi%3D1%7D%5En+l%5Cleft%28y_i%2C+%5Chat%7By%7D_i%5E%7Bt-1%7D+%2B+f_t%28x_i%29+%5Cright%29+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29++%5Cend%7Balign%7D+%5C%5C" alt="[公式]"></p><p>求此时最优化目标函数，就相当于求解 <img src="https://www.zhihu.com/equation?tex=f_t%28x_i%29" alt="[公式]"> 。</p><blockquote><p>泰勒公式是将一个在 <img src="https://www.zhihu.com/equation?tex=x%3Dx_0" alt="[公式]"> 处具有 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 阶导数的函数 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 利用关于 <img src="https://www.zhihu.com/equation?tex=x-x_0" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 次多项式来逼近函数的方法，若函数 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 在包含 <img src="https://www.zhihu.com/equation?tex=x_0" alt="[公式]"> 的某个闭区间 <img src="https://www.zhihu.com/equation?tex=%5Ba%2Cb%5D" alt="[公式]"> 上具有 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 阶导数，且在开区间 <img src="https://www.zhihu.com/equation?tex=%28a%2Cb%29" alt="[公式]"> 上具有 <img src="https://www.zhihu.com/equation?tex=n%2B1" alt="[公式]"> 阶导数，则对闭区间 <img src="https://www.zhihu.com/equation?tex=%5Ba%2Cb%5D" alt="[公式]"> 上任意一点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 有 <img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%29%3D%5Csum_%7Bi%3D0%7D%5E%7Bn%7D%5Cfrac%7Bf%5E%7B%28i%29%7D%28x_0%29%7D%7Bi%21%7D%28x-x_0%29%5E+i%2BR_n%28x%29+" alt="[公式]"> ，其中的多项式称为函数在 <img src="https://www.zhihu.com/equation?tex=x_0" alt="[公式]"> 处的泰勒展开式， <img src="https://www.zhihu.com/equation?tex=R_n%28x%29" alt="[公式]"> 是泰勒公式的余项且是 <img src="https://www.zhihu.com/equation?tex=%28x%E2%88%92x_0%29%5En" alt="[公式]"> 的高阶无穷小。</p></blockquote><p>根据泰勒公式我们把函数 <img src="https://www.zhihu.com/equation?tex=f%28x%2B%5CDelta+x%29" alt="[公式]"> 在点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 处进行泰勒的二阶展开，可得到如下等式：</p><p><img src="https://www.zhihu.com/equation?tex=f%28x%2B%5CDelta+x%29+%5Capprox+f%28x%29+%2B+f%27%28x%29%5CDelta+x+%2B+%5Cfrac12+f%27%27%28x%29%5CDelta+x%5E2++%5C%5C" alt="[公式]"></p><p>我们把 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%5E%7Bt-1%7D" alt="[公式]"> 视为 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=f_t%28x_i%29" alt="[公式]"> 视为 <img src="https://www.zhihu.com/equation?tex=%5CDelta+x" alt="[公式]"> ，故可以将目标函数写为：</p><p><img src="https://www.zhihu.com/equation?tex=Obj%5E%7B%28t%29%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%5B+l%28y_i%2C+%5Chat%7By%7D_i%5E%7Bt-1%7D%29+%2B+g_if_t%28x_i%29+%2B+%5Cfrac12h_if_t%5E2%28x_i%29+%5Cright%5D+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29+%5C%5C" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=g_%7Bi%7D" alt="[公式]"> 为损失函数的一阶导， <img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D" alt="[公式]"> 为损失函数的二阶导，<strong>注意这里的导是对 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%5E%7Bt-1%7D" alt="[公式]"> 求导</strong>。</p><p>我们以平方损失函数为例：</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28y_i+-+%28%5Chat%7By%7D_i%5E%7Bt-1%7D+%2B+f_t%28x_i%29%29+%5Cright%29%5E2++%5C%5C" alt="[公式]"></p><p>则：</p><p><img src="https://www.zhihu.com/equation?tex=++%5Cbegin%7Balign%7D++++++g_i+%26%3D+%5Cfrac%7B%5Cpartial+%28%5Chat%7By%7D%5E%7Bt-1%7D+-+y_i%29%5E2%7D%7B%5Cpartial+%7B%5Chat%7By%7D%5E%7Bt-1%7D%7D%7D+%3D+2%28%5Chat%7By%7D%5E%7Bt-1%7D+-+y_i%29+%5C%5C++++++h_i+%26%3D%5Cfrac%7B%5Cpartial%5E2%28%5Chat%7By%7D%5E%7Bt-1%7D+-+y_i%29%5E2%7D%7B%7B%5Chat%7By%7D%5E%7Bt-1%7D%7D%7D+%3D+2++++%5Cend%7Balign%7D++%5C%5C" alt="[公式]"></p><p>由于在第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 步时 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%5E%7Bt-1%7D" alt="[公式]"> 其实是一个已知的值，所以 <img src="https://www.zhihu.com/equation?tex=l%28y_i%2C+%5Chat%7By%7D_i%5E%7Bt-1%7D%29" alt="[公式]"> 是一个常数，其对函数的优化不会产生影响，因此目标函数可以写成：</p><p><img src="https://www.zhihu.com/equation?tex=+Obj%5E%7B%28t%29%7D+%5Capprox+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%5B+g_if_t%28x_i%29+%2B+%5Cfrac12h_if_t%5E2%28x_i%29+%5Cright%5D+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29+%5C%5C" alt="[公式]"></p><p>所以<strong>我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E%7Bt-1%7D" alt="[公式]"> 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> ，最后根据加法模型得到一个整体模型。</strong></p><h4 id="基于决策树的目标函数"><a href="#基于决策树的目标函数" class="headerlink" title="基于决策树的目标函数"></a>基于决策树的目标函数</h4><p>损失函数可由预测值 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D" alt="[公式]"> 与真实值 <img src="https://www.zhihu.com/equation?tex=y_%7Bi%7D" alt="[公式]"> 进行表示：</p><p><img src="https://www.zhihu.com/equation?tex=L+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bl%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%29%7D+%5C%5C" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 为样本的数量。</p><p>我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要在目标函数中添加正则项，用于防止过拟合。所以目标函数由模型的损失函数 <img src="https://www.zhihu.com/equation?tex=L" alt="[公式]"> 与抑制模型复杂度的正则项 <img src="https://www.zhihu.com/equation?tex=%5COmega" alt="[公式]"> 组成，目标函数的定义如下：</p><p><img src="https://www.zhihu.com/equation?tex=Obj+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bl%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%29%7D+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%7B%5COmega%28f_%7Bi%7D%29%7D+%5C%5C" alt="[公式]"></p><p>其中，<img src="https://www.zhihu.com/equation?tex=+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%7B%5COmega%28f_%7Bi%7D%29%7D" alt="[公式]"> 是将全部 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 棵树的复杂度进行求和，添加到目标函数中作为正则化项，用于防止模型过度拟合。</p><p>由于XGBoost是boosting族中的算法，所以遵从前向分步加法，以第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 步的模型为例，模型对第 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 个样本 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 的预测值为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D%5E%7B%28t%29%7D+%3D+%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D+%2B+f_%7Bt%7D%28x_%7Bi%7D%29+%5C%5C" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D" alt="[公式]"> 是由第 <img src="https://www.zhihu.com/equation?tex=+t-1+" alt="[公式]"> 步的模型给出的预测值，是已知常数， <img src="https://www.zhihu.com/equation?tex=+f_%7Bt%7D%28x_%7Bi%7D%29+" alt="[公式]"> 是这次需要加入的新模型的预测值。此时，目标函数就可以写成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+Obj%5E%7B%28t%29%7D+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bl%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t%29%7D%29%7D+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%7B%5COmega%28f_%7Bi%7D%29%7D+%5C%5C+%26+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bl%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%2Bf_%7Bt%7D%28x_%7Bi%7D%29%29%7D+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%7B%5COmega%28f_%7Bi%7D%29%7D+%5C%5C+%26%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bl%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%2Bf_%7Bt%7D%28x_%7Bi%7D%29%29%7D+%2B+%5COmega%28f_%7Bt%7D%29+%2Bconstant++%5Cend%7Baligned%7D+%5Cend%7Bequation%7D+%5C%5C" alt="[公式]"></p><p>注意上式中，只有一个变量，那就是第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 棵树<img src="https://www.zhihu.com/equation?tex=f_%7Bt%7D%28x_%7Bi%7D%29" alt="[公式]"> ，其余都是已知量或可通过已知量可以计算出来的。细心的同学可能会问，上式中的第二行到第三行是如何得到的呢？这里我们将正则化项进行拆分，由于前<img src="https://www.zhihu.com/equation?tex=t-1" alt="[公式]"> 棵树的结构已经确定，因此前<img src="https://www.zhihu.com/equation?tex=+t-1+" alt="[公式]"> 棵树的复杂度之和可以用一个常量表示，如下所示：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%7B%5COmega%28f_%7Bi%7D%29%7D+%26%3D%5COmega%28f_%7Bt%7D%29+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bt-1%7D%7B%5COmega%28f_%7Bi%7D%29%7D+%5C%5C+%26%3D+%5COmega%28f_%7Bt%7D%29+%2B+constant+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D+%5C%5C" alt="[公式]"></p><h4 id="泰勒公式展开"><a href="#泰勒公式展开" class="headerlink" title="泰勒公式展开"></a><strong>泰勒公式展开</strong></h4><p>泰勒公式是将一个在 <img src="https://www.zhihu.com/equation?tex=x%3Dx_%7B0%7D" alt="[公式]"> 处具有<img src="https://www.zhihu.com/equation?tex=+n" alt="[公式]"> 阶导数的函数<img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 利用关于<img src="https://www.zhihu.com/equation?tex=%28x-x_%7B0%7D%29" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 次多项式来逼近函数的方法。若函数<img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 在包含 <img src="https://www.zhihu.com/equation?tex=x_%7B0%7D" alt="[公式]"> 的某个闭区间 <img src="https://www.zhihu.com/equation?tex=%5Ba%2Cb%5D" alt="[公式]"> 上具有 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 阶导数，且在开区间 <img src="https://www.zhihu.com/equation?tex=%28a%2Cb%29" alt="[公式]"> 上具有 <img src="https://www.zhihu.com/equation?tex=n%2B1+" alt="[公式]"> 阶导数，则对闭区间 <img src="https://www.zhihu.com/equation?tex=%5Ba%2Cb%5D" alt="[公式]"> 上任意一点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 有：</p><p><img src="https://www.zhihu.com/equation?tex=f%28x%29+%3D+%5Csum_%7Bi%3D0%7D%5E%7Bn%7D%7B%5Cfrac%7Bf%5E%7B%28i%29%7D%28x_%7B0%7D%29%7D%7Bi%21%7D%7D%28x-x_%7B0%7D%29%5E%7Bi%7D%2BR_%7Bn%7D%28x%29+%5C%5C" alt="[公式]"></p><p>其中的多项式称为函数在 <img src="https://www.zhihu.com/equation?tex=x_%7B0%7D" alt="[公式]"> 处的泰勒展开式，<img src="https://www.zhihu.com/equation?tex=R_%7Bn%7D%28x%29" alt="[公式]"> 是泰勒公式的余项且是 <img src="https://www.zhihu.com/equation?tex=%28x-x_%7B0%7D%29%5E%7Bn%7D" alt="[公式]"> 的高阶无穷小。</p><p>根据泰勒公式，把函数 <img src="https://www.zhihu.com/equation?tex=f%28x%2B%5CDelta+x%29" alt="[公式]"> 在点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 处进行泰勒的二阶展开，可得如下等式：</p><p><img src="https://www.zhihu.com/equation?tex=f%28x%2B%5CDelta+x%29+%5Capprox+f%28x%29%2Bf%27%28x%29%5CDelta+x+%2B+%5Cfrac%7B1%7D%7B2%7D+f%27%27%28x%29%5CDelta+x%5E%7B2%7D+%5C%5C" alt="[公式]"></p><p>回到XGBoost的目标函数上来， <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 对应损失函数 <img src="https://www.zhihu.com/equation?tex=l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%2Bf_%7Bt%7D%28x_%7Bi%7D%29%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 对应前 <img src="https://www.zhihu.com/equation?tex=t-1+" alt="[公式]"> 棵树的预测值 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=%5CDelta+x" alt="[公式]"> 对应于我们正在训练的第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 棵树 <img src="https://www.zhihu.com/equation?tex=f_%7Bt%7D%28x_%7Bi%7D%29" alt="[公式]"> ，则可以将损失函数写为：</p><p><img src="https://www.zhihu.com/equation?tex=l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%2Bf_%7Bt%7D%28x_%7Bi%7D%29%29+%3D+l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29+%2B+g_%7Bi%7Df_%7Bt%7D%28x_%7Bi%7D%29+%2B+%5Cfrac%7B1%7D%7B2%7Dh_%7Bi%7Df_%7Bt%7D%5E%7B2%7D%28x_%7Bi%7D%29+%5C%5C" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=g_%7Bi%7D" alt="[公式]"> 为损失函数的一阶导， <img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D" alt="[公式]"> 为损失函数的二阶导，注意这里的求导是对 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D" alt="[公式]"> 求导。</p><p>我们以平方损失函数为例：</p><p><img src="https://www.zhihu.com/equation?tex=l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29+%3D+%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29%5E%7B2%7D+%5C%5C" alt="[公式]"></p><p>则：</p><p><img src="https://www.zhihu.com/equation?tex=g_%7Bi%7D+%3D+%5Cfrac%7B%5Cpartial+l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29%7D%7B%5Cpartial+%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D+%7D+%3D++-2%28y_%7Bi%7D-%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29+%5C%5C" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D+%3D+%5Cfrac%7B%5Cpartial+%5E%7B2%7D+l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29%7D%7B%5Cpartial+%28%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29%5E%7B2%7D+%7D+%3D++2+%5C%5C" alt="[公式]"></p><p>将上述的二阶展开式，带入到XGBoost的目标函数中，可以得到目标函数的近似值：</p><p><img src="https://www.zhihu.com/equation?tex=Obj%5E%7B%28t%29%7D+%5Csimeq+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Bl%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7Bt-1%7D%29%2Bg_%7Bi%7Df_%7Bt%7D%28x_%7Bi%7D%29%2B%5Cfrac%7B1%7D%7B2%7Dh_%7Bi%7Df_%7Bt%7D%5E%7B2%7D%28x_%7Bi%7D%29%5D%7D+%2B+%5COmega%28f_%7Bt%7D%29%2Bconstant+%5C%5C" alt="[公式]"></p><p>由于在第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 步时 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D" alt="[公式]"> 其实是一个已知的值，所以 <img src="https://www.zhihu.com/equation?tex=l%28y_%7Bi%7D%2C%5Chat%7By%7D_%7Bi%7D%5E%7B%28t-1%29%7D%29+" alt="[公式]"> 是一个常数，其对函数的优化不会产生影响。因此，去掉全部的常数项，得到目标函数为：</p><p><img src="https://www.zhihu.com/equation?tex=Obj%5E%7B%28t%29%7D+%5Csimeq+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Bg_%7Bi%7Df_%7Bt%7D%28x_%7Bi%7D%29%2B%5Cfrac%7B1%7D%7B2%7Dh_%7Bi%7Df_%7Bt%7D%5E%7B2%7D%28x_%7Bi%7D%29%5D%7D%2B%5COmega%28f_%7Bt%7D%29+%5C%5C" alt="[公式]"></p><p>所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E%7B%28t-1%29%7D" alt="[公式]"> 是已知的，所以这两个值就是常数），然后最优化目标函数，就可以得到每一步的 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> ，最后根据加法模型得到一个整体模型。</p><h3 id="一棵树的生长细节"><a href="#一棵树的生长细节" class="headerlink" title="一棵树的生长细节"></a>一棵树的生长细节</h3><h4 id="分裂结点"><a href="#分裂结点" class="headerlink" title="分裂结点"></a>分裂结点</h4><p>在实际训练过程中，当建立第 t 棵树时，XGBoost采用贪心法进行树结点的分裂：</p><p>从树深为0时开始：</p><ul><li>对树中的每个叶子结点尝试进行分裂；</li><li>每次分裂后，原来的一个叶子结点继续分裂为左右两个子叶子结点，原叶子结点中的样本集将根据该结点的判断规则分散到左右两个叶子结点中；</li><li>新分裂一个结点后，我们需要检测这次分裂是否会给损失函数带来增益，增益的定义如下：</li></ul><p><img src="https://pic4.zhimg.com/80/v2-61e13bb229a8574a8ff9a1f9d8fcc87b_hd.jpg" alt="img"></p><p>如果增益Gain&gt;0，即分裂为两个叶子节点后，目标函数下降了，那么我们会考虑此次分裂的结果。</p><p>但是，在一个结点分裂时，可能有很多个分裂点，每个分裂点都会产生一个增益，如何才能寻找到最优的分裂点呢？接下来会讲到。</p><h4 id="寻找最佳分裂点"><a href="#寻找最佳分裂点" class="headerlink" title="寻找最佳分裂点"></a>寻找最佳分裂点</h4><blockquote><p>在实际训练过程中，当建立第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 棵树时，一个非常关键的问题是如何找到叶子节点的最优切分点，XGBoost支持两种分裂节点的方法——<strong>贪心算法</strong>和<strong>近似算法</strong>。</p></blockquote><h6 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h6><p>  从树的深度为0开始：</p><blockquote><ol><li>对每个叶节点枚举所有的可用特征；</li><li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过<strong>线性扫描</strong>的方式来决定该特征的最佳分裂点，并记录该特征的<strong>分裂收益</strong>；</li><li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集；</li><li>回到第1步，递归执行直到满足特定条件为止；</li></ol></blockquote><p> <strong>那么如何计算每个特征的分裂收益呢？</strong></p><p>  假设我们在某一节点完成特征分裂，则分裂前的目标函数可以写为：</p><p>  <img src="https://www.zhihu.com/equation?tex=Obj_%7B1%7D+%3D-%5Cfrac12+%5B%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5D+%2B+%5Cgamma++%5C%5C" alt="[公式]"></p><p>  分裂后的目标函数为：</p><p>  <img src="https://www.zhihu.com/equation?tex=Obj_2+%3D++-%5Cfrac12+%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D%5D+%2B2%5Cgamma+%5C%5C" alt="[公式]"></p><p>  则对于目标函数来说，分裂后的收益为：</p><p>  <img src="https://www.zhihu.com/equation?tex=Gain%3D%5Cfrac12+%5Cleft%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D+-+%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5Cright%5D+-+%5Cgamma+%5C%5C" alt="[公式]"></p><p>  <strong>注意：</strong>该特征收益也可作为特征重要性输出的重要依据。</p><p>  <strong>对于每次分裂，我们都需要枚举所有特征可能的分割方案，如何高效地枚举所有的分割呢？</strong></p><p>  假设我们要枚举某个特征所有 <img src="https://www.zhihu.com/equation?tex=x+%3C+a" alt="[公式]"> 这样条件的样本，对于某个特定的分割点 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> 我们要计算 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> 左边和右边的导数和。</p><p>  <img src="https://pic2.zhimg.com/80/v2-973173d22eeb508eb1b6f26acbf9f2d1_hd.jpg" alt="img"></p><p>  我们可以发现对于所有的分裂点 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> ，只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <img src="https://www.zhihu.com/equation?tex=G_L" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=G_R" alt="[公式]"> 。然后用上面的公式计算每个分割方案的收益就可以了。</p><p>  观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入新叶子的惩罚项，也就是说引入的分割带来的增益如果小于一个阀值的时候，我们可以剪掉这个分割。</p><p>上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。</p><p>但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。</p><blockquote><p>基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案：</p><ul><li><p><strong>特征预排序+缓存：</strong>XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。</p></li><li><p><strong>分位点近似法：</strong>对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。</p></li><li><p><strong>并行查找：</strong>由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。</p></li></ul></blockquote><h6 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h6><p>  贪心算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪心算法这一缺点给出了近似最优解。</p><p>  对于每个特征，只考察分位点可以减少计算复杂度。</p><p>  该算法首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。</p><p>  在提出候选切分点时有两种策略：</p><ul><li><p><strong>Global：</strong>学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</p></li><li><p><strong>Local：</strong>每次分裂前将重新提出候选切分点。</p><p>直观上来看，Local策略需要更多的计算步骤，而Global策略因为节点已有划分所以需要更多的候选点。</p><p>下图给出不同种分裂策略的AUC变化曲线，横坐标为迭代次数，纵坐标为测试集AUC，eps为近似算法的精度，其倒数为桶的数量。</p><p><img src="https://pic4.zhimg.com/80/v2-3081183127c025ee9f3a1436bb873b07_hd.jpg" alt="img"></p><p>从上图我们可以看到， Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在eps取值合理的情况下，<strong>分位数策略</strong>可以获得与贪心算法相同的精度。</p><p>近似算法简单来说，就是根据特征 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 的分布来确定 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]"> 个候选切分点 <img src="https://www.zhihu.com/equation?tex=S_%7Bk%7D+%3D+%5Cleft%5C%7B+s_%7Bk1%7D%2C+s_%7Bk2%7D%2C...%2C+s_%7Bkl%7D+%5Cright%5C%7D" alt="[公式]"> ，然后根据这些候选切分点把相应的样本放入对应的桶中，对每个桶的 <img src="https://www.zhihu.com/equation?tex=G%2CH" alt="[公式]"> 进行累加。最后在候选切分点集合上贪心查找。该算法描述如下：</p><p><img src="https://pic1.zhimg.com/80/v2-1fe2882f8ef3b0a80068c57905ceaba0_hd.jpg" alt="img"></p></li></ul><p>  <strong>算法讲解：</strong></p><ul><li><p><strong>第一个for循环：</strong>对特征k根据该特征分布的分位数找到切割点的候选集合 <img src="https://www.zhihu.com/equation?tex=S_k%3D%5C%7Bs_%7Bk1%7D%2Cs_%7Bk2%7D%2C...%2Cs_%7Bkl%7D+%5C%7D" alt="[公式]"> 。这样做的目的是提取出部分的切分点不用遍历所有的切分点。其中获取某个特征k的候选切割点的方式叫<code>proposal</code>(策略)。XGBoost 支持 Global 策略和 Local 策略。</p></li><li><p><strong>第二个for循环：</strong>将每个特征的取值映射到由该特征对应的候选点集划分的分桶区间，即 <img src="https://www.zhihu.com/equation?tex=%7Bs_%7Bk%2Cv%7D%E2%89%A5x_%7Bjk%7D%3Es_%7Bk%2Cv%E2%88%921%7D%7D" alt="[公式]"> 。对每个桶区间内的样本统计值 G,H并进行累加，最后在这些累计的统计量上寻找最佳分裂点。这样做的目的是获取每个特征的候选分割点的 G,H值。</p><p>下图给出近似算法的具体例子，以三分位为例：</p><p><img src="https://pic2.zhimg.com/80/v2-cfecb2f6ad675e6e3bf536562e5c06dd_hd.jpg" alt="img"></p><p>根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 G,H 值，最终求解节点划分的增益。</p></li></ul><h4 id="停止生长"><a href="#停止生长" class="headerlink" title="停止生长"></a>停止生长</h4><p>一棵树不会一直生长下去，下面是一些常见的限制条件。</p><p><strong>(1) 当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。</strong></p><p><img src="https://pic1.zhimg.com/80/v2-46c88b4258c2b9740d89c87d203ed0c0_hd.jpg" alt="img"></p><p><strong>(2) 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。</strong></p><p><strong>(3) 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。</strong>这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施。</p><p>每个叶子结点的样本权值和计算方式如下：</p><img src="https://pic3.zhimg.com/80/v2-4ecca09165ffb7a76123401d2009191a_hd.jpg" alt="img" style="zoom:33%;"><p>总结推导过程：</p><p><img src="https://pic2.zhimg.com/80/v2-def00357a06b469b6144d6acb8ab75a9_hd.jpg" alt="总结推导过程"></p><h2 id="算法工程优化"><a href="#算法工程优化" class="headerlink" title="算法工程优化"></a>算法工程优化</h2><h3 id="对内存的优化：列块并行学习"><a href="#对内存的优化：列块并行学习" class="headerlink" title="对内存的优化：列块并行学习"></a>对内存的优化：<strong>列块并行学习</strong></h3><p>在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而 XGBoost 在训练之前会根据特征对数据进行排序，然后保存到<strong>块结构</strong>中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</p><p>作者提出通过按特征进行分块并排序，在块里面保存排序后的特征值及对应样本的引用，以便于获取样本的一阶、二阶导数值。具体流程为：</p><ul><li><p>整体训练数据可以看做一个 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+m" alt="[公式]"> 的超大规模稀疏矩阵</p></li><li><p>按照mini-batch的方式横向分割，可以切成很多个“Block”</p></li><li><p>每一个“Block”内部采用一种Compress Sparse Column的稀疏短阵格式，每一列特征分别做好升序排列，便于搜索切分点，整体的时间复杂度有效降低。</p></li><li><p>通过Block的设置，可以采用并行计算，从而提升模型训练速度。</p></li></ul><p>具体方式如图：</p><p><img src="https://pic2.zhimg.com/80/v2-3a93e4d9940cf6e2e9fd89dfa38dc62d_hd.jpg" alt=" 列分块的升序排列优化示意图"></p><p>通过顺序访问排序后的块遍历样本特征的特征值，方便进行切分点的查找。此外分块存储后多个特征之间互不干涉，可以使用多线程同时对不同的特征进行切分点查找，即特征的并行化处理。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 XGBoost 能够实现分布式或者多线程计算的原因。</p><h3 id="对CPU-Cache的优化：缓存优化"><a href="#对CPU-Cache的优化：缓存优化" class="headerlink" title="对CPU Cache的优化：缓存优化"></a>对<strong>CPU Cache</strong>的优化：缓存优化</h3><p>针对一个具体的块(block)，其中存储了排序好的特征值，以及指向特征值所属样本的索引指针，算法需要间接地利用索引指针来获得样本的梯度值。列块并行学习的设计可以减少节点分裂时的计算量，在顺序访问特征值时，访问的是一块连续的内存空间，但通过特征值持有的索引（样本索引）访问样本获取一阶、二阶导数时，这个访问操作访问的内存空间并不连续，这样可能造成cpu缓存命中率低，影响算法效率。由于块中数据是按特征值来排序的，当索引指针指向内存中不连续的样本时，无法充分利用CPU缓存来提速。</p><p>为了解决缓存命中率低的问题，XGBoost 提出了两种优化思路。</p><p><strong>（1）提前取数（Prefetching）</strong></p><p>对于精确搜索，利用多线程的方式，给每个线程划分一个连续的缓存空间，当training线程在按特征值的顺序计算梯度的累加时，prefetching线程可以提前将接下来的一批特征值对应的梯度加载到CPU缓存中。为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就实现了非连续空间到连续空间的转换，提高了算法效率。</p><p><strong>（2）合理设置分块大小</strong></p><p>对于近似分桶搜索，按行分块时需要准确地选择块的大小。块太小会导致每个线程的工作量太少，切换线程的成本过高，不利于并行计算；块太大导致缓存命中率低，需要花费更多时间在读取数据上。经过反复实验，作者找到一个合理的<code>block_size</code>为 <img src="https://www.zhihu.com/equation?tex=2%5E%7B16%7D" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=2%5E%7B16%7D" alt="[公式]">。</p><h3 id="对IO的优化：核外块计算"><a href="#对IO的优化：核外块计算" class="headerlink" title="对IO的优化：核外块计算"></a>对IO的优化：核外块计算</h3><p>当数据量非常大时，我们不能把所有的数据都加载到内存中。那么就必须将一部分需要加载进内存的数据先存放在硬盘中，当需要时再加载进内存。这样操作具有很明显的瓶颈，即硬盘的IO操作速度远远低于内存的处理速度，肯定会存在大量等待硬盘IO操作的情况。针对这个问题作者提出了“核外”计算的优化方法。具体操作为，将数据集分成多个块存放在硬盘中，使用一个独立的线程专门从硬盘读取数据，加载到内存中，这样算法在内存中处理数据就可以和从硬盘读取数据同时进行。此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p><ul><li><strong>块压缩</strong>（<strong>Block Compression</strong>）。论文使用的是按列进行压缩，读取的时候用另外的线程解压。对于行索引，只保存第一个索引值，然后用16位的整数保存与该block第一个索引的差值。作者通过测试在block设置为 <img src="https://www.zhihu.com/equation?tex=2%5E%7B16%7D" alt="[公式]"> 个样本大小时，压缩比率几乎达到26% <img src="https://www.zhihu.com/equation?tex=%5Csim" alt="[公式]"> 29%。</li><li><strong>块分区</strong>（<strong>Block Sharding</strong> ）。块分区是将特征block分区存放在不同的硬盘上，以此来增加硬盘IO的吞吐量。</li></ul><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li><strong>精度更高：</strong>GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li><li><strong>灵活性更强：</strong>GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li><li><strong>正则化：</strong>XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是XGBoost优于传统GBDT的一个特性。</li><li><strong>Shrinkage（缩减）：</strong>相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统GBDT的实现也有学习速率；</li><li><strong>列抽样：</strong>XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是XGBoost异于传统GBDT的一个特性；</li><li><strong>缺失值处理：</strong>对于特征的值有缺失的样本，XGBoost 采用的稀疏感知算法可以自动学习出它的分裂方向；</li><li><strong>XGBoost工具支持并行：</strong>boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li><li><strong>可并行的近似算法：</strong>树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，用于高效地生成候选的分割点。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li><p>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</p></li><li><p>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</p></li></ul><h3 id="XGBoost与GBDT的差异"><a href="#XGBoost与GBDT的差异" class="headerlink" title="XGBoost与GBDT的差异"></a>XGBoost与GBDT的差异</h3><p>在分析XGBooting优缺点的时候，通过比较该算法与GBDT的差异，即可有较清楚的描述，具体表现在如下方面。</p><p><strong>（1）基分类器的差异</strong></p><ul><li>GBDT算法只能利用CART树作为基学习器，满足分类应用；</li><li>XGBoost算法除了回归树之外还支持线性的基学习器，因此其一方面可以解决带L1与L2正则化项的逻辑回归分类问题，也可以解决线性回问题。</li></ul><p><strong>（2）节点分类方法的差异</strong></p><ul><li>GBDT算法主要是利用Gini impurity针对特征进行节点划分；</li><li>XGBoost经过公式推导，提出的weighted quantile sketch（<strong>加权分位数缩略图</strong>）划分方法，依据影响Loss的程度来确定连续特征的切分值。</li></ul><p><strong>（3）模型损失函数的差异</strong></p><ul><li>传统GBDT在优化时只用到一阶导数信息；</li><li>xgboost则对代价函数进行了二阶泰勒展开，二阶导数有利于梯度下降的更快更准。</li></ul><p><strong>（4）模型防止过拟合的差异</strong></p><ul><li>GBDT算法无正则项，可能出现过拟合；</li><li>Xgboost在代价函数里加入了正则项，用于控制模型的复杂度，降低了过拟合的可能性。</li></ul><p><strong>（5）模型实现上的差异</strong></p><p>决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。其能够实现在特征粒度的并行。</p><h2 id="XGBoost代码实现"><a href="#XGBoost代码实现" class="headerlink" title="XGBoost代码实现"></a>XGBoost代码实现</h2><h3 id="安装XGBoost依赖包"><a href="#安装XGBoost依赖包" class="headerlink" title="安装XGBoost依赖包"></a><strong>安装XGBoost依赖包</strong></h3><pre class="line-numbers language-python"><code class="language-python">pip install xgboost<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="XGBoost分类和回归"><a href="#XGBoost分类和回归" class="headerlink" title="XGBoost分类和回归"></a><strong>XGBoost分类和回归</strong></h3><p>XGBoost有两大类接口：XGBoost原生接口 和 scikit-learn接口 ，并且XGBoost能够实现分类和回归两种任务。</p><p><strong>（1）基于XGBoost原生接口的分类</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris<span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token keyword">from</span> xgboost <span class="token keyword">import</span> plot_importance<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token comment" spellcheck="true"># read in the iris data</span>iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> iris<span class="token punctuation">.</span>datay <span class="token operator">=</span> iris<span class="token punctuation">.</span>target<span class="token comment" spellcheck="true"># split train data and test data</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">1234565</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># set XGBoost's parameters</span>params <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'booster'</span><span class="token punctuation">:</span> <span class="token string">'gbtree'</span><span class="token punctuation">,</span>    <span class="token string">'objective'</span><span class="token punctuation">:</span> <span class="token string">'multi:softmax'</span><span class="token punctuation">,</span>   <span class="token comment" spellcheck="true"># 回归任务设置为：'objective': 'reg:gamma',</span>    <span class="token string">'num_class'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span>      <span class="token comment" spellcheck="true"># 回归任务没有这个参数</span>    <span class="token string">'gamma'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>    <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span>    <span class="token string">'lambda'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>    <span class="token string">'subsample'</span><span class="token punctuation">:</span> <span class="token number">0.7</span><span class="token punctuation">,</span>    <span class="token string">'colsample_bytree'</span><span class="token punctuation">:</span> <span class="token number">0.7</span><span class="token punctuation">,</span>    <span class="token string">'min_child_weight'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span>    <span class="token string">'silent'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token string">'eta'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>    <span class="token string">'seed'</span><span class="token punctuation">:</span> <span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token string">'nthread'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">}</span>plst <span class="token operator">=</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span>dtrain <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>num_rounds <span class="token operator">=</span> <span class="token number">500</span>model <span class="token operator">=</span> xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>plst<span class="token punctuation">,</span> dtrain<span class="token punctuation">,</span> num_rounds<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 对测试集进行预测</span>dtest <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>ans <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 计算准确率</span>cnt1 <span class="token operator">=</span> <span class="token number">0</span>cnt2 <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> ans<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> y_test<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>        cnt1 <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        cnt2 <span class="token operator">+=</span> <span class="token number">1</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: %.2f %% "</span> <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">*</span> cnt1 <span class="token operator">/</span> <span class="token punctuation">(</span>cnt1 <span class="token operator">+</span> cnt2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示重要特征</span>plot_importance<span class="token punctuation">(</span>model<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>（2）基于Scikit-learn接口的回归</strong></p><p>这里，我们用Kaggle比赛中回归问题：House Prices: Advanced Regression Techniques，地址：<a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/house-prices-advanced-regression-techniques">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a> 来进行实例讲解。</p><p>该房价预测的训练数据集中一共有81列，第一列是Id，最后一列是label，中间79列是特征。这79列特征中，有43列是分类型变量，33列是整数变量，3列是浮点型变量。训练数据集中存在缺失值。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>impute <span class="token keyword">import</span> SimpleImputer<span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_absolute_error<span class="token comment" spellcheck="true"># 1.读文件</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'./dataset/train.csv'</span><span class="token punctuation">)</span>data<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> subset<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.切分数据输入：特征 输出：预测目标变量</span>y <span class="token operator">=</span> data<span class="token punctuation">.</span>SalePriceX <span class="token operator">=</span> data<span class="token punctuation">.</span>drop<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select_dtypes<span class="token punctuation">(</span>exclude<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 3.切分训练集、测试集,切分比例7.5 : 2.5</span>train_X<span class="token punctuation">,</span> test_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_y <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">.</span>values<span class="token punctuation">,</span> y<span class="token punctuation">.</span>values<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 4.空值处理，默认方法：使用特征列的平均值进行填充</span>my_imputer <span class="token operator">=</span> SimpleImputer<span class="token punctuation">(</span><span class="token punctuation">)</span>train_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>train_X<span class="token punctuation">)</span>test_X <span class="token operator">=</span> my_imputer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 5.调用XGBoost模型，使用训练集数据进行训练（拟合）</span><span class="token comment" spellcheck="true"># Add verbosity=2 to print messages while running boosting</span>my_model <span class="token operator">=</span> xgb<span class="token punctuation">.</span>XGBRegressor<span class="token punctuation">(</span>objective<span class="token operator">=</span><span class="token string">'reg:squarederror'</span><span class="token punctuation">,</span> verbosity<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># xgb.XGBClassifier() XGBoost分类模型</span>my_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 6.使用模型对测试集数据进行预测</span>predictions <span class="token operator">=</span> my_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 7.对模型的预测结果进行评判（平均绝对误差）</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Mean Absolute Error : "</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>mean_absolute_error<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="XGBoost调参"><a href="#XGBoost调参" class="headerlink" title="XGBoost调参"></a><strong>XGBoost调参</strong></h3><p>在上一部分中，XGBoot模型的参数都使用了模型的默认参数，但默认参数并不是最好的。要想让XGBoost表现的更好，需要对XGBoost模型进行参数微调。XGBoost需要调的参数不算多，他们可以分成三个部分：</p><blockquote><p><strong>1、General Parameters，即与整个模型属基调相关的参数；</strong></p><p><strong>2、Booster Parameters，即与单颗树生成有关的参数；</strong></p><p><strong>3、Learning Task Parameters，与模型调优相关的参数；</strong></p></blockquote><h4 id="General-Parameters"><a href="#General-Parameters" class="headerlink" title="General Parameters"></a><strong>General Parameters</strong></h4><p><strong>1、booster [default=gbtree]</strong></p><p>即xgboost中基学习器类型，有两种选择，分别是树模型（gbtree）和线性模型（linear models）</p><p><strong>2、silent [default=0]</strong></p><p>即控制迭代日志的是否输出，默认输出；</p><p><strong>3、nthread [default to maximum number of threads available if not set]</strong></p><p>即控制模型训练调用机器的核心数，与sklearn中<em>n_jobs的含义相似；</em></p><h4 id="Booster-parameters"><a href="#Booster-parameters" class="headerlink" title="Booster parameters"></a><strong>Booster parameters</strong></h4><p>因为booster有两种类型，常用的一般是树模型，这里只列树模型相关的参数：</p><p><strong>1、eta [default=0.3]</strong> <strong>：学习率</strong></p><p>学习率，这个相当于sklearn中的learning_rate，常见的设置范围在0.01-0.2之间</p><p><strong>2、min_child_weight [default=1]：叶节点的最小权重值</strong></p><p>这个参数与GBM（sklearn）中的“min_samples_leaf”很相似，只不过这里不是样本数，而是权重值，如果样本的权重都是1，这两个参数是等同的；这个值设置较大时，通常树不会太深，可以控制过拟合，但太大时，容易造成欠拟合的现象，具体调参需要cv；</p><p><strong>3、max_depth：树的最大深度</strong></p><p>树的最大深度，含义很直白，控制树的复杂性；通常取值范围在3-10；</p><p><strong>4、max_leaf_nodes：最大叶节点数</strong></p><p>一般这个参数与max_depth二选一控制即可；</p><p><strong>5、gamma [default=0]：分裂收益阈值</strong></p><p>即用来比较每次节点分裂带来的收益，有效控制节点的过度分裂；</p><p>这个参数的变化范围受损失函数的选取影响；</p><p><strong>6、max_delta_step [default=0]</strong></p><p>这个参数暂时不是很理解它的作用范围，一般可以忽略它；</p><p><strong>7、subsample [default=1]：采样比例</strong></p><p>与sklearn中的参数一样，即每颗树的生成可以不去全部样本，这样可以控制模型的过拟合；通常取值范围0.5-1；</p><p><strong>8、colsample_bytree [default=1]：特征采样的比例（每棵树）</strong></p><p>即每棵树不使用全部的特征，控制模型的过拟合；</p><p>通常取值范围0.5-1；</p><p><strong>9、colsample_bylevel [default=1]</strong></p><p>特征采样的比例（每次分裂）；</p><p>这个与随机森林的思想很相似，即每次分裂都不取全部变量；</p><p>当7、8的参数设置较好时，该参数可以不用在意；</p><p><strong>10、lambda [default=1]</strong></p><p>L2范数的惩罚系数，叶子结点的分数？；</p><p><strong>11、alpha [default=0]</strong></p><p>L1范数的惩罚系数，叶子结点数？；</p><p><strong>12、scale_pos_weight [default=1]</strong></p><p>这个参数也不是很理解，貌似与类别不平衡的问题相关；</p><h4 id="Learning-Task-Parameters"><a href="#Learning-Task-Parameters" class="headerlink" title="Learning Task Parameters"></a><strong>Learning Task Parameters</strong></h4><p><strong>1、objective [default=reg:linear]：目标函数</strong></p><p>通常的选项分别是：binary:logistic，用于二分类，产生每类的概率值；multi:softmax，用于多分类，但不产生概率值，直接产生类别结果；multi:softprob，类似softmax，但产生多分类的概率值；</p><p><strong>2、eval_metric [ default according to objective ]：评价指标</strong></p><p>当你给模型一个验证集时，会输出对应的评价指标值；</p><p>一般有：rmse ，均方误差；mae ，绝对平均误差；logloss ，对数似然值；error ，二分类错误率；merror ，多分类错误率；mlogloss ；auc</p><p><strong>3、seed：即随机种子</strong></p><h2 id="关于XGBoost若干问题的思考"><a href="#关于XGBoost若干问题的思考" class="headerlink" title="关于XGBoost若干问题的思考"></a><strong>关于XGBoost若干问题的思考</strong></h2><h3 id="XGBoost与GBDT的联系和区别有哪些？"><a href="#XGBoost与GBDT的联系和区别有哪些？" class="headerlink" title="XGBoost与GBDT的联系和区别有哪些？"></a><strong>XGBoost与GBDT的联系和区别有哪些？</strong></h3><p>（1）GBDT是机器学习算法，XGBoost是该算法的工程实现。</p><p>（2）<strong>正则项：</strong>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。</p><p>（3）<strong>导数信息：</strong>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。</p><p>（4）<strong>基分类器：</strong>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。</p><p>（5）<strong>子采样：</strong>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。</p><p>（6）<strong>缺失值处理：</strong>传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。</p><p>（7）<strong>并行化</strong>：传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</p><h3 id="为什么XGBoost泰勒二阶展开后效果就比较好呢？"><a href="#为什么XGBoost泰勒二阶展开后效果就比较好呢？" class="headerlink" title="为什么XGBoost泰勒二阶展开后效果就比较好呢？"></a><strong>为什么XGBoost泰勒二阶展开后效果就比较好呢？</strong></h3><p>（1）<strong>从为什么会想到引入泰勒二阶的角度来说（可扩展性）：</strong>XGBoost官网上有说，当目标函数是MSE时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如logistic loss的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把MSE推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与MSE统一？是因为MSE是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与MSE统一了，那就只用推导MSE就好了。</p><p>（2）<strong>从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：</strong>二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。</p><h3 id="XGBoost对缺失值是怎么处理的？"><a href="#XGBoost对缺失值是怎么处理的？" class="headerlink" title="XGBoost对缺失值是怎么处理的？"></a><strong>XGBoost对缺失值是怎么处理的？</strong></h3><p>在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。</p><h3 id="XGBoost为什么可以并行训练？"><a href="#XGBoost为什么可以并行训练？" class="headerlink" title="XGBoost为什么可以并行训练？"></a><strong>XGBoost为什么可以并行训练？</strong></h3><p>（1）XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</p><p>（2）XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</p><h2 id="20道XGBoost面试题"><a href="#20道XGBoost面试题" class="headerlink" title="20道XGBoost面试题"></a>20道XGBoost面试题</h2><h3 id="简单介绍一下XGBoost"><a href="#简单介绍一下XGBoost" class="headerlink" title="简单介绍一下XGBoost"></a>简单介绍一下XGBoost</h3><p>首先需要说一说GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。</p><p>XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。</p><h3 id="XGBoost与GBDT有什么不同"><a href="#XGBoost与GBDT有什么不同" class="headerlink" title="XGBoost与GBDT有什么不同"></a>XGBoost与GBDT有什么不同</h3><ul><li><strong>基分类器</strong>：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li><li><strong>导数信息</strong>：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。</li><li><strong>正则项</strong>：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li><li><strong>列抽样</strong>：XGBoost支持列采样，与随机森林类似，用于防止过拟合。</li><li><strong>缺失值处理</strong>：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li><li><strong>并行化</strong>：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li></ul><h3 id="XGBoost为什么使用泰勒二阶展开"><a href="#XGBoost为什么使用泰勒二阶展开" class="headerlink" title="XGBoost为什么使用泰勒二阶展开"></a>XGBoost为什么使用泰勒二阶展开</h3><ul><li><strong>精准性</strong>：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数</li><li><strong>可扩展性</strong>：损失函数支持自定义，只需要新的损失函数二阶可导。</li></ul><h3 id="XGBoost为什么可以并行训练"><a href="#XGBoost为什么可以并行训练" class="headerlink" title="XGBoost为什么可以并行训练"></a>XGBoost为什么可以并行训练</h3><ul><li>XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li><li>XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</li></ul><h3 id="XGBoost为什么快"><a href="#XGBoost为什么快" class="headerlink" title="XGBoost为什么快"></a>XGBoost为什么快</h3><ul><li><strong>分块并行</strong>：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点</li><li><strong>候选分位点</strong>：每个特征采用常数个分位点作为候选分割点</li><li><strong>CPU cache 命中优化</strong>： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。</li><li><strong>Block 处理优化</strong>：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐</li></ul><h3 id="XGBoost防止过拟合的方法"><a href="#XGBoost防止过拟合的方法" class="headerlink" title="XGBoost防止过拟合的方法"></a>XGBoost防止过拟合的方法</h3><p>XGBoost在设计时，为了防止过拟合做了很多优化，具体如下：</p><ul><li><strong>目标函数添加正则项</strong>：叶子节点个数+叶子节点权重的L2正则化</li><li><strong>列抽样</strong>：训练的时候只用一部分特征（不考虑剩余的block块即可）</li><li><strong>子采样</strong>：每轮计算可以不使用全部样本，使算法更加保守</li><li><strong>shrinkage</strong>: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间</li></ul><h3 id="XGBoost如何处理缺失值"><a href="#XGBoost如何处理缺失值" class="headerlink" title="XGBoost如何处理缺失值"></a>XGBoost如何处理缺失值</h3><p>XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：</p><ul><li>在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。</li></ul><ul><li>在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</li></ul><ul><li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li></ul><img src="https://mmbiz.qpic.cn/mmbiz_png/90dLE6ibsg0fkqnx5yOhtlvx8dFgk1DvVfp2pmTsZ0yX0A2usH3afam4cJb7lQNIJGb3N2VZicclrfoRqM6MHhtQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:33%;"><p>find_split时，缺失值处理的伪代码</p><h3 id="XGBoost中叶子结点的权重如何计算出来"><a href="#XGBoost中叶子结点的权重如何计算出来" class="headerlink" title="XGBoost中叶子结点的权重如何计算出来"></a>XGBoost中叶子结点的权重如何计算出来</h3><p>XGBoost目标函数最终推导形式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/90dLE6ibsg0fDfLgXV02BLFJ9eaFEJB0ERQaHDopzOeSvCyaPGicmHqArjzlJYDejcTs9YJoAFdAqwyVrdpUPZQA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>利用一元二次函数求最值的知识，当目标函数达到最小值Obj<em>时，每个叶子结点的权重为wj</em>。</p><p>具体公式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/90dLE6ibsg0fDfLgXV02BLFJ9eaFEJB0EURBYpwF4xF4x2lLh7BroeKUjRqk17VXpkZqPEjaskia4kiazjs9nyg0A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h3 id="XGBoost中的一棵树的停止生长条件"><a href="#XGBoost中的一棵树的停止生长条件" class="headerlink" title="XGBoost中的一棵树的停止生长条件"></a>XGBoost中的一棵树的停止生长条件</h3><ul><li>当新引入的一次分裂所带来的增益Gain&lt;0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。</li><li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。</li><li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</li></ul><h3 id="RF和GBDT的区别"><a href="#RF和GBDT的区别" class="headerlink" title="RF和GBDT的区别"></a>RF和GBDT的区别</h3><p><strong>相同点：</strong></p><ul><li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li></ul><p><strong>不同点：</strong></p><ul><li><strong>集成学习</strong>：RF属于bagging思想，而GBDT是boosting思想</li><li><strong>偏差-方差权衡</strong>：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差</li><li><strong>训练样本</strong>：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本</li><li><strong>并行性</strong>：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)</li><li><strong>最终结果</strong>：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合</li><li><strong>数据敏感性</strong>：RF对异常值不敏感，而GBDT对异常值比较敏感</li><li><strong>泛化能力</strong>：RF不易过拟合，而GBDT容易过拟合</li></ul><h3 id="XGBoost如何处理不平衡数据"><a href="#XGBoost如何处理不平衡数据" class="headerlink" title="XGBoost如何处理不平衡数据"></a>XGBoost如何处理不平衡数据</h3><p>对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决：</p><p>第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10；</p><p>第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。</p><p>原话是这么说的：</p><pre class="line-numbers language-python"><code class="language-python">For common cases such <span class="token keyword">as</span> ads clickthrough log<span class="token punctuation">,</span> the dataset <span class="token keyword">is</span> extremely imbalanced<span class="token punctuation">.</span> This can affect the training of xgboost model<span class="token punctuation">,</span> <span class="token operator">and</span> there are two ways to improve it<span class="token punctuation">.</span>  If you care only about the ranking order <span class="token punctuation">(</span>AUC<span class="token punctuation">)</span> of your prediction      Balance the positive <span class="token operator">and</span> negative weights<span class="token punctuation">,</span> via scale_pos_weight      Use AUC <span class="token keyword">for</span> evaluation  If you care about predicting the right probability      In such a case<span class="token punctuation">,</span> you cannot re<span class="token operator">-</span>balance the dataset      In such a case<span class="token punctuation">,</span> set parameter max_delta_step to a finite number <span class="token punctuation">(</span>say <span class="token number">1</span><span class="token punctuation">)</span> will help convergence<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>那么，源码到底是怎么利用<strong>scale_pos_weight</strong>来平衡样本的呢，是调节权重还是过采样呢？请看源码：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">if</span> <span class="token punctuation">(</span>info<span class="token punctuation">.</span>labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1.</span><span class="token number">0f</span><span class="token punctuation">)</span>  w <span class="token operator">*=</span> param_<span class="token punctuation">.</span>scale_pos_weight<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看出，应该是增大了少数样本的权重。</p><p>除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。</p><h3 id="比较LR和GBDT，说说什么情景下GBDT不如LR"><a href="#比较LR和GBDT，说说什么情景下GBDT不如LR" class="headerlink" title="比较LR和GBDT，说说什么情景下GBDT不如LR"></a>比较LR和GBDT，说说什么情景下GBDT不如LR</h3><p>先说说LR和GBDT的区别：</p><ul><li>LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程</li><li>GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li></ul><p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：</p><p>先看一个例子：</p><blockquote><p>假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。</p><p>我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。</p></blockquote><p>那么这种情况下，如果采用LR的话，应该也会出现类似过拟合的情况呀：y = W1<em>f1 + Wi</em>fi+….，其中 W1特别大以拟合这10个样本。为什么此时树模型就过拟合的更严重呢？</p><p>仔细想想发现，因为现在的模型普遍都会带着正则项，而 LR 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大。但是，树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。</p><p>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：<strong>带正则化的线性模型比较不容易对稀疏特征过拟合。</strong></p><h3 id="XGBoost中如何对树进行剪枝"><a href="#XGBoost中如何对树进行剪枝" class="headerlink" title="XGBoost中如何对树进行剪枝"></a>XGBoost中如何对树进行剪枝</h3><ul><li>在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。</li><li>在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。</li><li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li><li>XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li></ul><h3 id="XGBoost如何选择最佳分裂点？"><a href="#XGBoost如何选择最佳分裂点？" class="headerlink" title="XGBoost如何选择最佳分裂点？"></a>XGBoost如何选择最佳分裂点？</h3><p>XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。</p><p>因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。</p><p>如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。</p><h3 id="XGBoost的Scalable性如何体现"><a href="#XGBoost的Scalable性如何体现" class="headerlink" title="XGBoost的Scalable性如何体现"></a>XGBoost的Scalable性如何体现</h3><ul><li><strong>基分类器的scalability</strong>：弱分类器可以支持CART决策树，也可以支持LR和Linear。</li><li><strong>目标函数的scalability</strong>：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。</li><li><strong>学习方法的scalability</strong>：Block结构支持并行化，支持 Out-of-core计算。</li></ul><h3 id="XGBoost如何评价特征的重要性"><a href="#XGBoost如何评价特征的重要性" class="headerlink" title="XGBoost如何评价特征的重要性"></a>XGBoost如何评价特征的重要性</h3><p>我们采用三种方法来评判XGBoost模型中特征的重要程度：</p><pre><code> 官方文档：（1）weight - the number of times a feature is used to split the data across all trees. （2）gain - the average gain of the feature when it is used in trees. （3）cover - the average coverage of the feature when it is used in trees.</code></pre><ul><li><strong>weight</strong> ：该特征在所有树中被用作分割样本的特征的总次数。</li><li><strong>gain</strong> ：该特征在其出现过的所有树中产生的平均增益。</li><li><strong>cover</strong> ：该特征在其出现过的所有树中的平均覆盖范围。</li></ul><blockquote><p>注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。</p></blockquote><h3 id="XGBooost参数调优的一般步骤"><a href="#XGBooost参数调优的一般步骤" class="headerlink" title="XGBooost参数调优的一般步骤"></a>XGBooost参数调优的一般步骤</h3><p>首先需要初始化一些基本变量，例如：</p><ul><li>max_depth = 5</li><li>min_child_weight = 1</li><li>gamma = 0</li><li>subsample, colsample_bytree = 0.8</li><li>scale_pos_weight = 1</li></ul><p><strong>(1) 确定learning rate和estimator的数量</strong></p><p>learning rate可以先用0.1，用cv来寻找最优的estimators</p><p><strong>(2) max_depth和 min_child_weight</strong></p><p>我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。</p><p>max_depth，每棵子树的最大深度，check from range(3,10,2)。</p><p>min_child_weight，子节点的权重阈值，check from range(1,6,2)。</p><p>如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p><p><strong>(3) gamma</strong></p><p>也称作最小划分损失<code>min_split_loss</code>，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。</p><ul><li>如果大于该阈值，则该叶子节点值得继续划分</li><li>如果小于该阈值，则该叶子节点不值得继续划分</li></ul><p><strong>(4) subsample, colsample_bytree</strong></p><p>subsample是对训练的采样比例</p><p>colsample_bytree是对特征的采样比例</p><p>both check from 0.6 to 0.9</p><p><strong>(5) 正则化参数</strong></p><p>alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100</p><p>lambda 是L2正则化系数</p><p><strong>(6) 降低学习率</strong></p><p>降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1</p><h3 id="XGBoost模型如果过拟合了怎么解决"><a href="#XGBoost模型如果过拟合了怎么解决" class="headerlink" title="XGBoost模型如果过拟合了怎么解决"></a>XGBoost模型如果过拟合了怎么解决</h3><p>当出现过拟合时，有两类参数可以缓解：</p><p>第一类参数：用于直接控制模型的复杂度。包括<code>max_depth,min_child_weight,gamma</code> 等参数</p><p>第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括<code>subsample,colsample_bytree</code></p><p>还有就是直接减小<code>learning rate</code>，但需要同时增加<code>estimator</code> 参数。</p><h3 id="为什么XGBoost相比某些模型对缺失值不敏感"><a href="#为什么XGBoost相比某些模型对缺失值不敏感" class="headerlink" title="为什么XGBoost相比某些模型对缺失值不敏感"></a>为什么XGBoost相比某些模型对缺失值不敏感</h3><p>对存在缺失值的特征，一般的解决方法是：</p><ul><li>离散型变量：用出现次数最多的特征值填充；</li><li>连续型变量：用中位数或均值填充；</li></ul><p>一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。</p><p>而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。</p><p>XGBoost对缺失数据有特定的处理方法，<a href="http://mp.weixin.qq.com/s?__biz=Mzg2MjI5Mzk0MA==&amp;mid=2247484181&amp;idx=1&amp;sn=8d0e51fb0cb974f042e66659e1daf447&amp;chksm=ce0b59cef97cd0d8cf7f9ae1e91e41017ff6d4c4b43a4c19b476c0b6d37f15769f954c2965ef&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">详情参考上篇文章第7题</a>。</p><p>因此，对于有缺失值的数据在经过缺失处理后：</p><ul><li>当数据量很小时，优先用朴素贝叶斯</li><li>数据量适中或者较大，用树模型，优先XGBoost</li><li>数据量较大，也可以用神经网络</li><li>避免使用距离度量相关的模型，如KNN和SVM</li></ul><h3 id="XGBoost和LightGBM的区别"><a href="#XGBoost和LightGBM的区别" class="headerlink" title="XGBoost和LightGBM的区别"></a>XGBoost和LightGBM的区别</h3><p><img src="https://mmbiz.qpic.cn/mmbiz_png/90dLE6ibsg0cassUTLvbQlGic1CW6ialKxxJ2S8XI3VokUBf5TBOSDG8zb6gZXe0q63b4TyDlDPCX9G6cPXlmR4cw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>（1）树生长策略：XGB采用<code>level-wise</code>的分裂策略，LGB采用<code>leaf-wise</code>的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。</p><p>（2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：</p><ul><li>减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。</li><li>计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑏𝑖𝑛𝑠)。</li><li>LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算</li></ul><blockquote><p>但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？</p><p>xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。</p></blockquote><p>（3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。</p><p>（4）缓存命中率：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p><p>（5）LightGBM 与 XGboost 的并行策略不同：</p><ul><li><strong>特征并行</strong> ：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。</li><li><strong>数据并行</strong> ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。</li><li><strong>投票并行（LGB）</strong>：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><strong>XGBoost论文解读：</strong></p><p>【1】Chen T , Guestrin C . XGBoost: A Scalable Tree Boosting System[J]. 2016.</p><p>【2】<a href="https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf" target="_blank" rel="noopener">Tianqi Chen的XGBoost的Slides</a></p><p>【3】<a href="https://zhuanlan.zhihu.com/p/75217528" target="_blank" rel="noopener">对xgboost的理解 - 金贵涛的文章 - 知乎</a></p><p>【4】<a href="https://blog.csdn.net/Dby_freedom/article/details/84301725" target="_blank" rel="noopener">CTR预估 论文精读(一)–XGBoost</a></p><p>【5】<a href="https://zhuanlan.zhihu.com/p/36794802" target="_blank" rel="noopener">XGBoost论文阅读及其原理 - Salon sai的文章 - 知乎</a></p><p>【6】<a href="https://blog.csdn.net/qdbszsj/article/details/79615712" target="_blank" rel="noopener">XGBoost 论文翻译+个人注释</a></p><p><strong>XGBoost算法讲解：</strong></p><p>【7】<a href="https://mp.weixin.qq.com/s/wLE9yb7MtE208IVLFlZNkw" target="_blank" rel="noopener">XGBoost超详细推导，终于有人讲明白了！</a></p><p>【8】<a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！</a></p><p>【9】<a href="https://www.zhihu.com/question/41354392/answer/98658997" target="_blank" rel="noopener">机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - wepon的回答 - 知乎</a> </p><p>【10】<a href="http://wepon.me/files/gbdt.pdf" target="_blank" rel="noopener">GBDT算法原理与系统设计简介，wepon</a></p><p><strong>XGBoost实例：</strong></p><p>【11】<a href="https://www.jianshu.com/p/7e0e2d66b3d4" target="_blank" rel="noopener">Kaggle 神器 xgboost</a></p><p>【12】<a href="https://mp.weixin.qq.com/s/X4K6UFZPxL05v2uolId7Lw" target="_blank" rel="noopener">干货 | XGBoost在携程搜索排序中的应用</a></p><p>【13】<a href="https://zhuanlan.zhihu.com/p/31182879" target="_blank" rel="noopener">史上最详细的XGBoost实战 - 章华燕的文章 - 知乎</a></p><p>【14】<a href="https://zhuanlan.zhihu.com/p/61150141" target="_blank" rel="noopener">XGBoost模型构建流程及模型参数微调（房价预测附代码讲解） - 人工智能学术前沿的文章 - 知乎</a></p><p><strong>XGBoost面试题：</strong></p><p>【15】<a href="https://mp.weixin.qq.com/s/_QgnYoW827GDgVH9lexkNA" target="_blank" rel="noopener">珍藏版 | 20道XGBoost面试题，你会几个？(上篇)</a></p><p>【16】<a href="https://mp.weixin.qq.com/s/BbelOsYgsiOvwfwYs5QfpQ" target="_blank" rel="noopener">珍藏版 | 20道XGBoost面试题，你会几个？(下篇</a>)</p><p>【17】<a href="https://mp.weixin.qq.com/s/RSQWx4fH3uI_sjZzAKVyKQ" target="_blank" rel="noopener">推荐收藏 | 10道XGBoost面试题送给你</a></p><p>【18】<a href="https://mp.weixin.qq.com/s/vjLPVhg_UavZIJrOzu_u1w" target="_blank" rel="noopener">面试题：xgboost怎么给特征评分？</a></p><p>【19】<a href="https://zhuanlan.zhihu.com/p/81368182" target="_blank" rel="noopener">[校招-基础算法]GBDT/XGBoost常见问题 - Jack Stark的文章 - 知乎</a></p><p>【20】《百面机器学习》诸葛越主编、葫芦娃著，P295-P297。</p><p>【21】<a href="https://zhuanlan.zhihu.com/p/86816771" target="_blank" rel="noopener">灵魂拷问，你看过Xgboost原文吗？ - 小雨姑娘的文章 - 知乎</a></p><p>【22】<a href="https://www.zhihu.com/question/277638585/answer/522272201" target="_blank" rel="noopener">为什么xgboost泰勒二阶展开后效果就比较好了呢？ - Zsank的回答 - 知乎</a> </p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（05）：梯度提升树算法GBDT</title>
      <link href="/2019/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8805%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95GBDT/"/>
      <url>/2019/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8805%EF%BC%89%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95GBDT/</url>
      
        <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>前面讲述了<a href="https://dataquaner.github.io/2019/12/17/机器学习系列之决策树算法（01）：决策树特征选择/">《决策树的特征选择》</a>、<a href="https://dataquaner.github.io/2019/12/19/机器学习系列之决策树算法（02）：决策树的生成/">《决策树的生成》</a>、<a href="https://dataquaner.github.io/2019/12/19/机器学习系列之决策树算法（03）：决策树的剪枝/">《决策树的剪枝》</a>，熟悉了单棵决策树的的实现细节，在实际应用时，往往采用多棵决策树组合的形式完成目标任务。那么如何组合单棵决策树可以使得模型效果更优呢？目前主要有两种思想：<strong>bagging</strong>和<strong>boosting</strong>，分别对应的典型算法<strong>随机森林</strong>和<strong>Adaboost</strong>、<strong>GBDT</strong>等。</p><blockquote><p><strong>Bagging</strong>的思想比较简单，即每一次从原始数据中根据<strong>均匀概率分布有放回的抽取和原始数据大小相同的样本集合</strong>，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。典型实现算法<strong>随机森林</strong></p><p><strong>boosting</strong>的每一次抽样的<strong>样本分布都是不一样的</strong>。每一次迭代，都根据上一次迭代的结果，<strong>增加被错误分类的样本的权重</strong>，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个<strong>不断学习的过程，也是一个不断提升</strong>的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。典型实现算法是<strong>GBDT</strong></p></blockquote><p>boosting的思想如下图：</p><p><img src="https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_hd.jpg" alt="boosting思想"></p><p>基于boosting思想的经典算法是<strong>Adaboost</strong>和<strong>GBDT</strong>。关于Adaboost的介绍可以参考《Adaboost算法》，本文重点介绍GBDT。</p><h2 id="2-什么是GBDT"><a href="#2-什么是GBDT" class="headerlink" title="2 什么是GBDT"></a>2 什么是GBDT</h2><blockquote><p>GBDT(Gradient Boosting Decision Tree) 是一种迭代的决策树算法，是<strong>回归树</strong>，而不是分类树。该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。</p><p>GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性。</p></blockquote><p>GBDT用来做回归预测，调整后也可以用于分类。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。具体训练过程如下图示意：</p><p><img src="https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_hd.jpg" alt="GBDT训练过程"></p><h2 id="3-GBDT算法原理"><a href="#3-GBDT算法原理" class="headerlink" title="3 GBDT算法原理"></a>3 GBDT算法原理</h2><p>GBDT算法的核心思想</p><blockquote><p>GBDT的核心就在于：<strong>每一棵树学的是之前所有树结论和的残差</strong>，这个残差就是一个加预测值后能得真实值的累加量。即所有弱分类器相加等于预测值，下一个弱分类器去拟合误差函数对预测值的梯度。</p></blockquote><blockquote><p>GBDT加入了简单的<strong>数值优化</strong>思想。</p><p><strong>Xgboost</strong>更加有效应用了数值优化。相比于gbdt，最重要是对损失函数变得更复杂。目标函数依然是所有树想加等于预测值。损失函数引入了一阶导数，二阶导数。</p><p>不同于随机森林所有树的预测求均值，GBDT所有的树的预测值加起来是最终的预测值，可以不断接近真实值。</p></blockquote><p>GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。</p><p>在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p><p>GBDT的思想的通俗解释</p><blockquote><p>假如有个人30岁，</p><p>第一棵树，我们首先用20岁去拟合，发现损失有10岁，</p><p>第二颗，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，</p><p>第三颗，我们用3岁拟合剩下的差距，差距就只有一岁了。</p><p><strong>三棵树加起来为29岁，距离30最近。</strong></p></blockquote><p>从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？</p><h2 id="4-负梯度拟合"><a href="#4-负梯度拟合" class="headerlink" title="4 负梯度拟合"></a>4 <strong>负梯度拟合</strong></h2><p>在上一节中，我们介绍了GBDT的基本思路，但是没有解决<strong>损失函数拟合方法</strong>的问题。针对这个问题，大牛<strong>Freidman</strong>提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyMP27fEskIYa0Y00VyUqTGZLvXic6rwLTApiaqawpGBqoY1b4zNNTGwAw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>利用(xi,rti)(i=1,2,..m),我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域Rtj,j=1,2,…,J。其中J为叶子节点的个数。</p><p>针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyCHtHNTNtpZHNxboDKqMzy43MyLicZFOt8A46iajZMSHbEAW4UEMeoIhw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>这样就得到了本轮的决策树拟合函数如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8ly8Wty7SEqX3Z7MNpiaArS5uNYUu53sb4dp7TsHQMe5Rraw2ZjtbmH84g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>从而本轮最终得到的强学习器的表达式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyOz13MCp5uicnZkqmXQpMubJAuFndxSJ7fzycvBicyZdwnDgoez4ZXbBQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>通过损失函数的负梯度来拟合，找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p><p>传统模型中，我们定义一个固定结构的函数，然后通过样本训练拟合更新该函数的参数，获得最后的最优函数。</p><p>GBDT提升树并非如此。它是加法模型，是不定结构的函数，通过不断加入新的子函数来使得模型能更加拟合训练数据，直到最优。函数更新的迭代方式可以写作：<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOHhkSYuFVakKkzs8bV1G1x0kTAtekib1cxFnKxQ6Kic59f53ckjEnM8MQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)。所以如果要更快逼近最优的函数，我们就需要在正确的方向上添加子函数，这个“正确的方向”当然就是损失减少最快的方向。所以我们需要用损失函数<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOvzmypoOy2AgFtciavA7xoa2n0JWZd5X30lGibWLBSYHR4Mp3vQXc24xA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)对函数<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOq82zrPJcdR69oOdqjadV52MHoDXRUA3ickHfwRPMLwD8DJINtj20Fpg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)求导（注意不是对x求导），求得的导数，就是接下来<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOq82zrPJcdR69oOdqjadV52MHoDXRUA3ickHfwRPMLwD8DJINtj20Fpg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)需要弥补的方向。在上式中<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOwoQiciaNbtJwTJcKw0EcEwuEwkBAnh9cp72mIAFhOfXM5Wk86ywWorYg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">就是表示导数的拟合。</p><p>导数值跟损失函数的选择有关系。如果选择平方损失误差<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOS3I0b3IJ8V0xxjVob1ol6YvFKklAOsnqa1HlIdFicbPuzsnFfd9hPDg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">，那么它的导数就是：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOh5pz5Hy4euGM4ohUKWRQAAQn3z1l3QE7I1OCfrqGnbPo0rGBic8L2Vw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>令人惊喜的是这正是真实值和估计值之间的残差！ 这就是为什么谈到GBDT的时候，很多文章都提到“残差”的拟合，却没有说“梯度”的拟合。其实它们在平方损失误差条件下是一个意思！BTW，上面之所以用了<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOu0blrBa87g5KTo3JKNJG3bISFc303NjWothbmK3SsSs5ibIUn3nIH9g/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">是为了计算方便，常数项并不会影响平方损失误差，以及残差的比较。</p><p>现在让我们重新理解这个式子：<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOHhkSYuFVakKkzs8bV1G1x0kTAtekib1cxFnKxQ6Kic59f53ckjEnM8MQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>1）先求取一个拟合函数Fm-1(x)</p><p>2）用Fm-1(x)进行预测，计算预测值和实际值的残差</p><p>3）为了弥补上面的残差，用一个函数△F(x)来拟合这个残差</p><p>4）这样最终的函数就变成了<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOHhkSYuFVakKkzs8bV1G1x0kTAtekib1cxFnKxQ6Kic59f53ckjEnM8MQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)，其中Fm-1(x)用来拟合原数据，△F(x)用来拟合残差</p><p>5）如果目前还有较大的残差，则循环2)~4)，更新函数到Fm+1(x) , Fm+2(x), …..直到残差满足条件。</p><p>针对以上流程，我们用实例来说明</p><h2 id="5-提升树的生成过程"><a href="#5-提升树的生成过程" class="headerlink" title="5 提升树的生成过程"></a><strong>5 提升树的生成过程</strong></h2><p>有以下数据需要用回归，并要求平方损失误差小于0.2（这0.2就是我们人为设置的最优条件，否则训练可能会无休止地进行下去）时，可以停止建树：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcO5GxKf7OxmtUG46swUJwHNyUFv8VOOpj0ShaibKVlPciaPk7lk6O9l4DA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>第一棵树</strong></p><p><strong>1） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点：</strong></p><p>比如s=1.5,分割成了两个子集：<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOQeodYeygMxPDwt2E8fP7ic0rwiatLYvcag6VOas4WOvDxeA7H0bxHxiag/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"> 通过公式<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOXTFAGhmwVnVjIAib047xJn00uibicmZcZjxq5YKy6olDr62Eac625L0tA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">求平方损失误差</p><p>而其中<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOAvj5NaNw2gmlQ76SqdDOFpTycMKHRr3cyFlpMnWaJT3eF6HpHE54pQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)为各自子集的平均值<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcO8o2CmsIo8IgPBCRHeMeibaUsrWlfYUn1E72DQOdbq86GibuXhBRQJW2w/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">时，可以使得每个子集的平方损失误差最小。</p><p>求平均值为：<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOiaYE1zAZnMhO7xiayKZvQmDWFrepYQWftLc2NB5b6LuXpd289gvQOGFw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">)，进而求得平方损失误差为<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOZ1zYYIfBrETSG24JJyLwUYqWqVeIz7TV0l1Rn5DtQCfiaMiaBQUfkjuA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>同样的方法求得其它切分点的平方损失误差，列表入下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOvkj8fRtjicJNMdXIKRDpUawqevqdwWQSYgiamJTY2hcT7KESYOlW8EnA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>可见，当s=6.5时,<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOxveLkvcSxckAMlbqx4I6xtfnEOd8EL9IjZE8DXwCkpqWicR1aAf1MEw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">为所有切分点里平方损失误差最小的</p><p><strong>2) 选择切分点s=6.5构建第一颗回归树，各分支数值使用</strong></p><p><em><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOCng6RAzOkxCibd02ZnfLXHapLY1rxGWJiaXIA1vVyB5VvtBibiadZCvibkw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">：</em></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcO0sGV4htSbcmYA79VtOVKIick610xGZfoOyIWePX3FOT75yn6j551Uog/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>第一轮</strong>过后，我们提升树为:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOsDfeU5cr9IsT5c1rgN8mnibodsaT6KkiaPB67thj2dZrvvCm7828ocjA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>3) 求提升树拟合数据的残差和平方损失误差：</strong></p><p>提升树拟合数据的残差计算：<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOTRzru4dDaCMSFjszEicw3ibPgTKobG5jQL71BISIUeiciasnFUicqKic4E0A/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>各个点的计算结果：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOQYbumw3ZlTVFWP9ZzXcMkoazH2F75AoQZYWxkdPtpfh8IE9uPibRV4g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>提升树拟合数据的平方损失误差计算：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcO3M9WFJP7O5lJsyW8F5ajrLeOiak5iaRsBylu9NT3uIhSNqC6FRw80rqQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>大于0.2，则还需要继续建树。</p><p><strong>第二棵树</strong></p><p><strong>4) 确定需要拟合的训练数据为上一棵树的残差：</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOQYbumw3ZlTVFWP9ZzXcMkoazH2F75AoQZYWxkdPtpfh8IE9uPibRV4g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>5） 遍历各个切分点s=1.5,2.5,…,9.5找到平方损失误差最小值的切分点：</strong></p><p>同样的方法求得其它切分点的平方损失误差，列表入下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOAnbNhuZUsHQHo0F7GVl0GU3Michbmyia3b8iatic03xJ4mOdMIouPqmRyg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>可见，当s=3.5时,<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOX0uGb3louwe22OkRoQdbd3nk722bLU9WuNnFscqq5H3TlXxrjkfhVg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">为所有切分点里平方损失误差最小的</p><p><strong>6) 选择切分点s=3.5构建第二颗回归树，各分支数值使用</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOjJLm5bHOFULrY4xFYZZzWlfnENVicnxhQYOiaTd3hOWJJjIgt4cXibPwA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOamWhe3GALv8PnuCOca6w70P1lzYjY25KRYrWjVMwY6Pxn11QSwH6fg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>第二轮过后，我们提升树为:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOYeITOyhABGetdc4z7STdupyzS9fuFa6v95NjpP5he9nzecPDVKOVLA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>7) 求提升树拟合数据的残差和平方损失误差：</strong></p><p>提升树拟合数据的残差计算：<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOlmY5ic3Nfg5VPRFVSeup8xjGudqGdAzI3j5bNWslcRR62AxV2XRYKqQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>各个点的计算结果，同时对比初始值和上一颗树的残差：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOt6HhibiciaSQazVLKP3lv4kLvzLF5wfaV2nuCYohOJEU2VkI4WgnZC74w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>可以看见，随着树的增多，残差一直在减少。</p><p>到目前为止，提升树拟合数据的平方损失误差计算：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcO5Rzh6ibyicU5VfBHyhsePVC2wQ8LlJeDn8jWcWicKNiarRWtzSF5uprFhg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>多说一句，这里是从全局提升树的角度去计算损失，其实和上面第5）步中从最后一颗树的角度去计算损失，结果是一样的</p><p>目前损失大于0.2的阈值，还需要继续建树</p><p>… </p><p>… </p><p><strong>第六棵树</strong></p><p>到第六颗树的时候，我们已经累计获得了：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOsh5duEdgszicGk8UbiafAta5NnHyMNVLYOAiaT0Ju4DLGKibDYPteXYBUA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">     <img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOUVO2SLfYFcibbOuMyHjICibYBh2EJrIict8lrtBTZWmfCaBFVE628Lypw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOr1ChtQiaO9CXxNCCGSjUDHrz1iayesKv8vMSCGiavrcJhsRXfSyCicJ30A/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">     <img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcODicRF1aptDuvhgKNadZHkhGOsIre4xibUuG60I4miaUkw5LNfo71gS9xw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>此时提升树为：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOv42KBJrAXur7luickhNHjCagFicZAiaTspyoEaibwhNRz2vr5EjibCjSctg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>此时用<img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOiaxnBL9gfbJ7zfDblcMKKgbwsm6iaTbnfR1MGsva6fJ6ypBUkT9poxUw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">拟合训练数据的平方损失误差为：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOQR444ODwtuSps7BDcbJGPXg70DVN6u5kHLojyQ9qprb27slQnbDV9A/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>平方损失误差小于0.2的阈值，停止建树。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/mqaP0ypnYKjPEIdtO1Jevr25pRyOXYcOiaxnBL9gfbJ7zfDblcMKKgbwsm6iaTbnfR1MGsva6fJ6ypBUkT9poxUw/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">为我们最终所求的提升树。</p><h2 id="6-回归算法"><a href="#6-回归算法" class="headerlink" title="6 回归算法"></a>6 回归算法</h2><p><strong>输入：</strong> 最大迭代次数T, 损失函数L，训练样本集</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyodcqQg8f6LDBSria8Wa2zKKrJ313X0ulTbVHBx2cCNwBqdaQWWrT5ug/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>输出：</strong> 强学习器f(x)</p><p><strong>1）</strong> 初始化弱学习器</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyNt6z0iafAWflN2BF8dBd4nlZNC5icuhiaoyAeqQxmur7BN4SEp7cN3k1w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>2）</strong>对迭代轮数t=1,2,…T有：</p><p>　  <strong>a)</strong> 对样本i=1,2，…m，计算负梯度</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyMP27fEskIYa0Y00VyUqTGZLvXic6rwLTApiaqawpGBqoY1b4zNNTGwAw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>　  <strong>b)</strong> 利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,…,J。其中J为回归树t的叶子节点的个数。</p><p>　 <strong>c)</strong> 对叶子区域j =1,2,..J,计算最佳拟合值</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyobJ7nYicsGlUQibywvuCuXXYEIo2XNIVF6Qtz5FeQFaMGFCqYnSnYcSQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>   <strong>(d)</strong> 更新强学习器</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyOz13MCp5uicnZkqmXQpMubJAuFndxSJ7fzycvBicyZdwnDgoez4ZXbBQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p><strong>3）</strong> 得到强学习器f(x)的表达式</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyB0YrMT8hPj4HkNiacdM1iaBIXQgRP1YKxibibgMcCht1hSJooCuIfxMEfg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h2 id="7-分类算法"><a href="#7-分类算法" class="headerlink" title="7 分类算法"></a>7 分类算法</h2><p>GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。</p><p>为了解决这个问题，主要有两个方法，</p><p><strong>1）一个是用指数损失函数，此时GBDT退化为Adaboost算法。</strong></p><p><strong>2）另一种方法是用类似于逻辑回归的对数似然损失函数的方法。</strong></p><p>也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。</p><h3 id="7-1-二元分类算法"><a href="#7-1-二元分类算法" class="headerlink" title="7.1 二元分类算法"></a>7.1 二元分类算法</h3><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyor0u8iatLzt15YpczNnbOsoRr1sEr2RvP3jTWs8qQgAGZgrhYKhbiaEw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>其中y∈{−1,+1}。则此时的负梯度误差为</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyiaKu2ichtrUfyDRkFbwibz1WyxNxLK62ePp2OMyKcGd3eupg2jGptoJzg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyzmvLpfZt9ACruqtJct6Mdic2x1ibt92yducrmTWtCO5qg8XvDDmBIsLg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>由于上式比较难优化，我们一般使用近似值代替</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyd3gAujuSrODXLfhr0ODJ2QJ7r1AG9KY4FpMvsP8JHLw06qS4Xx3sUA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。</p><h3 id="7-2-多元分类算法"><a href="#7-2-多元分类算法" class="headerlink" title="7.2 多元分类算法"></a>7.2 <strong>多元分类算法</strong></h3><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lynUXdO0WoeZx3kmacQGpC0vj9ny3ageHT4BcLG4sxJ3PeUwZY2EaTvA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyfGo24kcicXMRRpfZrwUdy5MCEmttkCkL18kBibEryicNmlGiba4wgjBn8g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyjDA8kSib3lX4UPdal2YQNADJibOQdec9gyRod0oXd01h37WWet50QfXA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lytoTyickEpRJluwyooIWm65M2vbM8yVf2LRwMibMUacYzx3N9EHRN3UtA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>由于上式比较难优化，我们一般使用近似值代替</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyyw8DyXqxF36pAWYHEh7AdB0BHzy8OJvBCBaygzicdUUeclNrLheGN3w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p><h2 id="8-正则化"><a href="#8-正则化" class="headerlink" title="8 正则化"></a>8 <strong>正则化</strong></h2><p>和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。</p><p>GBDT的正则化主要有三种方式。</p><p><strong>第一种是和Adaboost类似的正则化项</strong>，即<strong>步长(learning rate)</strong>。定义为ν,对于前面的弱学习器的迭代</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyDzGSAZPzHsznm49bIdewQ5CibDjbjUK37E0BzhIe7Szcr0lRTx8Oib3A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>如果我们加上了正则化项，则有</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHCMaFT1BjrnWicmQzJOrs8lyhicpZnfvsEAUeINAqfLibtT0qNzVxy6LpDmbh6oBibmWOJhQgKNSXGK0g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>ν的取值范围为0&lt;ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p><p><strong>第二种正则化的方式是通过子采样比例（subsample）。</strong>取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。<strong>使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)</strong>。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 </p><p><strong>第三种是对于弱学习器即CART回归树进行正则化剪枝。</strong>在决策树原理篇里我们已经讲过，这里就不重复了</p><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9 总结"></a>9 总结</h2><p>GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p><p><strong>优点</strong></p><p><strong>1)</strong> 可以灵活处理各种类型的数据，包括连续值和离散值。</p><p><strong>2)</strong> 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p><p><strong>3）</strong>使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p><p><strong>缺点</strong></p><p><strong>1)</strong> 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GBDT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（09）：ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结</title>
      <link href="/2019/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8809%EF%BC%89%EF%BC%9AID3%E3%80%81C4.5%E3%80%81CART%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E3%80%81bagging%E3%80%81boosting%E3%80%81Adaboost%E3%80%81GBDT%E3%80%81xgboost%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
      <url>/2019/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8809%EF%BC%89%EF%BC%9AID3%E3%80%81C4.5%E3%80%81CART%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E3%80%81bagging%E3%80%81boosting%E3%80%81Adaboost%E3%80%81GBDT%E3%80%81xgboost%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>最近心血来潮，整理了一下和树有关的方法和模型，请多担待！</p><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a><strong>决策树</strong></h2><p>首先，决策树是一个有监督的分类模型，其本质是选择一个能带来最大信息增益的特征值进行树的分割，直到到达结束条件或者叶子结点纯度到达一定阈值。下图是决策树的一个简单例子</p><p><img src="https://pic1.zhimg.com/80/v2-a0af608723c15fc3d2ce3fc650cee4d4_hd.jpg" alt="img"></p><p>按照分割指标和分割方法，决策树的经典模型可以分为ID3、C4.5以及CART</p><h3 id="ID3：以信息增益为准则来选择最优划分属性"><a href="#ID3：以信息增益为准则来选择最优划分属性" class="headerlink" title="ID3：以信息增益为准则来选择最优划分属性"></a><strong>ID3：以信息增益为准则来选择最优划分属性</strong></h3><p>信息增益的计算要基于信息熵（度量样本集合纯度的指标）</p><p><img src="https://pic2.zhimg.com/80/v2-e0ce667594401d2f8daf3ee6a7da9151_hd.jpg" alt="img">信息熵越小，数据集X的纯度越大</p><p>因此，假设于数据集D上建立决策树，数据有K个类别：</p><p><img src="https://pic1.zhimg.com/80/v2-f6d10699fdbe216617836c7e8732ba58_hd.jpg" alt="img"></p><p>公式（1）中：</p><p><img src="https://pic2.zhimg.com/80/v2-181bbc695d6de40eff56d86518d84f29_hd.jpg" alt="img">表示第k类样本的数据占数据集D样本总数的比例</p><p>公式（2）表示的是以特征A作为分割的属性，得到的信息熵：</p><p>Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合</p><p>因此，该公式求的是以属性A为划分，n个分支的信息熵总和</p><p>公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好</p><p>但是这种分割算法存在一定的<strong>缺陷</strong>：</p><p>假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，<strong>ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。</strong>为减小这一影响，有学者提出C4.5的分类算法。</p><h3 id="C4-5：基于信息增益率准则选择最优分割属性"><a href="#C4-5：基于信息增益率准则选择最优分割属性" class="headerlink" title="C4.5：基于信息增益率准则选择最优分割属性"></a><strong>C4.5：基于信息增益率准则选择最优分割属性</strong></h3><p>信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的属性。</p><p><img src="https://pic2.zhimg.com/80/v2-c35719627c479737cb680c3f4d8cdf6d_hd.jpg" alt="img"></p><p>上式，分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征。因此C4.5决策树先从候选划分属性中找出<strong>信息增益高于平均水平</strong>的属性，在从中选择<strong>增益率最高</strong>的。</p><p>对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。</p><h3 id="CART：以基尼系数为准则选择最优划分属性"><a href="#CART：以基尼系数为准则选择最优划分属性" class="headerlink" title="CART：以基尼系数为准则选择最优划分属性"></a><strong>CART：以基尼系数为准则选择最优划分属性</strong></h3><p>CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。</p><p><img src="https://pic3.zhimg.com/80/v2-79214da261d75829046953ab9cb8b03a_hd.jpg" alt="img">Di表示以A是属性值划分成n个分支里的数目</p><p>Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。</p><h3 id="分类树和回归树"><a href="#分类树和回归树" class="headerlink" title="分类树和回归树"></a><strong>分类树和回归树</strong></h3><p>提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。</p><p>作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。</p><p>回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a><strong>随机森林</strong></h2><p>在讲随机森林之前，我们需要补充一点<strong>组合分类器</strong>的概念，将多个分类器的结果进行多票表决或者是取平均值，以此作为最终的结果。</p><p>1、构建组合分类器的好处：</p><p>（1）、提升模型精度：整合各个模型的分类结果，得到更合理的决策边界，减少整体错误，实现更好的分类效果；</p><p><img src="https://pic1.zhimg.com/80/v2-8fc5ff86df06a3c4d3f7d4b055642224_hd.jpg" alt="img"></p><p>（2）、处理过大或过小的数据集：数据集较大时，可以将数据集划分成多个子集，对子集构建分类器；数据集较小时，可通过多种抽样方式（bootstrap）从原始数据集抽样产生多组不同的数据集，构建分类器。</p><p>（3）、若决策边界过于复杂，则线性模型不能很好地描述真实情况。因此先对于特定区域的数据集，训练多个线性分类器，再将它们集成。</p><p><img src="https://pic4.zhimg.com/80/v2-fdd9a957b915ec7711c68af1b365b30f_hd.jpg" alt="img"></p><p>（4）、比较适合处理多源异构数据（存储方式不同（关系型、非关系型），类别不同（时序型、离散型、连续型、网络结构数据））</p><p><img src="https://pic3.zhimg.com/80/v2-2743e0131bdb39de81f01649bc0f3b4e_hd.jpg" alt="img"></p><p>随机森林是一个典型的多个决策树的组合分类器。主要包括两个方面：数据的随机性选取，以及待选特征的随机选取。</p><p>（1）、数据的随机选取：<br>第一，从原始的数据集中采取有放回的抽样（bootstrap），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。<br>第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。</p><p><img src="https://pic1.zhimg.com/80/v2-a1c3ce43528dbc274be8952c06d2b9b4_hd.jpg" alt="img"></p><p>（2）、待选特征的随机选取：<br>与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p><p><img src="https://pic1.zhimg.com/80/v2-569009cc3ccd3e9922b77c1e4cbf4ca0_hd.jpg" alt="img">组合树示例图</p><h2 id="GBDT和xgboost"><a href="#GBDT和xgboost" class="headerlink" title="GBDT和xgboost"></a><strong>GBDT和xgboost</strong></h2><h3 id="bagging和boosting"><a href="#bagging和boosting" class="headerlink" title="bagging和boosting"></a><strong>bagging和boosting</strong></h3><p>Bagging的思想比较简单，即每一次从原始数据中根据<strong>均匀概率分布有放回的抽取和原始数据大小相同的样本集合</strong>，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。</p><p>boosting的每一次抽样的<strong>样本分布都是不一样的</strong>。每一次迭代，都根据上一次迭代的结果，<strong>增加被错误分类的样本的权重</strong>，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个<strong>不断学习的过程，也是一个不断提升</strong>的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。</p><p><img src="https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_hd.jpg" alt="img">boosting算法结构图</p><p>拿著名的<strong>Adaboost算法</strong>举例：</p><p><img src="https://pic4.zhimg.com/80/v2-42d79b3f2d50c86679f7c9bbc088d5f7_hd.jpg" alt="img">我们有一个数据集，样本大小为N，每一个样本对应一个原始标签起初，我们初始化样本的权重为1/N</p><p><img src="https://pic2.zhimg.com/80/v2-3f8463843d3f88642a288666ecb94ff1_hd.jpg" alt="img">em计算的是当前数据下，模型的分类误差率，模型的系数值是基于分类误差率的</p><p><img src="https://pic1.zhimg.com/80/v2-8d2590f60815d6389572d4f09ed9a658_hd.jpg" alt="img">根据模型的分类结果，更新原始数据中数据的分布，增加被错分的数据被抽中的概率，以便下一次迭代的时候能被模型重新训练</p><p><img src="https://pic1.zhimg.com/80/v2-7000a239700933215671f4f66066ddd4_hd.jpg" alt="img">最终的分类器是各个基分类器的组合</p><h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>GBDT是以决策树（CART）为基学习器的GB算法，是<strong>迭代树</strong>，而不是分类树。Boost是”提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。有了前面Adaboost的铺垫，大家应该能很容易理解大体思想。</p><p><img src="https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_hd.jpg" alt="img"></p><p>GBDT的核心就在于：<strong>每一棵树学的是之前所有树结论和的残差</strong>，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。</p><p><img src="https://pic3.zhimg.com/80/v2-a384924b89b1bdd581cef7d75b56e226_hd.jpg" alt="img"></p><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a><strong>xgboost</strong></h3><p>Xgboost相比于GBDT来说，更加有效应用了<strong>数值优化，最重要是对损失函数</strong>（预测值和真实值的误差）<strong>变得更复杂</strong>。目标函数依然是所有树的预测值相加等于预测值。</p><p>损失函数如下，引入了一阶导数，二阶导数。：</p><p><img src="https://pic2.zhimg.com/80/v2-1c0706e463f78b6036b3923048ac9149_hd.jpg" alt="img">好的模型需要具备两个基本要素：一是要有好的精度（即好的拟合程度），二是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项）</p><p>常用的误差项有平方误差和逻辑斯蒂误差，常见的惩罚项有l1，l2正则，l1正则是将模型各个元素进行求和，l2正则是对元素求平方。</p><p><img src="https://pic4.zhimg.com/80/v2-a9b82954ae62e9e6da256c69ba22d38b_hd.jpg" alt="img">每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差</p><p><img src="https://pic1.zhimg.com/80/v2-f0cd240fcc70e7615dae7c2a29856bfc_hd.jpg" alt="img">目标函数如上图，最后一行画圈部分实际上就是预测值和真实值之间的残差</p><p>先对训练误差进行展开：</p><p><img src="https://pic1.zhimg.com/80/v2-9f9f93d4a9d618d14201836d8f45a918_hd.jpg" alt="img">xgboost则对代价函数进行了二阶泰勒展开，同时用到了残差平方和的一阶和二阶导数</p><p>再研究目标函数中的正则项：</p><p><img src="https://pic3.zhimg.com/80/v2-142ca609c9ff3dc2df877a00c30756ca_hd.jpg" alt="img"></p><p>树的复杂度可以用树的分支数目来衡量，树的分支我们可以用叶子结点的数量来表示</p><p>那么树的复杂度式子：右边第一项是叶子结点的数量T，第二项是树的叶子结点权重w的l2正则化，正则化是为了防止叶子结点过多</p><p>此时，每一次迭代，相当于在原有模型中增加一棵树，目标函数中，我们用wq（x）表示一棵树，包括了树的结构以及叶子结点的权重，w表示权重（反映预测的概率），q表示样本所在的索引号（反映树的结构）</p><p>将最终得到的目标函数对参数w求导，带回目标函数，可知目标函数值由红色方框部分决定：</p><p><img src="https://pic1.zhimg.com/80/v2-c7ab2fcfd3196dbc0bce05d17b11d220_hd.jpg" alt="img"></p><p>因此，xgboost的迭代是以下图中gain式子定义的指标选择最优分割点的：</p><p><img src="https://pic4.zhimg.com/80/v2-d0cf0063c23679e711146f861d36fc17_hd.jpg" alt="img"></p><p>那么如何得到优秀的组合树呢？</p><p>一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。</p><p>另一种方法：XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照<strong>特征值的密度分布</strong>，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将<strong>bucket边界上的特征值</strong>作为split<br>point的候选，<strong>遍历所有的候选分裂点</strong>来找到最佳分裂点。</p><p>上图近似算法公式的解释：将特征k的特征值进行排序，计算特征值分布，rk（z）表示的是对于特征k而言，其特征值小于z的权重之和占总权重的比例，代表了这些特征值的重要程度，我们按照这个比例计算公式，将特征值分成若干个bucket，每个bucket的比例相同，选取这几类特征值的边界作为划分候选点，构成候选集；选择候选集的条件是要使得相邻的两个候选分裂节点差值小于某个阈值。</p><p>综合以上的解说，我们可以得到xgboost相比于GBDT的创新之处：</p><p>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p><ul><li>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了<strong>二阶泰勒展开，同时用到了一阶和二阶导数</strong>。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</li><li>xgboost在<strong>代价函数里加入了正则项，用于控制模型的复杂度</strong>。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</li><li><strong>Shrinkage（缩减），相当于学习速率（xgboost中的eta）</strong>。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象；</li><li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</li><li>忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销</li><li>指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。</li><li>并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>极客时间《数据分析45讲总结》</title>
      <link href="/2019/12/17/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4%E3%80%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%9045%E8%AE%B2%E6%80%BB%E7%BB%93%E3%80%8B/"/>
      <url>/2019/12/17/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4%E3%80%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%9045%E8%AE%B2%E6%80%BB%E7%BB%93%E3%80%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h2><p>该讲主要引导读者从全局去了解什么是数据分析？为什么做数据分析？怎么去做数据分析？答案就是：掌握数据，就是掌握规律。当你了解了市场数据，对它进行分析，就可以得到市场规律。当你掌握了产品自身的数据，对它进行分析，就可以了解产品的用户来源、用户画像等等。所以说数据是个全新的视角。数据分析如此重要，它不仅是新时代的“数据结构 + 算法”，也更是企业争夺人才的高地。 谈到数据分析，我们一般都会从3个方面入手：</p><blockquote><p>数据采集 – 数据源，我们要用的原材料</p><p>数据挖掘 – 它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值（所谓的商业智能BI）</p><p>数据的可视化 – 数据领域中的万金油，直观了解数据分析结构</p></blockquote><p>　　数据分析的三驾马车的关系如下：</p><p>　　<img src="https://img2018.cnblogs.com/blog/1169428/201901/1169428-20190119101700106-215720167.png" alt="img"></p><p>　　下面来大致认识下这三驾马车：</p><h2 id="2-数据采集："><a href="#2-数据采集：" class="headerlink" title="2.数据采集："></a>2.数据采集：</h2><p>数据的采集，主要是和数据打交道，用工具对数据进行采集，常用的数据源，如何获取它们。在专栏里，后续会将介绍如何掌握“八爪鱼”这个自动抓取的神器，它可以帮你抓取 99% 的页面源。也会教读者如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如“王祖贤”的海报，还能自动给微博加粉丝，让你掌握自动化的快感。</p><p>　　<img src="https://img2018.cnblogs.com/blog/1169428/201901/1169428-20190119102510649-1229981320.png" alt="img"></p><h2 id="3-数据挖掘："><a href="#3-数据挖掘：" class="headerlink" title="3.数据挖掘："></a>3.数据挖掘：</h2><p>数据挖掘，它可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。</p><p>掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的。</p><p>　　<img src="https://img2018.cnblogs.com/blog/1169428/201901/1169428-20190119102828928-1543716662.png" alt="img"></p><h2 id="4-数据可视化"><a href="#4-数据可视化" class="headerlink" title="4.数据可视化"></a>4.数据可视化</h2><p> 　为什么说数据要可视化，因为数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。</p><p> 数据可视化的两种方法：</p><ul><li><p>　Python ：在 Python 对数据进行清洗、挖掘的过程中，很多的库可以使用，像 Matplotlib、Seaborn 等第三方库进行呈现。</p></li><li><p>　第三方工具：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。</p><p>　<img src="https://img2018.cnblogs.com/blog/1169428/201901/1169428-20190119103259753-1199946581.png" alt="img"></p></li></ul><p>数据分析包括数据采集、数据挖掘、数据可视化这三个部分。乍看你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点“高深莫测”，掌握起来是不是会吃力。其实这些都是不必要的烦恼。个人觉得只要内心笃定，认为自己一定能做成，学成，其他一切都是“纸老虎”哈。</p><p>再说下，陈博在文章中提到的如何来快速掌握数据分析，核心就是<strong>认知</strong>。我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程就是认知升级的过程。</p><p>　　<img src="https://img2018.cnblogs.com/blog/1169428/201901/1169428-20190119103852266-1630283111.png" alt="img"></p><p>　　我本人也是很赞同这种说法，简单一句就是“知行合一”</p><p>　　<strong>总结</strong></p><ul><li><strong>记录下你每天的认知</strong>　　</li><li><strong>这些认知对应工具的哪些操作</strong></li><li><strong>做更多练习来巩固你的认知</strong></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Data Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习系列之决策树算法（01）：决策树特征选择</title>
      <link href="/2019/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8801%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
      <url>/2019/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%EF%BC%8801%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是特征选择"><a href="#1-什么是特征选择" class="headerlink" title="1.什么是特征选择"></a>1.什么是特征选择</h2><p>【特征选择】顾名思义就是对特征进行选择，以达到提高决策树学习的效率的目的。</p><p>【那么选择的是什么样的特征呢？】这里我们选择的特征需要是<strong>对训练数据有分类能力</strong>的特征，如果一个特征参与分类与否和随机分类的结果差别不大的话，我们就说这个特征<strong>没有分类能力</strong>，舍去这个特征对学习的精度不会有特别大的影响。</p><blockquote><p>特征选择是决定用哪个特征来划分特征空间。</p></blockquote><p>比如女生找男朋友，可能这个女生<strong>首先</strong>会问「这个男生帅不帅」，其次再是「身高如何」、「有无房子」、「收入区间」、「做什么工作」等等，那么「帅否」这个特征就是这位女生心中有着最好分类能力的特征了</p><p>【那怎么判断哪个特征有更好的分类能力呢？】这时候【<strong>信息增益</strong>】就要出场了。</p><h2 id="2-信息增益"><a href="#2-信息增益" class="headerlink" title="2.信息增益"></a>2.信息增益</h2><p>为了解释什么是信息增益，我们首先要讲解一下什么是【<strong>熵（entropy）</strong>】</p><h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a><strong>熵（Entropy）</strong></h3><blockquote><p>在热力学与化学中：</p><p>熵是一种测量在动力学方面【<strong>不能做功的能量的总数</strong>】，当总体熵增加，其<strong>做功能力</strong>也下降，熵的度量是<strong>能量退化</strong>的指标。</p></blockquote><p>1948 年，香农把热力学中的熵引入到信息论中，称为<strong>香农熵</strong>。根据维基百科的描述：</p><blockquote><p>在信息论中，熵是接收的<strong>每条消息</strong>中包含的<strong>信息的平均量</strong>。</p></blockquote><p>更一般的，【<strong>熵表示随机变量的不确定性</strong>】。假设一个有限取值的离散随机变量 X 的概率分布如下：</p><p><img src="https://www.zhihu.com/equation?tex=P%28X+%3D+x_i%29+%3D+p_i%2C%5C+%5C+%5C+%5C+i+%3D+1%2C+2%2C+%5Ccdots%2C+n" alt="[公式]"></p><p>那么它的熵定义为：</p><p><img src="https://www.zhihu.com/equation?tex=H%28X%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+p_i+%5Clog_%7Bb%7D+p_i" alt="[公式]"></p><p>上式中的 b 通常取 2 或者自然对数 <em>e</em>，这时熵的单位就分别称为比特（bit）或纳特（nat），这也是信息论中，信息量的单位。</p><p>从上式中，我们可以看到，<strong>熵与 X 的取值是没有关系的，它只与 X 的分布有关</strong>，所以 H 也可以写作 p 的函数：</p><p><img src="https://www.zhihu.com/equation?tex=H%28p%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+p_i%5Clog+p_i" alt="[公式]"></p><p>我们现在来看两个随机变量的情况。</p><p>假设随机变量 (X, Y) 的联合概率分布如下：</p><p><img src="https://www.zhihu.com/equation?tex=P%28X+%3D+x_i%2C+Y+%3D+y_j%29+%3D+p_%7Bij%7D%2C%5C+%5C+%5C+%5C+i+%3D+1%2C+2%2C+%5Ccdots%2C+n%3B%5C+j+%3D+1%2C+2%2C+%5Ccdots%2C+m" alt="[公式]"></p><p>我们使用<strong>条件熵（conditional entropy）H(Y|X)</strong>来度量在已知随机变量 X 的条件下随机变量 Y 的不确定性。</p><blockquote><p>条件熵定义为：X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。</p></blockquote><p>是不是看晕了，没关系，我们来看数学公式，这才是最简单直接让你晕过去的方法：</p><p><img src="https://www.zhihu.com/equation?tex=H%28Y%7CX%29+%3D+%5Csum_%7Bi%3D1%7D%5En+p_i+H%28Y%7CX%3Dx_i%29%2C%5C+%5C+%5C+%5C+p_i+%3D+P%28X%3Dx_i%29%2C%5C+i+%3D+1%2C+2%2C+%5Ccdots%2C+n" alt="[公式]"></p><p>有了上面的公式以后，条件熵的定义就非常容易理解了。</p><p>那么这些奇奇怪怪的熵又和我们要讲的信息增益有什么关系呢？</p><h3 id="信息增益的定义与信息增益算法"><a href="#信息增益的定义与信息增益算法" class="headerlink" title="信息增益的定义与信息增益算法"></a>信息增益的定义与信息增益算法</h3><p>既然熵是信息量的一种度量，那么信息增益就是熵的增加咯？</p><p>没错，由于熵表示不确定性，严格来说，<strong>信息增益（information gain）表示的是「得知了特征 X 的信息之后，类别 Y 的信息的不确定性减少的程度」</strong>。</p><p>我们给出信息增益的最终定义：</p><blockquote><p>特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为，集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差。</p></blockquote><p><img src="https://www.zhihu.com/equation?tex=g%28D%2C+A%29+%3D+H%28D%29+-+H%28D%7CA%29" alt="[公式]"></p><p><em>这里你只要知道经验熵和经验条件熵就是依据经验（由数据估计特别是极大似然估计）得出来的熵就可以了。</em></p><p>假设我们有一个训练集 D 和一个特征 A，那么，经验熵 H(D) 就是对 D 进行分类的不确定性，经验条件熵 H(D|A) 就是给定 A 后，对 D 分类的不确定性，经验熵 H(D) 与经验条件熵 H(D|A) 的差就是信息增益。</p><p>很明显的，不同的特征有不同的信息增益，信息增益大的特征分类能力更强。我们就是要根据信息增益来选择特征。</p><blockquote><p><strong>ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要</strong></p><p><strong>信息熵体现了信息的不确定程度，熵越大表示特征越不稳定，对于此次的分类，越大表示类别之间的数据差别越大</strong></p><p><strong>条件熵体现了根据该特征分类后的不确定程度，越小说明分类后越稳定</strong></p><p><strong>信息增益=信息熵-条件熵，越大说明熵的变化越大，熵的变化越大越有利于分类</strong></p></blockquote><p>下面我们给出信息增益的算法。</p><p>首先对数据做一些介绍：</p><ul><li>假设我们有一个训练集 D，训练集的总的样本个数即样本容量为 |D|，最后的结果有 K 个类别，每个类别表示为 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%7CC_k%7C" alt="[公式]"> 为属于这个类的样本的个数，很显然 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bk%3D1%7D%5EK+%7CC_k%7C+%3D+%7CD%7C" alt="[公式]"> 。</li><li>再假设我们有一个特征叫 A，A 有 n 个不同的取值 <img src="https://www.zhihu.com/equation?tex=%5C%7Ba_1%2C+a_2%2C+%5Ccdots%2C+a_n%5C%7D" alt="[公式]"> ，那么根据 A 我们可以将 D 分成 n 个子集，每个子集表示为 <img src="https://www.zhihu.com/equation?tex=D_i" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%7CD_i%7C" alt="[公式]"> 是这个子集的样本个数，很显然 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En+%7CD_i%7C+%3D+%7CD%7C" alt="[公式]"> 。</li><li>我们把 <img src="https://www.zhihu.com/equation?tex=D_i" alt="[公式]"> 中属于类别 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> 的集合称作 <img src="https://www.zhihu.com/equation?tex=D_%7Bik%7D%2C%5C+D_%7Bik%7D+%3D+D_i+%5Cbigcap+C_k" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%7CD_%7Bik%7D%7C" alt="[公式]"> 是其样本个数。</li></ul><p>信息增益的计算就分为如下几个步骤：</p><ol><li>计算 D 的经验熵 H(D)：</li></ol><p><img src="https://www.zhihu.com/equation?tex=H%28D%29+%3D+-%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D+%5Clog_2+%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D" alt="[公式]"></p><p>\2. 计算 A 对 D 的经验条件熵 H(D|A)：</p><p><img src="https://www.zhihu.com/equation?tex=H%28D%7CA%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+H%28D_i%29+%3D+-+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D+%5Clog_2+%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D" alt="[公式]"></p><p>\3. 计算信息增益 g(D, A)：</p><p><img src="https://www.zhihu.com/equation?tex=g%28D%2C+A%29+%3D+H%28D%29+-+H%28D%7CA%29" alt="[公式]"></p><h2 id="3-信息增益比"><a href="#3-信息增益比" class="headerlink" title="3.信息增益比"></a>3.信息增益比</h2><p>看到这个小标题，可能有人会问，信息增益我知道了，信息增益比又是个什么玩意儿？</p><p>按照经验来看，【<strong>以信息增益准则来选择划分数据集的特征，其实倾向于选择有更多取值的特征，而有时这种倾向会在决策树的构造时带来一定的误差</strong>】。</p><p><strong>ps：信息增益体现了特征的重要性，信息增益越大说明特征越重要。</strong>类别越多代表特征越不确定，即熵越多，类别的信息增益越小。</p><p>为了校正这一误差，我们引入了【<strong>信息增益比（information gain ratio）</strong>】，又叫做信息增益率，它的定义如下：</p><p>特征 A 对训练数据集 D 的信息增益比 <img src="https://www.zhihu.com/equation?tex=g_R%28D%2C+A%29" alt="[公式]"> 定义为其信息增益 <img src="https://www.zhihu.com/equation?tex=g%28D%2C+A%29" alt="[公式]"> 与训练数据集 D 关于特征 A 的值的熵 <img src="https://www.zhihu.com/equation?tex=H_A%28D%29" alt="[公式]"> 之比。</p><p><img src="https://www.zhihu.com/equation?tex=g_R%28D%2C+A%29+%3D+%5Cfrac%7Bg%28D%2C+A%29%7D%7BH_A%28D%29%7D" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=H_A%28D%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+%5Clog_2+%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D" alt="[公式]"> ，n 是 A 取值的个数。</p><p>两个经典的决策树算法 ID3 算法和 C4.5 算法，分别会采用信息增益和信息增益比作为特征选择的依据。</p><h2 id="4-ID3-：-最大信息增益"><a href="#4-ID3-：-最大信息增益" class="headerlink" title="4. ID3 ： 最大信息增益"></a>4. ID3 ： 最大信息增益</h2><blockquote><p>ID3以信息增益为准则来选择最优划分属性</p></blockquote><p>信息增益的计算要基于信息熵（度量样本集合纯度的指标）</p><p><img src="https://pic2.zhimg.com/80/v2-e0ce667594401d2f8daf3ee6a7da9151_hd.jpg" alt="img">信息熵越小，数据集X的纯度越大</p><p>因此，假设于数据集D上建立决策树，数据有K个类别：</p><p><img src="https://pic1.zhimg.com/80/v2-f6d10699fdbe216617836c7e8732ba58_hd.jpg" alt="img"></p><p>公式（1）中：</p><p><img src="https://pic2.zhimg.com/80/v2-181bbc695d6de40eff56d86518d84f29_hd.jpg" alt="img">表示第k类样本的数据占数据集D样本总数的比例</p><p>公式（2）表示的是以特征A作为分割的属性，得到的信息熵：</p><p>Di表示的是以属性A为划分，分成n个分支，第i个分支的节点集合</p><p>因此，该公式求的是以属性A为划分，n个分支的信息熵总和</p><p>公式（3）为分割后与分割前的信息熵的差值，也就是信息增益，越大越好</p><p>但是这种分割算法存在一定的<strong>缺陷</strong>：</p><blockquote><p>假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，<strong>ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。</strong>为减小这一影响，有学者提出C4.5的分类算法。</p></blockquote><h2 id="5-C4-5-：最大信息增益率"><a href="#5-C4-5-：最大信息增益率" class="headerlink" title="5. C4.5 ：最大信息增益率"></a>5. C4.5 ：<strong>最大信息增益率</strong></h2><blockquote><p>C4.5基于信息增益率准则选择最优分割属性的算法</p></blockquote><p>信息增益比率通过引入一个被称作【<strong>分裂信息(Split information)</strong>】的项来惩罚取值较多的属性。</p><p><img src="https://pic2.zhimg.com/80/v2-c35719627c479737cb680c3f4d8cdf6d_hd.jpg" alt="img"></p><p>上式，<strong>分子计算与ID3一样，分母是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小，这样就可以避免模型偏好特征值多的属性，但是聪明的人一看就会发现，如果简单的按照这个规则来分割，模型又会偏向特征数少的特征</strong>。因此C4.5决策树先从候选划分属性中找出<strong>信息增益高于平均水平</strong>的属性，在从中选择<strong>增益率最高</strong>的。</p><p>对于连续值属性来说，可取值数目不再有限，因此可以采用<strong>离散化技术</strong>（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。</p><h2 id="6-CART-：最小基尼指数"><a href="#6-CART-：最小基尼指数" class="headerlink" title="6.CART ：最小基尼指数"></a>6.CART ：<strong>最小基尼指数</strong></h2><blockquote><p>CART以基尼系数为准则选择最优划分属性，可以应用于分类和回归</p></blockquote><p>CART是一棵<strong>二叉树</strong>，采用【<strong>二元切分法</strong>】，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，<strong>既可以用于分类也可以用于回归</strong>。CART分类时，使用<strong>基尼指数（Gini）</strong>来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。<strong>CART中每一次迭代都会降低GINI系数。</strong></p><p><img src="https://pic3.zhimg.com/80/v2-79214da261d75829046953ab9cb8b03a_hd.jpg" alt="img">Di表示以A是属性值划分成n个分支里的数目</p><p>Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后<strong>基尼指数最小的属性作为最优化分属性。</strong></p><h3 id="7-分类树和回归树"><a href="#7-分类树和回归树" class="headerlink" title="7.分类树和回归树"></a>7.<strong>分类树和回归树</strong></h3><p>提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。</p><p>作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。</p><p>回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decision Tree </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
