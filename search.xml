<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>xgboost算法模型输出的解释</title>
      <link href="/2019/12/16/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/"/>
      <url>/2019/12/16/xgboost%E7%AE%97%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p> 近来, 在python环境下使用xgboost算法作若干的机器学习任务, 在这个过程中也使用了其内置的函数来可视化树的结果, 但对leaf value的值一知半解; 同时, 也遇到过使用xgboost 内置的predict 对测试集进行打分预测, 发现若干样本集的输出分值是一样的. 这个问题该怎么解释呢? 通过翻阅Stack Overflow 上的相关问题, 以及搜索到的github上的issue回答, 应该算初步对这个问题有了一定的理解。</p><h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2. 数据集"></a>2. 数据集</h2><p> 在这里, 使用经典的鸢尾花的数据来说明. 使用二分类的问题来说明, 故在这里只取前100行的数据.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"> </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">data = iris.data[:<span class="number">100</span>]</span><br><span class="line"><span class="keyword">print</span> data.shape</span><br><span class="line"><span class="comment">#(100L, 4L)</span></span><br><span class="line"><span class="comment">#一共有100个样本数据, 维度为4维</span></span><br><span class="line"> </span><br><span class="line">label = iris.target[:<span class="number">100</span>]</span><br><span class="line"><span class="keyword">print</span> label</span><br><span class="line"><span class="comment">#正好选取label为0和1的数据</span></span><br><span class="line">[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="3-训练集与测试集"><a href="#3-训练集与测试集" class="headerlink" title="3. 训练集与测试集"></a>3. 训练集与测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"> </span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data, label, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="4-Xgboost建模"><a href="#4-Xgboost建模" class="headerlink" title="4. Xgboost建模"></a>4. Xgboost建模</h2><h3 id="4-1-模型初始化设置"><a href="#4-1-模型初始化设置" class="headerlink" title="4.1 模型初始化设置"></a>4.1 模型初始化设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">dtrain=xgb.DMatrix(train_x,label=train_y)</span><br><span class="line">dtest=xgb.DMatrix(test_x)</span><br><span class="line"> </span><br><span class="line">params=&#123;<span class="string">'booster'</span>:<span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'binary:logistic'</span>,</span><br><span class="line">    <span class="string">'eval_metric'</span>: <span class="string">'auc'</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="string">'lambda'</span>:<span class="number">10</span>,</span><br><span class="line">    <span class="string">'subsample'</span>:<span class="number">0.75</span>,</span><br><span class="line">    <span class="string">'colsample_bytree'</span>:<span class="number">0.75</span>,</span><br><span class="line">    <span class="string">'min_child_weight'</span>:<span class="number">2</span>,</span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.025</span>,</span><br><span class="line">    <span class="string">'seed'</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">'nthread'</span>:<span class="number">8</span>,</span><br><span class="line">     <span class="string">'silent'</span>:<span class="number">1</span>&#125;</span><br><span class="line"> </span><br><span class="line">watchlist = [(dtrain,<span class="string">'train'</span>)]</span><br></pre></td></tr></table></figure><h3 id="4-2-建模与预测"><a href="#4-2-建模与预测" class="headerlink" title="4.2 建模与预测"></a>4.2 建模与预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bst=xgb.train(params,dtrain,num_boost_round=<span class="number">100</span>,evals=watchlist)</span><br><span class="line"> </span><br><span class="line">ypred=bst.predict(dtest)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 设置阈值, 输出一些评价指标</span></span><br><span class="line">y_pred = (ypred &gt;= <span class="number">0.5</span>)*<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> <span class="string">'AUC: %.4f'</span> % metrics.roc_auc_score(test_y,ypred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'ACC: %.4f'</span> % metrics.accuracy_score(test_y,y_pred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Recall: %.4f'</span> % metrics.recall_score(test_y,y_pred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'F1-score: %.4f'</span> %metrics.f1_score(test_y,y_pred)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Precesion: %.4f'</span> %metrics.precision_score(test_y,y_pred)</span><br><span class="line">metrics.confusion_matrix(test_y,y_pred)</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">AUC: <span class="number">1.0000</span></span><br><span class="line">ACC: <span class="number">1.0000</span></span><br><span class="line">Recall: <span class="number">1.0000</span></span><br><span class="line">F1-score: <span class="number">1.0000</span></span><br><span class="line">Precesion: <span class="number">1.0000</span></span><br><span class="line">array([[<span class="number">13</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>, <span class="number">12</span>]], dtype=int64)</span><br></pre></td></tr></table></figure><p>Yeah, 完美的模型, 完美的预测!</p><h3 id="4-3-可视化输出"><a href="#4-3-可视化输出" class="headerlink" title="4.3 可视化输出"></a>4.3 可视化输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对于预测的输出有三种方式</span></span><br><span class="line">?bst.predict</span><br><span class="line">Signature: bst.predict(data, output_margin=<span class="literal">False</span>, ntree_limit=<span class="number">0</span>, pred_leaf=<span class="literal">False</span>, pred_contribs=<span class="literal">False</span>, approx_contribs=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line">pred_leaf : bool</span><br><span class="line">    When this option <span class="keyword">is</span> on, the output will be a matrix of (nsample, ntrees)</span><br><span class="line">    <span class="keyword">with</span> each record indicating the predicted leaf index of each sample <span class="keyword">in</span> each tree.</span><br><span class="line">    Note that the leaf index of a tree <span class="keyword">is</span> unique per tree, so you may find leaf <span class="number">1</span></span><br><span class="line">    <span class="keyword">in</span> both tree <span class="number">1</span> <span class="keyword">and</span> tree <span class="number">0.</span></span><br><span class="line"> </span><br><span class="line">pred_contribs : bool</span><br><span class="line">    When this option <span class="keyword">is</span> on, the output will be a matrix of (nsample, nfeats+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">with</span> each record indicating the feature contributions (SHAP values) <span class="keyword">for</span> that</span><br><span class="line">    prediction. The sum of all feature contributions <span class="keyword">is</span> equal to the prediction.</span><br><span class="line">    Note that the bias <span class="keyword">is</span> added <span class="keyword">as</span> the final column, on top of the regular features.</span><br></pre></td></tr></table></figure><h4 id="4-3-1-得分"><a href="#4-3-1-得分" class="headerlink" title="4.3.1 得分"></a>4.3.1 得分</h4><p>默认的输出就是得分, 这没什么好说的, 直接上code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ypred = bst.predict(dtest)</span><br><span class="line">ypred</span><br></pre></td></tr></table></figure><p>Out[32]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,</span><br><span class="line">        <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,</span><br><span class="line">        <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,</span><br><span class="line">        <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,  <span class="number">0.20081411</span>,</span><br><span class="line">        <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>,  <span class="number">0.80391562</span>,  <span class="number">0.20081411</span>], dtype=float32)</span><br></pre></td></tr></table></figure><p>在这里, 就可以观察到文章最开始遇到的问题: 为什么得分几乎都是一样的值? 先不急, 看看另外两种输出.</p><h4 id="4-3-2-所属的叶子节点"><a href="#4-3-2-所属的叶子节点" class="headerlink" title="4.3.2 所属的叶子节点"></a>4.3.2 所属的叶子节点</h4><p>当设置<code>pred_leaf=True</code>的时候, 这时就会输出每个样本在所有树中的叶子节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ypred_leaf = bst.predict(dtest, pred_leaf=<span class="literal">True</span>)</span><br><span class="line">ypred_leaf</span><br></pre></td></tr></table></figure><p>Out[33]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, ..., <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       ..., </span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, ..., <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>输出的维度为[样本数, 树的数量], 树的数量默认是100, 所以<code>ypred_leaf</code>的维度为<code>[100*100]</code>.</p><p>对于第一行数据的解释就是, 在xgboost所有的100棵树里, 预测的叶子节点都是1(相对于每颗树).</p><p>那怎么看每颗树以及相应的叶子节点的分值呢?这里有两种方法, 可视化树或者直接输出模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xgb.to_graphviz(bst, num_trees=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#可视化第一棵树的生成情况</span></span><br></pre></td></tr></table></figure><p><img alt="img" data-src="https://images2017.cnblogs.com/blog/957413/201710/957413-20171017204407818-1932629185.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接输出模型的迭代工程</span></span><br><span class="line">bst.dump_model(<span class="string">"model.txt"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">booster[<span class="number">0</span>]:</span><br><span class="line"><span class="number">0</span>:[f3&lt;<span class="number">0.75</span>] yes=<span class="number">1</span>,no=<span class="number">2</span>,missing=<span class="number">1</span></span><br><span class="line">    <span class="number">1</span>:leaf=<span class="number">-0.019697</span></span><br><span class="line">    <span class="number">2</span>:leaf=<span class="number">0.0214286</span></span><br><span class="line">booster[<span class="number">1</span>]:</span><br><span class="line"><span class="number">0</span>:[f2&lt;<span class="number">2.35</span>] yes=<span class="number">1</span>,no=<span class="number">2</span>,missing=<span class="number">1</span></span><br><span class="line">    <span class="number">1</span>:leaf=<span class="number">-0.0212184</span></span><br><span class="line">    <span class="number">2</span>:leaf=<span class="number">0.0212</span></span><br><span class="line">booster[<span class="number">2</span>]:</span><br><span class="line"><span class="number">0</span>:[f2&lt;<span class="number">2.35</span>] yes=<span class="number">1</span>,no=<span class="number">2</span>,missing=<span class="number">1</span></span><br><span class="line">    <span class="number">1</span>:leaf=<span class="number">-0.0197404</span></span><br><span class="line">    <span class="number">2</span>:leaf=<span class="number">0.0197235</span></span><br><span class="line">booster[<span class="number">3</span>]: ……</span><br></pre></td></tr></table></figure><p>通过上述命令就可以输出模型的迭代过程, 可以看到每颗树都有两个叶子节点(树比较简单). 然后我们对每颗树中的叶子节点1的value进行累加求和, 同时进行相应的函数转换, 就是第一个样本的预测值.</p><p>在这里, 以第一个样本为例, 可以看到, 该样本在所有树中都属于第一个叶子, 所以累加值, 得到以下值.</p><p>同样, 以第二个样本为例, 可以看到, 该样本在所有树中都属于第二个叶子, 所以累加值, 得到以下值.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leaf1   <span class="number">-1.381214</span></span><br><span class="line">leaf2    <span class="number">1.410950</span></span><br></pre></td></tr></table></figure><p>在使用xgboost模型最开始, 模型初始化的时候, 我们就设置了<code>&#39;objective&#39;: &#39;binary:logistic&#39;</code>, 因此使用函数将累加的值转换为实际的打分:</p><p>f(x)=1/(1+exp(−x))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>/float(<span class="number">1</span>+np.exp(<span class="number">1.38121416</span>))</span><br><span class="line">Out[<span class="number">24</span>]: <span class="number">0.20081407112186503</span></span><br><span class="line"><span class="number">1</span>/float(<span class="number">1</span>+np.exp(<span class="number">-1.410950</span>))</span><br><span class="line">Out[<span class="number">25</span>]: <span class="number">0.8039157403338895</span></span><br></pre></td></tr></table></figure><p>这就与<code>ypred = bst.predict(dtest)</code> 的分值相对应上了.</p><h4 id="4-3-2-特征重要性"><a href="#4-3-2-特征重要性" class="headerlink" title="4.3.2 特征重要性"></a>4.3.2 特征重要性</h4><p>接着, 我们看另一种输出方式, 输出的是特征相对于得分的重要性.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ypred_contribs = bst.predict(dtest, pred_contribs=<span class="literal">True</span>)</span><br><span class="line">ypred_contribs</span><br></pre></td></tr></table></figure><p>Out[37]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.96967536</span>,  <span class="number">0.39522746</span>,  <span class="number">0.04604663</span>],</span><br><span class="line">       [ <span class="number">0.</span>        ,  <span class="number">0.</span>        , <span class="number">-1.01448286</span>, <span class="number">-0.41277751</span>,  <span class="number">0.04604663</span>]], dtype=float32)</span><br></pre></td></tr></table></figure><p>输出的<code>ypred_contribs</code>的维度为<code>[100,5]</code>, 通过阅读前面的文档注释就可以知道, 最后一列是<code>bias</code>, 前面的四列分别是每个特征对最后打分的影响因子, 可以看出, 前面两个特征是不起作用的.</p><p>通过这个输出, 怎么和最后的打分进行关联呢? 原理也是一样的, 还是以前两列为例.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">score_a = sum(ypred_contribs[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> score_a</span><br><span class="line"><span class="comment"># -1.38121373579</span></span><br><span class="line">score_b = sum(ypred_contribs[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> score_b</span><br><span class="line"><span class="comment"># 1.41094945744</span></span><br></pre></td></tr></table></figure><p>相同的分值, 相同的处理情况.</p><p>到此, 这期关于在python上关于xgboost算法的简单实现, 以及在实现的过程中: 得分的输出、样本对应到树的节点、每个样本中单独特征对得分的影响, 以及上述三者之间的联系, 均已介绍完毕。</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> xgboost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据存储之MySQL系列（01）：MySQL体系结构</title>
      <link href="/2019/12/04/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
      <url>/2019/12/04/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B9%8BMySQL%E7%B3%BB%E5%88%97%EF%BC%8801%EF%BC%89%EF%BC%9AMySQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 数据存储 </category>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> 体系结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LightGBM算法基础系列之基础理论篇（1）</title>
      <link href="/2019/11/14/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/"/>
      <url>/2019/11/14/LightGBM%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>这是lightgbm算法基础系列的第一篇，讲述lightgbm基础理论。</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> LightGBM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LightGBM </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
